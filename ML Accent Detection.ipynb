{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:19.816653Z",
     "start_time": "2024-06-11T09:42:19.059474Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:19.877984Z",
     "start_time": "2024-06-11T09:42:19.820170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "data = pd.read_csv('preprocessed.csv')"
   ],
   "id": "6478eebbfebdd48c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:19.887044Z",
     "start_time": "2024-06-11T09:42:19.879688Z"
    }
   },
   "cell_type": "code",
   "source": "data = data.drop(['idx'],axis = 1)",
   "id": "f3418738a2accd21",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:19.926907Z",
     "start_time": "2024-06-11T09:42:19.890156Z"
    }
   },
   "cell_type": "code",
   "source": "data.groupby(['Label']).count()",
   "id": "4bdf62ebd0be8f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           col3  col4  col5  col6  col7  col8  col9  col10  col11  col12  ...  \\\n",
       "Label                                                                     ...   \n",
       "afrikaans     5     5     5     5     5     5     5      5      5      5  ...   \n",
       "agni          1     1     1     1     1     1     1      1      1      1  ...   \n",
       "akan          1     1     1     1     1     1     1      1      1      1  ...   \n",
       "albanian      9     9     9     9     9     9     9      9      9      9  ...   \n",
       "amazigh       2     2     2     2     2     2     2      2      2      2  ...   \n",
       "...         ...   ...   ...   ...   ...   ...   ...    ...    ...    ...  ...   \n",
       "yapese        1     1     1     1     1     1     1      1      1      1  ...   \n",
       "yiddish       5     5     5     5     5     5     5      5      5      5  ...   \n",
       "yoruba        5     5     5     5     5     5     5      5      5      5  ...   \n",
       "yupik         1     1     1     1     1     1     1      1      1      1  ...   \n",
       "zulu          1     1     1     1     1     1     1      1      1      1  ...   \n",
       "\n",
       "           col93  col94  col95  col96  col97  col98  col99  col100  col101  \\\n",
       "Label                                                                        \n",
       "afrikaans      5      5      5      5      5      5      5       5       5   \n",
       "agni           1      1      1      1      1      1      1       1       1   \n",
       "akan           1      1      1      1      1      1      1       1       1   \n",
       "albanian       9      9      9      9      9      9      9       9       9   \n",
       "amazigh        2      2      2      2      2      2      2       2       2   \n",
       "...          ...    ...    ...    ...    ...    ...    ...     ...     ...   \n",
       "yapese         1      1      1      1      1      1      1       1       1   \n",
       "yiddish        5      5      5      5      5      5      5       5       5   \n",
       "yoruba         5      5      5      5      5      5      5       5       5   \n",
       "yupik          1      1      1      1      1      1      1       1       1   \n",
       "zulu           1      1      1      1      1      1      1       1       1   \n",
       "\n",
       "           col102  \n",
       "Label              \n",
       "afrikaans       5  \n",
       "agni            1  \n",
       "akan            1  \n",
       "albanian        9  \n",
       "amazigh         2  \n",
       "...           ...  \n",
       "yapese          1  \n",
       "yiddish         5  \n",
       "yoruba          5  \n",
       "yupik           1  \n",
       "zulu            1  \n",
       "\n",
       "[200 rows x 100 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>col11</th>\n",
       "      <th>col12</th>\n",
       "      <th>...</th>\n",
       "      <th>col93</th>\n",
       "      <th>col94</th>\n",
       "      <th>col95</th>\n",
       "      <th>col96</th>\n",
       "      <th>col97</th>\n",
       "      <th>col98</th>\n",
       "      <th>col99</th>\n",
       "      <th>col100</th>\n",
       "      <th>col101</th>\n",
       "      <th>col102</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>afrikaans</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agni</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akan</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albanian</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazigh</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yapese</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yiddish</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoruba</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yupik</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zulu</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 100 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:19.936917Z",
     "start_time": "2024-06-11T09:42:19.929918Z"
    }
   },
   "cell_type": "code",
   "source": "data['Label'].unique()",
   "id": "6175dbfc355409cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['afrikaans', 'agni', 'akan', 'albanian', 'amazigh', 'amharic',\n",
       "       'arabic', 'armenian', 'ashanti', 'azerbaijani', 'bafang', 'baga',\n",
       "       'bai', 'bambara', 'bamun', 'bari', 'basque', 'bavarian',\n",
       "       'belarusan', 'bengali', 'bosnian', 'bulgarian', 'burmese',\n",
       "       'cantonese', 'carolinian', 'catalan', 'cebuano', 'chaldean',\n",
       "       'chamorro', 'chichewa', 'chittagonian', 'croatian', 'czech',\n",
       "       'danish', 'dari', 'dinka', 'dutch', 'ebira', 'edo', 'english',\n",
       "       'estonian', 'ewe', 'fang', 'fanti', 'faroese', 'farsi', 'fataluku',\n",
       "       'fijian', 'filipino', 'finnish', 'french', 'frisian', 'ga', 'gan',\n",
       "       'ganda', 'garifuna', 'gedeo', 'georgian', 'german', 'greek',\n",
       "       'gujarati', 'gusii', 'hadiyya', 'hainanese', 'hakka', 'hausa',\n",
       "       'hebrew', 'hindi', 'hindko', 'hmong', 'hungarian', 'ibibio',\n",
       "       'icelandic', 'ife', 'igbo', 'ilonggo', 'indonesian', 'irish',\n",
       "       'italian', 'japanese', 'jola', 'kabyle', 'kalanga', 'kambaata',\n",
       "       'kannada', 'kanuri', 'kazakh', 'khmer', 'kikongo', 'kikuyu',\n",
       "       'kirghiz', 'kiswahili', 'konkani', 'korean', 'krio', 'kru',\n",
       "       'kurdish', 'lamaholot', 'lamotrekese', 'lao', 'latvian', 'lingala',\n",
       "       'lithuanian', 'luo', 'luxembourgeois', 'macedonian', 'malagasy',\n",
       "       'malay', 'malayalam', 'maltese', 'mandarin', 'mandingo',\n",
       "       'mandinka', 'mankanya', 'marathi', 'mauritian', 'mende', 'miskito',\n",
       "       'mizo', 'moba', 'mongolian', 'moore', 'mortlockese', 'nama',\n",
       "       'nandi', 'naxi', 'nepali', 'newari', 'ngemba', 'norwegian', 'nuer',\n",
       "       'oriya', 'oromo', 'papiamentu', 'pashto', 'pohnpeian', 'polish',\n",
       "       'portuguese', 'pulaar', 'punjabi', 'quechua', 'romanian',\n",
       "       'rotuman', 'rundi', 'russian', 'rwanda', 'saa', 'sardinian',\n",
       "       'sarua', 'satawalese', 'serbian', 'serer', 'sesotho', 'shan',\n",
       "       'shilluk', 'shona', 'sicilian', 'sindhi', 'sinhala', 'slovak',\n",
       "       'slovenian', 'somali', 'spanish', 'sundanese', 'susu', 'swedish',\n",
       "       'sylheti', 'synthesized', 'tagalog', 'taishan', 'taiwanese',\n",
       "       'tajiki', 'tamil', 'tatar', 'telugu', 'temne', 'teochew', 'thai',\n",
       "       'tibetan', 'tigrigna', 'tswana', 'turkish', 'turkmen', 'twi',\n",
       "       'ukrainian', 'urdu', 'uyghur', 'uzbek', 'vietnamese', 'vlaams',\n",
       "       'wolof', 'wu', 'xasonga', 'xiang', 'yakut', 'yapese', 'yiddish',\n",
       "       'yoruba', 'yupik', 'zulu'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T10:03:25.085505Z",
     "start_time": "2024-06-11T10:03:25.080608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network._multilayer_perceptron import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "id": "6c1269f480063f7e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:31.407044Z",
     "start_time": "2024-06-11T09:42:31.393477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = data.drop(['Label'],axis = 1)\n",
    "y = data['Label']\n",
    "\n",
    "class_counts = Counter(y)\n",
    "\n",
    "# Find classes with fewer than 5 samples\n",
    "classes_to_remove = [cls for cls, count in class_counts.items() if count < 5]\n",
    "\n",
    "# Remove samples belonging to classes with fewer than 5 samples\n",
    "X_filtered = x[~y.isin(classes_to_remove)]\n",
    "y_filtered = y[~y.isin(classes_to_remove)]\n",
    "\n",
    "print(\"Filtered dataset shape:\", Counter(y_filtered))"
   ],
   "id": "9eab53d63ec0b971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset shape: Counter({'english': 579, 'spanish': 162, 'arabic': 102, 'mandarin': 65, 'french': 63, 'korean': 52, 'portuguese': 48, 'russian': 48, 'dutch': 47, 'turkish': 37, 'german': 36, 'polish': 34, 'italian': 33, 'japanese': 27, 'macedonian': 26, 'cantonese': 23, 'farsi': 23, 'vietnamese': 22, 'amharic': 20, 'romanian': 20, 'swedish': 20, 'bulgarian': 18, 'hindi': 18, 'serbian': 18, 'tagalog': 18, 'bengali': 17, 'urdu': 16, 'greek': 15, 'thai': 15, 'nepali': 13, 'miskito': 11, 'ukrainian': 11, 'kurdish': 10, 'pashto': 10, 'punjabi': 10, 'albanian': 9, 'bosnian': 9, 'czech': 9, 'gujarati': 9, 'hausa': 9, 'hebrew': 9, 'hungarian': 9, 'kiswahili': 9, 'mongolian': 9, 'armenian': 8, 'croatian': 8, 'danish': 8, 'finnish': 8, 'indonesian': 8, 'taiwanese': 8, 'tigrigna': 8, 'khmer': 7, 'ga': 6, 'krio': 6, 'lithuanian': 6, 'norwegian': 6, 'somali': 6, 'tamil': 6, 'wolof': 6, 'afrikaans': 5, 'bambara': 5, 'dari': 5, 'georgian': 5, 'malay': 5, 'slovak': 5, 'twi': 5, 'yiddish': 5, 'yoruba': 5})\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:42:38.918347Z",
     "start_time": "2024-06-11T09:42:38.060975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_filtered)\n",
    "\n",
    "# Step 2: Splitting the dataset into training and testing sets\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_train_pca, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Resampling with BorderlineSMOTE\n",
    "class_counts = Counter(y_train)\n",
    "min_class = min(class_counts, key=class_counts.get)\n",
    "min_class_count = class_counts[min_class]\n",
    "\n",
    "# Ensure k_neighbors is at least 1\n",
    "k_neighbors = max(1, min(min_class_count - 1, 5))\n",
    "smote = smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_pca, y_train)\n"
   ],
   "id": "4257d8806b4b0a75",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:48:39.672999Z",
     "start_time": "2024-06-11T09:43:14.243775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42,verbose=True)\n",
    "clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "id": "2a417e679aabb831",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.22039713\n",
      "Iteration 2, loss = 4.19843743\n",
      "Iteration 3, loss = 4.16072607\n",
      "Iteration 4, loss = 4.10696506\n",
      "Iteration 5, loss = 4.05099785\n",
      "Iteration 6, loss = 3.99548001\n",
      "Iteration 7, loss = 3.94095057\n",
      "Iteration 8, loss = 3.88634409\n",
      "Iteration 9, loss = 3.83383263\n",
      "Iteration 10, loss = 3.78422398\n",
      "Iteration 11, loss = 3.73684332\n",
      "Iteration 12, loss = 3.69313870\n",
      "Iteration 13, loss = 3.65096521\n",
      "Iteration 14, loss = 3.61076649\n",
      "Iteration 15, loss = 3.57232543\n",
      "Iteration 16, loss = 3.53546673\n",
      "Iteration 17, loss = 3.50018843\n",
      "Iteration 18, loss = 3.46610271\n",
      "Iteration 19, loss = 3.43303729\n",
      "Iteration 20, loss = 3.40114414\n",
      "Iteration 21, loss = 3.37023184\n",
      "Iteration 22, loss = 3.34003116\n",
      "Iteration 23, loss = 3.31133365\n",
      "Iteration 24, loss = 3.28276178\n",
      "Iteration 25, loss = 3.25514770\n",
      "Iteration 26, loss = 3.22886236\n",
      "Iteration 27, loss = 3.20303996\n",
      "Iteration 28, loss = 3.17770392\n",
      "Iteration 29, loss = 3.15318644\n",
      "Iteration 30, loss = 3.13034925\n",
      "Iteration 31, loss = 3.10697128\n",
      "Iteration 32, loss = 3.08499859\n",
      "Iteration 33, loss = 3.06302846\n",
      "Iteration 34, loss = 3.04242725\n",
      "Iteration 35, loss = 3.02208320\n",
      "Iteration 36, loss = 3.00196793\n",
      "Iteration 37, loss = 2.98329813\n",
      "Iteration 38, loss = 2.96436836\n",
      "Iteration 39, loss = 2.94636213\n",
      "Iteration 40, loss = 2.92838668\n",
      "Iteration 41, loss = 2.91130427\n",
      "Iteration 42, loss = 2.89413353\n",
      "Iteration 43, loss = 2.87794988\n",
      "Iteration 44, loss = 2.86251204\n",
      "Iteration 45, loss = 2.84627240\n",
      "Iteration 46, loss = 2.83190736\n",
      "Iteration 47, loss = 2.81679319\n",
      "Iteration 48, loss = 2.80256821\n",
      "Iteration 49, loss = 2.78877718\n",
      "Iteration 50, loss = 2.77490267\n",
      "Iteration 51, loss = 2.76132230\n",
      "Iteration 52, loss = 2.74795111\n",
      "Iteration 53, loss = 2.73576925\n",
      "Iteration 54, loss = 2.72277816\n",
      "Iteration 55, loss = 2.71071004\n",
      "Iteration 56, loss = 2.69908901\n",
      "Iteration 57, loss = 2.68717692\n",
      "Iteration 58, loss = 2.67583635\n",
      "Iteration 59, loss = 2.66444790\n",
      "Iteration 60, loss = 2.65356588\n",
      "Iteration 61, loss = 2.64264922\n",
      "Iteration 62, loss = 2.63230928\n",
      "Iteration 63, loss = 2.62213073\n",
      "Iteration 64, loss = 2.61129699\n",
      "Iteration 65, loss = 2.60162406\n",
      "Iteration 66, loss = 2.59130662\n",
      "Iteration 67, loss = 2.58200774\n",
      "Iteration 68, loss = 2.57225831\n",
      "Iteration 69, loss = 2.56313926\n",
      "Iteration 70, loss = 2.55336917\n",
      "Iteration 71, loss = 2.54471187\n",
      "Iteration 72, loss = 2.53571169\n",
      "Iteration 73, loss = 2.52692468\n",
      "Iteration 74, loss = 2.51780493\n",
      "Iteration 75, loss = 2.50955804\n",
      "Iteration 76, loss = 2.50101269\n",
      "Iteration 77, loss = 2.49267832\n",
      "Iteration 78, loss = 2.48435142\n",
      "Iteration 79, loss = 2.47631086\n",
      "Iteration 80, loss = 2.46834791\n",
      "Iteration 81, loss = 2.46058130\n",
      "Iteration 82, loss = 2.45264464\n",
      "Iteration 83, loss = 2.44480297\n",
      "Iteration 84, loss = 2.43724870\n",
      "Iteration 85, loss = 2.43000715\n",
      "Iteration 86, loss = 2.42213026\n",
      "Iteration 87, loss = 2.41492269\n",
      "Iteration 88, loss = 2.40737900\n",
      "Iteration 89, loss = 2.40031030\n",
      "Iteration 90, loss = 2.39342650\n",
      "Iteration 91, loss = 2.38619062\n",
      "Iteration 92, loss = 2.37944917\n",
      "Iteration 93, loss = 2.37312956\n",
      "Iteration 94, loss = 2.36598132\n",
      "Iteration 95, loss = 2.35948222\n",
      "Iteration 96, loss = 2.35219018\n",
      "Iteration 97, loss = 2.34629035\n",
      "Iteration 98, loss = 2.33957943\n",
      "Iteration 99, loss = 2.33295056\n",
      "Iteration 100, loss = 2.32659560\n",
      "Iteration 101, loss = 2.31993406\n",
      "Iteration 102, loss = 2.31451525\n",
      "Iteration 103, loss = 2.30800327\n",
      "Iteration 104, loss = 2.30190264\n",
      "Iteration 105, loss = 2.29602194\n",
      "Iteration 106, loss = 2.29021510\n",
      "Iteration 107, loss = 2.28421020\n",
      "Iteration 108, loss = 2.27810477\n",
      "Iteration 109, loss = 2.27227467\n",
      "Iteration 110, loss = 2.26668922\n",
      "Iteration 111, loss = 2.26063767\n",
      "Iteration 112, loss = 2.25509669\n",
      "Iteration 113, loss = 2.24979680\n",
      "Iteration 114, loss = 2.24428685\n",
      "Iteration 115, loss = 2.23850626\n",
      "Iteration 116, loss = 2.23341806\n",
      "Iteration 117, loss = 2.22786140\n",
      "Iteration 118, loss = 2.22256676\n",
      "Iteration 119, loss = 2.21738031\n",
      "Iteration 120, loss = 2.21253338\n",
      "Iteration 121, loss = 2.20684540\n",
      "Iteration 122, loss = 2.20163845\n",
      "Iteration 123, loss = 2.19647696\n",
      "Iteration 124, loss = 2.19165540\n",
      "Iteration 125, loss = 2.18651569\n",
      "Iteration 126, loss = 2.18165183\n",
      "Iteration 127, loss = 2.17643317\n",
      "Iteration 128, loss = 2.17185432\n",
      "Iteration 129, loss = 2.16681233\n",
      "Iteration 130, loss = 2.16221655\n",
      "Iteration 131, loss = 2.15832865\n",
      "Iteration 132, loss = 2.15314801\n",
      "Iteration 133, loss = 2.14819329\n",
      "Iteration 134, loss = 2.14374851\n",
      "Iteration 135, loss = 2.13917649\n",
      "Iteration 136, loss = 2.13406973\n",
      "Iteration 137, loss = 2.13028790\n",
      "Iteration 138, loss = 2.12584998\n",
      "Iteration 139, loss = 2.12139888\n",
      "Iteration 140, loss = 2.11748689\n",
      "Iteration 141, loss = 2.11269957\n",
      "Iteration 142, loss = 2.10794075\n",
      "Iteration 143, loss = 2.10407469\n",
      "Iteration 144, loss = 2.09983056\n",
      "Iteration 145, loss = 2.09601525\n",
      "Iteration 146, loss = 2.09163318\n",
      "Iteration 147, loss = 2.08766437\n",
      "Iteration 148, loss = 2.08366682\n",
      "Iteration 149, loss = 2.07922091\n",
      "Iteration 150, loss = 2.07559549\n",
      "Iteration 151, loss = 2.07198865\n",
      "Iteration 152, loss = 2.06773449\n",
      "Iteration 153, loss = 2.06419766\n",
      "Iteration 154, loss = 2.06030695\n",
      "Iteration 155, loss = 2.05584251\n",
      "Iteration 156, loss = 2.05274051\n",
      "Iteration 157, loss = 2.04850713\n",
      "Iteration 158, loss = 2.04525733\n",
      "Iteration 159, loss = 2.04175492\n",
      "Iteration 160, loss = 2.03790604\n",
      "Iteration 161, loss = 2.03441103\n",
      "Iteration 162, loss = 2.03063208\n",
      "Iteration 163, loss = 2.02757155\n",
      "Iteration 164, loss = 2.02406632\n",
      "Iteration 165, loss = 2.01989280\n",
      "Iteration 166, loss = 2.01718596\n",
      "Iteration 167, loss = 2.01326369\n",
      "Iteration 168, loss = 2.01021064\n",
      "Iteration 169, loss = 2.00726142\n",
      "Iteration 170, loss = 2.00352453\n",
      "Iteration 171, loss = 2.00011328\n",
      "Iteration 172, loss = 1.99726023\n",
      "Iteration 173, loss = 1.99371899\n",
      "Iteration 174, loss = 1.99091426\n",
      "Iteration 175, loss = 1.98791807\n",
      "Iteration 176, loss = 1.98452787\n",
      "Iteration 177, loss = 1.98146350\n",
      "Iteration 178, loss = 1.97889887\n",
      "Iteration 179, loss = 1.97567141\n",
      "Iteration 180, loss = 1.97215047\n",
      "Iteration 181, loss = 1.96931739\n",
      "Iteration 182, loss = 1.96713316\n",
      "Iteration 183, loss = 1.96363285\n",
      "Iteration 184, loss = 1.96158940\n",
      "Iteration 185, loss = 1.95831106\n",
      "Iteration 186, loss = 1.95533793\n",
      "Iteration 187, loss = 1.95232492\n",
      "Iteration 188, loss = 1.94953553\n",
      "Iteration 189, loss = 1.94690281\n",
      "Iteration 190, loss = 1.94447806\n",
      "Iteration 191, loss = 1.94144446\n",
      "Iteration 192, loss = 1.93898699\n",
      "Iteration 193, loss = 1.93571856\n",
      "Iteration 194, loss = 1.93390796\n",
      "Iteration 195, loss = 1.93058152\n",
      "Iteration 196, loss = 1.92883709\n",
      "Iteration 197, loss = 1.92611452\n",
      "Iteration 198, loss = 1.92314169\n",
      "Iteration 199, loss = 1.92096920\n",
      "Iteration 200, loss = 1.91804321\n",
      "Iteration 201, loss = 1.91576382\n",
      "Iteration 202, loss = 1.91350209\n",
      "Iteration 203, loss = 1.91147351\n",
      "Iteration 204, loss = 1.90891675\n",
      "Iteration 205, loss = 1.90644756\n",
      "Iteration 206, loss = 1.90405512\n",
      "Iteration 207, loss = 1.90171624\n",
      "Iteration 208, loss = 1.89941296\n",
      "Iteration 209, loss = 1.89686897\n",
      "Iteration 210, loss = 1.89434205\n",
      "Iteration 211, loss = 1.89262053\n",
      "Iteration 212, loss = 1.89070375\n",
      "Iteration 213, loss = 1.88844489\n",
      "Iteration 214, loss = 1.88613913\n",
      "Iteration 215, loss = 1.88367569\n",
      "Iteration 216, loss = 1.88135810\n",
      "Iteration 217, loss = 1.87924661\n",
      "Iteration 218, loss = 1.87743127\n",
      "Iteration 219, loss = 1.87529678\n",
      "Iteration 220, loss = 1.87286618\n",
      "Iteration 221, loss = 1.87026529\n",
      "Iteration 222, loss = 1.86928624\n",
      "Iteration 223, loss = 1.86681989\n",
      "Iteration 224, loss = 1.86468363\n",
      "Iteration 225, loss = 1.86218334\n",
      "Iteration 226, loss = 1.86104544\n",
      "Iteration 227, loss = 1.85820507\n",
      "Iteration 228, loss = 1.85699826\n",
      "Iteration 229, loss = 1.85512034\n",
      "Iteration 230, loss = 1.85300541\n",
      "Iteration 231, loss = 1.85187510\n",
      "Iteration 232, loss = 1.84862952\n",
      "Iteration 233, loss = 1.84760656\n",
      "Iteration 234, loss = 1.84509476\n",
      "Iteration 235, loss = 1.84358186\n",
      "Iteration 236, loss = 1.84153818\n",
      "Iteration 237, loss = 1.84005155\n",
      "Iteration 238, loss = 1.83722002\n",
      "Iteration 239, loss = 1.83631051\n",
      "Iteration 240, loss = 1.83388940\n",
      "Iteration 241, loss = 1.83207734\n",
      "Iteration 242, loss = 1.83037825\n",
      "Iteration 243, loss = 1.82909902\n",
      "Iteration 244, loss = 1.82705353\n",
      "Iteration 245, loss = 1.82521240\n",
      "Iteration 246, loss = 1.82349779\n",
      "Iteration 247, loss = 1.82207529\n",
      "Iteration 248, loss = 1.82006244\n",
      "Iteration 249, loss = 1.81747391\n",
      "Iteration 250, loss = 1.81701669\n",
      "Iteration 251, loss = 1.81451665\n",
      "Iteration 252, loss = 1.81317268\n",
      "Iteration 253, loss = 1.81149335\n",
      "Iteration 254, loss = 1.81005839\n",
      "Iteration 255, loss = 1.80842512\n",
      "Iteration 256, loss = 1.80634260\n",
      "Iteration 257, loss = 1.80505640\n",
      "Iteration 258, loss = 1.80315397\n",
      "Iteration 259, loss = 1.80178333\n",
      "Iteration 260, loss = 1.80030290\n",
      "Iteration 261, loss = 1.79867533\n",
      "Iteration 262, loss = 1.79691193\n",
      "Iteration 263, loss = 1.79538403\n",
      "Iteration 264, loss = 1.79364263\n",
      "Iteration 265, loss = 1.79232361\n",
      "Iteration 266, loss = 1.79088950\n",
      "Iteration 267, loss = 1.78927800\n",
      "Iteration 268, loss = 1.78785289\n",
      "Iteration 269, loss = 1.78580190\n",
      "Iteration 270, loss = 1.78461063\n",
      "Iteration 271, loss = 1.78341331\n",
      "Iteration 272, loss = 1.78166668\n",
      "Iteration 273, loss = 1.77994452\n",
      "Iteration 274, loss = 1.77896458\n",
      "Iteration 275, loss = 1.77710666\n",
      "Iteration 276, loss = 1.77599730\n",
      "Iteration 277, loss = 1.77427890\n",
      "Iteration 278, loss = 1.77383711\n",
      "Iteration 279, loss = 1.77163699\n",
      "Iteration 280, loss = 1.77035252\n",
      "Iteration 281, loss = 1.76864652\n",
      "Iteration 282, loss = 1.76722614\n",
      "Iteration 283, loss = 1.76577543\n",
      "Iteration 284, loss = 1.76489039\n",
      "Iteration 285, loss = 1.76353102\n",
      "Iteration 286, loss = 1.76232136\n",
      "Iteration 287, loss = 1.76013740\n",
      "Iteration 288, loss = 1.75937867\n",
      "Iteration 289, loss = 1.75781352\n",
      "Iteration 290, loss = 1.75675615\n",
      "Iteration 291, loss = 1.75521508\n",
      "Iteration 292, loss = 1.75406924\n",
      "Iteration 293, loss = 1.75313918\n",
      "Iteration 294, loss = 1.75138782\n",
      "Iteration 295, loss = 1.74951030\n",
      "Iteration 296, loss = 1.74861752\n",
      "Iteration 297, loss = 1.74673098\n",
      "Iteration 298, loss = 1.74587280\n",
      "Iteration 299, loss = 1.74485151\n",
      "Iteration 300, loss = 1.74367684\n",
      "Iteration 301, loss = 1.74208270\n",
      "Iteration 302, loss = 1.74145159\n",
      "Iteration 303, loss = 1.73944033\n",
      "Iteration 304, loss = 1.73871314\n",
      "Iteration 305, loss = 1.73712129\n",
      "Iteration 306, loss = 1.73575464\n",
      "Iteration 307, loss = 1.73498150\n",
      "Iteration 308, loss = 1.73287591\n",
      "Iteration 309, loss = 1.73255306\n",
      "Iteration 310, loss = 1.73109344\n",
      "Iteration 311, loss = 1.72992250\n",
      "Iteration 312, loss = 1.72809467\n",
      "Iteration 313, loss = 1.72744301\n",
      "Iteration 314, loss = 1.72603310\n",
      "Iteration 315, loss = 1.72502073\n",
      "Iteration 316, loss = 1.72411193\n",
      "Iteration 317, loss = 1.72217183\n",
      "Iteration 318, loss = 1.72137822\n",
      "Iteration 319, loss = 1.72055702\n",
      "Iteration 320, loss = 1.71959290\n",
      "Iteration 321, loss = 1.71784374\n",
      "Iteration 322, loss = 1.71676159\n",
      "Iteration 323, loss = 1.71564242\n",
      "Iteration 324, loss = 1.71460019\n",
      "Iteration 325, loss = 1.71335757\n",
      "Iteration 326, loss = 1.71247074\n",
      "Iteration 327, loss = 1.71127212\n",
      "Iteration 328, loss = 1.71081183\n",
      "Iteration 329, loss = 1.70869983\n",
      "Iteration 330, loss = 1.70804109\n",
      "Iteration 331, loss = 1.70695426\n",
      "Iteration 332, loss = 1.70590853\n",
      "Iteration 333, loss = 1.70524245\n",
      "Iteration 334, loss = 1.70384597\n",
      "Iteration 335, loss = 1.70246906\n",
      "Iteration 336, loss = 1.70185715\n",
      "Iteration 337, loss = 1.70049043\n",
      "Iteration 338, loss = 1.69914465\n",
      "Iteration 339, loss = 1.69864321\n",
      "Iteration 340, loss = 1.69716426\n",
      "Iteration 341, loss = 1.69604622\n",
      "Iteration 342, loss = 1.69510862\n",
      "Iteration 343, loss = 1.69364336\n",
      "Iteration 344, loss = 1.69339540\n",
      "Iteration 345, loss = 1.69190180\n",
      "Iteration 346, loss = 1.69146631\n",
      "Iteration 347, loss = 1.69020295\n",
      "Iteration 348, loss = 1.68941493\n",
      "Iteration 349, loss = 1.68835507\n",
      "Iteration 350, loss = 1.68753213\n",
      "Iteration 351, loss = 1.68666926\n",
      "Iteration 352, loss = 1.68518573\n",
      "Iteration 353, loss = 1.68396950\n",
      "Iteration 354, loss = 1.68374124\n",
      "Iteration 355, loss = 1.68220143\n",
      "Iteration 356, loss = 1.68136790\n",
      "Iteration 357, loss = 1.68051628\n",
      "Iteration 358, loss = 1.67987087\n",
      "Iteration 359, loss = 1.67855189\n",
      "Iteration 360, loss = 1.67803799\n",
      "Iteration 361, loss = 1.67660841\n",
      "Iteration 362, loss = 1.67584420\n",
      "Iteration 363, loss = 1.67462360\n",
      "Iteration 364, loss = 1.67432383\n",
      "Iteration 365, loss = 1.67255739\n",
      "Iteration 366, loss = 1.67203091\n",
      "Iteration 367, loss = 1.67121435\n",
      "Iteration 368, loss = 1.67049932\n",
      "Iteration 369, loss = 1.67007064\n",
      "Iteration 370, loss = 1.66870986\n",
      "Iteration 371, loss = 1.66765894\n",
      "Iteration 372, loss = 1.66663490\n",
      "Iteration 373, loss = 1.66626469\n",
      "Iteration 374, loss = 1.66468131\n",
      "Iteration 375, loss = 1.66364603\n",
      "Iteration 376, loss = 1.66321114\n",
      "Iteration 377, loss = 1.66251770\n",
      "Iteration 378, loss = 1.66136663\n",
      "Iteration 379, loss = 1.66101548\n",
      "Iteration 380, loss = 1.66014396\n",
      "Iteration 381, loss = 1.65894684\n",
      "Iteration 382, loss = 1.65785577\n",
      "Iteration 383, loss = 1.65737067\n",
      "Iteration 384, loss = 1.65665921\n",
      "Iteration 385, loss = 1.65534191\n",
      "Iteration 386, loss = 1.65406878\n",
      "Iteration 387, loss = 1.65432308\n",
      "Iteration 388, loss = 1.65334169\n",
      "Iteration 389, loss = 1.65275623\n",
      "Iteration 390, loss = 1.65076715\n",
      "Iteration 391, loss = 1.65052965\n",
      "Iteration 392, loss = 1.65028369\n",
      "Iteration 393, loss = 1.64876919\n",
      "Iteration 394, loss = 1.64865299\n",
      "Iteration 395, loss = 1.64726866\n",
      "Iteration 396, loss = 1.64670047\n",
      "Iteration 397, loss = 1.64595555\n",
      "Iteration 398, loss = 1.64559109\n",
      "Iteration 399, loss = 1.64509151\n",
      "Iteration 400, loss = 1.64353244\n",
      "Iteration 401, loss = 1.64307438\n",
      "Iteration 402, loss = 1.64205322\n",
      "Iteration 403, loss = 1.64104883\n",
      "Iteration 404, loss = 1.64082201\n",
      "Iteration 405, loss = 1.64029790\n",
      "Iteration 406, loss = 1.63884709\n",
      "Iteration 407, loss = 1.63895269\n",
      "Iteration 408, loss = 1.63742676\n",
      "Iteration 409, loss = 1.63717201\n",
      "Iteration 410, loss = 1.63596526\n",
      "Iteration 411, loss = 1.63592247\n",
      "Iteration 412, loss = 1.63463123\n",
      "Iteration 413, loss = 1.63385747\n",
      "Iteration 414, loss = 1.63322362\n",
      "Iteration 415, loss = 1.63257754\n",
      "Iteration 416, loss = 1.63197639\n",
      "Iteration 417, loss = 1.63096832\n",
      "Iteration 418, loss = 1.62968513\n",
      "Iteration 419, loss = 1.62987970\n",
      "Iteration 420, loss = 1.62954251\n",
      "Iteration 421, loss = 1.62768091\n",
      "Iteration 422, loss = 1.62796033\n",
      "Iteration 423, loss = 1.62794247\n",
      "Iteration 424, loss = 1.62636953\n",
      "Iteration 425, loss = 1.62592949\n",
      "Iteration 426, loss = 1.62469249\n",
      "Iteration 427, loss = 1.62430003\n",
      "Iteration 428, loss = 1.62364127\n",
      "Iteration 429, loss = 1.62276220\n",
      "Iteration 430, loss = 1.62226995\n",
      "Iteration 431, loss = 1.62149387\n",
      "Iteration 432, loss = 1.62168632\n",
      "Iteration 433, loss = 1.61962173\n",
      "Iteration 434, loss = 1.61914296\n",
      "Iteration 435, loss = 1.61905908\n",
      "Iteration 436, loss = 1.61847751\n",
      "Iteration 437, loss = 1.61769503\n",
      "Iteration 438, loss = 1.61724280\n",
      "Iteration 439, loss = 1.61596495\n",
      "Iteration 440, loss = 1.61524884\n",
      "Iteration 441, loss = 1.61436389\n",
      "Iteration 442, loss = 1.61420864\n",
      "Iteration 443, loss = 1.61355159\n",
      "Iteration 444, loss = 1.61295320\n",
      "Iteration 445, loss = 1.61208091\n",
      "Iteration 446, loss = 1.61169462\n",
      "Iteration 447, loss = 1.61121884\n",
      "Iteration 448, loss = 1.60996970\n",
      "Iteration 449, loss = 1.61023483\n",
      "Iteration 450, loss = 1.60944609\n",
      "Iteration 451, loss = 1.60782408\n",
      "Iteration 452, loss = 1.60757250\n",
      "Iteration 453, loss = 1.60712067\n",
      "Iteration 454, loss = 1.60665265\n",
      "Iteration 455, loss = 1.60559825\n",
      "Iteration 456, loss = 1.60482713\n",
      "Iteration 457, loss = 1.60538934\n",
      "Iteration 458, loss = 1.60386489\n",
      "Iteration 459, loss = 1.60327318\n",
      "Iteration 460, loss = 1.60244907\n",
      "Iteration 461, loss = 1.60198351\n",
      "Iteration 462, loss = 1.60208793\n",
      "Iteration 463, loss = 1.60089425\n",
      "Iteration 464, loss = 1.59981081\n",
      "Iteration 465, loss = 1.59991374\n",
      "Iteration 466, loss = 1.59943452\n",
      "Iteration 467, loss = 1.59899174\n",
      "Iteration 468, loss = 1.59847960\n",
      "Iteration 469, loss = 1.59811733\n",
      "Iteration 470, loss = 1.59656746\n",
      "Iteration 471, loss = 1.59665297\n",
      "Iteration 472, loss = 1.59572789\n",
      "Iteration 473, loss = 1.59532047\n",
      "Iteration 474, loss = 1.59492703\n",
      "Iteration 475, loss = 1.59489443\n",
      "Iteration 476, loss = 1.59357582\n",
      "Iteration 477, loss = 1.59284615\n",
      "Iteration 478, loss = 1.59198154\n",
      "Iteration 479, loss = 1.59267265\n",
      "Iteration 480, loss = 1.59206434\n",
      "Iteration 481, loss = 1.59065526\n",
      "Iteration 482, loss = 1.58971227\n",
      "Iteration 483, loss = 1.58999471\n",
      "Iteration 484, loss = 1.58973970\n",
      "Iteration 485, loss = 1.58845417\n",
      "Iteration 486, loss = 1.58795029\n",
      "Iteration 487, loss = 1.58774637\n",
      "Iteration 488, loss = 1.58679943\n",
      "Iteration 489, loss = 1.58630896\n",
      "Iteration 490, loss = 1.58592038\n",
      "Iteration 491, loss = 1.58525518\n",
      "Iteration 492, loss = 1.58466045\n",
      "Iteration 493, loss = 1.58431289\n",
      "Iteration 494, loss = 1.58311147\n",
      "Iteration 495, loss = 1.58324698\n",
      "Iteration 496, loss = 1.58284190\n",
      "Iteration 497, loss = 1.58197431\n",
      "Iteration 498, loss = 1.58090640\n",
      "Iteration 499, loss = 1.58099469\n",
      "Iteration 500, loss = 1.58037805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Step 5: Model Evaluation\u001B[39;00m\n\u001B[0;32m      5\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mpredict(X_test_pca)\n\u001B[1;32m----> 6\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43maccuracy_score\u001B[49m(y_test, y_pred)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T09:48:39.675004Z",
     "start_time": "2024-06-11T09:48:39.675004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)"
   ],
   "id": "3b92c73db0218186",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = clf.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100)"
   ],
   "id": "e8f983105b79d07e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T11:38:30.849121Z",
     "start_time": "2024-06-11T11:25:52.562849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (150,), (250,), (300,)], # Example values for hidden layer sizes\n",
    "    'activation': ['logistic', 'tanh', 'relu'], # Example activation functions\n",
    "    'alpha': [0.0001, 0.001, 0.01], # Example values for regularization parameter\n",
    "    'max_iter': randint(100, 1000) # Randomly select max_iter from range [100, 500)\n",
    "}\n",
    "# Instantiate MLPClassifier\n",
    "clf = MLPClassifier(random_state=42,verbose=True)\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=50, cv=5, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Get the best estimator\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best estimator\n",
    "accuracy = best_estimator.score(X_test_pca, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "id": "f09d17daf373c00e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.23295702\n",
      "Iteration 2, loss = 4.13465739\n",
      "Iteration 3, loss = 4.04084714\n",
      "Iteration 4, loss = 3.94565069\n",
      "Iteration 5, loss = 3.84557572\n",
      "Iteration 6, loss = 3.74237041\n",
      "Iteration 7, loss = 3.63851109\n",
      "Iteration 8, loss = 3.53566996\n",
      "Iteration 9, loss = 3.43859429\n",
      "Iteration 10, loss = 3.35549892\n",
      "Iteration 11, loss = 3.28975807\n",
      "Iteration 12, loss = 3.24895100\n",
      "Iteration 13, loss = 3.23109762\n",
      "Iteration 14, loss = 3.22767573\n",
      "Iteration 15, loss = 3.22637564\n",
      "Iteration 16, loss = 3.22423988\n",
      "Iteration 17, loss = 3.22102620\n",
      "Iteration 18, loss = 3.22012253\n",
      "Iteration 19, loss = 3.21880306\n",
      "Iteration 20, loss = 3.21756108\n",
      "Iteration 21, loss = 3.21707742\n",
      "Iteration 22, loss = 3.21604861\n",
      "Iteration 23, loss = 3.21487316\n",
      "Iteration 24, loss = 3.21387871\n",
      "Iteration 25, loss = 3.21344627\n",
      "Iteration 26, loss = 3.21230221\n",
      "Iteration 27, loss = 3.21079626\n",
      "Iteration 28, loss = 3.20981730\n",
      "Iteration 29, loss = 3.20945671\n",
      "Iteration 30, loss = 3.20865333\n",
      "Iteration 31, loss = 3.20708430\n",
      "Iteration 32, loss = 3.20600654\n",
      "Iteration 33, loss = 3.20463887\n",
      "Iteration 34, loss = 3.20425717\n",
      "Iteration 35, loss = 3.20275555\n",
      "Iteration 36, loss = 3.20165213\n",
      "Iteration 37, loss = 3.20057530\n",
      "Iteration 38, loss = 3.19924246\n",
      "Iteration 39, loss = 3.19833655\n",
      "Iteration 40, loss = 3.19731419\n",
      "Iteration 41, loss = 3.19624431\n",
      "Iteration 42, loss = 3.19540388\n",
      "Iteration 43, loss = 3.19429221\n",
      "Iteration 44, loss = 3.19283538\n",
      "Iteration 45, loss = 3.19210562\n",
      "Iteration 46, loss = 3.19057154\n",
      "Iteration 47, loss = 3.18900457\n",
      "Iteration 48, loss = 3.18751102\n",
      "Iteration 49, loss = 3.18712497\n",
      "Iteration 50, loss = 3.18530704\n",
      "Iteration 51, loss = 3.18412788\n",
      "Iteration 52, loss = 3.18220057\n",
      "Iteration 53, loss = 3.18222190\n",
      "Iteration 54, loss = 3.18080056\n",
      "Iteration 55, loss = 3.17944870\n",
      "Iteration 56, loss = 3.17765827\n",
      "Iteration 57, loss = 3.17757612\n",
      "Iteration 58, loss = 3.17610642\n",
      "Iteration 59, loss = 3.17481124\n",
      "Iteration 60, loss = 3.17312580\n",
      "Iteration 61, loss = 3.17152619\n",
      "Iteration 62, loss = 3.17036252\n",
      "Iteration 63, loss = 3.16886391\n",
      "Iteration 64, loss = 3.16759413\n",
      "Iteration 65, loss = 3.16607660\n",
      "Iteration 66, loss = 3.16475915\n",
      "Iteration 67, loss = 3.16389765\n",
      "Iteration 68, loss = 3.16224590\n",
      "Iteration 69, loss = 3.16117267\n",
      "Iteration 70, loss = 3.15900013\n",
      "Iteration 71, loss = 3.15768018\n",
      "Iteration 72, loss = 3.15603043\n",
      "Iteration 73, loss = 3.15473550\n",
      "Iteration 74, loss = 3.15290106\n",
      "Iteration 75, loss = 3.15275600\n",
      "Iteration 76, loss = 3.15092055\n",
      "Iteration 77, loss = 3.14869401\n",
      "Iteration 78, loss = 3.14826017\n",
      "Iteration 79, loss = 3.14640013\n",
      "Iteration 80, loss = 3.14507685\n",
      "Iteration 81, loss = 3.14335757\n",
      "Iteration 82, loss = 3.14136527\n",
      "Iteration 83, loss = 3.14010799\n",
      "Iteration 84, loss = 3.13881617\n",
      "Iteration 85, loss = 3.13721311\n",
      "Iteration 86, loss = 3.13563718\n",
      "Iteration 87, loss = 3.13431140\n",
      "Iteration 88, loss = 3.13233931\n",
      "Iteration 89, loss = 3.13040078\n",
      "Iteration 90, loss = 3.12945319\n",
      "Iteration 91, loss = 3.12699872\n",
      "Iteration 92, loss = 3.12653198\n",
      "Iteration 93, loss = 3.12445961\n",
      "Iteration 94, loss = 3.12220922\n",
      "Iteration 95, loss = 3.12123025\n",
      "Iteration 96, loss = 3.11922188\n",
      "Iteration 97, loss = 3.11755977\n",
      "Iteration 98, loss = 3.11693261\n",
      "Iteration 99, loss = 3.11440602\n",
      "Iteration 100, loss = 3.11224810\n",
      "Iteration 101, loss = 3.11098335\n",
      "Iteration 102, loss = 3.10929709\n",
      "Iteration 103, loss = 3.10730805\n",
      "Iteration 104, loss = 3.10747642\n",
      "Iteration 105, loss = 3.10536661\n",
      "Iteration 106, loss = 3.10371190\n",
      "Iteration 107, loss = 3.10221857\n",
      "Iteration 108, loss = 3.10036563\n",
      "Iteration 109, loss = 3.09806063\n",
      "Iteration 110, loss = 3.09709968\n",
      "Iteration 111, loss = 3.09622160\n",
      "Iteration 112, loss = 3.09395677\n",
      "Iteration 113, loss = 3.09154212\n",
      "Iteration 114, loss = 3.09047797\n",
      "Iteration 115, loss = 3.08837627\n",
      "Iteration 116, loss = 3.08708596\n",
      "Iteration 117, loss = 3.08529755\n",
      "Iteration 118, loss = 3.08295198\n",
      "Iteration 119, loss = 3.08166155\n",
      "Iteration 120, loss = 3.08130351\n",
      "Iteration 121, loss = 3.07912402\n",
      "Iteration 122, loss = 3.07745093\n",
      "Iteration 123, loss = 3.07542650\n",
      "Iteration 124, loss = 3.07337393\n",
      "Iteration 125, loss = 3.07169739\n",
      "Iteration 126, loss = 3.07091752\n",
      "Iteration 127, loss = 3.06915058\n",
      "Iteration 128, loss = 3.06786940\n",
      "Iteration 129, loss = 3.06713349\n",
      "Iteration 130, loss = 3.06493637\n",
      "Iteration 131, loss = 3.06430179\n",
      "Iteration 132, loss = 3.06251799\n",
      "Iteration 133, loss = 3.06010624\n",
      "Iteration 134, loss = 3.05745412\n",
      "Iteration 135, loss = 3.05633281\n",
      "Iteration 136, loss = 3.05500040\n",
      "Iteration 137, loss = 3.05369853\n",
      "Iteration 138, loss = 3.05193876\n",
      "Iteration 139, loss = 3.04985455\n",
      "Iteration 140, loss = 3.04871305\n",
      "Iteration 141, loss = 3.04847358\n",
      "Iteration 142, loss = 3.04670763\n",
      "Iteration 143, loss = 3.04408756\n",
      "Iteration 144, loss = 3.04470808\n",
      "Iteration 145, loss = 3.04200080\n",
      "Iteration 146, loss = 3.03894795\n",
      "Iteration 147, loss = 3.03762292\n",
      "Iteration 148, loss = 3.03620724\n",
      "Iteration 149, loss = 3.03480179\n",
      "Iteration 150, loss = 3.03281580\n",
      "Iteration 151, loss = 3.03157352\n",
      "Iteration 152, loss = 3.03128269\n",
      "Iteration 153, loss = 3.02904413\n",
      "Iteration 154, loss = 3.02660442\n",
      "Iteration 155, loss = 3.02529473\n",
      "Iteration 156, loss = 3.02505737\n",
      "Iteration 157, loss = 3.02184922\n",
      "Iteration 158, loss = 3.02101331\n",
      "Iteration 159, loss = 3.01921777\n",
      "Iteration 160, loss = 3.01786233\n",
      "Iteration 161, loss = 3.01614044\n",
      "Iteration 162, loss = 3.01441287\n",
      "Iteration 163, loss = 3.01462870\n",
      "Iteration 164, loss = 3.01246623\n",
      "Iteration 165, loss = 3.01154518\n",
      "Iteration 166, loss = 3.01132251\n",
      "Iteration 167, loss = 3.00944430\n",
      "Iteration 168, loss = 3.00588249\n",
      "Iteration 169, loss = 3.00432307\n",
      "Iteration 170, loss = 3.00409121\n",
      "Iteration 171, loss = 3.00345695\n",
      "Iteration 172, loss = 3.00166271\n",
      "Iteration 173, loss = 2.99896798\n",
      "Iteration 174, loss = 2.99762865\n",
      "Iteration 175, loss = 2.99609813\n",
      "Iteration 176, loss = 2.99374394\n",
      "Iteration 177, loss = 2.99253334\n",
      "Iteration 178, loss = 2.99162020\n",
      "Iteration 179, loss = 2.99179714\n",
      "Iteration 180, loss = 2.98788764\n",
      "Iteration 181, loss = 2.98787637\n",
      "Iteration 182, loss = 2.98484241\n",
      "Iteration 183, loss = 2.98356948\n",
      "Iteration 184, loss = 2.98248936\n",
      "Iteration 185, loss = 2.98130956\n",
      "Iteration 186, loss = 2.97891988\n",
      "Iteration 187, loss = 2.97831040\n",
      "Iteration 188, loss = 2.97637155\n",
      "Iteration 189, loss = 2.97507960\n",
      "Iteration 190, loss = 2.97352959\n",
      "Iteration 191, loss = 2.97220501\n",
      "Iteration 192, loss = 2.97001920\n",
      "Iteration 193, loss = 2.97089292\n",
      "Iteration 194, loss = 2.96822732\n",
      "Iteration 195, loss = 2.96537724\n",
      "Iteration 196, loss = 2.96336149\n",
      "Iteration 197, loss = 2.96233510\n",
      "Iteration 198, loss = 2.96084930\n",
      "Iteration 199, loss = 2.95924984\n",
      "Iteration 200, loss = 2.95910575\n",
      "Iteration 201, loss = 2.95740747\n",
      "Iteration 202, loss = 2.95683067\n",
      "Iteration 203, loss = 2.95463953\n",
      "Iteration 204, loss = 2.95252850\n",
      "Iteration 205, loss = 2.95241273\n",
      "Iteration 206, loss = 2.95088313\n",
      "Iteration 207, loss = 2.94935869\n",
      "Iteration 208, loss = 2.94712533\n",
      "Iteration 209, loss = 2.94530388\n",
      "Iteration 210, loss = 2.94536512\n",
      "Iteration 211, loss = 2.94350894\n",
      "Iteration 212, loss = 2.94162714\n",
      "Iteration 213, loss = 2.94022585\n",
      "Iteration 214, loss = 2.94094010\n",
      "Iteration 215, loss = 2.93904398\n",
      "Iteration 216, loss = 2.93651225\n",
      "Iteration 217, loss = 2.93456035\n",
      "Iteration 218, loss = 2.93355464\n",
      "Iteration 219, loss = 2.93269855\n",
      "Iteration 220, loss = 2.93239826\n",
      "Iteration 221, loss = 2.92913611\n",
      "Iteration 222, loss = 2.92778853\n",
      "Iteration 223, loss = 2.92730903\n",
      "Iteration 224, loss = 2.92798743\n",
      "Iteration 225, loss = 2.92463654\n",
      "Iteration 226, loss = 2.92177175\n",
      "Iteration 227, loss = 2.92313410\n",
      "Iteration 228, loss = 2.92025616\n",
      "Iteration 229, loss = 2.91762571\n",
      "Iteration 230, loss = 2.91779594\n",
      "Iteration 231, loss = 2.91535926\n",
      "Iteration 232, loss = 2.91463952\n",
      "Iteration 233, loss = 2.91273264\n",
      "Iteration 234, loss = 2.91119917\n",
      "Iteration 235, loss = 2.91129290\n",
      "Iteration 236, loss = 2.90899798\n",
      "Iteration 237, loss = 2.90678969\n",
      "Iteration 238, loss = 2.90411866\n",
      "Iteration 239, loss = 2.90452577\n",
      "Iteration 240, loss = 2.90276724\n",
      "Iteration 241, loss = 2.90232813\n",
      "Iteration 242, loss = 2.90036015\n",
      "Iteration 243, loss = 2.89896875\n",
      "Iteration 244, loss = 2.89855381\n",
      "Iteration 245, loss = 2.89694678\n",
      "Iteration 246, loss = 2.89593380\n",
      "Iteration 247, loss = 2.89401986\n",
      "Iteration 248, loss = 2.89170964\n",
      "Iteration 249, loss = 2.89112095\n",
      "Iteration 250, loss = 2.89134011\n",
      "Iteration 251, loss = 2.88904712\n",
      "Iteration 252, loss = 2.88746805\n",
      "Iteration 253, loss = 2.88659391\n",
      "Iteration 254, loss = 2.88446586\n",
      "Iteration 255, loss = 2.88349635\n",
      "Iteration 256, loss = 2.88205754\n",
      "Iteration 257, loss = 2.88041284\n",
      "Iteration 258, loss = 2.88024973\n",
      "Iteration 259, loss = 2.87867603\n",
      "Iteration 260, loss = 2.87785732\n",
      "Iteration 261, loss = 2.87741443\n",
      "Iteration 262, loss = 2.87565321\n",
      "Iteration 263, loss = 2.87384382\n",
      "Iteration 264, loss = 2.87218917\n",
      "Iteration 265, loss = 2.87106361\n",
      "Iteration 266, loss = 2.87016295\n",
      "Iteration 267, loss = 2.86834391\n",
      "Iteration 268, loss = 2.86704031\n",
      "Iteration 269, loss = 2.86626215\n",
      "Iteration 270, loss = 2.86457512\n",
      "Iteration 271, loss = 2.86473048\n",
      "Iteration 272, loss = 2.86379516\n",
      "Iteration 273, loss = 2.86111029\n",
      "Iteration 274, loss = 2.86003754\n",
      "Iteration 275, loss = 2.85865980\n",
      "Iteration 276, loss = 2.85709917\n",
      "Iteration 277, loss = 2.85592736\n",
      "Iteration 278, loss = 2.85478778\n",
      "Iteration 279, loss = 2.85317680\n",
      "Iteration 280, loss = 2.85217343\n",
      "Iteration 281, loss = 2.84988874\n",
      "Iteration 282, loss = 2.84936069\n",
      "Iteration 283, loss = 2.84724274\n",
      "Iteration 284, loss = 2.84617423\n",
      "Iteration 285, loss = 2.84610583\n",
      "Iteration 286, loss = 2.84466952\n",
      "Iteration 287, loss = 2.84293472\n",
      "Iteration 288, loss = 2.84206796\n",
      "Iteration 289, loss = 2.84092917\n",
      "Iteration 290, loss = 2.83960389\n",
      "Iteration 291, loss = 2.83795324\n",
      "Iteration 292, loss = 2.83685911\n",
      "Iteration 293, loss = 2.83527865\n",
      "Iteration 294, loss = 2.83414351\n",
      "Iteration 295, loss = 2.83302887\n",
      "Iteration 296, loss = 2.83118709\n",
      "Iteration 297, loss = 2.82921616\n",
      "Iteration 298, loss = 2.82875457\n",
      "Iteration 299, loss = 2.82729638\n",
      "Iteration 300, loss = 2.82605188\n",
      "Iteration 301, loss = 2.82523022\n",
      "Iteration 302, loss = 2.82465408\n",
      "Iteration 303, loss = 2.82349202\n",
      "Iteration 304, loss = 2.82138603\n",
      "Iteration 305, loss = 2.82027715\n",
      "Iteration 306, loss = 2.81896717\n",
      "Iteration 307, loss = 2.81822677\n",
      "Iteration 308, loss = 2.81790162\n",
      "Iteration 309, loss = 2.81574676\n",
      "Iteration 310, loss = 2.81471517\n",
      "Iteration 311, loss = 2.81399469\n",
      "Iteration 312, loss = 2.81375558\n",
      "Iteration 313, loss = 2.81160800\n",
      "Iteration 314, loss = 2.80973471\n",
      "Iteration 315, loss = 2.80850849\n",
      "Iteration 316, loss = 2.80770745\n",
      "Iteration 317, loss = 2.80631239\n",
      "Iteration 318, loss = 2.80541010\n",
      "Iteration 319, loss = 2.80469128\n",
      "Iteration 320, loss = 2.80365857\n",
      "Iteration 321, loss = 2.80119048\n",
      "Iteration 322, loss = 2.80017001\n",
      "Iteration 323, loss = 2.80016876\n",
      "Iteration 324, loss = 2.79869490\n",
      "Iteration 325, loss = 2.79682124\n",
      "Iteration 326, loss = 2.79576595\n",
      "Iteration 327, loss = 2.79383176\n",
      "Iteration 328, loss = 2.79360205\n",
      "Iteration 329, loss = 2.79332564\n",
      "Iteration 330, loss = 2.79143885\n",
      "Iteration 331, loss = 2.78923879\n",
      "Iteration 332, loss = 2.78782379\n",
      "Iteration 333, loss = 2.78989173\n",
      "Iteration 334, loss = 2.78829603\n",
      "Iteration 335, loss = 2.78369799\n",
      "Iteration 336, loss = 2.78315501\n",
      "Iteration 337, loss = 2.78198217\n",
      "Iteration 338, loss = 2.78075061\n",
      "Iteration 339, loss = 2.78012830\n",
      "Iteration 340, loss = 2.77839513\n",
      "Iteration 341, loss = 2.77771903\n",
      "Iteration 342, loss = 2.77655509\n",
      "Iteration 343, loss = 2.77499570\n",
      "Iteration 344, loss = 2.77347710\n",
      "Iteration 345, loss = 2.77319996\n",
      "Iteration 346, loss = 2.77264034\n",
      "Iteration 347, loss = 2.77000790\n",
      "Iteration 348, loss = 2.76998833\n",
      "Iteration 349, loss = 2.77010037\n",
      "Iteration 350, loss = 2.76987217\n",
      "Iteration 351, loss = 2.76743381\n",
      "Iteration 352, loss = 2.76882917\n",
      "Iteration 353, loss = 2.76592253\n",
      "Iteration 354, loss = 2.76349176\n",
      "Iteration 355, loss = 2.76246625\n",
      "Iteration 356, loss = 2.76148091\n",
      "Iteration 357, loss = 2.76032012\n",
      "Iteration 358, loss = 2.75956503\n",
      "Iteration 359, loss = 2.75825630\n",
      "Iteration 360, loss = 2.75891664\n",
      "Iteration 361, loss = 2.75589906\n",
      "Iteration 362, loss = 2.75448431\n",
      "Iteration 363, loss = 2.75504747\n",
      "Iteration 364, loss = 2.75333371\n",
      "Iteration 365, loss = 2.75110103\n",
      "Iteration 366, loss = 2.75087439\n",
      "Iteration 367, loss = 2.74932416\n",
      "Iteration 368, loss = 2.74808919\n",
      "Iteration 369, loss = 2.74709813\n",
      "Iteration 370, loss = 2.74658933\n",
      "Iteration 371, loss = 2.74474619\n",
      "Iteration 372, loss = 2.74575454\n",
      "Iteration 373, loss = 2.74486382\n",
      "Iteration 374, loss = 2.74147407\n",
      "Iteration 375, loss = 2.74264692\n",
      "Iteration 376, loss = 2.74182444\n",
      "Iteration 377, loss = 2.73893886\n",
      "Iteration 378, loss = 2.73665321\n",
      "Iteration 379, loss = 2.73638199\n",
      "Iteration 380, loss = 2.73541307\n",
      "Iteration 381, loss = 2.73567974\n",
      "Iteration 382, loss = 2.73564668\n",
      "Iteration 383, loss = 2.73315091\n",
      "Iteration 384, loss = 2.73259977\n",
      "Iteration 385, loss = 2.72997792\n",
      "Iteration 386, loss = 2.72942597\n",
      "Iteration 387, loss = 2.72968644\n",
      "Iteration 388, loss = 2.72903268\n",
      "Iteration 389, loss = 2.72810053\n",
      "Iteration 390, loss = 2.72708179\n",
      "Iteration 391, loss = 2.72457326\n",
      "Iteration 392, loss = 2.72277687\n",
      "Iteration 393, loss = 2.72217922\n",
      "Iteration 394, loss = 2.72107966\n",
      "Iteration 395, loss = 2.71959327\n",
      "Iteration 396, loss = 2.71882399\n",
      "Iteration 397, loss = 2.71793739\n",
      "Iteration 398, loss = 2.71691883\n",
      "Iteration 399, loss = 2.71538529\n",
      "Iteration 400, loss = 2.71443080\n",
      "Iteration 401, loss = 2.71368413\n",
      "Iteration 402, loss = 2.71315903\n",
      "Iteration 403, loss = 2.71140935\n",
      "Iteration 404, loss = 2.71143715\n",
      "Iteration 405, loss = 2.70955122\n",
      "Iteration 406, loss = 2.71015452\n",
      "Iteration 407, loss = 2.70815837\n",
      "Iteration 408, loss = 2.70635756\n",
      "Iteration 409, loss = 2.70425982\n",
      "Iteration 410, loss = 2.70455477\n",
      "Iteration 411, loss = 2.70242140\n",
      "Iteration 412, loss = 2.70164672\n",
      "Iteration 413, loss = 2.69956405\n",
      "Iteration 414, loss = 2.69984196\n",
      "Iteration 415, loss = 2.69970064\n",
      "Iteration 416, loss = 2.69874088\n",
      "Iteration 417, loss = 2.69733123\n",
      "Iteration 418, loss = 2.69553805\n",
      "Iteration 419, loss = 2.69512690\n",
      "Iteration 420, loss = 2.69450479\n",
      "Iteration 421, loss = 2.69302587\n",
      "Iteration 422, loss = 2.69365037\n",
      "Iteration 423, loss = 2.69267518\n",
      "Iteration 424, loss = 2.69115646\n",
      "Iteration 425, loss = 2.68844305\n",
      "Iteration 426, loss = 2.68837099\n",
      "Iteration 427, loss = 2.68652374\n",
      "Iteration 428, loss = 2.68605285\n",
      "Iteration 429, loss = 2.68484164\n",
      "Iteration 430, loss = 2.68426124\n",
      "Iteration 431, loss = 2.68419607\n",
      "Iteration 432, loss = 2.68197797\n",
      "Iteration 433, loss = 2.68071833\n",
      "Iteration 434, loss = 2.68035920\n",
      "Iteration 435, loss = 2.67977177\n",
      "Iteration 436, loss = 2.67871543\n",
      "Iteration 437, loss = 2.67761844\n",
      "Iteration 438, loss = 2.67692239\n",
      "Iteration 439, loss = 2.67525934\n",
      "Iteration 440, loss = 2.67324266\n",
      "Iteration 441, loss = 2.67328767\n",
      "Iteration 442, loss = 2.67212310\n",
      "Iteration 443, loss = 2.67060045\n",
      "Iteration 444, loss = 2.67022941\n",
      "Iteration 445, loss = 2.66924511\n",
      "Iteration 446, loss = 2.66793096\n",
      "Iteration 447, loss = 2.66784007\n",
      "Iteration 448, loss = 2.66599828\n",
      "Iteration 449, loss = 2.66520334\n",
      "Iteration 450, loss = 2.66322318\n",
      "Iteration 451, loss = 2.66279893\n",
      "Iteration 452, loss = 2.66351088\n",
      "Iteration 453, loss = 2.66360347\n",
      "Iteration 454, loss = 2.66097106\n",
      "Iteration 455, loss = 2.66063437\n",
      "Iteration 456, loss = 2.65803974\n",
      "Iteration 457, loss = 2.65805490\n",
      "Iteration 458, loss = 2.65754175\n",
      "Iteration 459, loss = 2.65645457\n",
      "Iteration 460, loss = 2.65592518\n",
      "Iteration 461, loss = 2.65403127\n",
      "Iteration 462, loss = 2.65281230\n",
      "Iteration 463, loss = 2.65117764\n",
      "Iteration 464, loss = 2.65131315\n",
      "Iteration 465, loss = 2.65035485\n",
      "Iteration 466, loss = 2.64876151\n",
      "Iteration 467, loss = 2.64888999\n",
      "Iteration 468, loss = 2.64739828\n",
      "Iteration 469, loss = 2.64645484\n",
      "Iteration 470, loss = 2.64607333\n",
      "Iteration 471, loss = 2.64455547\n",
      "Iteration 472, loss = 2.64327640\n",
      "Iteration 473, loss = 2.64284251\n",
      "Iteration 474, loss = 2.64139964\n",
      "Iteration 475, loss = 2.63957702\n",
      "Iteration 476, loss = 2.64025745\n",
      "Iteration 477, loss = 2.63894962\n",
      "Iteration 478, loss = 2.63696228\n",
      "Iteration 479, loss = 2.63732499\n",
      "Iteration 480, loss = 2.63702846\n",
      "Iteration 481, loss = 2.63562085\n",
      "Iteration 482, loss = 2.63515671\n",
      "Iteration 483, loss = 2.63479159\n",
      "Iteration 484, loss = 2.63222349\n",
      "Iteration 485, loss = 2.63251872\n",
      "Iteration 486, loss = 2.63013281\n",
      "Iteration 487, loss = 2.63076894\n",
      "Iteration 488, loss = 2.62963450\n",
      "Iteration 489, loss = 2.62810268\n",
      "Iteration 490, loss = 2.62677098\n",
      "Iteration 491, loss = 2.62539835\n",
      "Iteration 492, loss = 2.62444006\n",
      "Iteration 493, loss = 2.62407973\n",
      "Iteration 494, loss = 2.62377243\n",
      "Iteration 495, loss = 2.62190824\n",
      "Iteration 496, loss = 2.62371354\n",
      "Iteration 497, loss = 2.62247037\n",
      "Iteration 498, loss = 2.61992545\n",
      "Iteration 499, loss = 2.61909419\n",
      "Iteration 500, loss = 2.61748638\n",
      "Iteration 501, loss = 2.61743477\n",
      "Iteration 502, loss = 2.61731961\n",
      "Iteration 503, loss = 2.61690642\n",
      "Iteration 504, loss = 2.61530219\n",
      "Iteration 505, loss = 2.61274052\n",
      "Iteration 506, loss = 2.61243353\n",
      "Iteration 507, loss = 2.61133990\n",
      "Iteration 508, loss = 2.61123290\n",
      "Iteration 509, loss = 2.61097118\n",
      "Iteration 510, loss = 2.60904674\n",
      "Iteration 511, loss = 2.60817995\n",
      "Iteration 512, loss = 2.60798559\n",
      "Iteration 513, loss = 2.60697690\n",
      "Iteration 514, loss = 2.60612523\n",
      "Iteration 515, loss = 2.60483754\n",
      "Iteration 516, loss = 2.60396602\n",
      "Iteration 517, loss = 2.60237115\n",
      "Iteration 518, loss = 2.60133486\n",
      "Iteration 519, loss = 2.60229320\n",
      "Iteration 520, loss = 2.60099899\n",
      "Iteration 521, loss = 2.59954960\n",
      "Iteration 522, loss = 2.59936629\n",
      "Iteration 523, loss = 2.59795302\n",
      "Iteration 524, loss = 2.59903349\n",
      "Iteration 525, loss = 2.59607915\n",
      "Iteration 526, loss = 2.59787732\n",
      "Iteration 527, loss = 2.59463972\n",
      "Iteration 528, loss = 2.59340331\n",
      "Iteration 529, loss = 2.59338550\n",
      "Iteration 530, loss = 2.59348061\n",
      "Iteration 531, loss = 2.59095405\n",
      "Iteration 532, loss = 2.58976581\n",
      "Iteration 533, loss = 2.59088475\n",
      "Iteration 534, loss = 2.59094651\n",
      "Iteration 535, loss = 2.58806466\n",
      "Iteration 536, loss = 2.58748909\n",
      "Iteration 537, loss = 2.58664759\n",
      "Iteration 538, loss = 2.58400110\n",
      "Iteration 539, loss = 2.58451348\n",
      "Iteration 540, loss = 2.58379757\n",
      "Iteration 541, loss = 2.58341172\n",
      "Iteration 542, loss = 2.58366858\n",
      "Iteration 543, loss = 2.58172206\n",
      "Iteration 544, loss = 2.57992120\n",
      "Iteration 545, loss = 2.57965031\n",
      "Iteration 546, loss = 2.57995071\n",
      "Iteration 547, loss = 2.57688005\n",
      "Iteration 548, loss = 2.57680776\n",
      "Iteration 549, loss = 2.57578250\n",
      "Iteration 550, loss = 2.57539829\n",
      "Iteration 551, loss = 2.57422043\n",
      "Iteration 552, loss = 2.57320534\n",
      "Iteration 553, loss = 2.57176967\n",
      "Iteration 554, loss = 2.57150593\n",
      "Iteration 555, loss = 2.57078572\n",
      "Iteration 556, loss = 2.57302434\n",
      "Iteration 557, loss = 2.57092306\n",
      "Iteration 558, loss = 2.56818942\n",
      "Iteration 559, loss = 2.56807602\n",
      "Iteration 560, loss = 2.56693286\n",
      "Iteration 561, loss = 2.56538331\n",
      "Iteration 562, loss = 2.56652316\n",
      "Iteration 563, loss = 2.56501664\n",
      "Iteration 564, loss = 2.56455515\n",
      "Iteration 565, loss = 2.56175364\n",
      "Iteration 566, loss = 2.56186814\n",
      "Iteration 567, loss = 2.56160004\n",
      "Iteration 568, loss = 2.56023230\n",
      "Iteration 569, loss = 2.55924485\n",
      "Iteration 570, loss = 2.55811026\n",
      "Iteration 571, loss = 2.55769454\n",
      "Iteration 572, loss = 2.55666732\n",
      "Iteration 573, loss = 2.55679702\n",
      "Iteration 574, loss = 2.55507767\n",
      "Iteration 575, loss = 2.55507975\n",
      "Iteration 576, loss = 2.55589452\n",
      "Iteration 577, loss = 2.55457847\n",
      "Iteration 578, loss = 2.55190674\n",
      "Iteration 579, loss = 2.55130051\n",
      "Iteration 580, loss = 2.55049297\n",
      "Iteration 581, loss = 2.54975207\n",
      "Iteration 582, loss = 2.55025761\n",
      "Iteration 583, loss = 2.54864933\n",
      "Iteration 584, loss = 2.54660895\n",
      "Iteration 585, loss = 2.54814452\n",
      "Iteration 586, loss = 2.54615966\n",
      "Iteration 587, loss = 2.54605140\n",
      "Iteration 588, loss = 2.54404611\n",
      "Iteration 589, loss = 2.54306211\n",
      "Iteration 590, loss = 2.54208730\n",
      "Iteration 591, loss = 2.54111522\n",
      "Iteration 592, loss = 2.54218234\n",
      "Iteration 593, loss = 2.54064202\n",
      "Iteration 594, loss = 2.53999723\n",
      "Iteration 595, loss = 2.54289851\n",
      "Iteration 596, loss = 2.54088712\n",
      "Iteration 597, loss = 2.53770945\n",
      "Iteration 598, loss = 2.53586011\n",
      "Iteration 599, loss = 2.53432942\n",
      "Iteration 600, loss = 2.53432969\n",
      "Iteration 601, loss = 2.53360961\n",
      "Iteration 602, loss = 2.53164716\n",
      "Iteration 603, loss = 2.53265084\n",
      "Iteration 604, loss = 2.53245687\n",
      "Iteration 605, loss = 2.53145693\n",
      "Iteration 606, loss = 2.53113401\n",
      "Iteration 607, loss = 2.52986496\n",
      "Iteration 608, loss = 2.52856509\n",
      "Iteration 609, loss = 2.52894892\n",
      "Iteration 610, loss = 2.52743418\n",
      "Iteration 611, loss = 2.52716214\n",
      "Iteration 612, loss = 2.52554495\n",
      "Iteration 613, loss = 2.52408889\n",
      "Iteration 614, loss = 2.52456231\n",
      "Iteration 615, loss = 2.52285862\n",
      "Iteration 616, loss = 2.52307211\n",
      "Iteration 617, loss = 2.52172678\n",
      "Iteration 618, loss = 2.52087529\n",
      "Iteration 619, loss = 2.52278255\n",
      "Iteration 620, loss = 2.51972570\n",
      "Iteration 621, loss = 2.51726818\n",
      "Iteration 622, loss = 2.51868090\n",
      "Iteration 623, loss = 2.51771461\n",
      "Iteration 624, loss = 2.51742648\n",
      "Iteration 625, loss = 2.51687628\n",
      "Iteration 626, loss = 2.51709211\n",
      "Iteration 627, loss = 2.51459074\n",
      "Iteration 628, loss = 2.51382507\n",
      "Iteration 629, loss = 2.51367158\n",
      "Iteration 630, loss = 2.51235731\n",
      "Iteration 631, loss = 2.50993344\n",
      "Iteration 632, loss = 2.51153970\n",
      "Iteration 633, loss = 2.50967405\n",
      "Iteration 634, loss = 2.50971920\n",
      "Iteration 635, loss = 2.50825086\n",
      "Iteration 636, loss = 2.50640305\n",
      "Iteration 637, loss = 2.51080079\n",
      "Iteration 638, loss = 2.50821575\n",
      "Iteration 639, loss = 2.50612167\n",
      "Iteration 640, loss = 2.50510379\n",
      "Iteration 641, loss = 2.50361305\n",
      "Iteration 642, loss = 2.50342303\n",
      "Iteration 643, loss = 2.50263502\n",
      "Iteration 644, loss = 2.50166755\n",
      "Iteration 645, loss = 2.50131056\n",
      "Iteration 646, loss = 2.50126895\n",
      "Iteration 647, loss = 2.49955764\n",
      "Iteration 648, loss = 2.49845658\n",
      "Iteration 649, loss = 2.49778515\n",
      "Iteration 650, loss = 2.49726588\n",
      "Iteration 651, loss = 2.49934973\n",
      "Iteration 652, loss = 2.49705964\n",
      "Iteration 653, loss = 2.49573341\n",
      "Iteration 654, loss = 2.49450767\n",
      "Iteration 655, loss = 2.49365930\n",
      "Iteration 656, loss = 2.49293456\n",
      "Iteration 657, loss = 2.49253192\n",
      "Iteration 658, loss = 2.49121346\n",
      "Iteration 659, loss = 2.49000430\n",
      "Iteration 660, loss = 2.48903504\n",
      "Iteration 661, loss = 2.48959594\n",
      "Iteration 662, loss = 2.48747329\n",
      "Iteration 663, loss = 2.48766943\n",
      "Iteration 664, loss = 2.48664056\n",
      "Iteration 665, loss = 2.48779577\n",
      "Iteration 666, loss = 2.48632159\n",
      "Iteration 667, loss = 2.48664910\n",
      "Iteration 668, loss = 2.48610166\n",
      "Iteration 669, loss = 2.48395017\n",
      "Iteration 670, loss = 2.48306318\n",
      "Iteration 671, loss = 2.48343361\n",
      "Iteration 672, loss = 2.48001593\n",
      "Iteration 673, loss = 2.48033407\n",
      "Iteration 674, loss = 2.47910586\n",
      "Iteration 675, loss = 2.47960305\n",
      "Iteration 676, loss = 2.47805588\n",
      "Iteration 677, loss = 2.47792892\n",
      "Iteration 678, loss = 2.47622574\n",
      "Iteration 679, loss = 2.47712994\n",
      "Iteration 680, loss = 2.47502134\n",
      "Iteration 681, loss = 2.47608048\n",
      "Iteration 682, loss = 2.47366412\n",
      "Iteration 683, loss = 2.47375790\n",
      "Iteration 684, loss = 2.47237368\n",
      "Iteration 685, loss = 2.47276220\n",
      "Iteration 686, loss = 2.47154342\n",
      "Iteration 687, loss = 2.47156720\n",
      "Iteration 688, loss = 2.46947374\n",
      "Iteration 689, loss = 2.46917828\n",
      "Iteration 690, loss = 2.46791335\n",
      "Iteration 691, loss = 2.46733612\n",
      "Iteration 692, loss = 2.46597503\n",
      "Iteration 693, loss = 2.46548946\n",
      "Iteration 694, loss = 2.46557511\n",
      "Iteration 695, loss = 2.46497478\n",
      "Iteration 696, loss = 2.46337202\n",
      "Iteration 697, loss = 2.46245983\n",
      "Iteration 698, loss = 2.46220375\n",
      "Iteration 699, loss = 2.46208958\n",
      "Iteration 700, loss = 2.46275379\n",
      "Iteration 701, loss = 2.46135252\n",
      "Iteration 702, loss = 2.46010082\n",
      "Iteration 703, loss = 2.45945822\n",
      "Iteration 704, loss = 2.45908018\n",
      "Iteration 705, loss = 2.45801420\n",
      "Iteration 706, loss = 2.45944604\n",
      "Iteration 707, loss = 2.45730365\n",
      "Iteration 708, loss = 2.45716142\n",
      "Iteration 709, loss = 2.45644648\n",
      "Iteration 710, loss = 2.45608662\n",
      "Iteration 711, loss = 2.45343306\n",
      "Iteration 712, loss = 2.45320061\n",
      "Iteration 713, loss = 2.45326417\n",
      "Iteration 714, loss = 2.45185285\n",
      "Iteration 715, loss = 2.45020398\n",
      "Iteration 716, loss = 2.44937952\n",
      "Iteration 717, loss = 2.45028956\n",
      "Iteration 718, loss = 2.45141914\n",
      "Iteration 719, loss = 2.44885065\n",
      "Iteration 720, loss = 2.44786173\n",
      "Iteration 721, loss = 2.44680483\n",
      "Iteration 722, loss = 2.44622163\n",
      "Iteration 723, loss = 2.44439360\n",
      "Iteration 724, loss = 2.44584410\n",
      "Iteration 725, loss = 2.44515142\n",
      "Iteration 726, loss = 2.44528885\n",
      "Iteration 727, loss = 2.44297055\n",
      "Iteration 728, loss = 2.44224712\n",
      "Iteration 729, loss = 2.44182929\n",
      "Iteration 730, loss = 2.44125429\n",
      "Iteration 731, loss = 2.44191611\n",
      "Iteration 732, loss = 2.43944620\n",
      "Iteration 733, loss = 2.43935484\n",
      "Iteration 734, loss = 2.43857121\n",
      "Iteration 735, loss = 2.43860358\n",
      "Iteration 736, loss = 2.43862346\n",
      "Iteration 737, loss = 2.43608297\n",
      "Iteration 738, loss = 2.43551630\n",
      "Iteration 739, loss = 2.43479928\n",
      "Iteration 740, loss = 2.43487523\n",
      "Iteration 741, loss = 2.43365204\n",
      "Iteration 742, loss = 2.43227823\n",
      "Iteration 743, loss = 2.43217742\n",
      "Iteration 744, loss = 2.43179756\n",
      "Iteration 745, loss = 2.43107800\n",
      "Iteration 746, loss = 2.43118883\n",
      "Iteration 747, loss = 2.43035203\n",
      "Iteration 748, loss = 2.43026649\n",
      "Iteration 749, loss = 2.42936236\n",
      "Iteration 750, loss = 2.42708785\n",
      "Iteration 751, loss = 2.42678631\n",
      "Iteration 752, loss = 2.42624545\n",
      "Iteration 753, loss = 2.42511992\n",
      "Iteration 754, loss = 2.42460905\n",
      "Iteration 755, loss = 2.42381837\n",
      "Iteration 756, loss = 2.42384807\n",
      "Iteration 757, loss = 2.42296878\n",
      "Iteration 758, loss = 2.42201154\n",
      "Iteration 759, loss = 2.42187057\n",
      "Iteration 760, loss = 2.42161369\n",
      "Iteration 761, loss = 2.41960367\n",
      "Iteration 762, loss = 2.42106986\n",
      "Iteration 763, loss = 2.41995988\n",
      "Iteration 764, loss = 2.42052563\n",
      "Iteration 765, loss = 2.41765905\n",
      "Iteration 766, loss = 2.41934717\n",
      "Iteration 767, loss = 2.41809224\n",
      "Iteration 768, loss = 2.41573252\n",
      "Iteration 769, loss = 2.41495918\n",
      "Iteration 770, loss = 2.41621315\n",
      "Iteration 771, loss = 2.41407827\n",
      "Iteration 772, loss = 2.41251749\n",
      "Iteration 773, loss = 2.41278086\n",
      "Iteration 774, loss = 2.41147742\n",
      "Iteration 775, loss = 2.41108235\n",
      "Iteration 776, loss = 2.41017442\n",
      "Iteration 777, loss = 2.41077635\n",
      "Iteration 778, loss = 2.40953182\n",
      "Iteration 779, loss = 2.40848621\n",
      "Iteration 780, loss = 2.40819318\n",
      "Iteration 781, loss = 2.40796323\n",
      "Iteration 782, loss = 2.40726459\n",
      "Iteration 783, loss = 2.40645132\n",
      "Iteration 784, loss = 2.40586417\n",
      "Iteration 785, loss = 2.40403129\n",
      "Iteration 786, loss = 2.40451242\n",
      "Iteration 787, loss = 2.40362877\n",
      "Iteration 788, loss = 2.40476865\n",
      "Iteration 789, loss = 2.40329146\n",
      "Iteration 790, loss = 2.40178909\n",
      "Iteration 791, loss = 2.40039248\n",
      "Iteration 792, loss = 2.40194234\n",
      "Iteration 793, loss = 2.40027812\n",
      "Iteration 794, loss = 2.39889789\n",
      "Iteration 795, loss = 2.39987344\n",
      "Iteration 796, loss = 2.39943072\n",
      "Iteration 797, loss = 2.39736523\n",
      "Iteration 798, loss = 2.39702138\n",
      "Iteration 799, loss = 2.39625654\n",
      "Iteration 800, loss = 2.39642714\n",
      "Iteration 801, loss = 2.39526641\n",
      "Iteration 802, loss = 2.39459184\n",
      "Iteration 803, loss = 2.39364357\n",
      "Iteration 804, loss = 2.39516612\n",
      "Iteration 805, loss = 2.39342264\n",
      "Iteration 806, loss = 2.39210781\n",
      "Iteration 807, loss = 2.39131322\n",
      "Iteration 808, loss = 2.39087859\n",
      "Iteration 809, loss = 2.39024797\n",
      "Iteration 810, loss = 2.38851401\n",
      "Iteration 811, loss = 2.38821492\n",
      "Iteration 812, loss = 2.38921443\n",
      "Iteration 813, loss = 2.38812977\n",
      "Iteration 814, loss = 2.38762069\n",
      "Iteration 815, loss = 2.38613222\n",
      "Iteration 816, loss = 2.38811038\n",
      "Iteration 817, loss = 2.38697822\n",
      "Iteration 818, loss = 2.38453829\n",
      "Iteration 819, loss = 2.38356477\n",
      "Iteration 820, loss = 2.38393152\n",
      "Iteration 821, loss = 2.38285938\n",
      "Iteration 822, loss = 2.38250058\n",
      "Iteration 823, loss = 2.38081734\n",
      "Iteration 824, loss = 2.38185690\n",
      "Iteration 825, loss = 2.38095248\n",
      "Iteration 826, loss = 2.38223279\n",
      "Iteration 827, loss = 2.37877810\n",
      "Iteration 828, loss = 2.37838526\n",
      "Iteration 829, loss = 2.37746407\n",
      "Iteration 830, loss = 2.37674949\n",
      "Iteration 831, loss = 2.37539077\n",
      "Iteration 832, loss = 2.37625515\n",
      "Iteration 833, loss = 2.37495956\n",
      "Iteration 834, loss = 2.37433531\n",
      "Iteration 835, loss = 2.37436242\n",
      "Iteration 836, loss = 2.37374419\n",
      "Iteration 837, loss = 2.37236566\n",
      "Iteration 838, loss = 2.37174668\n",
      "Iteration 839, loss = 2.37174482\n",
      "Iteration 840, loss = 2.37293071\n",
      "Iteration 841, loss = 2.37093030\n",
      "Iteration 842, loss = 2.37014614\n",
      "Iteration 843, loss = 2.37175895\n",
      "Iteration 844, loss = 2.36916682\n",
      "Iteration 845, loss = 2.36848746\n",
      "Iteration 846, loss = 2.36792361\n",
      "Iteration 847, loss = 2.36889332\n",
      "Iteration 848, loss = 2.36740434\n",
      "Iteration 849, loss = 2.36684748\n",
      "Iteration 850, loss = 2.36560702\n",
      "Iteration 851, loss = 2.36496047\n",
      "Iteration 852, loss = 2.36342963\n",
      "Iteration 853, loss = 2.36373229\n",
      "Iteration 854, loss = 2.36371502\n",
      "Iteration 855, loss = 2.36188825\n",
      "Iteration 856, loss = 2.36104153\n",
      "Iteration 857, loss = 2.36022228\n",
      "Iteration 858, loss = 2.36081761\n",
      "Iteration 859, loss = 2.35977163\n",
      "Iteration 860, loss = 2.35954679\n",
      "Iteration 861, loss = 2.35987274\n",
      "Iteration 862, loss = 2.35851587\n",
      "Iteration 863, loss = 2.35642225\n",
      "Iteration 864, loss = 2.35693812\n",
      "Iteration 865, loss = 2.35729999\n",
      "Iteration 866, loss = 2.35567243\n",
      "Iteration 867, loss = 2.35657084\n",
      "Iteration 868, loss = 2.35467335\n",
      "Iteration 869, loss = 2.35428119\n",
      "Iteration 870, loss = 2.35360496\n",
      "Iteration 871, loss = 2.35340252\n",
      "Iteration 872, loss = 2.35247705\n",
      "Iteration 873, loss = 2.35192052\n",
      "Iteration 874, loss = 2.35115253\n",
      "Iteration 875, loss = 2.35117840\n",
      "Iteration 876, loss = 2.34991696\n",
      "Iteration 877, loss = 2.35052553\n",
      "Iteration 878, loss = 2.35102159\n",
      "Iteration 879, loss = 2.34759852\n",
      "Iteration 880, loss = 2.34783515\n",
      "Iteration 881, loss = 2.34832949\n",
      "Iteration 882, loss = 2.34815360\n",
      "Iteration 883, loss = 2.34583294\n",
      "Iteration 884, loss = 2.34577277\n",
      "Iteration 885, loss = 2.34442621\n",
      "Iteration 886, loss = 2.34649043\n",
      "Iteration 887, loss = 2.34469832\n",
      "Iteration 888, loss = 2.34429761\n",
      "Iteration 889, loss = 2.34161289\n",
      "Iteration 890, loss = 2.34164613\n",
      "Iteration 891, loss = 2.34207847\n",
      "Iteration 892, loss = 2.34046161\n",
      "Iteration 893, loss = 2.34018738\n",
      "Iteration 894, loss = 2.33928319\n",
      "Iteration 895, loss = 2.34001158\n",
      "Iteration 896, loss = 2.33834886\n",
      "Iteration 897, loss = 2.33685443\n",
      "Iteration 898, loss = 2.33968365\n",
      "Iteration 899, loss = 2.33677376\n",
      "Iteration 900, loss = 2.33631891\n",
      "Iteration 901, loss = 2.33610026\n",
      "Iteration 902, loss = 2.33436186\n",
      "Iteration 903, loss = 2.33465719\n",
      "Iteration 904, loss = 2.33392262\n",
      "Iteration 905, loss = 2.33253979\n",
      "Iteration 906, loss = 2.33356596\n",
      "Iteration 907, loss = 2.33269866\n",
      "Iteration 908, loss = 2.33087502\n",
      "Iteration 909, loss = 2.33035587\n",
      "Iteration 910, loss = 2.33020192\n",
      "Iteration 911, loss = 2.32969681\n",
      "Iteration 912, loss = 2.32799861\n",
      "Iteration 913, loss = 2.32884825\n",
      "Iteration 914, loss = 2.32873884\n",
      "Iteration 915, loss = 2.32865055\n",
      "Iteration 916, loss = 2.32749565\n",
      "Iteration 917, loss = 2.32756642\n",
      "Iteration 918, loss = 2.32579295\n",
      "Iteration 919, loss = 2.32492858\n",
      "Iteration 920, loss = 2.32460063\n",
      "Iteration 921, loss = 2.32515157\n",
      "Iteration 922, loss = 2.32269463\n",
      "Iteration 923, loss = 2.32247263\n",
      "Iteration 924, loss = 2.32276128\n",
      "Iteration 925, loss = 2.31954220\n",
      "Iteration 926, loss = 2.32360095\n",
      "Iteration 927, loss = 2.32059820\n",
      "Iteration 928, loss = 2.32108936\n",
      "Iteration 929, loss = 2.31872242\n",
      "Iteration 930, loss = 2.31831149\n",
      "Iteration 931, loss = 2.31852422\n",
      "Iteration 932, loss = 2.31809875\n",
      "Iteration 933, loss = 2.31722878\n",
      "Iteration 934, loss = 2.31669699\n",
      "Iteration 935, loss = 2.31650197\n",
      "Iteration 936, loss = 2.31579910\n",
      "Iteration 937, loss = 2.31352635\n",
      "Iteration 938, loss = 2.31433961\n",
      "Iteration 939, loss = 2.31457299\n",
      "Iteration 940, loss = 2.31305391\n",
      "Iteration 941, loss = 2.31293860\n",
      "Iteration 942, loss = 2.31299882\n",
      "Iteration 943, loss = 2.31197336\n",
      "Iteration 944, loss = 2.31109151\n",
      "Iteration 945, loss = 2.31074153\n",
      "Iteration 946, loss = 2.30923451\n",
      "Iteration 947, loss = 2.31040284\n",
      "Iteration 948, loss = 2.31104732\n",
      "Iteration 949, loss = 2.30874613\n",
      "Iteration 950, loss = 2.30798136\n",
      "Iteration 951, loss = 2.30696732\n",
      "Iteration 952, loss = 2.30752592\n",
      "Iteration 953, loss = 2.30766947\n",
      "Iteration 954, loss = 2.30655499\n",
      "Iteration 955, loss = 2.30693482\n",
      "Iteration 956, loss = 2.30780342\n",
      "Iteration 957, loss = 2.30604327\n",
      "Iteration 958, loss = 2.30437122\n",
      "Iteration 959, loss = 2.30394056\n",
      "Iteration 960, loss = 2.30373481\n",
      "Iteration 961, loss = 2.30389879\n",
      "Iteration 962, loss = 2.30135922\n",
      "Iteration 963, loss = 2.30135630\n",
      "Iteration 964, loss = 2.30325943\n",
      "Iteration 965, loss = 2.30214354\n",
      "Iteration 966, loss = 2.29990143\n",
      "Iteration 967, loss = 2.29868126\n",
      "Iteration 968, loss = 2.29839997\n",
      "Iteration 969, loss = 2.29950697\n",
      "Iteration 970, loss = 2.29845322\n",
      "Iteration 971, loss = 2.29574411\n",
      "Iteration 972, loss = 2.29692005\n",
      "Iteration 973, loss = 2.29566119\n",
      "Iteration 974, loss = 2.29537648\n",
      "Iteration 975, loss = 2.29625427\n",
      "Iteration 976, loss = 2.29427024\n",
      "Iteration 977, loss = 2.29349630\n",
      "Iteration 978, loss = 2.29359588\n",
      "Iteration 979, loss = 2.29289621\n",
      "Iteration 980, loss = 2.29270482\n",
      "Iteration 981, loss = 2.29077143\n",
      "Iteration 982, loss = 2.29147356\n",
      "Iteration 983, loss = 2.29066551\n",
      "Iteration 984, loss = 2.28907712\n",
      "Iteration 985, loss = 2.28953388\n",
      "Iteration 986, loss = 2.28840750\n",
      "Iteration 987, loss = 2.28840793\n",
      "Iteration 988, loss = 2.28750211\n",
      "Iteration 989, loss = 2.28866770\n",
      "Iteration 990, loss = 2.28752329\n",
      "Iteration 991, loss = 2.28672485\n",
      "Iteration 992, loss = 2.28531859\n",
      "Iteration 993, loss = 2.28469326\n",
      "Iteration 994, loss = 2.28533064\n",
      "Iteration 995, loss = 2.28402895\n",
      "Iteration 996, loss = 2.28311300\n",
      "Iteration 997, loss = 2.28215440\n",
      "Iteration 998, loss = 2.28168163\n",
      "Iteration 999, loss = 2.28095922\n",
      "Iteration 1000, loss = 2.28099286\n",
      "Iteration 1001, loss = 2.28136719\n",
      "Iteration 1002, loss = 2.28063460\n",
      "Iteration 1003, loss = 2.27888376\n",
      "Iteration 1004, loss = 2.28001826\n",
      "Iteration 1005, loss = 2.27929505\n",
      "Iteration 1006, loss = 2.28039003\n",
      "Iteration 1007, loss = 2.27841377\n",
      "Iteration 1008, loss = 2.27721454\n",
      "Iteration 1009, loss = 2.27730701\n",
      "Iteration 1010, loss = 2.27613105\n",
      "Iteration 1011, loss = 2.27564474\n",
      "Iteration 1012, loss = 2.27669885\n",
      "Iteration 1013, loss = 2.27685411\n",
      "Iteration 1014, loss = 2.27532146\n",
      "Iteration 1015, loss = 2.27464359\n",
      "Iteration 1016, loss = 2.27312019\n",
      "Iteration 1017, loss = 2.27209166\n",
      "Iteration 1018, loss = 2.27229680\n",
      "Iteration 1019, loss = 2.27341522\n",
      "Iteration 1020, loss = 2.27209977\n",
      "Iteration 1021, loss = 2.27097068\n",
      "Iteration 1022, loss = 2.27012636\n",
      "Iteration 1023, loss = 2.27178024\n",
      "Iteration 1024, loss = 2.27203757\n",
      "Iteration 1025, loss = 2.26929836\n",
      "Iteration 1026, loss = 2.26824844\n",
      "Iteration 1027, loss = 2.26843251\n",
      "Iteration 1028, loss = 2.26793073\n",
      "Iteration 1029, loss = 2.26659391\n",
      "Iteration 1030, loss = 2.26530519\n",
      "Iteration 1031, loss = 2.26569033\n",
      "Iteration 1032, loss = 2.26573862\n",
      "Iteration 1033, loss = 2.26644386\n",
      "Iteration 1034, loss = 2.26399842\n",
      "Iteration 1035, loss = 2.26326803\n",
      "Iteration 1036, loss = 2.26360190\n",
      "Iteration 1037, loss = 2.26174025\n",
      "Iteration 1038, loss = 2.26090768\n",
      "Iteration 1039, loss = 2.26010798\n",
      "Iteration 1040, loss = 2.26202816\n",
      "Iteration 1041, loss = 2.26034836\n",
      "Iteration 1042, loss = 2.26095191\n",
      "Iteration 1043, loss = 2.25871059\n",
      "Iteration 1044, loss = 2.26086098\n",
      "Iteration 1045, loss = 2.25759340\n",
      "Iteration 1046, loss = 2.25741378\n",
      "Iteration 1047, loss = 2.25802069\n",
      "Iteration 1048, loss = 2.25747022\n",
      "Iteration 1049, loss = 2.25639337\n",
      "Iteration 1050, loss = 2.25631799\n",
      "Iteration 1051, loss = 2.25615288\n",
      "Iteration 1052, loss = 2.25546029\n",
      "Iteration 1053, loss = 2.25424661\n",
      "Iteration 1054, loss = 2.25423696\n",
      "Iteration 1055, loss = 2.25332661\n",
      "Iteration 1056, loss = 2.25345893\n",
      "Iteration 1057, loss = 2.25165744\n",
      "Iteration 1058, loss = 2.25200894\n",
      "Iteration 1059, loss = 2.25193648\n",
      "Iteration 1060, loss = 2.25135144\n",
      "Iteration 1061, loss = 2.25086558\n",
      "Iteration 1062, loss = 2.25169643\n",
      "Iteration 1063, loss = 2.25056186\n",
      "Iteration 1064, loss = 2.25063418\n",
      "Iteration 1065, loss = 2.24812038\n",
      "Iteration 1066, loss = 2.24834124\n",
      "Iteration 1067, loss = 2.24756104\n",
      "Iteration 1068, loss = 2.24657393\n",
      "Iteration 1069, loss = 2.24708539\n",
      "Iteration 1070, loss = 2.24678338\n",
      "Iteration 1071, loss = 2.24525337\n",
      "Iteration 1072, loss = 2.24634709\n",
      "Iteration 1073, loss = 2.24373823\n",
      "Iteration 1074, loss = 2.24269615\n",
      "Iteration 1075, loss = 2.24310790\n",
      "Iteration 1076, loss = 2.24218975\n",
      "Iteration 1077, loss = 2.24314860\n",
      "Iteration 1078, loss = 2.24379212\n",
      "Iteration 1079, loss = 2.24105547\n",
      "Iteration 1080, loss = 2.24195847\n",
      "Iteration 1081, loss = 2.24079028\n",
      "Iteration 1082, loss = 2.24064452\n",
      "Iteration 1083, loss = 2.23942718\n",
      "Iteration 1084, loss = 2.23770337\n",
      "Iteration 1085, loss = 2.23807655\n",
      "Iteration 1086, loss = 2.23787812\n",
      "Iteration 1087, loss = 2.23696000\n",
      "Iteration 1088, loss = 2.23700582\n",
      "Iteration 1089, loss = 2.23612910\n",
      "Iteration 1090, loss = 2.23620252\n",
      "Iteration 1091, loss = 2.23521824\n",
      "Iteration 1092, loss = 2.23547145\n",
      "Iteration 1093, loss = 2.23512577\n",
      "Iteration 1094, loss = 2.23432718\n",
      "Iteration 1095, loss = 2.23535432\n",
      "Iteration 1096, loss = 2.23439666\n",
      "Iteration 1097, loss = 2.23348113\n",
      "Iteration 1098, loss = 2.23188651\n",
      "Iteration 1099, loss = 2.23141873\n",
      "Iteration 1100, loss = 2.23062201\n",
      "Iteration 1101, loss = 2.23035460\n",
      "Iteration 1102, loss = 2.23014372\n",
      "Iteration 1103, loss = 2.23064124\n",
      "Iteration 1104, loss = 2.23047541\n",
      "Iteration 1105, loss = 2.23043531\n",
      "Iteration 1106, loss = 2.22857897\n",
      "Iteration 1107, loss = 2.22795539\n",
      "Iteration 1108, loss = 2.22741208\n",
      "Iteration 1109, loss = 2.22621137\n",
      "Iteration 1110, loss = 2.22507091\n",
      "Iteration 1111, loss = 2.22526312\n",
      "Iteration 1112, loss = 2.22578605\n",
      "Iteration 1113, loss = 2.22622648\n",
      "Iteration 1114, loss = 2.22537043\n",
      "Iteration 1115, loss = 2.22243624\n",
      "Iteration 1116, loss = 2.22493239\n",
      "Iteration 1117, loss = 2.22388891\n",
      "Iteration 1118, loss = 2.22188880\n",
      "Iteration 1119, loss = 2.22175834\n",
      "Iteration 1120, loss = 2.22284913\n",
      "Iteration 1121, loss = 2.22325795\n",
      "Iteration 1122, loss = 2.22036834\n",
      "Iteration 1123, loss = 2.22029497\n",
      "Iteration 1124, loss = 2.21916294\n",
      "Iteration 1125, loss = 2.21882895\n",
      "Iteration 1126, loss = 2.21835907\n",
      "Iteration 1127, loss = 2.21855837\n",
      "Iteration 1128, loss = 2.21913783\n",
      "Iteration 1129, loss = 2.21724842\n",
      "Iteration 1130, loss = 2.21971468\n",
      "Iteration 1131, loss = 2.21998398\n",
      "Iteration 1132, loss = 2.21829699\n",
      "Iteration 1133, loss = 2.21836398\n",
      "Iteration 1134, loss = 2.21674963\n",
      "Iteration 1135, loss = 2.21540912\n",
      "Iteration 1136, loss = 2.21354667\n",
      "Iteration 1137, loss = 2.21385961\n",
      "Iteration 1138, loss = 2.21374460\n",
      "Iteration 1139, loss = 2.21236288\n",
      "Iteration 1140, loss = 2.21175294\n",
      "Iteration 1141, loss = 2.21146798\n",
      "Iteration 1142, loss = 2.21073240\n",
      "Iteration 1143, loss = 2.21031659\n",
      "Iteration 1144, loss = 2.21102280\n",
      "Iteration 1145, loss = 2.21094192\n",
      "Iteration 1146, loss = 2.20869836\n",
      "Iteration 1147, loss = 2.20847860\n",
      "Iteration 1148, loss = 2.20902372\n",
      "Iteration 1149, loss = 2.20831870\n",
      "Iteration 1150, loss = 2.20776420\n",
      "Iteration 1151, loss = 2.20700657\n",
      "Iteration 1152, loss = 2.20780987\n",
      "Iteration 1153, loss = 2.20687802\n",
      "Iteration 1154, loss = 2.20595343\n",
      "Iteration 1155, loss = 2.20702422\n",
      "Iteration 1156, loss = 2.20835094\n",
      "Iteration 1157, loss = 2.20454862\n",
      "Iteration 1158, loss = 2.20477341\n",
      "Iteration 1159, loss = 2.20375123\n",
      "Iteration 1160, loss = 2.20210609\n",
      "Iteration 1161, loss = 2.20248842\n",
      "Iteration 1162, loss = 2.20206964\n",
      "Iteration 1163, loss = 2.20184948\n",
      "Iteration 1164, loss = 2.20051060\n",
      "Iteration 1165, loss = 2.20072725\n",
      "Iteration 1166, loss = 2.20173297\n",
      "Iteration 1167, loss = 2.19991264\n",
      "Iteration 1168, loss = 2.20019022\n",
      "Iteration 1169, loss = 2.19791508\n",
      "Iteration 1170, loss = 2.19950616\n",
      "Iteration 1171, loss = 2.19738037\n",
      "Iteration 1172, loss = 2.19734858\n",
      "Iteration 1173, loss = 2.19896595\n",
      "Iteration 1174, loss = 2.19771799\n",
      "Iteration 1175, loss = 2.19501944\n",
      "Iteration 1176, loss = 2.19604249\n",
      "Iteration 1177, loss = 2.19554274\n",
      "Iteration 1178, loss = 2.19520243\n",
      "Iteration 1179, loss = 2.19471795\n",
      "Iteration 1180, loss = 2.19214577\n",
      "Iteration 1181, loss = 2.19338108\n",
      "Iteration 1182, loss = 2.19495915\n",
      "Iteration 1183, loss = 2.19247844\n",
      "Iteration 1184, loss = 2.19268801\n",
      "Iteration 1185, loss = 2.19155442\n",
      "Iteration 1186, loss = 2.19179086\n",
      "Iteration 1187, loss = 2.19148136\n",
      "Iteration 1188, loss = 2.19020078\n",
      "Iteration 1189, loss = 2.18981797\n",
      "Iteration 1190, loss = 2.18806952\n",
      "Iteration 1191, loss = 2.18882197\n",
      "Iteration 1192, loss = 2.18732744\n",
      "Iteration 1193, loss = 2.18754677\n",
      "Iteration 1194, loss = 2.18815525\n",
      "Iteration 1195, loss = 2.18640493\n",
      "Iteration 1196, loss = 2.18685034\n",
      "Iteration 1197, loss = 2.18556098\n",
      "Iteration 1198, loss = 2.18452880\n",
      "Iteration 1199, loss = 2.18667306\n",
      "Iteration 1200, loss = 2.18528660\n",
      "Iteration 1201, loss = 2.18397345\n",
      "Iteration 1202, loss = 2.18565981\n",
      "Iteration 1203, loss = 2.18276358\n",
      "Iteration 1204, loss = 2.18334980\n",
      "Iteration 1205, loss = 2.18303550\n",
      "Iteration 1206, loss = 2.18216614\n",
      "Iteration 1207, loss = 2.18256464\n",
      "Iteration 1208, loss = 2.17985764\n",
      "Iteration 1209, loss = 2.18001823\n",
      "Iteration 1210, loss = 2.18103490\n",
      "Iteration 1211, loss = 2.18103629\n",
      "Iteration 1212, loss = 2.17944315\n",
      "Iteration 1213, loss = 2.18039278\n",
      "Iteration 1214, loss = 2.18064929\n",
      "Iteration 1215, loss = 2.17887519\n",
      "Iteration 1216, loss = 2.17825117\n",
      "Iteration 1217, loss = 2.17883824\n",
      "Iteration 1218, loss = 2.17827340\n",
      "Iteration 1219, loss = 2.17629474\n",
      "Iteration 1220, loss = 2.17616451\n",
      "Iteration 1221, loss = 2.17671897\n",
      "Iteration 1222, loss = 2.17656419\n",
      "Iteration 1223, loss = 2.17506764\n",
      "Iteration 1224, loss = 2.17343067\n",
      "Iteration 1225, loss = 2.17232546\n",
      "Iteration 1226, loss = 2.17303147\n",
      "Iteration 1227, loss = 2.17328221\n",
      "Iteration 1228, loss = 2.17171733\n",
      "Iteration 1229, loss = 2.17383146\n",
      "Iteration 1230, loss = 2.17042200\n",
      "Iteration 1231, loss = 2.17244828\n",
      "Iteration 1232, loss = 2.17220568\n",
      "Iteration 1233, loss = 2.17025216\n",
      "Iteration 1234, loss = 2.16856795\n",
      "Iteration 1235, loss = 2.17003870\n",
      "Iteration 1236, loss = 2.16779393\n",
      "Iteration 1237, loss = 2.16930569\n",
      "Iteration 1238, loss = 2.16834557\n",
      "Iteration 1239, loss = 2.16664416\n",
      "Iteration 1240, loss = 2.16652109\n",
      "Iteration 1241, loss = 2.16534475\n",
      "Iteration 1242, loss = 2.16559346\n",
      "Iteration 1243, loss = 2.16670704\n",
      "Iteration 1244, loss = 2.16546302\n",
      "Iteration 1245, loss = 2.16624754\n",
      "Iteration 1246, loss = 2.16505370\n",
      "Iteration 1247, loss = 2.16369602\n",
      "Iteration 1248, loss = 2.16428254\n",
      "Iteration 1249, loss = 2.16325119\n",
      "Iteration 1250, loss = 2.16230453\n",
      "Iteration 1251, loss = 2.16327665\n",
      "Iteration 1252, loss = 2.16245644\n",
      "Iteration 1253, loss = 2.16240135\n",
      "Iteration 1254, loss = 2.16085563\n",
      "Iteration 1255, loss = 2.16141959\n",
      "Iteration 1256, loss = 2.15970754\n",
      "Iteration 1257, loss = 2.15975030\n",
      "Iteration 1258, loss = 2.15919266\n",
      "Iteration 1259, loss = 2.15777752\n",
      "Iteration 1260, loss = 2.15762413\n",
      "Iteration 1261, loss = 2.15982733\n",
      "Iteration 1262, loss = 2.15834638\n",
      "Iteration 1263, loss = 2.15784137\n",
      "Iteration 1264, loss = 2.15653876\n",
      "Iteration 1265, loss = 2.15461012\n",
      "Iteration 1266, loss = 2.15719126\n",
      "Iteration 1267, loss = 2.15582094\n",
      "Iteration 1268, loss = 2.15495425\n",
      "Iteration 1269, loss = 2.15302781\n",
      "Iteration 1270, loss = 2.15417387\n",
      "Iteration 1271, loss = 2.15311694\n",
      "Iteration 1272, loss = 2.15284604\n",
      "Iteration 1273, loss = 2.15227172\n",
      "Iteration 1274, loss = 2.15348225\n",
      "Iteration 1275, loss = 2.15340608\n",
      "Iteration 1276, loss = 2.15352608\n",
      "Iteration 1277, loss = 2.14952345\n",
      "Iteration 1278, loss = 2.14914352\n",
      "Iteration 1279, loss = 2.15028095\n",
      "Iteration 1280, loss = 2.14962640\n",
      "Iteration 1281, loss = 2.15026465\n",
      "Iteration 1282, loss = 2.15057988\n",
      "Iteration 1283, loss = 2.14924152\n",
      "Iteration 1284, loss = 2.14834137\n",
      "Iteration 1285, loss = 2.14567232\n",
      "Iteration 1286, loss = 2.14600058\n",
      "Iteration 1287, loss = 2.14703654\n",
      "Iteration 1288, loss = 2.14580845\n",
      "Iteration 1289, loss = 2.14516437\n",
      "Iteration 1290, loss = 2.14543651\n",
      "Iteration 1291, loss = 2.14567967\n",
      "Iteration 1292, loss = 2.14519259\n",
      "Iteration 1293, loss = 2.14295102\n",
      "Iteration 1294, loss = 2.14399021\n",
      "Iteration 1295, loss = 2.14253180\n",
      "Iteration 1296, loss = 2.14336051\n",
      "Iteration 1297, loss = 2.14204954\n",
      "Iteration 1298, loss = 2.14179075\n",
      "Iteration 1299, loss = 2.14110297\n",
      "Iteration 1300, loss = 2.14155558\n",
      "Iteration 1301, loss = 2.14135740\n",
      "Iteration 1302, loss = 2.14057742\n",
      "Iteration 1303, loss = 2.13929704\n",
      "Iteration 1304, loss = 2.13966784\n",
      "Iteration 1305, loss = 2.13933493\n",
      "Iteration 1306, loss = 2.13818714\n",
      "Iteration 1307, loss = 2.13786122\n",
      "Iteration 1308, loss = 2.13695775\n",
      "Iteration 1309, loss = 2.13784802\n",
      "Iteration 1310, loss = 2.13772865\n",
      "Iteration 1311, loss = 2.13635622\n",
      "Iteration 1312, loss = 2.13650316\n",
      "Iteration 1313, loss = 2.13526629\n",
      "Iteration 1314, loss = 2.13675653\n",
      "Iteration 1315, loss = 2.13703258\n",
      "Iteration 1316, loss = 2.13360861\n",
      "Iteration 1317, loss = 2.13438586\n",
      "Iteration 1318, loss = 2.13344007\n",
      "Iteration 1319, loss = 2.13279562\n",
      "Iteration 1320, loss = 2.13268012\n",
      "Iteration 1321, loss = 2.13161681\n",
      "Iteration 1322, loss = 2.13062379\n",
      "Iteration 1323, loss = 2.13103385\n",
      "Iteration 1324, loss = 2.12982639\n",
      "Iteration 1325, loss = 2.13175214\n",
      "Iteration 1326, loss = 2.12972958\n",
      "Iteration 1327, loss = 2.12935495\n",
      "Iteration 1328, loss = 2.12939797\n",
      "Iteration 1329, loss = 2.13077192\n",
      "Iteration 1330, loss = 2.13111340\n",
      "Iteration 1331, loss = 2.12931795\n",
      "Iteration 1332, loss = 2.12960744\n",
      "Iteration 1333, loss = 2.12721101\n",
      "Iteration 1334, loss = 2.13119108\n",
      "Iteration 1335, loss = 2.12526684\n",
      "Iteration 1336, loss = 2.12678969\n",
      "Iteration 1337, loss = 2.12667723\n",
      "Iteration 1338, loss = 2.12810069\n",
      "Iteration 1339, loss = 2.12610004\n",
      "Iteration 1340, loss = 2.12612279\n",
      "Iteration 1341, loss = 2.12448395\n",
      "Iteration 1342, loss = 2.12269746\n",
      "Iteration 1343, loss = 2.12232950\n",
      "Iteration 1344, loss = 2.12514460\n",
      "Iteration 1345, loss = 2.12368680\n",
      "Iteration 1346, loss = 2.12337357\n",
      "Iteration 1347, loss = 2.12322020\n",
      "Iteration 1348, loss = 2.12298249\n",
      "Iteration 1349, loss = 2.12404778\n",
      "Iteration 1350, loss = 2.12306927\n",
      "Iteration 1351, loss = 2.11952776\n",
      "Iteration 1352, loss = 2.11985120\n",
      "Iteration 1353, loss = 2.11950331\n",
      "Iteration 1354, loss = 2.12018385\n",
      "Iteration 1355, loss = 2.11823501\n",
      "Iteration 1356, loss = 2.11862658\n",
      "Iteration 1357, loss = 2.11735897\n",
      "Iteration 1358, loss = 2.11711958\n",
      "Iteration 1359, loss = 2.11694158\n",
      "Iteration 1360, loss = 2.11570076\n",
      "Iteration 1361, loss = 2.11626211\n",
      "Iteration 1362, loss = 2.11549129\n",
      "Iteration 1363, loss = 2.11517869\n",
      "Iteration 1364, loss = 2.11504355\n",
      "Iteration 1365, loss = 2.11356167\n",
      "Iteration 1366, loss = 2.11384929\n",
      "Iteration 1367, loss = 2.11427491\n",
      "Iteration 1368, loss = 2.11312701\n",
      "Iteration 1369, loss = 2.11205506\n",
      "Iteration 1370, loss = 2.11135165\n",
      "Iteration 1371, loss = 2.11109281\n",
      "Iteration 1372, loss = 2.11011568\n",
      "Iteration 1373, loss = 2.10956086\n",
      "Iteration 1374, loss = 2.11023464\n",
      "Iteration 1375, loss = 2.10979452\n",
      "Iteration 1376, loss = 2.10873014\n",
      "Iteration 1377, loss = 2.10823582\n",
      "Iteration 1378, loss = 2.10880877\n",
      "Iteration 1379, loss = 2.10943570\n",
      "Iteration 1380, loss = 2.10939611\n",
      "Iteration 1381, loss = 2.10785659\n",
      "Iteration 1382, loss = 2.10771942\n",
      "Iteration 1383, loss = 2.10668453\n",
      "Iteration 1384, loss = 2.10604918\n",
      "Iteration 1385, loss = 2.10572713\n",
      "Iteration 1386, loss = 2.10496740\n",
      "Iteration 1387, loss = 2.10426627\n",
      "Iteration 1388, loss = 2.10558805\n",
      "Iteration 1389, loss = 2.10498106\n",
      "Iteration 1390, loss = 2.10477069\n",
      "Iteration 1391, loss = 2.10433951\n",
      "Iteration 1392, loss = 2.10295810\n",
      "Iteration 1393, loss = 2.10219732\n",
      "Iteration 1394, loss = 2.10088006\n",
      "Iteration 1395, loss = 2.10107797\n",
      "Iteration 1396, loss = 2.10137603\n",
      "Iteration 1397, loss = 2.10125417\n",
      "Iteration 1398, loss = 2.10091070\n",
      "Iteration 1399, loss = 2.10148439\n",
      "Iteration 1400, loss = 2.10139845\n",
      "Iteration 1401, loss = 2.09955633\n",
      "Iteration 1402, loss = 2.09948955\n",
      "Iteration 1403, loss = 2.09950170\n",
      "Iteration 1404, loss = 2.09944116\n",
      "Iteration 1405, loss = 2.09914327\n",
      "Iteration 1406, loss = 2.09691877\n",
      "Iteration 1407, loss = 2.09718026\n",
      "Iteration 1408, loss = 2.09773273\n",
      "Iteration 1409, loss = 2.09687153\n",
      "Iteration 1410, loss = 2.09515675\n",
      "Iteration 1411, loss = 2.09592265\n",
      "Iteration 1412, loss = 2.09564860\n",
      "Iteration 1413, loss = 2.09513614\n",
      "Iteration 1414, loss = 2.09561942\n",
      "Iteration 1415, loss = 2.09525515\n",
      "Iteration 1416, loss = 2.09490611\n",
      "Iteration 1417, loss = 2.09554584\n",
      "Iteration 1418, loss = 2.09289041\n",
      "Iteration 1419, loss = 2.09254257\n",
      "Iteration 1420, loss = 2.09347478\n",
      "Iteration 1421, loss = 2.09281276\n",
      "Iteration 1422, loss = 2.09133709\n",
      "Iteration 1423, loss = 2.09360021\n",
      "Iteration 1424, loss = 2.09126713\n",
      "Iteration 1425, loss = 2.09099066\n",
      "Iteration 1426, loss = 2.09168378\n",
      "Iteration 1427, loss = 2.08966494\n",
      "Iteration 1428, loss = 2.08854488\n",
      "Iteration 1429, loss = 2.08819362\n",
      "Iteration 1430, loss = 2.08835287\n",
      "Iteration 1431, loss = 2.08806199\n",
      "Iteration 1432, loss = 2.08722374\n",
      "Iteration 1433, loss = 2.08608865\n",
      "Iteration 1434, loss = 2.08669684\n",
      "Iteration 1435, loss = 2.08715967\n",
      "Iteration 1436, loss = 2.08733371\n",
      "Iteration 1437, loss = 2.08572086\n",
      "Iteration 1438, loss = 2.08468816\n",
      "Iteration 1439, loss = 2.08614769\n",
      "Iteration 1440, loss = 2.08546342\n",
      "Iteration 1441, loss = 2.08372785\n",
      "Iteration 1442, loss = 2.08350189\n",
      "Iteration 1443, loss = 2.08529145\n",
      "Iteration 1444, loss = 2.08217722\n",
      "Iteration 1445, loss = 2.08216025\n",
      "Iteration 1446, loss = 2.08376168\n",
      "Iteration 1447, loss = 2.08397435\n",
      "Iteration 1448, loss = 2.08294104\n",
      "Iteration 1449, loss = 2.08203350\n",
      "Iteration 1450, loss = 2.08162077\n",
      "Iteration 1451, loss = 2.08044201\n",
      "Iteration 1452, loss = 2.08039514\n",
      "Iteration 1453, loss = 2.08088276\n",
      "Iteration 1454, loss = 2.07899305\n",
      "Iteration 1455, loss = 2.07841102\n",
      "Iteration 1456, loss = 2.07813009\n",
      "Iteration 1457, loss = 2.07685637\n",
      "Iteration 1458, loss = 2.07681754\n",
      "Iteration 1459, loss = 2.07619651\n",
      "Iteration 1460, loss = 2.07626827\n",
      "Iteration 1461, loss = 2.07768260\n",
      "Iteration 1462, loss = 2.07565994\n",
      "Iteration 1463, loss = 2.07671821\n",
      "Iteration 1464, loss = 2.07552543\n",
      "Iteration 1465, loss = 2.07510401\n",
      "Iteration 1466, loss = 2.07442467\n",
      "Iteration 1467, loss = 2.07534629\n",
      "Iteration 1468, loss = 2.07499955\n",
      "Iteration 1469, loss = 2.07326498\n",
      "Iteration 1470, loss = 2.07358881\n",
      "Iteration 1471, loss = 2.07263167\n",
      "Iteration 1472, loss = 2.07451193\n",
      "Iteration 1473, loss = 2.07291108\n",
      "Iteration 1474, loss = 2.07284423\n",
      "Iteration 1475, loss = 2.07168613\n",
      "Iteration 1476, loss = 2.07037936\n",
      "Iteration 1477, loss = 2.07043627\n",
      "Iteration 1478, loss = 2.07006396\n",
      "Iteration 1479, loss = 2.06835537\n",
      "Iteration 1480, loss = 2.06891169\n",
      "Iteration 1481, loss = 2.06940738\n",
      "Iteration 1482, loss = 2.06798972\n",
      "Iteration 1483, loss = 2.06943573\n",
      "Iteration 1484, loss = 2.06888012\n",
      "Iteration 1485, loss = 2.06620321\n",
      "Iteration 1486, loss = 2.06928088\n",
      "Iteration 1487, loss = 2.06697123\n",
      "Iteration 1488, loss = 2.06728069\n",
      "Iteration 1489, loss = 2.06678784\n",
      "Iteration 1490, loss = 2.06654018\n",
      "Iteration 1491, loss = 2.06556861\n",
      "Iteration 1492, loss = 2.06699403\n",
      "Iteration 1493, loss = 2.06741508\n",
      "Iteration 1494, loss = 2.06420146\n",
      "Iteration 1495, loss = 2.06461591\n",
      "Iteration 1496, loss = 2.06264414\n",
      "Iteration 1497, loss = 2.06244643\n",
      "Iteration 1498, loss = 2.06152781\n",
      "Iteration 1499, loss = 2.06143771\n",
      "Iteration 1500, loss = 2.06123725\n",
      "Iteration 1501, loss = 2.06035071\n",
      "Iteration 1502, loss = 2.06177961\n",
      "Iteration 1503, loss = 2.06033949\n",
      "Iteration 1504, loss = 2.06195139\n",
      "Iteration 1505, loss = 2.05945878\n",
      "Iteration 1506, loss = 2.06243051\n",
      "Iteration 1507, loss = 2.06125158\n",
      "Iteration 1508, loss = 2.05875823\n",
      "Iteration 1509, loss = 2.05895819\n",
      "Iteration 1510, loss = 2.05843227\n",
      "Iteration 1511, loss = 2.05596110\n",
      "Iteration 1512, loss = 2.05625471\n",
      "Iteration 1513, loss = 2.05700909\n",
      "Iteration 1514, loss = 2.05585024\n",
      "Iteration 1515, loss = 2.05792930\n",
      "Iteration 1516, loss = 2.05764858\n",
      "Iteration 1517, loss = 2.05668564\n",
      "Iteration 1518, loss = 2.05752231\n",
      "Iteration 1519, loss = 2.05522918\n",
      "Iteration 1520, loss = 2.05717176\n",
      "Iteration 1521, loss = 2.05423080\n",
      "Iteration 1522, loss = 2.05300280\n",
      "Iteration 1523, loss = 2.05537404\n",
      "Iteration 1524, loss = 2.05599004\n",
      "Iteration 1525, loss = 2.05356124\n",
      "Iteration 1526, loss = 2.05288991\n",
      "Iteration 1527, loss = 2.05268469\n",
      "Iteration 1528, loss = 2.05154395\n",
      "Iteration 1529, loss = 2.05358787\n",
      "Iteration 1530, loss = 2.05104592\n",
      "Iteration 1531, loss = 2.04997329\n",
      "Iteration 1532, loss = 2.04964876\n",
      "Iteration 1533, loss = 2.05129300\n",
      "Iteration 1534, loss = 2.05094694\n",
      "Iteration 1535, loss = 2.05000125\n",
      "Iteration 1536, loss = 2.05061045\n",
      "Iteration 1537, loss = 2.04900013\n",
      "Iteration 1538, loss = 2.04764317\n",
      "Iteration 1539, loss = 2.04695917\n",
      "Iteration 1540, loss = 2.04734930\n",
      "Iteration 1541, loss = 2.04843851\n",
      "Iteration 1542, loss = 2.04611589\n",
      "Iteration 1543, loss = 2.04699631\n",
      "Iteration 1544, loss = 2.04601324\n",
      "Iteration 1545, loss = 2.04565560\n",
      "Iteration 1546, loss = 2.04462957\n",
      "Iteration 1547, loss = 2.04599941\n",
      "Iteration 1548, loss = 2.04547420\n",
      "Iteration 1549, loss = 2.04372207\n",
      "Iteration 1550, loss = 2.04361228\n",
      "Iteration 1551, loss = 2.04409486\n",
      "Iteration 1552, loss = 2.04417028\n",
      "Iteration 1553, loss = 2.04233641\n",
      "Iteration 1554, loss = 2.04305473\n",
      "Iteration 1555, loss = 2.04240810\n",
      "Iteration 1556, loss = 2.04166572\n",
      "Iteration 1557, loss = 2.04282072\n",
      "Iteration 1558, loss = 2.04085398\n",
      "Iteration 1559, loss = 2.04063372\n",
      "Iteration 1560, loss = 2.04130843\n",
      "Iteration 1561, loss = 2.04001928\n",
      "Iteration 1562, loss = 2.03900153\n",
      "Iteration 1563, loss = 2.03922528\n",
      "Iteration 1564, loss = 2.03905405\n",
      "Iteration 1565, loss = 2.03861807\n",
      "Iteration 1566, loss = 2.03825688\n",
      "Iteration 1567, loss = 2.03772682\n",
      "Iteration 1568, loss = 2.03750139\n",
      "Iteration 1569, loss = 2.03620781\n",
      "Iteration 1570, loss = 2.03754161\n",
      "Iteration 1571, loss = 2.03877142\n",
      "Iteration 1572, loss = 2.03728158\n",
      "Iteration 1573, loss = 2.03588135\n",
      "Iteration 1574, loss = 2.03569698\n",
      "Iteration 1575, loss = 2.03661718\n",
      "Iteration 1576, loss = 2.03622049\n",
      "Iteration 1577, loss = 2.03539291\n",
      "Iteration 1578, loss = 2.03550324\n",
      "Iteration 1579, loss = 2.03437771\n",
      "Iteration 1580, loss = 2.03235821\n",
      "Iteration 1581, loss = 2.03266812\n",
      "Iteration 1582, loss = 2.03459073\n",
      "Iteration 1583, loss = 2.03220365\n",
      "Iteration 1584, loss = 2.03111015\n",
      "Iteration 1585, loss = 2.03096788\n",
      "Iteration 1586, loss = 2.03066769\n",
      "Iteration 1587, loss = 2.03106956\n",
      "Iteration 1588, loss = 2.03207700\n",
      "Iteration 1589, loss = 2.02936267\n",
      "Iteration 1590, loss = 2.02962783\n",
      "Iteration 1591, loss = 2.03066306\n",
      "Iteration 1592, loss = 2.03040118\n",
      "Iteration 1593, loss = 2.02968697\n",
      "Iteration 1594, loss = 2.03132983\n",
      "Iteration 1595, loss = 2.02694903\n",
      "Iteration 1596, loss = 2.02849630\n",
      "Iteration 1597, loss = 2.02811080\n",
      "Iteration 1598, loss = 2.02785312\n",
      "Iteration 1599, loss = 2.02647235\n",
      "Iteration 1600, loss = 2.02512416\n",
      "Iteration 1601, loss = 2.02525880\n",
      "Iteration 1602, loss = 2.02684659\n",
      "Iteration 1603, loss = 2.02582166\n",
      "Iteration 1604, loss = 2.02349568\n",
      "Iteration 1605, loss = 2.02732382\n",
      "Iteration 1606, loss = 2.02298068\n",
      "Iteration 1607, loss = 2.02617608\n",
      "Iteration 1608, loss = 2.02477094\n",
      "Iteration 1609, loss = 2.02404314\n",
      "Iteration 1610, loss = 2.02344133\n",
      "Iteration 1611, loss = 2.02307653\n",
      "Iteration 1612, loss = 2.02217475\n",
      "Iteration 1613, loss = 2.02138817\n",
      "Iteration 1614, loss = 2.02253599\n",
      "Iteration 1615, loss = 2.02220761\n",
      "Iteration 1616, loss = 2.02355777\n",
      "Iteration 1617, loss = 2.02106890\n",
      "Iteration 1618, loss = 2.02058190\n",
      "Iteration 1619, loss = 2.02018183\n",
      "Iteration 1620, loss = 2.02081427\n",
      "Iteration 1621, loss = 2.01865343\n",
      "Iteration 1622, loss = 2.01837216\n",
      "Iteration 1623, loss = 2.01877369\n",
      "Iteration 1624, loss = 2.01842986\n",
      "Iteration 1625, loss = 2.01964854\n",
      "Iteration 1626, loss = 2.01856764\n",
      "Iteration 1627, loss = 2.01780182\n",
      "Iteration 1628, loss = 2.01827837\n",
      "Iteration 1629, loss = 2.01923508\n",
      "Iteration 1630, loss = 2.01823438\n",
      "Iteration 1631, loss = 2.01632847\n",
      "Iteration 1632, loss = 2.01514801\n",
      "Iteration 1633, loss = 2.01559489\n",
      "Iteration 1634, loss = 2.01643273\n",
      "Iteration 1635, loss = 2.01347496\n",
      "Iteration 1636, loss = 2.01538359\n",
      "Iteration 1637, loss = 2.01569248\n",
      "Iteration 1638, loss = 2.01524351\n",
      "Iteration 1639, loss = 2.01456213\n",
      "Iteration 1640, loss = 2.01393096\n",
      "Iteration 1641, loss = 2.01267450\n",
      "Iteration 1642, loss = 2.01345051\n",
      "Iteration 1643, loss = 2.01337661\n",
      "Iteration 1644, loss = 2.01137015\n",
      "Iteration 1645, loss = 2.01240725\n",
      "Iteration 1646, loss = 2.01291307\n",
      "Iteration 1647, loss = 2.01133620\n",
      "Iteration 1648, loss = 2.01057925\n",
      "Iteration 1649, loss = 2.00899611\n",
      "Iteration 1650, loss = 2.01087911\n",
      "Iteration 1651, loss = 2.00867868\n",
      "Iteration 1652, loss = 2.00933186\n",
      "Iteration 1653, loss = 2.00933976\n",
      "Iteration 1654, loss = 2.00843168\n",
      "Iteration 1655, loss = 2.00873919\n",
      "Iteration 1656, loss = 2.00902044\n",
      "Iteration 1657, loss = 2.00791961\n",
      "Iteration 1658, loss = 2.00754683\n",
      "Iteration 1659, loss = 2.00708183\n",
      "Iteration 1660, loss = 2.00634129\n",
      "Iteration 1661, loss = 2.00532569\n",
      "Iteration 1662, loss = 2.00615034\n",
      "Iteration 1663, loss = 2.00438432\n",
      "Iteration 1664, loss = 2.00339659\n",
      "Iteration 1665, loss = 2.00476514\n",
      "Iteration 1666, loss = 2.00528360\n",
      "Iteration 1667, loss = 2.00549464\n",
      "Iteration 1668, loss = 2.00500739\n",
      "Iteration 1669, loss = 2.00331982\n",
      "Iteration 1670, loss = 2.00424085\n",
      "Iteration 1671, loss = 2.00507207\n",
      "Iteration 1672, loss = 2.00414655\n",
      "Iteration 1673, loss = 2.00161222\n",
      "Iteration 1674, loss = 2.00176600\n",
      "Iteration 1675, loss = 2.00117722\n",
      "Iteration 1676, loss = 2.00259956\n",
      "Iteration 1677, loss = 2.00024418\n",
      "Iteration 1678, loss = 2.00113862\n",
      "Iteration 1679, loss = 2.00083573\n",
      "Iteration 1680, loss = 1.99997652\n",
      "Iteration 1681, loss = 1.99887495\n",
      "Iteration 1682, loss = 1.99950000\n",
      "Iteration 1683, loss = 1.99854849\n",
      "Iteration 1684, loss = 1.99865752\n",
      "Iteration 1685, loss = 1.99828131\n",
      "Iteration 1686, loss = 1.99741761\n",
      "Iteration 1687, loss = 1.99631050\n",
      "Iteration 1688, loss = 1.99622228\n",
      "Iteration 1689, loss = 1.99726694\n",
      "Iteration 1690, loss = 1.99780826\n",
      "Iteration 1691, loss = 1.99564191\n",
      "Iteration 1692, loss = 1.99737296\n",
      "Iteration 1693, loss = 1.99496921\n",
      "Iteration 1694, loss = 1.99669598\n",
      "Iteration 1695, loss = 1.99550760\n",
      "Iteration 1696, loss = 1.99562499\n",
      "Iteration 1697, loss = 1.99801804\n",
      "Iteration 1698, loss = 1.99389652\n",
      "Iteration 1699, loss = 1.99411650\n",
      "Iteration 1700, loss = 1.99435534\n",
      "Iteration 1701, loss = 1.99436936\n",
      "Iteration 1702, loss = 1.99280706\n",
      "Iteration 1703, loss = 1.99253861\n",
      "Iteration 1704, loss = 1.99201797\n",
      "Iteration 1705, loss = 1.99150369\n",
      "Iteration 1706, loss = 1.99113273\n",
      "Iteration 1707, loss = 1.99041936\n",
      "Iteration 1708, loss = 1.99029392\n",
      "Iteration 1709, loss = 1.98994214\n",
      "Iteration 1710, loss = 1.98958714\n",
      "Iteration 1711, loss = 1.99222143\n",
      "Iteration 1712, loss = 1.99075272\n",
      "Iteration 1713, loss = 1.99028697\n",
      "Iteration 1714, loss = 1.98709937\n",
      "Iteration 1715, loss = 1.98692496\n",
      "Iteration 1716, loss = 1.98686732\n",
      "Iteration 1717, loss = 1.98901006\n",
      "Iteration 1718, loss = 1.98808350\n",
      "Iteration 1719, loss = 1.98719773\n",
      "Iteration 1720, loss = 1.98747404\n",
      "Iteration 1721, loss = 1.98677560\n",
      "Iteration 1722, loss = 1.98705828\n",
      "Iteration 1723, loss = 1.98600846\n",
      "Iteration 1724, loss = 1.98660028\n",
      "Iteration 1725, loss = 1.98663048\n",
      "Iteration 1726, loss = 1.98478264\n",
      "Iteration 1727, loss = 1.98367134\n",
      "Iteration 1728, loss = 1.98404768\n",
      "Iteration 1729, loss = 1.98380041\n",
      "Iteration 1730, loss = 1.98426482\n",
      "Iteration 1731, loss = 1.98362930\n",
      "Iteration 1732, loss = 1.98206482\n",
      "Iteration 1733, loss = 1.98275715\n",
      "Iteration 1734, loss = 1.98545554\n",
      "Iteration 1735, loss = 1.98473551\n",
      "Iteration 1736, loss = 1.98142497\n",
      "Iteration 1737, loss = 1.98198929\n",
      "Iteration 1738, loss = 1.98137025\n",
      "Iteration 1739, loss = 1.98055793\n",
      "Iteration 1740, loss = 1.98260660\n",
      "Iteration 1741, loss = 1.98122320\n",
      "Iteration 1742, loss = 1.98055624\n",
      "Iteration 1743, loss = 1.97808844\n",
      "Iteration 1744, loss = 1.97982911\n",
      "Iteration 1745, loss = 1.97965756\n",
      "Iteration 1746, loss = 1.97985336\n",
      "Iteration 1747, loss = 1.97793087\n",
      "Iteration 1748, loss = 1.97839663\n",
      "Iteration 1749, loss = 1.97728293\n",
      "Iteration 1750, loss = 1.97652025\n",
      "Iteration 1751, loss = 1.97721659\n",
      "Iteration 1752, loss = 1.97866403\n",
      "Iteration 1753, loss = 1.97536110\n",
      "Iteration 1754, loss = 1.97700000\n",
      "Iteration 1755, loss = 1.97682192\n",
      "Iteration 1756, loss = 1.97559215\n",
      "Iteration 1757, loss = 1.97826704\n",
      "Iteration 1758, loss = 1.97498144\n",
      "Iteration 1759, loss = 1.97669286\n",
      "Iteration 1760, loss = 1.97883228\n",
      "Iteration 1761, loss = 1.97557622\n",
      "Iteration 1762, loss = 1.97760211\n",
      "Iteration 1763, loss = 1.97658370\n",
      "Iteration 1764, loss = 1.97314567\n",
      "Iteration 1765, loss = 1.97253536\n",
      "Iteration 1766, loss = 1.97178833\n",
      "Iteration 1767, loss = 1.97154642\n",
      "Iteration 1768, loss = 1.97235582\n",
      "Iteration 1769, loss = 1.97153622\n",
      "Iteration 1770, loss = 1.97053683\n",
      "Iteration 1771, loss = 1.96995124\n",
      "Iteration 1772, loss = 1.97094324\n",
      "Iteration 1773, loss = 1.96987707\n",
      "Iteration 1774, loss = 1.96925687\n",
      "Iteration 1775, loss = 1.96865560\n",
      "Iteration 1776, loss = 1.96946346\n",
      "Iteration 1777, loss = 1.96736008\n",
      "Iteration 1778, loss = 1.96748845\n",
      "Iteration 1779, loss = 1.96681087\n",
      "Iteration 1780, loss = 1.96769482\n",
      "Iteration 1781, loss = 1.96868431\n",
      "Iteration 1782, loss = 1.96853034\n",
      "Iteration 1783, loss = 1.96801000\n",
      "Iteration 1784, loss = 1.96626050\n",
      "Iteration 1785, loss = 1.96596870\n",
      "Iteration 1786, loss = 1.96536755\n",
      "Iteration 1787, loss = 1.96468208\n",
      "Iteration 1788, loss = 1.96701275\n",
      "Iteration 1789, loss = 1.96592984\n",
      "Iteration 1790, loss = 1.96442867\n",
      "Iteration 1791, loss = 1.96395038\n",
      "Iteration 1792, loss = 1.96312369\n",
      "Iteration 1793, loss = 1.96289316\n",
      "Iteration 1794, loss = 1.96112925\n",
      "Iteration 1795, loss = 1.96190119\n",
      "Iteration 1796, loss = 1.96282181\n",
      "Iteration 1797, loss = 1.96358194\n",
      "Iteration 1798, loss = 1.96247285\n",
      "Iteration 1799, loss = 1.96329358\n",
      "Iteration 1800, loss = 1.96178892\n",
      "Iteration 1801, loss = 1.96337144\n",
      "Iteration 1802, loss = 1.96287467\n",
      "Iteration 1803, loss = 1.96040045\n",
      "Iteration 1804, loss = 1.96091716\n",
      "Iteration 1805, loss = 1.95929696\n",
      "Iteration 1806, loss = 1.95910632\n",
      "Iteration 1807, loss = 1.95834005\n",
      "Iteration 1808, loss = 1.95990010\n",
      "Iteration 1809, loss = 1.95897594\n",
      "Iteration 1810, loss = 1.95814652\n",
      "Iteration 1811, loss = 1.95865232\n",
      "Iteration 1812, loss = 1.95726182\n",
      "Iteration 1813, loss = 1.95865066\n",
      "Iteration 1814, loss = 1.95738070\n",
      "Iteration 1815, loss = 1.95860673\n",
      "Iteration 1816, loss = 1.95551994\n",
      "Iteration 1817, loss = 1.95529407\n",
      "Iteration 1818, loss = 1.95985218\n",
      "Iteration 1819, loss = 1.95760946\n",
      "Iteration 1820, loss = 1.95621556\n",
      "Iteration 1821, loss = 1.95439745\n",
      "Iteration 1822, loss = 1.95525425\n",
      "Iteration 1823, loss = 1.95474132\n",
      "Iteration 1824, loss = 1.95477420\n",
      "Iteration 1825, loss = 1.95436749\n",
      "Iteration 1826, loss = 1.95363136\n",
      "Iteration 1827, loss = 1.95367766\n",
      "Iteration 1828, loss = 1.95477119\n",
      "Iteration 1829, loss = 1.95157599\n",
      "Iteration 1830, loss = 1.95402621\n",
      "Iteration 1831, loss = 1.95324563\n",
      "Iteration 1832, loss = 1.95314724\n",
      "Iteration 1833, loss = 1.95440335\n",
      "Iteration 1834, loss = 1.95187038\n",
      "Iteration 1835, loss = 1.95214922\n",
      "Iteration 1836, loss = 1.95291177\n",
      "Iteration 1837, loss = 1.95046043\n",
      "Iteration 1838, loss = 1.95024176\n",
      "Iteration 1839, loss = 1.95053976\n",
      "Iteration 1840, loss = 1.95290638\n",
      "Iteration 1841, loss = 1.94901573\n",
      "Iteration 1842, loss = 1.94820383\n",
      "Iteration 1843, loss = 1.94753165\n",
      "Iteration 1844, loss = 1.94805344\n",
      "Iteration 1845, loss = 1.94767769\n",
      "Iteration 1846, loss = 1.94929698\n",
      "Iteration 1847, loss = 1.94712621\n",
      "Iteration 1848, loss = 1.94975240\n",
      "Iteration 1849, loss = 1.94704195\n",
      "Iteration 1850, loss = 1.94657080\n",
      "Iteration 1851, loss = 1.94700072\n",
      "Iteration 1852, loss = 1.94544162\n",
      "Iteration 1853, loss = 1.94608315\n",
      "Iteration 1854, loss = 1.94761774\n",
      "Iteration 1855, loss = 1.94737195\n",
      "Iteration 1856, loss = 1.94373690\n",
      "Iteration 1857, loss = 1.94526073\n",
      "Iteration 1858, loss = 1.94478611\n",
      "Iteration 1859, loss = 1.94349757\n",
      "Iteration 1860, loss = 1.94271436\n",
      "Iteration 1861, loss = 1.94232716\n",
      "Iteration 1862, loss = 1.94298818\n",
      "Iteration 1863, loss = 1.94353444\n",
      "Iteration 1864, loss = 1.94124287\n",
      "Iteration 1865, loss = 1.94177470\n",
      "Iteration 1866, loss = 1.94323001\n",
      "Iteration 1867, loss = 1.94210236\n",
      "Iteration 1868, loss = 1.94310337\n",
      "Iteration 1869, loss = 1.94286805\n",
      "Iteration 1870, loss = 1.94397676\n",
      "Iteration 1871, loss = 1.94292154\n",
      "Iteration 1872, loss = 1.94610075\n",
      "Iteration 1873, loss = 1.94186711\n",
      "Iteration 1874, loss = 1.94254591\n",
      "Iteration 1875, loss = 1.93971033\n",
      "Iteration 1876, loss = 1.94067742\n",
      "Iteration 1877, loss = 1.93965575\n",
      "Iteration 1878, loss = 1.93994700\n",
      "Iteration 1879, loss = 1.93852845\n",
      "Iteration 1880, loss = 1.93862826\n",
      "Iteration 1881, loss = 1.93771372\n",
      "Iteration 1882, loss = 1.93796712\n",
      "Iteration 1883, loss = 1.93992586\n",
      "Iteration 1884, loss = 1.93781376\n",
      "Iteration 1885, loss = 1.93700803\n",
      "Iteration 1886, loss = 1.93804163\n",
      "Iteration 1887, loss = 1.93703023\n",
      "Iteration 1888, loss = 1.93549882\n",
      "Iteration 1889, loss = 1.93579818\n",
      "Iteration 1890, loss = 1.93618003\n",
      "Iteration 1891, loss = 1.93379486\n",
      "Iteration 1892, loss = 1.93456835\n",
      "Iteration 1893, loss = 1.93499117\n",
      "Iteration 1894, loss = 1.93417885\n",
      "Iteration 1895, loss = 1.93380948\n",
      "Iteration 1896, loss = 1.93524148\n",
      "Iteration 1897, loss = 1.93233788\n",
      "Iteration 1898, loss = 1.93184012\n",
      "Iteration 1899, loss = 1.93303717\n",
      "Iteration 1900, loss = 1.93179076\n",
      "Iteration 1901, loss = 1.93209913\n",
      "Iteration 1902, loss = 1.92997804\n",
      "Iteration 1903, loss = 1.93182152\n",
      "Iteration 1904, loss = 1.93228773\n",
      "Iteration 1905, loss = 1.93343299\n",
      "Iteration 1906, loss = 1.93243852\n",
      "Iteration 1907, loss = 1.93055937\n",
      "Iteration 1908, loss = 1.93355924\n",
      "Iteration 1909, loss = 1.93201179\n",
      "Iteration 1910, loss = 1.92972822\n",
      "Iteration 1911, loss = 1.92911023\n",
      "Iteration 1912, loss = 1.92802944\n",
      "Iteration 1913, loss = 1.92792929\n",
      "Iteration 1914, loss = 1.92714257\n",
      "Iteration 1915, loss = 1.92782385\n",
      "Iteration 1916, loss = 1.92810393\n",
      "Iteration 1917, loss = 1.92995405\n",
      "Iteration 1918, loss = 1.93132858\n",
      "Iteration 1919, loss = 1.93084532\n",
      "Iteration 1920, loss = 1.92795677\n",
      "Iteration 1921, loss = 1.92905564\n",
      "Iteration 1922, loss = 1.92644917\n",
      "Iteration 1923, loss = 1.92653127\n",
      "Iteration 1924, loss = 1.92688670\n",
      "Iteration 1925, loss = 1.92498755\n",
      "Iteration 1926, loss = 1.92438418\n",
      "Iteration 1927, loss = 1.92701259\n",
      "Iteration 1928, loss = 1.92606801\n",
      "Iteration 1929, loss = 1.92501552\n",
      "Iteration 1930, loss = 1.92249877\n",
      "Iteration 1931, loss = 1.92357026\n",
      "Iteration 1932, loss = 1.92299771\n",
      "Iteration 1933, loss = 1.92246722\n",
      "Iteration 1934, loss = 1.92231057\n",
      "Iteration 1935, loss = 1.92147221\n",
      "Iteration 1936, loss = 1.92270173\n",
      "Iteration 1937, loss = 1.92147606\n",
      "Iteration 1938, loss = 1.92228177\n",
      "Iteration 1939, loss = 1.92240998\n",
      "Iteration 1940, loss = 1.92165587\n",
      "Iteration 1941, loss = 1.92115665\n",
      "Iteration 1942, loss = 1.92034896\n",
      "Iteration 1943, loss = 1.91983881\n",
      "Iteration 1944, loss = 1.92222578\n",
      "Iteration 1945, loss = 1.92021369\n",
      "Iteration 1946, loss = 1.92022354\n",
      "Iteration 1947, loss = 1.91883594\n",
      "Iteration 1948, loss = 1.91968834\n",
      "Iteration 1949, loss = 1.91800080\n",
      "Iteration 1950, loss = 1.91826975\n",
      "Iteration 1951, loss = 1.91698311\n",
      "Iteration 1952, loss = 1.91745915\n",
      "Iteration 1953, loss = 1.91812790\n",
      "Iteration 1954, loss = 1.91715598\n",
      "Iteration 1955, loss = 1.91647842\n",
      "Iteration 1956, loss = 1.91622373\n",
      "Iteration 1957, loss = 1.91529273\n",
      "Iteration 1958, loss = 1.91708519\n",
      "Iteration 1959, loss = 1.91614036\n",
      "Iteration 1960, loss = 1.91573259\n",
      "Iteration 1961, loss = 1.91481618\n",
      "Iteration 1962, loss = 1.91437575\n",
      "Iteration 1963, loss = 1.91579446\n",
      "Iteration 1964, loss = 1.91509523\n",
      "Iteration 1965, loss = 1.91467435\n",
      "Iteration 1966, loss = 1.91319989\n",
      "Iteration 1967, loss = 1.91342575\n",
      "Iteration 1968, loss = 1.91428218\n",
      "Iteration 1969, loss = 1.91212402\n",
      "Iteration 1970, loss = 1.91306809\n",
      "Iteration 1971, loss = 1.91387425\n",
      "Iteration 1972, loss = 1.91224179\n",
      "Iteration 1973, loss = 1.91186028\n",
      "Iteration 1974, loss = 1.91268366\n",
      "Iteration 1975, loss = 1.91192222\n",
      "Iteration 1976, loss = 1.91151684\n",
      "Iteration 1977, loss = 1.91223353\n",
      "Iteration 1978, loss = 1.91060808\n",
      "Iteration 1979, loss = 1.90946358\n",
      "Iteration 1980, loss = 1.91018777\n",
      "Iteration 1981, loss = 1.91069530\n",
      "Iteration 1982, loss = 1.91135099\n",
      "Iteration 1983, loss = 1.91076521\n",
      "Iteration 1984, loss = 1.91076451\n",
      "Iteration 1985, loss = 1.91046410\n",
      "Iteration 1986, loss = 1.90916741\n",
      "Iteration 1987, loss = 1.90859342\n",
      "Iteration 1988, loss = 1.90874518\n",
      "Iteration 1989, loss = 1.90816902\n",
      "Iteration 1990, loss = 1.90729411\n",
      "Iteration 1991, loss = 1.90672933\n",
      "Iteration 1992, loss = 1.90724030\n",
      "Iteration 1993, loss = 1.90822697\n",
      "Iteration 1994, loss = 1.90701787\n",
      "Iteration 1995, loss = 1.90669849\n",
      "Iteration 1996, loss = 1.90571302\n",
      "Iteration 1997, loss = 1.90611011\n",
      "Iteration 1998, loss = 1.90666524\n",
      "Iteration 1999, loss = 1.90501466\n",
      "Iteration 2000, loss = 1.90415547\n",
      "Iteration 2001, loss = 1.90474400\n",
      "Iteration 2002, loss = 1.90507523\n",
      "Iteration 2003, loss = 1.90404942\n",
      "Iteration 2004, loss = 1.90343172\n",
      "Iteration 2005, loss = 1.90229792\n",
      "Iteration 2006, loss = 1.90227303\n",
      "Iteration 2007, loss = 1.90239448\n",
      "Iteration 2008, loss = 1.90374789\n",
      "Iteration 2009, loss = 1.90271223\n",
      "Iteration 2010, loss = 1.90169175\n",
      "Iteration 2011, loss = 1.90402093\n",
      "Iteration 2012, loss = 1.89986499\n",
      "Iteration 2013, loss = 1.90314548\n",
      "Iteration 2014, loss = 1.90453837\n",
      "Iteration 2015, loss = 1.90339246\n",
      "Iteration 2016, loss = 1.90250088\n",
      "Iteration 2017, loss = 1.89984824\n",
      "Iteration 2018, loss = 1.89981033\n",
      "Iteration 2019, loss = 1.90084586\n",
      "Iteration 2020, loss = 1.89980224\n",
      "Iteration 2021, loss = 1.90159597\n",
      "Iteration 2022, loss = 1.89892866\n",
      "Iteration 2023, loss = 1.90014395\n",
      "Iteration 2024, loss = 1.89935585\n",
      "Iteration 2025, loss = 1.89773891\n",
      "Iteration 2026, loss = 1.89636277\n",
      "Iteration 2027, loss = 1.89689092\n",
      "Iteration 2028, loss = 1.89777125\n",
      "Iteration 2029, loss = 1.89859958\n",
      "Iteration 2030, loss = 1.89876848\n",
      "Iteration 2031, loss = 1.89746559\n",
      "Iteration 2032, loss = 1.89837586\n",
      "Iteration 2033, loss = 1.89710131\n",
      "Iteration 2034, loss = 1.89562228\n",
      "Iteration 2035, loss = 1.89494229\n",
      "Iteration 2036, loss = 1.89425251\n",
      "Iteration 2037, loss = 1.89428215\n",
      "Iteration 2038, loss = 1.89430505\n",
      "Iteration 2039, loss = 1.89338342\n",
      "Iteration 2040, loss = 1.89330272\n",
      "Iteration 2041, loss = 1.89264609\n",
      "Iteration 2042, loss = 1.89337241\n",
      "Iteration 2043, loss = 1.89467105\n",
      "Iteration 2044, loss = 1.89380441\n",
      "Iteration 2045, loss = 1.89286051\n",
      "Iteration 2046, loss = 1.89127260\n",
      "Iteration 2047, loss = 1.89200513\n",
      "Iteration 2048, loss = 1.89101829\n",
      "Iteration 2049, loss = 1.89041797\n",
      "Iteration 2050, loss = 1.89341002\n",
      "Iteration 2051, loss = 1.89212838\n",
      "Iteration 2052, loss = 1.89161505\n",
      "Iteration 2053, loss = 1.89030259\n",
      "Iteration 2054, loss = 1.89363975\n",
      "Iteration 2055, loss = 1.88835394\n",
      "Iteration 2056, loss = 1.89065367\n",
      "Iteration 2057, loss = 1.89001156\n",
      "Iteration 2058, loss = 1.88979149\n",
      "Iteration 2059, loss = 1.88805989\n",
      "Iteration 2060, loss = 1.89087439\n",
      "Iteration 2061, loss = 1.88756521\n",
      "Iteration 2062, loss = 1.88878393\n",
      "Iteration 2063, loss = 1.88782727\n",
      "Iteration 2064, loss = 1.88979223\n",
      "Iteration 2065, loss = 1.88729771\n",
      "Iteration 2066, loss = 1.88710089\n",
      "Iteration 2067, loss = 1.88832554\n",
      "Iteration 2068, loss = 1.88649596\n",
      "Iteration 2069, loss = 1.88591193\n",
      "Iteration 2070, loss = 1.88550731\n",
      "Iteration 2071, loss = 1.88543038\n",
      "Iteration 2072, loss = 1.88497897\n",
      "Iteration 2073, loss = 1.88430676\n",
      "Iteration 2074, loss = 1.88478806\n",
      "Iteration 2075, loss = 1.88547237\n",
      "Iteration 2076, loss = 1.88496333\n",
      "Iteration 2077, loss = 1.88433315\n",
      "Iteration 2078, loss = 1.88231706\n",
      "Iteration 2079, loss = 1.88399880\n",
      "Iteration 2080, loss = 1.88312053\n",
      "Iteration 2081, loss = 1.88282292\n",
      "Iteration 2082, loss = 1.88285420\n",
      "Iteration 2083, loss = 1.88417218\n",
      "Iteration 2084, loss = 1.88358943\n",
      "Iteration 2085, loss = 1.88314240\n",
      "Iteration 2086, loss = 1.88261756\n",
      "Iteration 2087, loss = 1.88384286\n",
      "Iteration 2088, loss = 1.88172742\n",
      "Iteration 2089, loss = 1.88303452\n",
      "Iteration 2090, loss = 1.88203497\n",
      "Iteration 2091, loss = 1.87981197\n",
      "Iteration 2092, loss = 1.88033765\n",
      "Iteration 2093, loss = 1.88089057\n",
      "Iteration 2094, loss = 1.87976837\n",
      "Iteration 2095, loss = 1.87931480\n",
      "Iteration 2096, loss = 1.87880723\n",
      "Iteration 2097, loss = 1.87792480\n",
      "Iteration 2098, loss = 1.87912946\n",
      "Iteration 2099, loss = 1.87932529\n",
      "Iteration 2100, loss = 1.88027582\n",
      "Iteration 2101, loss = 1.87750507\n",
      "Iteration 2102, loss = 1.87977086\n",
      "Iteration 2103, loss = 1.87919817\n",
      "Iteration 2104, loss = 1.87977244\n",
      "Iteration 2105, loss = 1.87670210\n",
      "Iteration 2106, loss = 1.87726458\n",
      "Iteration 2107, loss = 1.87591833\n",
      "Iteration 2108, loss = 1.87704014\n",
      "Iteration 2109, loss = 1.87554441\n",
      "Iteration 2110, loss = 1.87586337\n",
      "Iteration 2111, loss = 1.87463105\n",
      "Iteration 2112, loss = 1.87802042\n",
      "Iteration 2113, loss = 1.87561226\n",
      "Iteration 2114, loss = 1.87599796\n",
      "Iteration 2115, loss = 1.87480092\n",
      "Iteration 2116, loss = 1.87461240\n",
      "Iteration 2117, loss = 1.87208022\n",
      "Iteration 2118, loss = 1.87239555\n",
      "Iteration 2119, loss = 1.87364017\n",
      "Iteration 2120, loss = 1.87486580\n",
      "Iteration 2121, loss = 1.87329333\n",
      "Iteration 2122, loss = 1.87335529\n",
      "Iteration 2123, loss = 1.87228919\n",
      "Iteration 2124, loss = 1.87179558\n",
      "Iteration 2125, loss = 1.87297103\n",
      "Iteration 2126, loss = 1.87089837\n",
      "Iteration 2127, loss = 1.87081706\n",
      "Iteration 2128, loss = 1.87087157\n",
      "Iteration 2129, loss = 1.87125972\n",
      "Iteration 2130, loss = 1.87047015\n",
      "Iteration 2131, loss = 1.87024820\n",
      "Iteration 2132, loss = 1.86979378\n",
      "Iteration 2133, loss = 1.86997881\n",
      "Iteration 2134, loss = 1.86971694\n",
      "Iteration 2135, loss = 1.86946044\n",
      "Iteration 2136, loss = 1.86848133\n",
      "Iteration 2137, loss = 1.87063529\n",
      "Iteration 2138, loss = 1.86872021\n",
      "Iteration 2139, loss = 1.86923722\n",
      "Iteration 2140, loss = 1.87088112\n",
      "Iteration 2141, loss = 1.86675615\n",
      "Iteration 2142, loss = 1.86885226\n",
      "Iteration 2143, loss = 1.86785866\n",
      "Iteration 2144, loss = 1.87021987\n",
      "Iteration 2145, loss = 1.86922173\n",
      "Iteration 2146, loss = 1.86740207\n",
      "Iteration 2147, loss = 1.86707611\n",
      "Iteration 2148, loss = 1.86756346\n",
      "Iteration 2149, loss = 1.86925387\n",
      "Iteration 2150, loss = 1.86715085\n",
      "Iteration 2151, loss = 1.86890313\n",
      "Iteration 2152, loss = 1.86804388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23405386\n",
      "Iteration 2, loss = 4.13603675\n",
      "Iteration 3, loss = 4.04391233\n",
      "Iteration 4, loss = 3.94814183\n",
      "Iteration 5, loss = 3.84946463\n",
      "Iteration 6, loss = 3.74386779\n",
      "Iteration 7, loss = 3.63339662\n",
      "Iteration 8, loss = 3.52754606\n",
      "Iteration 9, loss = 3.42724792\n",
      "Iteration 10, loss = 3.34679068\n",
      "Iteration 11, loss = 3.28255925\n",
      "Iteration 12, loss = 3.24865396\n",
      "Iteration 13, loss = 3.23351665\n",
      "Iteration 14, loss = 3.23023039\n",
      "Iteration 15, loss = 3.22810417\n",
      "Iteration 16, loss = 3.22596436\n",
      "Iteration 17, loss = 3.22288071\n",
      "Iteration 18, loss = 3.22196515\n",
      "Iteration 19, loss = 3.22086052\n",
      "Iteration 20, loss = 3.21891825\n",
      "Iteration 21, loss = 3.21772404\n",
      "Iteration 22, loss = 3.21675866\n",
      "Iteration 23, loss = 3.21565885\n",
      "Iteration 24, loss = 3.21394870\n",
      "Iteration 25, loss = 3.21268958\n",
      "Iteration 26, loss = 3.21145722\n",
      "Iteration 27, loss = 3.21077617\n",
      "Iteration 28, loss = 3.20949631\n",
      "Iteration 29, loss = 3.20748207\n",
      "Iteration 30, loss = 3.20600465\n",
      "Iteration 31, loss = 3.20513626\n",
      "Iteration 32, loss = 3.20434441\n",
      "Iteration 33, loss = 3.20292487\n",
      "Iteration 34, loss = 3.20185671\n",
      "Iteration 35, loss = 3.20056244\n",
      "Iteration 36, loss = 3.19871167\n",
      "Iteration 37, loss = 3.19778478\n",
      "Iteration 38, loss = 3.19605661\n",
      "Iteration 39, loss = 3.19488533\n",
      "Iteration 40, loss = 3.19377160\n",
      "Iteration 41, loss = 3.19301325\n",
      "Iteration 42, loss = 3.19237522\n",
      "Iteration 43, loss = 3.19060558\n",
      "Iteration 44, loss = 3.18840295\n",
      "Iteration 45, loss = 3.18674953\n",
      "Iteration 46, loss = 3.18498201\n",
      "Iteration 47, loss = 3.18311504\n",
      "Iteration 48, loss = 3.18354113\n",
      "Iteration 49, loss = 3.18197914\n",
      "Iteration 50, loss = 3.17977397\n",
      "Iteration 51, loss = 3.17884347\n",
      "Iteration 52, loss = 3.17715728\n",
      "Iteration 53, loss = 3.17577793\n",
      "Iteration 54, loss = 3.17366455\n",
      "Iteration 55, loss = 3.17240849\n",
      "Iteration 56, loss = 3.17017561\n",
      "Iteration 57, loss = 3.16967464\n",
      "Iteration 58, loss = 3.16787678\n",
      "Iteration 59, loss = 3.16657167\n",
      "Iteration 60, loss = 3.16620224\n",
      "Iteration 61, loss = 3.16524273\n",
      "Iteration 62, loss = 3.16394528\n",
      "Iteration 63, loss = 3.16211883\n",
      "Iteration 64, loss = 3.16036345\n",
      "Iteration 65, loss = 3.15804220\n",
      "Iteration 66, loss = 3.15715639\n",
      "Iteration 67, loss = 3.15627115\n",
      "Iteration 68, loss = 3.15437369\n",
      "Iteration 69, loss = 3.15297030\n",
      "Iteration 70, loss = 3.15237766\n",
      "Iteration 71, loss = 3.15015168\n",
      "Iteration 72, loss = 3.14787066\n",
      "Iteration 73, loss = 3.14700316\n",
      "Iteration 74, loss = 3.14562792\n",
      "Iteration 75, loss = 3.14459689\n",
      "Iteration 76, loss = 3.14398778\n",
      "Iteration 77, loss = 3.14211510\n",
      "Iteration 78, loss = 3.13971807\n",
      "Iteration 79, loss = 3.13952118\n",
      "Iteration 80, loss = 3.13797825\n",
      "Iteration 81, loss = 3.13567033\n",
      "Iteration 82, loss = 3.13439958\n",
      "Iteration 83, loss = 3.13342974\n",
      "Iteration 84, loss = 3.13195269\n",
      "Iteration 85, loss = 3.13030960\n",
      "Iteration 86, loss = 3.12857395\n",
      "Iteration 87, loss = 3.12807014\n",
      "Iteration 88, loss = 3.12722109\n",
      "Iteration 89, loss = 3.12452163\n",
      "Iteration 90, loss = 3.12280888\n",
      "Iteration 91, loss = 3.12117449\n",
      "Iteration 92, loss = 3.11885908\n",
      "Iteration 93, loss = 3.11741612\n",
      "Iteration 94, loss = 3.11570925\n",
      "Iteration 95, loss = 3.11499798\n",
      "Iteration 96, loss = 3.11301067\n",
      "Iteration 97, loss = 3.11122470\n",
      "Iteration 98, loss = 3.10999990\n",
      "Iteration 99, loss = 3.10898474\n",
      "Iteration 100, loss = 3.10696097\n",
      "Iteration 101, loss = 3.10560620\n",
      "Iteration 102, loss = 3.10405280\n",
      "Iteration 103, loss = 3.10235091\n",
      "Iteration 104, loss = 3.10111333\n",
      "Iteration 105, loss = 3.10076556\n",
      "Iteration 106, loss = 3.09931976\n",
      "Iteration 107, loss = 3.09691185\n",
      "Iteration 108, loss = 3.09454977\n",
      "Iteration 109, loss = 3.09305946\n",
      "Iteration 110, loss = 3.09148547\n",
      "Iteration 111, loss = 3.09073879\n",
      "Iteration 112, loss = 3.08810897\n",
      "Iteration 113, loss = 3.08742736\n",
      "Iteration 114, loss = 3.08726774\n",
      "Iteration 115, loss = 3.08684785\n",
      "Iteration 116, loss = 3.08424821\n",
      "Iteration 117, loss = 3.08294030\n",
      "Iteration 118, loss = 3.08136001\n",
      "Iteration 119, loss = 3.07981306\n",
      "Iteration 120, loss = 3.08048437\n",
      "Iteration 121, loss = 3.07766192\n",
      "Iteration 122, loss = 3.07525498\n",
      "Iteration 123, loss = 3.07258127\n",
      "Iteration 124, loss = 3.07159107\n",
      "Iteration 125, loss = 3.06949558\n",
      "Iteration 126, loss = 3.06971780\n",
      "Iteration 127, loss = 3.06834629\n",
      "Iteration 128, loss = 3.06601413\n",
      "Iteration 129, loss = 3.06408927\n",
      "Iteration 130, loss = 3.06339390\n",
      "Iteration 131, loss = 3.06109712\n",
      "Iteration 132, loss = 3.06036314\n",
      "Iteration 133, loss = 3.05791898\n",
      "Iteration 134, loss = 3.05588925\n",
      "Iteration 135, loss = 3.05632663\n",
      "Iteration 136, loss = 3.05390946\n",
      "Iteration 137, loss = 3.05224315\n",
      "Iteration 138, loss = 3.05066052\n",
      "Iteration 139, loss = 3.04980410\n",
      "Iteration 140, loss = 3.04816677\n",
      "Iteration 141, loss = 3.04685760\n",
      "Iteration 142, loss = 3.04424705\n",
      "Iteration 143, loss = 3.04323648\n",
      "Iteration 144, loss = 3.04191994\n",
      "Iteration 145, loss = 3.04006816\n",
      "Iteration 146, loss = 3.03894678\n",
      "Iteration 147, loss = 3.03655216\n",
      "Iteration 148, loss = 3.03496578\n",
      "Iteration 149, loss = 3.03373356\n",
      "Iteration 150, loss = 3.03333570\n",
      "Iteration 151, loss = 3.03115632\n",
      "Iteration 152, loss = 3.03064961\n",
      "Iteration 153, loss = 3.02804834\n",
      "Iteration 154, loss = 3.02855100\n",
      "Iteration 155, loss = 3.02671314\n",
      "Iteration 156, loss = 3.02378462\n",
      "Iteration 157, loss = 3.02263240\n",
      "Iteration 158, loss = 3.02116298\n",
      "Iteration 159, loss = 3.01959161\n",
      "Iteration 160, loss = 3.01807390\n",
      "Iteration 161, loss = 3.01653144\n",
      "Iteration 162, loss = 3.01518827\n",
      "Iteration 163, loss = 3.01483849\n",
      "Iteration 164, loss = 3.01425847\n",
      "Iteration 165, loss = 3.01131484\n",
      "Iteration 166, loss = 3.00918524\n",
      "Iteration 167, loss = 3.00880210\n",
      "Iteration 168, loss = 3.00721417\n",
      "Iteration 169, loss = 3.00534128\n",
      "Iteration 170, loss = 3.00482497\n",
      "Iteration 171, loss = 3.00254444\n",
      "Iteration 172, loss = 3.00426457\n",
      "Iteration 173, loss = 3.00025917\n",
      "Iteration 174, loss = 2.99858620\n",
      "Iteration 175, loss = 2.99918382\n",
      "Iteration 176, loss = 2.99782044\n",
      "Iteration 177, loss = 2.99438673\n",
      "Iteration 178, loss = 2.99246863\n",
      "Iteration 179, loss = 2.99221417\n",
      "Iteration 180, loss = 2.99151272\n",
      "Iteration 181, loss = 2.98894784\n",
      "Iteration 182, loss = 2.98806647\n",
      "Iteration 183, loss = 2.98654126\n",
      "Iteration 184, loss = 2.98512171\n",
      "Iteration 185, loss = 2.98444350\n",
      "Iteration 186, loss = 2.98243837\n",
      "Iteration 187, loss = 2.98059361\n",
      "Iteration 188, loss = 2.97859797\n",
      "Iteration 189, loss = 2.97759460\n",
      "Iteration 190, loss = 2.97655556\n",
      "Iteration 191, loss = 2.97639593\n",
      "Iteration 192, loss = 2.97311804\n",
      "Iteration 193, loss = 2.97114177\n",
      "Iteration 194, loss = 2.96949067\n",
      "Iteration 195, loss = 2.96886951\n",
      "Iteration 196, loss = 2.96694436\n",
      "Iteration 197, loss = 2.96582515\n",
      "Iteration 198, loss = 2.96470735\n",
      "Iteration 199, loss = 2.96296285\n",
      "Iteration 200, loss = 2.96111837\n",
      "Iteration 201, loss = 2.96042634\n",
      "Iteration 202, loss = 2.95909627\n",
      "Iteration 203, loss = 2.95817826\n",
      "Iteration 204, loss = 2.95765845\n",
      "Iteration 205, loss = 2.95576186\n",
      "Iteration 206, loss = 2.95408898\n",
      "Iteration 207, loss = 2.95337784\n",
      "Iteration 208, loss = 2.95136239\n",
      "Iteration 209, loss = 2.95026012\n",
      "Iteration 210, loss = 2.94893478\n",
      "Iteration 211, loss = 2.94687628\n",
      "Iteration 212, loss = 2.94460583\n",
      "Iteration 213, loss = 2.94450618\n",
      "Iteration 214, loss = 2.94368699\n",
      "Iteration 215, loss = 2.94211958\n",
      "Iteration 216, loss = 2.93957994\n",
      "Iteration 217, loss = 2.93970388\n",
      "Iteration 218, loss = 2.93856181\n",
      "Iteration 219, loss = 2.93720890\n",
      "Iteration 220, loss = 2.93463924\n",
      "Iteration 221, loss = 2.93298939\n",
      "Iteration 222, loss = 2.93257908\n",
      "Iteration 223, loss = 2.93070727\n",
      "Iteration 224, loss = 2.93008642\n",
      "Iteration 225, loss = 2.92869811\n",
      "Iteration 226, loss = 2.92678843\n",
      "Iteration 227, loss = 2.92580641\n",
      "Iteration 228, loss = 2.92421189\n",
      "Iteration 229, loss = 2.92304277\n",
      "Iteration 230, loss = 2.92080358\n",
      "Iteration 231, loss = 2.91978158\n",
      "Iteration 232, loss = 2.91898907\n",
      "Iteration 233, loss = 2.91716621\n",
      "Iteration 234, loss = 2.91633216\n",
      "Iteration 235, loss = 2.91565635\n",
      "Iteration 236, loss = 2.91375585\n",
      "Iteration 237, loss = 2.91242379\n",
      "Iteration 238, loss = 2.91173531\n",
      "Iteration 239, loss = 2.91051881\n",
      "Iteration 240, loss = 2.90818607\n",
      "Iteration 241, loss = 2.90791955\n",
      "Iteration 242, loss = 2.90676998\n",
      "Iteration 243, loss = 2.90675955\n",
      "Iteration 244, loss = 2.90324312\n",
      "Iteration 245, loss = 2.90266007\n",
      "Iteration 246, loss = 2.90134115\n",
      "Iteration 247, loss = 2.89915491\n",
      "Iteration 248, loss = 2.89746954\n",
      "Iteration 249, loss = 2.89670935\n",
      "Iteration 250, loss = 2.89615782\n",
      "Iteration 251, loss = 2.89580893\n",
      "Iteration 252, loss = 2.89413610\n",
      "Iteration 253, loss = 2.89217775\n",
      "Iteration 254, loss = 2.89120138\n",
      "Iteration 255, loss = 2.88969775\n",
      "Iteration 256, loss = 2.88827834\n",
      "Iteration 257, loss = 2.88648743\n",
      "Iteration 258, loss = 2.88541858\n",
      "Iteration 259, loss = 2.88460492\n",
      "Iteration 260, loss = 2.88390017\n",
      "Iteration 261, loss = 2.88201373\n",
      "Iteration 262, loss = 2.87990351\n",
      "Iteration 263, loss = 2.87913249\n",
      "Iteration 264, loss = 2.87838085\n",
      "Iteration 265, loss = 2.87690016\n",
      "Iteration 266, loss = 2.87565486\n",
      "Iteration 267, loss = 2.87452468\n",
      "Iteration 268, loss = 2.87485110\n",
      "Iteration 269, loss = 2.87263931\n",
      "Iteration 270, loss = 2.87030530\n",
      "Iteration 271, loss = 2.86921172\n",
      "Iteration 272, loss = 2.86862020\n",
      "Iteration 273, loss = 2.86718536\n",
      "Iteration 274, loss = 2.86539094\n",
      "Iteration 275, loss = 2.86418697\n",
      "Iteration 276, loss = 2.86326847\n",
      "Iteration 277, loss = 2.86222993\n",
      "Iteration 278, loss = 2.86040608\n",
      "Iteration 279, loss = 2.85959965\n",
      "Iteration 280, loss = 2.85908394\n",
      "Iteration 281, loss = 2.85771960\n",
      "Iteration 282, loss = 2.85585175\n",
      "Iteration 283, loss = 2.85446955\n",
      "Iteration 284, loss = 2.85360768\n",
      "Iteration 285, loss = 2.85209303\n",
      "Iteration 286, loss = 2.85166371\n",
      "Iteration 287, loss = 2.85127841\n",
      "Iteration 288, loss = 2.84961561\n",
      "Iteration 289, loss = 2.84845098\n",
      "Iteration 290, loss = 2.84619403\n",
      "Iteration 291, loss = 2.84574639\n",
      "Iteration 292, loss = 2.84430424\n",
      "Iteration 293, loss = 2.84251106\n",
      "Iteration 294, loss = 2.84164040\n",
      "Iteration 295, loss = 2.84111273\n",
      "Iteration 296, loss = 2.84002639\n",
      "Iteration 297, loss = 2.83817701\n",
      "Iteration 298, loss = 2.83706902\n",
      "Iteration 299, loss = 2.83479103\n",
      "Iteration 300, loss = 2.83466444\n",
      "Iteration 301, loss = 2.83372399\n",
      "Iteration 302, loss = 2.83294112\n",
      "Iteration 303, loss = 2.83148041\n",
      "Iteration 304, loss = 2.83046043\n",
      "Iteration 305, loss = 2.82854316\n",
      "Iteration 306, loss = 2.82818158\n",
      "Iteration 307, loss = 2.82717126\n",
      "Iteration 308, loss = 2.82624701\n",
      "Iteration 309, loss = 2.82444539\n",
      "Iteration 310, loss = 2.82295365\n",
      "Iteration 311, loss = 2.82154426\n",
      "Iteration 312, loss = 2.82080267\n",
      "Iteration 313, loss = 2.82050463\n",
      "Iteration 314, loss = 2.81784262\n",
      "Iteration 315, loss = 2.81687643\n",
      "Iteration 316, loss = 2.81660687\n",
      "Iteration 317, loss = 2.81536700\n",
      "Iteration 318, loss = 2.81334475\n",
      "Iteration 319, loss = 2.81168212\n",
      "Iteration 320, loss = 2.81113968\n",
      "Iteration 321, loss = 2.80922022\n",
      "Iteration 322, loss = 2.80845832\n",
      "Iteration 323, loss = 2.80747668\n",
      "Iteration 324, loss = 2.80647522\n",
      "Iteration 325, loss = 2.80632626\n",
      "Iteration 326, loss = 2.80458807\n",
      "Iteration 327, loss = 2.80202314\n",
      "Iteration 328, loss = 2.80201244\n",
      "Iteration 329, loss = 2.80133460\n",
      "Iteration 330, loss = 2.80108743\n",
      "Iteration 331, loss = 2.80023824\n",
      "Iteration 332, loss = 2.79775467\n",
      "Iteration 333, loss = 2.79858975\n",
      "Iteration 334, loss = 2.79672050\n",
      "Iteration 335, loss = 2.79391504\n",
      "Iteration 336, loss = 2.79356497\n",
      "Iteration 337, loss = 2.79235781\n",
      "Iteration 338, loss = 2.79198513\n",
      "Iteration 339, loss = 2.79050964\n",
      "Iteration 340, loss = 2.78982719\n",
      "Iteration 341, loss = 2.78865299\n",
      "Iteration 342, loss = 2.78697585\n",
      "Iteration 343, loss = 2.78606753\n",
      "Iteration 344, loss = 2.78526875\n",
      "Iteration 345, loss = 2.78458090\n",
      "Iteration 346, loss = 2.78299369\n",
      "Iteration 347, loss = 2.78221446\n",
      "Iteration 348, loss = 2.78046338\n",
      "Iteration 349, loss = 2.77887244\n",
      "Iteration 350, loss = 2.77759968\n",
      "Iteration 351, loss = 2.77641599\n",
      "Iteration 352, loss = 2.77645099\n",
      "Iteration 353, loss = 2.77511975\n",
      "Iteration 354, loss = 2.77344686\n",
      "Iteration 355, loss = 2.77382191\n",
      "Iteration 356, loss = 2.77291771\n",
      "Iteration 357, loss = 2.77067848\n",
      "Iteration 358, loss = 2.77074317\n",
      "Iteration 359, loss = 2.76790738\n",
      "Iteration 360, loss = 2.76789903\n",
      "Iteration 361, loss = 2.76721904\n",
      "Iteration 362, loss = 2.76542496\n",
      "Iteration 363, loss = 2.76423954\n",
      "Iteration 364, loss = 2.76418038\n",
      "Iteration 365, loss = 2.76407065\n",
      "Iteration 366, loss = 2.76275549\n",
      "Iteration 367, loss = 2.76025840\n",
      "Iteration 368, loss = 2.75893030\n",
      "Iteration 369, loss = 2.75852390\n",
      "Iteration 370, loss = 2.75768341\n",
      "Iteration 371, loss = 2.75580621\n",
      "Iteration 372, loss = 2.75546230\n",
      "Iteration 373, loss = 2.75395930\n",
      "Iteration 374, loss = 2.75312773\n",
      "Iteration 375, loss = 2.75207001\n",
      "Iteration 376, loss = 2.75103148\n",
      "Iteration 377, loss = 2.74955011\n",
      "Iteration 378, loss = 2.74805077\n",
      "Iteration 379, loss = 2.74661240\n",
      "Iteration 380, loss = 2.74638375\n",
      "Iteration 381, loss = 2.74630912\n",
      "Iteration 382, loss = 2.74535698\n",
      "Iteration 383, loss = 2.74534085\n",
      "Iteration 384, loss = 2.74505535\n",
      "Iteration 385, loss = 2.74240971\n",
      "Iteration 386, loss = 2.74155076\n",
      "Iteration 387, loss = 2.74007700\n",
      "Iteration 388, loss = 2.74112563\n",
      "Iteration 389, loss = 2.73886648\n",
      "Iteration 390, loss = 2.73783133\n",
      "Iteration 391, loss = 2.73504978\n",
      "Iteration 392, loss = 2.73437669\n",
      "Iteration 393, loss = 2.73609546\n",
      "Iteration 394, loss = 2.73466539\n",
      "Iteration 395, loss = 2.73231314\n",
      "Iteration 396, loss = 2.73128614\n",
      "Iteration 397, loss = 2.72908023\n",
      "Iteration 398, loss = 2.73024368\n",
      "Iteration 399, loss = 2.72864263\n",
      "Iteration 400, loss = 2.72773760\n",
      "Iteration 401, loss = 2.72449326\n",
      "Iteration 402, loss = 2.72780139\n",
      "Iteration 403, loss = 2.72656406\n",
      "Iteration 404, loss = 2.72465125\n",
      "Iteration 405, loss = 2.72423071\n",
      "Iteration 406, loss = 2.72174552\n",
      "Iteration 407, loss = 2.71995666\n",
      "Iteration 408, loss = 2.71974079\n",
      "Iteration 409, loss = 2.71794908\n",
      "Iteration 410, loss = 2.71792690\n",
      "Iteration 411, loss = 2.71667779\n",
      "Iteration 412, loss = 2.71500558\n",
      "Iteration 413, loss = 2.71372325\n",
      "Iteration 414, loss = 2.71257335\n",
      "Iteration 415, loss = 2.71224598\n",
      "Iteration 416, loss = 2.71071366\n",
      "Iteration 417, loss = 2.71020176\n",
      "Iteration 418, loss = 2.70947976\n",
      "Iteration 419, loss = 2.70863882\n",
      "Iteration 420, loss = 2.70735514\n",
      "Iteration 421, loss = 2.70655978\n",
      "Iteration 422, loss = 2.70500704\n",
      "Iteration 423, loss = 2.70421365\n",
      "Iteration 424, loss = 2.70380047\n",
      "Iteration 425, loss = 2.70222963\n",
      "Iteration 426, loss = 2.70127639\n",
      "Iteration 427, loss = 2.69962840\n",
      "Iteration 428, loss = 2.70095568\n",
      "Iteration 429, loss = 2.69990564\n",
      "Iteration 430, loss = 2.69803179\n",
      "Iteration 431, loss = 2.69778139\n",
      "Iteration 432, loss = 2.69651692\n",
      "Iteration 433, loss = 2.69533254\n",
      "Iteration 434, loss = 2.69329189\n",
      "Iteration 435, loss = 2.69154112\n",
      "Iteration 436, loss = 2.69241989\n",
      "Iteration 437, loss = 2.69124883\n",
      "Iteration 438, loss = 2.69077119\n",
      "Iteration 439, loss = 2.68879210\n",
      "Iteration 440, loss = 2.68770889\n",
      "Iteration 441, loss = 2.68738555\n",
      "Iteration 442, loss = 2.68673541\n",
      "Iteration 443, loss = 2.68517309\n",
      "Iteration 444, loss = 2.68271924\n",
      "Iteration 445, loss = 2.68501095\n",
      "Iteration 446, loss = 2.68256189\n",
      "Iteration 447, loss = 2.68063471\n",
      "Iteration 448, loss = 2.67978390\n",
      "Iteration 449, loss = 2.67950577\n",
      "Iteration 450, loss = 2.67842355\n",
      "Iteration 451, loss = 2.67703942\n",
      "Iteration 452, loss = 2.67610606\n",
      "Iteration 453, loss = 2.67492206\n",
      "Iteration 454, loss = 2.67387178\n",
      "Iteration 455, loss = 2.67298763\n",
      "Iteration 456, loss = 2.67249460\n",
      "Iteration 457, loss = 2.67206514\n",
      "Iteration 458, loss = 2.67133578\n",
      "Iteration 459, loss = 2.66997720\n",
      "Iteration 460, loss = 2.66901510\n",
      "Iteration 461, loss = 2.66808832\n",
      "Iteration 462, loss = 2.66699853\n",
      "Iteration 463, loss = 2.66597278\n",
      "Iteration 464, loss = 2.66494510\n",
      "Iteration 465, loss = 2.66419767\n",
      "Iteration 466, loss = 2.66310247\n",
      "Iteration 467, loss = 2.66201935\n",
      "Iteration 468, loss = 2.66146881\n",
      "Iteration 469, loss = 2.66145715\n",
      "Iteration 470, loss = 2.66087877\n",
      "Iteration 471, loss = 2.65981899\n",
      "Iteration 472, loss = 2.65800279\n",
      "Iteration 473, loss = 2.65648510\n",
      "Iteration 474, loss = 2.65588913\n",
      "Iteration 475, loss = 2.65515833\n",
      "Iteration 476, loss = 2.65443676\n",
      "Iteration 477, loss = 2.65287005\n",
      "Iteration 478, loss = 2.65170608\n",
      "Iteration 479, loss = 2.65047259\n",
      "Iteration 480, loss = 2.65049802\n",
      "Iteration 481, loss = 2.64891598\n",
      "Iteration 482, loss = 2.65088113\n",
      "Iteration 483, loss = 2.64874846\n",
      "Iteration 484, loss = 2.64947771\n",
      "Iteration 485, loss = 2.64658515\n",
      "Iteration 486, loss = 2.64535908\n",
      "Iteration 487, loss = 2.64446878\n",
      "Iteration 488, loss = 2.64302951\n",
      "Iteration 489, loss = 2.64198674\n",
      "Iteration 490, loss = 2.64186648\n",
      "Iteration 491, loss = 2.64179149\n",
      "Iteration 492, loss = 2.64075064\n",
      "Iteration 493, loss = 2.63926730\n",
      "Iteration 494, loss = 2.63691750\n",
      "Iteration 495, loss = 2.63650484\n",
      "Iteration 496, loss = 2.63613297\n",
      "Iteration 497, loss = 2.63516416\n",
      "Iteration 498, loss = 2.63458238\n",
      "Iteration 499, loss = 2.63509583\n",
      "Iteration 500, loss = 2.63341803\n",
      "Iteration 501, loss = 2.63299383\n",
      "Iteration 502, loss = 2.63125425\n",
      "Iteration 503, loss = 2.63005158\n",
      "Iteration 504, loss = 2.63000720\n",
      "Iteration 505, loss = 2.62820680\n",
      "Iteration 506, loss = 2.62664294\n",
      "Iteration 507, loss = 2.62546582\n",
      "Iteration 508, loss = 2.62496693\n",
      "Iteration 509, loss = 2.62462316\n",
      "Iteration 510, loss = 2.62419884\n",
      "Iteration 511, loss = 2.62178540\n",
      "Iteration 512, loss = 2.62114350\n",
      "Iteration 513, loss = 2.61926486\n",
      "Iteration 514, loss = 2.61889213\n",
      "Iteration 515, loss = 2.61862868\n",
      "Iteration 516, loss = 2.61877191\n",
      "Iteration 517, loss = 2.61769886\n",
      "Iteration 518, loss = 2.61586425\n",
      "Iteration 519, loss = 2.61685358\n",
      "Iteration 520, loss = 2.61660244\n",
      "Iteration 521, loss = 2.61569589\n",
      "Iteration 522, loss = 2.61357351\n",
      "Iteration 523, loss = 2.61175158\n",
      "Iteration 524, loss = 2.61091448\n",
      "Iteration 525, loss = 2.60934143\n",
      "Iteration 526, loss = 2.60853564\n",
      "Iteration 527, loss = 2.60979074\n",
      "Iteration 528, loss = 2.60787506\n",
      "Iteration 529, loss = 2.60828194\n",
      "Iteration 530, loss = 2.60706647\n",
      "Iteration 531, loss = 2.60494502\n",
      "Iteration 532, loss = 2.60446775\n",
      "Iteration 533, loss = 2.60354412\n",
      "Iteration 534, loss = 2.60312846\n",
      "Iteration 535, loss = 2.60264085\n",
      "Iteration 536, loss = 2.60189806\n",
      "Iteration 537, loss = 2.60005276\n",
      "Iteration 538, loss = 2.60116104\n",
      "Iteration 539, loss = 2.59910850\n",
      "Iteration 540, loss = 2.59772033\n",
      "Iteration 541, loss = 2.59755346\n",
      "Iteration 542, loss = 2.59659721\n",
      "Iteration 543, loss = 2.59682838\n",
      "Iteration 544, loss = 2.59592468\n",
      "Iteration 545, loss = 2.59395795\n",
      "Iteration 546, loss = 2.59267600\n",
      "Iteration 547, loss = 2.59116853\n",
      "Iteration 548, loss = 2.59311563\n",
      "Iteration 549, loss = 2.59264751\n",
      "Iteration 550, loss = 2.59055434\n",
      "Iteration 551, loss = 2.59109641\n",
      "Iteration 552, loss = 2.58963785\n",
      "Iteration 553, loss = 2.58664540\n",
      "Iteration 554, loss = 2.58520086\n",
      "Iteration 555, loss = 2.58527662\n",
      "Iteration 556, loss = 2.58399558\n",
      "Iteration 557, loss = 2.58423613\n",
      "Iteration 558, loss = 2.58304458\n",
      "Iteration 559, loss = 2.58150589\n",
      "Iteration 560, loss = 2.58248076\n",
      "Iteration 561, loss = 2.58003121\n",
      "Iteration 562, loss = 2.58098612\n",
      "Iteration 563, loss = 2.57781748\n",
      "Iteration 564, loss = 2.57796562\n",
      "Iteration 565, loss = 2.57710722\n",
      "Iteration 566, loss = 2.57565986\n",
      "Iteration 567, loss = 2.57646159\n",
      "Iteration 568, loss = 2.57530205\n",
      "Iteration 569, loss = 2.57406084\n",
      "Iteration 570, loss = 2.57300801\n",
      "Iteration 571, loss = 2.57251138\n",
      "Iteration 572, loss = 2.57127582\n",
      "Iteration 573, loss = 2.56924546\n",
      "Iteration 574, loss = 2.56851624\n",
      "Iteration 575, loss = 2.56747615\n",
      "Iteration 576, loss = 2.56731700\n",
      "Iteration 577, loss = 2.56622334\n",
      "Iteration 578, loss = 2.56657419\n",
      "Iteration 579, loss = 2.56978595\n",
      "Iteration 580, loss = 2.56606642\n",
      "Iteration 581, loss = 2.56254957\n",
      "Iteration 582, loss = 2.56293159\n",
      "Iteration 583, loss = 2.56239859\n",
      "Iteration 584, loss = 2.56055621\n",
      "Iteration 585, loss = 2.56043210\n",
      "Iteration 586, loss = 2.55936674\n",
      "Iteration 587, loss = 2.55813750\n",
      "Iteration 588, loss = 2.55711526\n",
      "Iteration 589, loss = 2.55664578\n",
      "Iteration 590, loss = 2.55514713\n",
      "Iteration 591, loss = 2.55444060\n",
      "Iteration 592, loss = 2.55404300\n",
      "Iteration 593, loss = 2.55540557\n",
      "Iteration 594, loss = 2.55450154\n",
      "Iteration 595, loss = 2.55298356\n",
      "Iteration 596, loss = 2.55135526\n",
      "Iteration 597, loss = 2.55049472\n",
      "Iteration 598, loss = 2.54930632\n",
      "Iteration 599, loss = 2.54929126\n",
      "Iteration 600, loss = 2.54853521\n",
      "Iteration 601, loss = 2.54821912\n",
      "Iteration 602, loss = 2.54766798\n",
      "Iteration 603, loss = 2.54598660\n",
      "Iteration 604, loss = 2.54513783\n",
      "Iteration 605, loss = 2.54456529\n",
      "Iteration 606, loss = 2.54298781\n",
      "Iteration 607, loss = 2.54165469\n",
      "Iteration 608, loss = 2.54025052\n",
      "Iteration 609, loss = 2.54084105\n",
      "Iteration 610, loss = 2.53923379\n",
      "Iteration 611, loss = 2.53931680\n",
      "Iteration 612, loss = 2.53774189\n",
      "Iteration 613, loss = 2.53655703\n",
      "Iteration 614, loss = 2.53661460\n",
      "Iteration 615, loss = 2.53576042\n",
      "Iteration 616, loss = 2.53496369\n",
      "Iteration 617, loss = 2.53375051\n",
      "Iteration 618, loss = 2.53324910\n",
      "Iteration 619, loss = 2.53416747\n",
      "Iteration 620, loss = 2.53174059\n",
      "Iteration 621, loss = 2.53066143\n",
      "Iteration 622, loss = 2.53031183\n",
      "Iteration 623, loss = 2.53112325\n",
      "Iteration 624, loss = 2.53007130\n",
      "Iteration 625, loss = 2.52744658\n",
      "Iteration 626, loss = 2.52817150\n",
      "Iteration 627, loss = 2.52690352\n",
      "Iteration 628, loss = 2.52662565\n",
      "Iteration 629, loss = 2.52678078\n",
      "Iteration 630, loss = 2.52534418\n",
      "Iteration 631, loss = 2.52430178\n",
      "Iteration 632, loss = 2.52275231\n",
      "Iteration 633, loss = 2.52159090\n",
      "Iteration 634, loss = 2.52176321\n",
      "Iteration 635, loss = 2.52007282\n",
      "Iteration 636, loss = 2.51954051\n",
      "Iteration 637, loss = 2.52060011\n",
      "Iteration 638, loss = 2.51977857\n",
      "Iteration 639, loss = 2.51706829\n",
      "Iteration 640, loss = 2.51695353\n",
      "Iteration 641, loss = 2.51623797\n",
      "Iteration 642, loss = 2.51483994\n",
      "Iteration 643, loss = 2.51416303\n",
      "Iteration 644, loss = 2.51299651\n",
      "Iteration 645, loss = 2.51280736\n",
      "Iteration 646, loss = 2.51136562\n",
      "Iteration 647, loss = 2.51148461\n",
      "Iteration 648, loss = 2.50985657\n",
      "Iteration 649, loss = 2.50995756\n",
      "Iteration 650, loss = 2.50811284\n",
      "Iteration 651, loss = 2.50755004\n",
      "Iteration 652, loss = 2.50711205\n",
      "Iteration 653, loss = 2.50671366\n",
      "Iteration 654, loss = 2.50720901\n",
      "Iteration 655, loss = 2.50577520\n",
      "Iteration 656, loss = 2.50460653\n",
      "Iteration 657, loss = 2.50366876\n",
      "Iteration 658, loss = 2.50337369\n",
      "Iteration 659, loss = 2.50303791\n",
      "Iteration 660, loss = 2.50115072\n",
      "Iteration 661, loss = 2.49968336\n",
      "Iteration 662, loss = 2.49995133\n",
      "Iteration 663, loss = 2.49900958\n",
      "Iteration 664, loss = 2.49881710\n",
      "Iteration 665, loss = 2.49693050\n",
      "Iteration 666, loss = 2.49803912\n",
      "Iteration 667, loss = 2.49699604\n",
      "Iteration 668, loss = 2.49567177\n",
      "Iteration 669, loss = 2.49433786\n",
      "Iteration 670, loss = 2.49446144\n",
      "Iteration 671, loss = 2.49382431\n",
      "Iteration 672, loss = 2.49439046\n",
      "Iteration 673, loss = 2.49133887\n",
      "Iteration 674, loss = 2.49260552\n",
      "Iteration 675, loss = 2.49175981\n",
      "Iteration 676, loss = 2.49161690\n",
      "Iteration 677, loss = 2.49016459\n",
      "Iteration 678, loss = 2.48910837\n",
      "Iteration 679, loss = 2.48854230\n",
      "Iteration 680, loss = 2.48719775\n",
      "Iteration 681, loss = 2.48699600\n",
      "Iteration 682, loss = 2.48543353\n",
      "Iteration 683, loss = 2.48523226\n",
      "Iteration 684, loss = 2.48362897\n",
      "Iteration 685, loss = 2.48409846\n",
      "Iteration 686, loss = 2.48221915\n",
      "Iteration 687, loss = 2.48134512\n",
      "Iteration 688, loss = 2.48073710\n",
      "Iteration 689, loss = 2.48095415\n",
      "Iteration 690, loss = 2.48049456\n",
      "Iteration 691, loss = 2.47793833\n",
      "Iteration 692, loss = 2.47905540\n",
      "Iteration 693, loss = 2.47735528\n",
      "Iteration 694, loss = 2.47602494\n",
      "Iteration 695, loss = 2.47555352\n",
      "Iteration 696, loss = 2.47451363\n",
      "Iteration 697, loss = 2.47471637\n",
      "Iteration 698, loss = 2.47509919\n",
      "Iteration 699, loss = 2.47301148\n",
      "Iteration 700, loss = 2.47230815\n",
      "Iteration 701, loss = 2.47201244\n",
      "Iteration 702, loss = 2.47122545\n",
      "Iteration 703, loss = 2.46953194\n",
      "Iteration 704, loss = 2.46859308\n",
      "Iteration 705, loss = 2.46892076\n",
      "Iteration 706, loss = 2.46844554\n",
      "Iteration 707, loss = 2.46780743\n",
      "Iteration 708, loss = 2.46634699\n",
      "Iteration 709, loss = 2.46797229\n",
      "Iteration 710, loss = 2.46667997\n",
      "Iteration 711, loss = 2.46586604\n",
      "Iteration 712, loss = 2.46319615\n",
      "Iteration 713, loss = 2.46506113\n",
      "Iteration 714, loss = 2.46401247\n",
      "Iteration 715, loss = 2.46272485\n",
      "Iteration 716, loss = 2.46087901\n",
      "Iteration 717, loss = 2.46121531\n",
      "Iteration 718, loss = 2.46016969\n",
      "Iteration 719, loss = 2.45867748\n",
      "Iteration 720, loss = 2.45788211\n",
      "Iteration 721, loss = 2.45700902\n",
      "Iteration 722, loss = 2.45638054\n",
      "Iteration 723, loss = 2.45641487\n",
      "Iteration 724, loss = 2.45531615\n",
      "Iteration 725, loss = 2.45616652\n",
      "Iteration 726, loss = 2.45402644\n",
      "Iteration 727, loss = 2.45353662\n",
      "Iteration 728, loss = 2.45536737\n",
      "Iteration 729, loss = 2.45240348\n",
      "Iteration 730, loss = 2.45171252\n",
      "Iteration 731, loss = 2.45068547\n",
      "Iteration 732, loss = 2.44879827\n",
      "Iteration 733, loss = 2.44850970\n",
      "Iteration 734, loss = 2.44896176\n",
      "Iteration 735, loss = 2.44899714\n",
      "Iteration 736, loss = 2.44684693\n",
      "Iteration 737, loss = 2.44688008\n",
      "Iteration 738, loss = 2.44611151\n",
      "Iteration 739, loss = 2.44504527\n",
      "Iteration 740, loss = 2.44522412\n",
      "Iteration 741, loss = 2.44534043\n",
      "Iteration 742, loss = 2.44432447\n",
      "Iteration 743, loss = 2.44186596\n",
      "Iteration 744, loss = 2.44428593\n",
      "Iteration 745, loss = 2.44169201\n",
      "Iteration 746, loss = 2.44183556\n",
      "Iteration 747, loss = 2.44060244\n",
      "Iteration 748, loss = 2.43990303\n",
      "Iteration 749, loss = 2.43928438\n",
      "Iteration 750, loss = 2.43777959\n",
      "Iteration 751, loss = 2.43660796\n",
      "Iteration 752, loss = 2.43747176\n",
      "Iteration 753, loss = 2.43702606\n",
      "Iteration 754, loss = 2.43593505\n",
      "Iteration 755, loss = 2.43604399\n",
      "Iteration 756, loss = 2.43509297\n",
      "Iteration 757, loss = 2.43427923\n",
      "Iteration 758, loss = 2.43264927\n",
      "Iteration 759, loss = 2.43113676\n",
      "Iteration 760, loss = 2.43122568\n",
      "Iteration 761, loss = 2.43047771\n",
      "Iteration 762, loss = 2.42998000\n",
      "Iteration 763, loss = 2.42940626\n",
      "Iteration 764, loss = 2.42878885\n",
      "Iteration 765, loss = 2.42856390\n",
      "Iteration 766, loss = 2.42899018\n",
      "Iteration 767, loss = 2.42647391\n",
      "Iteration 768, loss = 2.42705912\n",
      "Iteration 769, loss = 2.42617957\n",
      "Iteration 770, loss = 2.42461231\n",
      "Iteration 771, loss = 2.42498472\n",
      "Iteration 772, loss = 2.42477205\n",
      "Iteration 773, loss = 2.42410282\n",
      "Iteration 774, loss = 2.42279103\n",
      "Iteration 775, loss = 2.42068173\n",
      "Iteration 776, loss = 2.42085722\n",
      "Iteration 777, loss = 2.42057576\n",
      "Iteration 778, loss = 2.42008624\n",
      "Iteration 779, loss = 2.41797925\n",
      "Iteration 780, loss = 2.41752416\n",
      "Iteration 781, loss = 2.41619853\n",
      "Iteration 782, loss = 2.41540360\n",
      "Iteration 783, loss = 2.41679161\n",
      "Iteration 784, loss = 2.41723532\n",
      "Iteration 785, loss = 2.41593699\n",
      "Iteration 786, loss = 2.41392149\n",
      "Iteration 787, loss = 2.41312567\n",
      "Iteration 788, loss = 2.41252740\n",
      "Iteration 789, loss = 2.41195683\n",
      "Iteration 790, loss = 2.41117108\n",
      "Iteration 791, loss = 2.41162059\n",
      "Iteration 792, loss = 2.41080861\n",
      "Iteration 793, loss = 2.41282811\n",
      "Iteration 794, loss = 2.41095987\n",
      "Iteration 795, loss = 2.40980956\n",
      "Iteration 796, loss = 2.40880100\n",
      "Iteration 797, loss = 2.40787781\n",
      "Iteration 798, loss = 2.40636351\n",
      "Iteration 799, loss = 2.40671549\n",
      "Iteration 800, loss = 2.40844712\n",
      "Iteration 801, loss = 2.40561192\n",
      "Iteration 802, loss = 2.40462565\n",
      "Iteration 803, loss = 2.40439471\n",
      "Iteration 804, loss = 2.40250280\n",
      "Iteration 805, loss = 2.40158444\n",
      "Iteration 806, loss = 2.40029553\n",
      "Iteration 807, loss = 2.39983786\n",
      "Iteration 808, loss = 2.40005104\n",
      "Iteration 809, loss = 2.39900695\n",
      "Iteration 810, loss = 2.39918302\n",
      "Iteration 811, loss = 2.39867654\n",
      "Iteration 812, loss = 2.39655204\n",
      "Iteration 813, loss = 2.39803970\n",
      "Iteration 814, loss = 2.39755326\n",
      "Iteration 815, loss = 2.39606334\n",
      "Iteration 816, loss = 2.39495462\n",
      "Iteration 817, loss = 2.39411674\n",
      "Iteration 818, loss = 2.39400085\n",
      "Iteration 819, loss = 2.39257693\n",
      "Iteration 820, loss = 2.39256899\n",
      "Iteration 821, loss = 2.39105280\n",
      "Iteration 822, loss = 2.39259552\n",
      "Iteration 823, loss = 2.39133280\n",
      "Iteration 824, loss = 2.39101329\n",
      "Iteration 825, loss = 2.38967270\n",
      "Iteration 826, loss = 2.38967426\n",
      "Iteration 827, loss = 2.38842552\n",
      "Iteration 828, loss = 2.38694587\n",
      "Iteration 829, loss = 2.38610057\n",
      "Iteration 830, loss = 2.38637484\n",
      "Iteration 831, loss = 2.38511806\n",
      "Iteration 832, loss = 2.38515756\n",
      "Iteration 833, loss = 2.38476462\n",
      "Iteration 834, loss = 2.38397163\n",
      "Iteration 835, loss = 2.38253846\n",
      "Iteration 836, loss = 2.38185262\n",
      "Iteration 837, loss = 2.38122341\n",
      "Iteration 838, loss = 2.38106795\n",
      "Iteration 839, loss = 2.38024337\n",
      "Iteration 840, loss = 2.37937539\n",
      "Iteration 841, loss = 2.37801258\n",
      "Iteration 842, loss = 2.37777834\n",
      "Iteration 843, loss = 2.37970985\n",
      "Iteration 844, loss = 2.37801569\n",
      "Iteration 845, loss = 2.37855358\n",
      "Iteration 846, loss = 2.37677772\n",
      "Iteration 847, loss = 2.37646564\n",
      "Iteration 848, loss = 2.37541087\n",
      "Iteration 849, loss = 2.37461452\n",
      "Iteration 850, loss = 2.37585973\n",
      "Iteration 851, loss = 2.37482594\n",
      "Iteration 852, loss = 2.37266733\n",
      "Iteration 853, loss = 2.37321821\n",
      "Iteration 854, loss = 2.37245369\n",
      "Iteration 855, loss = 2.37475704\n",
      "Iteration 856, loss = 2.37138195\n",
      "Iteration 857, loss = 2.37018055\n",
      "Iteration 858, loss = 2.37164476\n",
      "Iteration 859, loss = 2.36953668\n",
      "Iteration 860, loss = 2.36999047\n",
      "Iteration 861, loss = 2.36890627\n",
      "Iteration 862, loss = 2.36607243\n",
      "Iteration 863, loss = 2.36595855\n",
      "Iteration 864, loss = 2.36617316\n",
      "Iteration 865, loss = 2.36625474\n",
      "Iteration 866, loss = 2.36596252\n",
      "Iteration 867, loss = 2.36242965\n",
      "Iteration 868, loss = 2.36372936\n",
      "Iteration 869, loss = 2.36194796\n",
      "Iteration 870, loss = 2.36137620\n",
      "Iteration 871, loss = 2.36092363\n",
      "Iteration 872, loss = 2.36052637\n",
      "Iteration 873, loss = 2.36052918\n",
      "Iteration 874, loss = 2.35920873\n",
      "Iteration 875, loss = 2.35984647\n",
      "Iteration 876, loss = 2.35835007\n",
      "Iteration 877, loss = 2.36036634\n",
      "Iteration 878, loss = 2.35920522\n",
      "Iteration 879, loss = 2.35568729\n",
      "Iteration 880, loss = 2.35511685\n",
      "Iteration 881, loss = 2.35592893\n",
      "Iteration 882, loss = 2.35472028\n",
      "Iteration 883, loss = 2.35527259\n",
      "Iteration 884, loss = 2.35357064\n",
      "Iteration 885, loss = 2.35403874\n",
      "Iteration 886, loss = 2.35375689\n",
      "Iteration 887, loss = 2.35089616\n",
      "Iteration 888, loss = 2.35028346\n",
      "Iteration 889, loss = 2.35097438\n",
      "Iteration 890, loss = 2.35240361\n",
      "Iteration 891, loss = 2.35264844\n",
      "Iteration 892, loss = 2.34927791\n",
      "Iteration 893, loss = 2.34994092\n",
      "Iteration 894, loss = 2.34912614\n",
      "Iteration 895, loss = 2.34756305\n",
      "Iteration 896, loss = 2.34612614\n",
      "Iteration 897, loss = 2.34671114\n",
      "Iteration 898, loss = 2.34580160\n",
      "Iteration 899, loss = 2.34421004\n",
      "Iteration 900, loss = 2.34432489\n",
      "Iteration 901, loss = 2.34348476\n",
      "Iteration 902, loss = 2.34151146\n",
      "Iteration 903, loss = 2.34411746\n",
      "Iteration 904, loss = 2.34268050\n",
      "Iteration 905, loss = 2.34156729\n",
      "Iteration 906, loss = 2.34060493\n",
      "Iteration 907, loss = 2.34127672\n",
      "Iteration 908, loss = 2.33987194\n",
      "Iteration 909, loss = 2.33985032\n",
      "Iteration 910, loss = 2.33981763\n",
      "Iteration 911, loss = 2.33758621\n",
      "Iteration 912, loss = 2.33637653\n",
      "Iteration 913, loss = 2.33751651\n",
      "Iteration 914, loss = 2.33612353\n",
      "Iteration 915, loss = 2.33541405\n",
      "Iteration 916, loss = 2.33614855\n",
      "Iteration 917, loss = 2.33466254\n",
      "Iteration 918, loss = 2.33374652\n",
      "Iteration 919, loss = 2.33407594\n",
      "Iteration 920, loss = 2.33330620\n",
      "Iteration 921, loss = 2.33267721\n",
      "Iteration 922, loss = 2.33218280\n",
      "Iteration 923, loss = 2.33076928\n",
      "Iteration 924, loss = 2.33024165\n",
      "Iteration 925, loss = 2.33020312\n",
      "Iteration 926, loss = 2.32971948\n",
      "Iteration 927, loss = 2.32941465\n",
      "Iteration 928, loss = 2.32842080\n",
      "Iteration 929, loss = 2.32934673\n",
      "Iteration 930, loss = 2.32690812\n",
      "Iteration 931, loss = 2.32582766\n",
      "Iteration 932, loss = 2.32556906\n",
      "Iteration 933, loss = 2.32457717\n",
      "Iteration 934, loss = 2.32459870\n",
      "Iteration 935, loss = 2.32381535\n",
      "Iteration 936, loss = 2.32405638\n",
      "Iteration 937, loss = 2.32255617\n",
      "Iteration 938, loss = 2.32170293\n",
      "Iteration 939, loss = 2.32242683\n",
      "Iteration 940, loss = 2.32110102\n",
      "Iteration 941, loss = 2.32192421\n",
      "Iteration 942, loss = 2.31865966\n",
      "Iteration 943, loss = 2.32162247\n",
      "Iteration 944, loss = 2.31968999\n",
      "Iteration 945, loss = 2.31831305\n",
      "Iteration 946, loss = 2.31803578\n",
      "Iteration 947, loss = 2.31888153\n",
      "Iteration 948, loss = 2.31795740\n",
      "Iteration 949, loss = 2.31965527\n",
      "Iteration 950, loss = 2.31773890\n",
      "Iteration 951, loss = 2.31666104\n",
      "Iteration 952, loss = 2.31588601\n",
      "Iteration 953, loss = 2.31332449\n",
      "Iteration 954, loss = 2.31379103\n",
      "Iteration 955, loss = 2.31359571\n",
      "Iteration 956, loss = 2.31345679\n",
      "Iteration 957, loss = 2.31173337\n",
      "Iteration 958, loss = 2.31094291\n",
      "Iteration 959, loss = 2.31075808\n",
      "Iteration 960, loss = 2.31035211\n",
      "Iteration 961, loss = 2.31026790\n",
      "Iteration 962, loss = 2.30901479\n",
      "Iteration 963, loss = 2.30888551\n",
      "Iteration 964, loss = 2.30990893\n",
      "Iteration 965, loss = 2.30802794\n",
      "Iteration 966, loss = 2.30778221\n",
      "Iteration 967, loss = 2.30783563\n",
      "Iteration 968, loss = 2.30500949\n",
      "Iteration 969, loss = 2.30509555\n",
      "Iteration 970, loss = 2.30448880\n",
      "Iteration 971, loss = 2.30384451\n",
      "Iteration 972, loss = 2.30323013\n",
      "Iteration 973, loss = 2.30233660\n",
      "Iteration 974, loss = 2.30239941\n",
      "Iteration 975, loss = 2.30332987\n",
      "Iteration 976, loss = 2.30216998\n",
      "Iteration 977, loss = 2.30153464\n",
      "Iteration 978, loss = 2.30132603\n",
      "Iteration 979, loss = 2.30086200\n",
      "Iteration 980, loss = 2.30014345\n",
      "Iteration 981, loss = 2.29936493\n",
      "Iteration 982, loss = 2.29931119\n",
      "Iteration 983, loss = 2.29858977\n",
      "Iteration 984, loss = 2.29692390\n",
      "Iteration 985, loss = 2.29871274\n",
      "Iteration 986, loss = 2.29586104\n",
      "Iteration 987, loss = 2.29674264\n",
      "Iteration 988, loss = 2.29449406\n",
      "Iteration 989, loss = 2.29594999\n",
      "Iteration 990, loss = 2.29414735\n",
      "Iteration 991, loss = 2.29457582\n",
      "Iteration 992, loss = 2.29283524\n",
      "Iteration 993, loss = 2.29270920\n",
      "Iteration 994, loss = 2.29066056\n",
      "Iteration 995, loss = 2.29064389\n",
      "Iteration 996, loss = 2.29091210\n",
      "Iteration 997, loss = 2.29102164\n",
      "Iteration 998, loss = 2.28954467\n",
      "Iteration 999, loss = 2.28916086\n",
      "Iteration 1000, loss = 2.28864848\n",
      "Iteration 1001, loss = 2.28874454\n",
      "Iteration 1002, loss = 2.28753408\n",
      "Iteration 1003, loss = 2.28816114\n",
      "Iteration 1004, loss = 2.28776018\n",
      "Iteration 1005, loss = 2.28670773\n",
      "Iteration 1006, loss = 2.28516340\n",
      "Iteration 1007, loss = 2.28564256\n",
      "Iteration 1008, loss = 2.28453973\n",
      "Iteration 1009, loss = 2.28454964\n",
      "Iteration 1010, loss = 2.28368991\n",
      "Iteration 1011, loss = 2.28234678\n",
      "Iteration 1012, loss = 2.28195704\n",
      "Iteration 1013, loss = 2.28213976\n",
      "Iteration 1014, loss = 2.28134927\n",
      "Iteration 1015, loss = 2.28184892\n",
      "Iteration 1016, loss = 2.28056552\n",
      "Iteration 1017, loss = 2.28118822\n",
      "Iteration 1018, loss = 2.28014245\n",
      "Iteration 1019, loss = 2.28116915\n",
      "Iteration 1020, loss = 2.27787143\n",
      "Iteration 1021, loss = 2.27708007\n",
      "Iteration 1022, loss = 2.27597445\n",
      "Iteration 1023, loss = 2.27870820\n",
      "Iteration 1024, loss = 2.27675277\n",
      "Iteration 1025, loss = 2.27797054\n",
      "Iteration 1026, loss = 2.27685335\n",
      "Iteration 1027, loss = 2.27448213\n",
      "Iteration 1028, loss = 2.27389924\n",
      "Iteration 1029, loss = 2.27391488\n",
      "Iteration 1030, loss = 2.27257856\n",
      "Iteration 1031, loss = 2.27307126\n",
      "Iteration 1032, loss = 2.27375863\n",
      "Iteration 1033, loss = 2.27248309\n",
      "Iteration 1034, loss = 2.27100376\n",
      "Iteration 1035, loss = 2.27179601\n",
      "Iteration 1036, loss = 2.27104925\n",
      "Iteration 1037, loss = 2.26872397\n",
      "Iteration 1038, loss = 2.26859113\n",
      "Iteration 1039, loss = 2.26900107\n",
      "Iteration 1040, loss = 2.26765747\n",
      "Iteration 1041, loss = 2.26737939\n",
      "Iteration 1042, loss = 2.26620664\n",
      "Iteration 1043, loss = 2.26600983\n",
      "Iteration 1044, loss = 2.26662790\n",
      "Iteration 1045, loss = 2.26679444\n",
      "Iteration 1046, loss = 2.26434059\n",
      "Iteration 1047, loss = 2.26479648\n",
      "Iteration 1048, loss = 2.26483050\n",
      "Iteration 1049, loss = 2.26410837\n",
      "Iteration 1050, loss = 2.26322867\n",
      "Iteration 1051, loss = 2.26207846\n",
      "Iteration 1052, loss = 2.26242502\n",
      "Iteration 1053, loss = 2.26188126\n",
      "Iteration 1054, loss = 2.26118198\n",
      "Iteration 1055, loss = 2.26038703\n",
      "Iteration 1056, loss = 2.26057868\n",
      "Iteration 1057, loss = 2.25998969\n",
      "Iteration 1058, loss = 2.26095208\n",
      "Iteration 1059, loss = 2.25887942\n",
      "Iteration 1060, loss = 2.25836520\n",
      "Iteration 1061, loss = 2.25743835\n",
      "Iteration 1062, loss = 2.25806879\n",
      "Iteration 1063, loss = 2.25782580\n",
      "Iteration 1064, loss = 2.25794105\n",
      "Iteration 1065, loss = 2.25659019\n",
      "Iteration 1066, loss = 2.25584965\n",
      "Iteration 1067, loss = 2.25493973\n",
      "Iteration 1068, loss = 2.25516733\n",
      "Iteration 1069, loss = 2.25337083\n",
      "Iteration 1070, loss = 2.25454143\n",
      "Iteration 1071, loss = 2.25227161\n",
      "Iteration 1072, loss = 2.25175023\n",
      "Iteration 1073, loss = 2.25198858\n",
      "Iteration 1074, loss = 2.25178222\n",
      "Iteration 1075, loss = 2.25207355\n",
      "Iteration 1076, loss = 2.25255395\n",
      "Iteration 1077, loss = 2.24998530\n",
      "Iteration 1078, loss = 2.24926438\n",
      "Iteration 1079, loss = 2.24956338\n",
      "Iteration 1080, loss = 2.24875992\n",
      "Iteration 1081, loss = 2.24781504\n",
      "Iteration 1082, loss = 2.24801462\n",
      "Iteration 1083, loss = 2.24608636\n",
      "Iteration 1084, loss = 2.24609141\n",
      "Iteration 1085, loss = 2.24478790\n",
      "Iteration 1086, loss = 2.24547115\n",
      "Iteration 1087, loss = 2.24488016\n",
      "Iteration 1088, loss = 2.24516991\n",
      "Iteration 1089, loss = 2.24353876\n",
      "Iteration 1090, loss = 2.24322057\n",
      "Iteration 1091, loss = 2.24381797\n",
      "Iteration 1092, loss = 2.24393466\n",
      "Iteration 1093, loss = 2.24489549\n",
      "Iteration 1094, loss = 2.24218609\n",
      "Iteration 1095, loss = 2.24379990\n",
      "Iteration 1096, loss = 2.24031536\n",
      "Iteration 1097, loss = 2.23999231\n",
      "Iteration 1098, loss = 2.24088541\n",
      "Iteration 1099, loss = 2.23979434\n",
      "Iteration 1100, loss = 2.23757982\n",
      "Iteration 1101, loss = 2.24070945\n",
      "Iteration 1102, loss = 2.23777169\n",
      "Iteration 1103, loss = 2.23874760\n",
      "Iteration 1104, loss = 2.23696631\n",
      "Iteration 1105, loss = 2.23526046\n",
      "Iteration 1106, loss = 2.23775380\n",
      "Iteration 1107, loss = 2.23852745\n",
      "Iteration 1108, loss = 2.23597257\n",
      "Iteration 1109, loss = 2.23469102\n",
      "Iteration 1110, loss = 2.23318900\n",
      "Iteration 1111, loss = 2.23260277\n",
      "Iteration 1112, loss = 2.23267978\n",
      "Iteration 1113, loss = 2.23175341\n",
      "Iteration 1114, loss = 2.23058123\n",
      "Iteration 1115, loss = 2.22961542\n",
      "Iteration 1116, loss = 2.22983469\n",
      "Iteration 1117, loss = 2.23007473\n",
      "Iteration 1118, loss = 2.23126675\n",
      "Iteration 1119, loss = 2.22831551\n",
      "Iteration 1120, loss = 2.23231639\n",
      "Iteration 1121, loss = 2.22884462\n",
      "Iteration 1122, loss = 2.22847966\n",
      "Iteration 1123, loss = 2.22842139\n",
      "Iteration 1124, loss = 2.22835893\n",
      "Iteration 1125, loss = 2.22824909\n",
      "Iteration 1126, loss = 2.22680182\n",
      "Iteration 1127, loss = 2.22628273\n",
      "Iteration 1128, loss = 2.22441671\n",
      "Iteration 1129, loss = 2.22364625\n",
      "Iteration 1130, loss = 2.22374399\n",
      "Iteration 1131, loss = 2.22307535\n",
      "Iteration 1132, loss = 2.22229655\n",
      "Iteration 1133, loss = 2.22271988\n",
      "Iteration 1134, loss = 2.22296813\n",
      "Iteration 1135, loss = 2.22147304\n",
      "Iteration 1136, loss = 2.22045887\n",
      "Iteration 1137, loss = 2.22238813\n",
      "Iteration 1138, loss = 2.22001699\n",
      "Iteration 1139, loss = 2.22080365\n",
      "Iteration 1140, loss = 2.21734390\n",
      "Iteration 1141, loss = 2.22330383\n",
      "Iteration 1142, loss = 2.21840819\n",
      "Iteration 1143, loss = 2.21844812\n",
      "Iteration 1144, loss = 2.21816935\n",
      "Iteration 1145, loss = 2.21781534\n",
      "Iteration 1146, loss = 2.21624744\n",
      "Iteration 1147, loss = 2.21629651\n",
      "Iteration 1148, loss = 2.21622151\n",
      "Iteration 1149, loss = 2.21607333\n",
      "Iteration 1150, loss = 2.21562661\n",
      "Iteration 1151, loss = 2.21357452\n",
      "Iteration 1152, loss = 2.21370502\n",
      "Iteration 1153, loss = 2.21300474\n",
      "Iteration 1154, loss = 2.21182852\n",
      "Iteration 1155, loss = 2.21191059\n",
      "Iteration 1156, loss = 2.21161539\n",
      "Iteration 1157, loss = 2.21158370\n",
      "Iteration 1158, loss = 2.21038748\n",
      "Iteration 1159, loss = 2.20882659\n",
      "Iteration 1160, loss = 2.20979815\n",
      "Iteration 1161, loss = 2.20846626\n",
      "Iteration 1162, loss = 2.20714421\n",
      "Iteration 1163, loss = 2.20655234\n",
      "Iteration 1164, loss = 2.20888985\n",
      "Iteration 1165, loss = 2.20727558\n",
      "Iteration 1166, loss = 2.20816945\n",
      "Iteration 1167, loss = 2.20603686\n",
      "Iteration 1168, loss = 2.20576702\n",
      "Iteration 1169, loss = 2.20655315\n",
      "Iteration 1170, loss = 2.20662332\n",
      "Iteration 1171, loss = 2.20359840\n",
      "Iteration 1172, loss = 2.20458312\n",
      "Iteration 1173, loss = 2.20354841\n",
      "Iteration 1174, loss = 2.20256713\n",
      "Iteration 1175, loss = 2.20255699\n",
      "Iteration 1176, loss = 2.20133928\n",
      "Iteration 1177, loss = 2.20063821\n",
      "Iteration 1178, loss = 2.20040590\n",
      "Iteration 1179, loss = 2.20115654\n",
      "Iteration 1180, loss = 2.19993276\n",
      "Iteration 1181, loss = 2.19953849\n",
      "Iteration 1182, loss = 2.19916050\n",
      "Iteration 1183, loss = 2.19939687\n",
      "Iteration 1184, loss = 2.19825303\n",
      "Iteration 1185, loss = 2.19852901\n",
      "Iteration 1186, loss = 2.19736402\n",
      "Iteration 1187, loss = 2.19594456\n",
      "Iteration 1188, loss = 2.19776581\n",
      "Iteration 1189, loss = 2.19815232\n",
      "Iteration 1190, loss = 2.19554624\n",
      "Iteration 1191, loss = 2.19430638\n",
      "Iteration 1192, loss = 2.19630507\n",
      "Iteration 1193, loss = 2.19493519\n",
      "Iteration 1194, loss = 2.19460189\n",
      "Iteration 1195, loss = 2.19359119\n",
      "Iteration 1196, loss = 2.19276536\n",
      "Iteration 1197, loss = 2.19281380\n",
      "Iteration 1198, loss = 2.19080532\n",
      "Iteration 1199, loss = 2.19095231\n",
      "Iteration 1200, loss = 2.19209028\n",
      "Iteration 1201, loss = 2.18935360\n",
      "Iteration 1202, loss = 2.19016229\n",
      "Iteration 1203, loss = 2.19019337\n",
      "Iteration 1204, loss = 2.18895949\n",
      "Iteration 1205, loss = 2.18934489\n",
      "Iteration 1206, loss = 2.18898268\n",
      "Iteration 1207, loss = 2.18819577\n",
      "Iteration 1208, loss = 2.18822774\n",
      "Iteration 1209, loss = 2.18734514\n",
      "Iteration 1210, loss = 2.18656307\n",
      "Iteration 1211, loss = 2.18757604\n",
      "Iteration 1212, loss = 2.18587140\n",
      "Iteration 1213, loss = 2.18620650\n",
      "Iteration 1214, loss = 2.18571192\n",
      "Iteration 1215, loss = 2.18639915\n",
      "Iteration 1216, loss = 2.18530659\n",
      "Iteration 1217, loss = 2.18309411\n",
      "Iteration 1218, loss = 2.18255529\n",
      "Iteration 1219, loss = 2.18411798\n",
      "Iteration 1220, loss = 2.18349806\n",
      "Iteration 1221, loss = 2.18455957\n",
      "Iteration 1222, loss = 2.18344503\n",
      "Iteration 1223, loss = 2.18104418\n",
      "Iteration 1224, loss = 2.18041086\n",
      "Iteration 1225, loss = 2.18056361\n",
      "Iteration 1226, loss = 2.17997071\n",
      "Iteration 1227, loss = 2.17990222\n",
      "Iteration 1228, loss = 2.18041533\n",
      "Iteration 1229, loss = 2.17741040\n",
      "Iteration 1230, loss = 2.17904835\n",
      "Iteration 1231, loss = 2.17630506\n",
      "Iteration 1232, loss = 2.18008510\n",
      "Iteration 1233, loss = 2.17781577\n",
      "Iteration 1234, loss = 2.17708378\n",
      "Iteration 1235, loss = 2.17934720\n",
      "Iteration 1236, loss = 2.17591387\n",
      "Iteration 1237, loss = 2.17410211\n",
      "Iteration 1238, loss = 2.17585948\n",
      "Iteration 1239, loss = 2.17252174\n",
      "Iteration 1240, loss = 2.17359566\n",
      "Iteration 1241, loss = 2.17217538\n",
      "Iteration 1242, loss = 2.17281520\n",
      "Iteration 1243, loss = 2.17416409\n",
      "Iteration 1244, loss = 2.17131465\n",
      "Iteration 1245, loss = 2.17031644\n",
      "Iteration 1246, loss = 2.17312572\n",
      "Iteration 1247, loss = 2.17251269\n",
      "Iteration 1248, loss = 2.17062044\n",
      "Iteration 1249, loss = 2.16816376\n",
      "Iteration 1250, loss = 2.16853142\n",
      "Iteration 1251, loss = 2.16773433\n",
      "Iteration 1252, loss = 2.16675687\n",
      "Iteration 1253, loss = 2.16700361\n",
      "Iteration 1254, loss = 2.16590416\n",
      "Iteration 1255, loss = 2.16603328\n",
      "Iteration 1256, loss = 2.16484368\n",
      "Iteration 1257, loss = 2.16418494\n",
      "Iteration 1258, loss = 2.16396219\n",
      "Iteration 1259, loss = 2.16424175\n",
      "Iteration 1260, loss = 2.16216986\n",
      "Iteration 1261, loss = 2.16451073\n",
      "Iteration 1262, loss = 2.16464293\n",
      "Iteration 1263, loss = 2.16247380\n",
      "Iteration 1264, loss = 2.16196968\n",
      "Iteration 1265, loss = 2.16160316\n",
      "Iteration 1266, loss = 2.16174561\n",
      "Iteration 1267, loss = 2.16334064\n",
      "Iteration 1268, loss = 2.16040918\n",
      "Iteration 1269, loss = 2.16186556\n",
      "Iteration 1270, loss = 2.15942415\n",
      "Iteration 1271, loss = 2.15933448\n",
      "Iteration 1272, loss = 2.15860368\n",
      "Iteration 1273, loss = 2.15840442\n",
      "Iteration 1274, loss = 2.15827518\n",
      "Iteration 1275, loss = 2.15776895\n",
      "Iteration 1276, loss = 2.15674154\n",
      "Iteration 1277, loss = 2.15646242\n",
      "Iteration 1278, loss = 2.15728562\n",
      "Iteration 1279, loss = 2.15646151\n",
      "Iteration 1280, loss = 2.15706615\n",
      "Iteration 1281, loss = 2.15752871\n",
      "Iteration 1282, loss = 2.15765226\n",
      "Iteration 1283, loss = 2.15632951\n",
      "Iteration 1284, loss = 2.15541641\n",
      "Iteration 1285, loss = 2.15405922\n",
      "Iteration 1286, loss = 2.15307104\n",
      "Iteration 1287, loss = 2.15344937\n",
      "Iteration 1288, loss = 2.15151358\n",
      "Iteration 1289, loss = 2.15114587\n",
      "Iteration 1290, loss = 2.15097307\n",
      "Iteration 1291, loss = 2.15063259\n",
      "Iteration 1292, loss = 2.14977509\n",
      "Iteration 1293, loss = 2.14925322\n",
      "Iteration 1294, loss = 2.14950983\n",
      "Iteration 1295, loss = 2.14880635\n",
      "Iteration 1296, loss = 2.14824907\n",
      "Iteration 1297, loss = 2.14893840\n",
      "Iteration 1298, loss = 2.14744483\n",
      "Iteration 1299, loss = 2.14806405\n",
      "Iteration 1300, loss = 2.14733129\n",
      "Iteration 1301, loss = 2.14710058\n",
      "Iteration 1302, loss = 2.14580091\n",
      "Iteration 1303, loss = 2.14581818\n",
      "Iteration 1304, loss = 2.14521587\n",
      "Iteration 1305, loss = 2.14494114\n",
      "Iteration 1306, loss = 2.14346005\n",
      "Iteration 1307, loss = 2.14328251\n",
      "Iteration 1308, loss = 2.14379383\n",
      "Iteration 1309, loss = 2.14326841\n",
      "Iteration 1310, loss = 2.14326991\n",
      "Iteration 1311, loss = 2.14307340\n",
      "Iteration 1312, loss = 2.14016728\n",
      "Iteration 1313, loss = 2.14202639\n",
      "Iteration 1314, loss = 2.14063173\n",
      "Iteration 1315, loss = 2.14118593\n",
      "Iteration 1316, loss = 2.13971575\n",
      "Iteration 1317, loss = 2.13962623\n",
      "Iteration 1318, loss = 2.14198907\n",
      "Iteration 1319, loss = 2.13915961\n",
      "Iteration 1320, loss = 2.13931742\n",
      "Iteration 1321, loss = 2.13754845\n",
      "Iteration 1322, loss = 2.13716336\n",
      "Iteration 1323, loss = 2.13679817\n",
      "Iteration 1324, loss = 2.13512864\n",
      "Iteration 1325, loss = 2.13554957\n",
      "Iteration 1326, loss = 2.13767776\n",
      "Iteration 1327, loss = 2.13430227\n",
      "Iteration 1328, loss = 2.13388218\n",
      "Iteration 1329, loss = 2.13442328\n",
      "Iteration 1330, loss = 2.13535092\n",
      "Iteration 1331, loss = 2.13494408\n",
      "Iteration 1332, loss = 2.13334073\n",
      "Iteration 1333, loss = 2.13220157\n",
      "Iteration 1334, loss = 2.13219232\n",
      "Iteration 1335, loss = 2.13198620\n",
      "Iteration 1336, loss = 2.13133884\n",
      "Iteration 1337, loss = 2.13255548\n",
      "Iteration 1338, loss = 2.13092623\n",
      "Iteration 1339, loss = 2.12891546\n",
      "Iteration 1340, loss = 2.12897213\n",
      "Iteration 1341, loss = 2.12907723\n",
      "Iteration 1342, loss = 2.13033348\n",
      "Iteration 1343, loss = 2.12857690\n",
      "Iteration 1344, loss = 2.12910405\n",
      "Iteration 1345, loss = 2.12876301\n",
      "Iteration 1346, loss = 2.12858734\n",
      "Iteration 1347, loss = 2.12670500\n",
      "Iteration 1348, loss = 2.12711410\n",
      "Iteration 1349, loss = 2.12672846\n",
      "Iteration 1350, loss = 2.12619020\n",
      "Iteration 1351, loss = 2.12658045\n",
      "Iteration 1352, loss = 2.12564089\n",
      "Iteration 1353, loss = 2.12540271\n",
      "Iteration 1354, loss = 2.12485310\n",
      "Iteration 1355, loss = 2.12325209\n",
      "Iteration 1356, loss = 2.12399849\n",
      "Iteration 1357, loss = 2.12309191\n",
      "Iteration 1358, loss = 2.12169310\n",
      "Iteration 1359, loss = 2.12184351\n",
      "Iteration 1360, loss = 2.12123163\n",
      "Iteration 1361, loss = 2.11987359\n",
      "Iteration 1362, loss = 2.12066173\n",
      "Iteration 1363, loss = 2.12280202\n",
      "Iteration 1364, loss = 2.12422097\n",
      "Iteration 1365, loss = 2.12064322\n",
      "Iteration 1366, loss = 2.11887508\n",
      "Iteration 1367, loss = 2.11872743\n",
      "Iteration 1368, loss = 2.12057192\n",
      "Iteration 1369, loss = 2.11988224\n",
      "Iteration 1370, loss = 2.11930880\n",
      "Iteration 1371, loss = 2.11877048\n",
      "Iteration 1372, loss = 2.11787336\n",
      "Iteration 1373, loss = 2.11593011\n",
      "Iteration 1374, loss = 2.11550958\n",
      "Iteration 1375, loss = 2.11552843\n",
      "Iteration 1376, loss = 2.11485173\n",
      "Iteration 1377, loss = 2.11492122\n",
      "Iteration 1378, loss = 2.11520597\n",
      "Iteration 1379, loss = 2.11563153\n",
      "Iteration 1380, loss = 2.11379957\n",
      "Iteration 1381, loss = 2.11463138\n",
      "Iteration 1382, loss = 2.11320129\n",
      "Iteration 1383, loss = 2.11223954\n",
      "Iteration 1384, loss = 2.11401432\n",
      "Iteration 1385, loss = 2.11154172\n",
      "Iteration 1386, loss = 2.11085883\n",
      "Iteration 1387, loss = 2.11068875\n",
      "Iteration 1388, loss = 2.10981272\n",
      "Iteration 1389, loss = 2.11044474\n",
      "Iteration 1390, loss = 2.11038122\n",
      "Iteration 1391, loss = 2.10921216\n",
      "Iteration 1392, loss = 2.11037368\n",
      "Iteration 1393, loss = 2.10820254\n",
      "Iteration 1394, loss = 2.10782705\n",
      "Iteration 1395, loss = 2.10767150\n",
      "Iteration 1396, loss = 2.10609361\n",
      "Iteration 1397, loss = 2.10610278\n",
      "Iteration 1398, loss = 2.10535038\n",
      "Iteration 1399, loss = 2.10621601\n",
      "Iteration 1400, loss = 2.10544540\n",
      "Iteration 1401, loss = 2.10504968\n",
      "Iteration 1402, loss = 2.10490407\n",
      "Iteration 1403, loss = 2.10679493\n",
      "Iteration 1404, loss = 2.10591019\n",
      "Iteration 1405, loss = 2.10331551\n",
      "Iteration 1406, loss = 2.10421888\n",
      "Iteration 1407, loss = 2.10180657\n",
      "Iteration 1408, loss = 2.10279376\n",
      "Iteration 1409, loss = 2.10212552\n",
      "Iteration 1410, loss = 2.10162418\n",
      "Iteration 1411, loss = 2.10044173\n",
      "Iteration 1412, loss = 2.10208570\n",
      "Iteration 1413, loss = 2.10194595\n",
      "Iteration 1414, loss = 2.10057347\n",
      "Iteration 1415, loss = 2.10169937\n",
      "Iteration 1416, loss = 2.10012904\n",
      "Iteration 1417, loss = 2.09967588\n",
      "Iteration 1418, loss = 2.09961472\n",
      "Iteration 1419, loss = 2.09786814\n",
      "Iteration 1420, loss = 2.09870914\n",
      "Iteration 1421, loss = 2.09681900\n",
      "Iteration 1422, loss = 2.09638923\n",
      "Iteration 1423, loss = 2.09619088\n",
      "Iteration 1424, loss = 2.09592020\n",
      "Iteration 1425, loss = 2.09604697\n",
      "Iteration 1426, loss = 2.09546678\n",
      "Iteration 1427, loss = 2.09360894\n",
      "Iteration 1428, loss = 2.09645809\n",
      "Iteration 1429, loss = 2.09436056\n",
      "Iteration 1430, loss = 2.09340339\n",
      "Iteration 1431, loss = 2.09317920\n",
      "Iteration 1432, loss = 2.09301269\n",
      "Iteration 1433, loss = 2.09307686\n",
      "Iteration 1434, loss = 2.09130944\n",
      "Iteration 1435, loss = 2.09216049\n",
      "Iteration 1436, loss = 2.09152436\n",
      "Iteration 1437, loss = 2.09301536\n",
      "Iteration 1438, loss = 2.09183628\n",
      "Iteration 1439, loss = 2.09338015\n",
      "Iteration 1440, loss = 2.09027409\n",
      "Iteration 1441, loss = 2.08956487\n",
      "Iteration 1442, loss = 2.08834364\n",
      "Iteration 1443, loss = 2.08798819\n",
      "Iteration 1444, loss = 2.08718692\n",
      "Iteration 1445, loss = 2.08706517\n",
      "Iteration 1446, loss = 2.08745962\n",
      "Iteration 1447, loss = 2.08735723\n",
      "Iteration 1448, loss = 2.08773502\n",
      "Iteration 1449, loss = 2.08651134\n",
      "Iteration 1450, loss = 2.08577524\n",
      "Iteration 1451, loss = 2.08578593\n",
      "Iteration 1452, loss = 2.08562289\n",
      "Iteration 1453, loss = 2.08504956\n",
      "Iteration 1454, loss = 2.08562156\n",
      "Iteration 1455, loss = 2.08377128\n",
      "Iteration 1456, loss = 2.08304198\n",
      "Iteration 1457, loss = 2.08267341\n",
      "Iteration 1458, loss = 2.08236265\n",
      "Iteration 1459, loss = 2.08246586\n",
      "Iteration 1460, loss = 2.08472540\n",
      "Iteration 1461, loss = 2.08373211\n",
      "Iteration 1462, loss = 2.08239350\n",
      "Iteration 1463, loss = 2.08145172\n",
      "Iteration 1464, loss = 2.08123599\n",
      "Iteration 1465, loss = 2.08133839\n",
      "Iteration 1466, loss = 2.08019876\n",
      "Iteration 1467, loss = 2.07988979\n",
      "Iteration 1468, loss = 2.07897419\n",
      "Iteration 1469, loss = 2.07946877\n",
      "Iteration 1470, loss = 2.07788119\n",
      "Iteration 1471, loss = 2.07702717\n",
      "Iteration 1472, loss = 2.07714473\n",
      "Iteration 1473, loss = 2.07610427\n",
      "Iteration 1474, loss = 2.07610360\n",
      "Iteration 1475, loss = 2.07735589\n",
      "Iteration 1476, loss = 2.07581017\n",
      "Iteration 1477, loss = 2.07565449\n",
      "Iteration 1478, loss = 2.07541863\n",
      "Iteration 1479, loss = 2.07443785\n",
      "Iteration 1480, loss = 2.07360711\n",
      "Iteration 1481, loss = 2.07450415\n",
      "Iteration 1482, loss = 2.07348923\n",
      "Iteration 1483, loss = 2.07408717\n",
      "Iteration 1484, loss = 2.07579091\n",
      "Iteration 1485, loss = 2.07453154\n",
      "Iteration 1486, loss = 2.07566806\n",
      "Iteration 1487, loss = 2.07238505\n",
      "Iteration 1488, loss = 2.07172287\n",
      "Iteration 1489, loss = 2.07081471\n",
      "Iteration 1490, loss = 2.07104763\n",
      "Iteration 1491, loss = 2.07024102\n",
      "Iteration 1492, loss = 2.06929380\n",
      "Iteration 1493, loss = 2.06943980\n",
      "Iteration 1494, loss = 2.07245064\n",
      "Iteration 1495, loss = 2.07055715\n",
      "Iteration 1496, loss = 2.06993154\n",
      "Iteration 1497, loss = 2.06932496\n",
      "Iteration 1498, loss = 2.06885382\n",
      "Iteration 1499, loss = 2.06735451\n",
      "Iteration 1500, loss = 2.06847650\n",
      "Iteration 1501, loss = 2.06692571\n",
      "Iteration 1502, loss = 2.06523873\n",
      "Iteration 1503, loss = 2.06514780\n",
      "Iteration 1504, loss = 2.06395508\n",
      "Iteration 1505, loss = 2.06496850\n",
      "Iteration 1506, loss = 2.06498308\n",
      "Iteration 1507, loss = 2.06337342\n",
      "Iteration 1508, loss = 2.06389115\n",
      "Iteration 1509, loss = 2.06376167\n",
      "Iteration 1510, loss = 2.06379448\n",
      "Iteration 1511, loss = 2.06170189\n",
      "Iteration 1512, loss = 2.06262660\n",
      "Iteration 1513, loss = 2.06081859\n",
      "Iteration 1514, loss = 2.06326997\n",
      "Iteration 1515, loss = 2.06215561\n",
      "Iteration 1516, loss = 2.06105391\n",
      "Iteration 1517, loss = 2.05944496\n",
      "Iteration 1518, loss = 2.05811723\n",
      "Iteration 1519, loss = 2.05984108\n",
      "Iteration 1520, loss = 2.05934473\n",
      "Iteration 1521, loss = 2.05967202\n",
      "Iteration 1522, loss = 2.06085418\n",
      "Iteration 1523, loss = 2.05788609\n",
      "Iteration 1524, loss = 2.05846619\n",
      "Iteration 1525, loss = 2.05973491\n",
      "Iteration 1526, loss = 2.06045073\n",
      "Iteration 1527, loss = 2.05698858\n",
      "Iteration 1528, loss = 2.05755009\n",
      "Iteration 1529, loss = 2.05688074\n",
      "Iteration 1530, loss = 2.05712019\n",
      "Iteration 1531, loss = 2.05557164\n",
      "Iteration 1532, loss = 2.05580307\n",
      "Iteration 1533, loss = 2.05470571\n",
      "Iteration 1534, loss = 2.05423863\n",
      "Iteration 1535, loss = 2.05325923\n",
      "Iteration 1536, loss = 2.05346248\n",
      "Iteration 1537, loss = 2.05368070\n",
      "Iteration 1538, loss = 2.05394167\n",
      "Iteration 1539, loss = 2.05470509\n",
      "Iteration 1540, loss = 2.05061824\n",
      "Iteration 1541, loss = 2.05266729\n",
      "Iteration 1542, loss = 2.05162302\n",
      "Iteration 1543, loss = 2.05229954\n",
      "Iteration 1544, loss = 2.05294649\n",
      "Iteration 1545, loss = 2.05185918\n",
      "Iteration 1546, loss = 2.05132988\n",
      "Iteration 1547, loss = 2.05106503\n",
      "Iteration 1548, loss = 2.04898378\n",
      "Iteration 1549, loss = 2.04935478\n",
      "Iteration 1550, loss = 2.04809289\n",
      "Iteration 1551, loss = 2.04783643\n",
      "Iteration 1552, loss = 2.04669281\n",
      "Iteration 1553, loss = 2.05097818\n",
      "Iteration 1554, loss = 2.04696923\n",
      "Iteration 1555, loss = 2.04833497\n",
      "Iteration 1556, loss = 2.04738842\n",
      "Iteration 1557, loss = 2.04613706\n",
      "Iteration 1558, loss = 2.04987872\n",
      "Iteration 1559, loss = 2.04801966\n",
      "Iteration 1560, loss = 2.04575881\n",
      "Iteration 1561, loss = 2.04453003\n",
      "Iteration 1562, loss = 2.04490291\n",
      "Iteration 1563, loss = 2.04676903\n",
      "Iteration 1564, loss = 2.04394845\n",
      "Iteration 1565, loss = 2.04438903\n",
      "Iteration 1566, loss = 2.04309636\n",
      "Iteration 1567, loss = 2.04385138\n",
      "Iteration 1568, loss = 2.04269598\n",
      "Iteration 1569, loss = 2.04416612\n",
      "Iteration 1570, loss = 2.04227969\n",
      "Iteration 1571, loss = 2.04071040\n",
      "Iteration 1572, loss = 2.03973234\n",
      "Iteration 1573, loss = 2.04008591\n",
      "Iteration 1574, loss = 2.04075462\n",
      "Iteration 1575, loss = 2.04145046\n",
      "Iteration 1576, loss = 2.03904713\n",
      "Iteration 1577, loss = 2.03985924\n",
      "Iteration 1578, loss = 2.03919700\n",
      "Iteration 1579, loss = 2.03826582\n",
      "Iteration 1580, loss = 2.03903170\n",
      "Iteration 1581, loss = 2.03903663\n",
      "Iteration 1582, loss = 2.03876038\n",
      "Iteration 1583, loss = 2.03784484\n",
      "Iteration 1584, loss = 2.03730464\n",
      "Iteration 1585, loss = 2.03570780\n",
      "Iteration 1586, loss = 2.03628494\n",
      "Iteration 1587, loss = 2.03520675\n",
      "Iteration 1588, loss = 2.03574912\n",
      "Iteration 1589, loss = 2.03667662\n",
      "Iteration 1590, loss = 2.03509827\n",
      "Iteration 1591, loss = 2.03439281\n",
      "Iteration 1592, loss = 2.03372212\n",
      "Iteration 1593, loss = 2.03356848\n",
      "Iteration 1594, loss = 2.03225887\n",
      "Iteration 1595, loss = 2.03262106\n",
      "Iteration 1596, loss = 2.03293151\n",
      "Iteration 1597, loss = 2.03423755\n",
      "Iteration 1598, loss = 2.03372362\n",
      "Iteration 1599, loss = 2.03088536\n",
      "Iteration 1600, loss = 2.03224917\n",
      "Iteration 1601, loss = 2.03295140\n",
      "Iteration 1602, loss = 2.03461856\n",
      "Iteration 1603, loss = 2.03206038\n",
      "Iteration 1604, loss = 2.03082906\n",
      "Iteration 1605, loss = 2.03170224\n",
      "Iteration 1606, loss = 2.03027224\n",
      "Iteration 1607, loss = 2.02931488\n",
      "Iteration 1608, loss = 2.02836515\n",
      "Iteration 1609, loss = 2.02815816\n",
      "Iteration 1610, loss = 2.02690350\n",
      "Iteration 1611, loss = 2.02725378\n",
      "Iteration 1612, loss = 2.02743840\n",
      "Iteration 1613, loss = 2.02703137\n",
      "Iteration 1614, loss = 2.02750031\n",
      "Iteration 1615, loss = 2.02585525\n",
      "Iteration 1616, loss = 2.02556163\n",
      "Iteration 1617, loss = 2.02575992\n",
      "Iteration 1618, loss = 2.02546660\n",
      "Iteration 1619, loss = 2.02550314\n",
      "Iteration 1620, loss = 2.02591666\n",
      "Iteration 1621, loss = 2.02491193\n",
      "Iteration 1622, loss = 2.02379104\n",
      "Iteration 1623, loss = 2.02303576\n",
      "Iteration 1624, loss = 2.02424860\n",
      "Iteration 1625, loss = 2.02332508\n",
      "Iteration 1626, loss = 2.02222559\n",
      "Iteration 1627, loss = 2.02175899\n",
      "Iteration 1628, loss = 2.02241663\n",
      "Iteration 1629, loss = 2.01979624\n",
      "Iteration 1630, loss = 2.01979756\n",
      "Iteration 1631, loss = 2.02384548\n",
      "Iteration 1632, loss = 2.02043584\n",
      "Iteration 1633, loss = 2.01954004\n",
      "Iteration 1634, loss = 2.02211085\n",
      "Iteration 1635, loss = 2.02268850\n",
      "Iteration 1636, loss = 2.01938377\n",
      "Iteration 1637, loss = 2.01763672\n",
      "Iteration 1638, loss = 2.01820345\n",
      "Iteration 1639, loss = 2.01687853\n",
      "Iteration 1640, loss = 2.01649835\n",
      "Iteration 1641, loss = 2.01656929\n",
      "Iteration 1642, loss = 2.01812741\n",
      "Iteration 1643, loss = 2.01726670\n",
      "Iteration 1644, loss = 2.01511655\n",
      "Iteration 1645, loss = 2.01550399\n",
      "Iteration 1646, loss = 2.01736016\n",
      "Iteration 1647, loss = 2.01417506\n",
      "Iteration 1648, loss = 2.01496588\n",
      "Iteration 1649, loss = 2.01528860\n",
      "Iteration 1650, loss = 2.01370773\n",
      "Iteration 1651, loss = 2.01308970\n",
      "Iteration 1652, loss = 2.01381307\n",
      "Iteration 1653, loss = 2.01468326\n",
      "Iteration 1654, loss = 2.01736481\n",
      "Iteration 1655, loss = 2.01385950\n",
      "Iteration 1656, loss = 2.01236375\n",
      "Iteration 1657, loss = 2.01366440\n",
      "Iteration 1658, loss = 2.01142156\n",
      "Iteration 1659, loss = 2.01098636\n",
      "Iteration 1660, loss = 2.01049079\n",
      "Iteration 1661, loss = 2.01217255\n",
      "Iteration 1662, loss = 2.01018754\n",
      "Iteration 1663, loss = 2.00930661\n",
      "Iteration 1664, loss = 2.01032940\n",
      "Iteration 1665, loss = 2.00951146\n",
      "Iteration 1666, loss = 2.00959969\n",
      "Iteration 1667, loss = 2.00893928\n",
      "Iteration 1668, loss = 2.00909756\n",
      "Iteration 1669, loss = 2.00812623\n",
      "Iteration 1670, loss = 2.01004105\n",
      "Iteration 1671, loss = 2.00966608\n",
      "Iteration 1672, loss = 2.00700277\n",
      "Iteration 1673, loss = 2.00893798\n",
      "Iteration 1674, loss = 2.00784405\n",
      "Iteration 1675, loss = 2.00770055\n",
      "Iteration 1676, loss = 2.00710456\n",
      "Iteration 1677, loss = 2.00930405\n",
      "Iteration 1678, loss = 2.00695171\n",
      "Iteration 1679, loss = 2.00642511\n",
      "Iteration 1680, loss = 2.00373565\n",
      "Iteration 1681, loss = 2.00562856\n",
      "Iteration 1682, loss = 2.00495352\n",
      "Iteration 1683, loss = 2.00286516\n",
      "Iteration 1684, loss = 2.00273108\n",
      "Iteration 1685, loss = 2.00367772\n",
      "Iteration 1686, loss = 2.00244546\n",
      "Iteration 1687, loss = 2.00232928\n",
      "Iteration 1688, loss = 2.00262170\n",
      "Iteration 1689, loss = 2.00162389\n",
      "Iteration 1690, loss = 2.00143847\n",
      "Iteration 1691, loss = 2.00145024\n",
      "Iteration 1692, loss = 2.00122085\n",
      "Iteration 1693, loss = 1.99949954\n",
      "Iteration 1694, loss = 2.00408468\n",
      "Iteration 1695, loss = 2.00085197\n",
      "Iteration 1696, loss = 2.00286135\n",
      "Iteration 1697, loss = 2.00118961\n",
      "Iteration 1698, loss = 2.00053983\n",
      "Iteration 1699, loss = 1.99915575\n",
      "Iteration 1700, loss = 1.99788317\n",
      "Iteration 1701, loss = 1.99638591\n",
      "Iteration 1702, loss = 1.99686716\n",
      "Iteration 1703, loss = 1.99695234\n",
      "Iteration 1704, loss = 1.99746032\n",
      "Iteration 1705, loss = 1.99770310\n",
      "Iteration 1706, loss = 1.99589866\n",
      "Iteration 1707, loss = 1.99710591\n",
      "Iteration 1708, loss = 1.99654263\n",
      "Iteration 1709, loss = 1.99701078\n",
      "Iteration 1710, loss = 1.99467314\n",
      "Iteration 1711, loss = 1.99500443\n",
      "Iteration 1712, loss = 1.99590651\n",
      "Iteration 1713, loss = 1.99622516\n",
      "Iteration 1714, loss = 1.99421011\n",
      "Iteration 1715, loss = 1.99401694\n",
      "Iteration 1716, loss = 1.99321580\n",
      "Iteration 1717, loss = 1.99361267\n",
      "Iteration 1718, loss = 1.99327410\n",
      "Iteration 1719, loss = 1.99288654\n",
      "Iteration 1720, loss = 1.99037153\n",
      "Iteration 1721, loss = 1.99461614\n",
      "Iteration 1722, loss = 1.99078774\n",
      "Iteration 1723, loss = 1.99370089\n",
      "Iteration 1724, loss = 1.99081534\n",
      "Iteration 1725, loss = 1.99177279\n",
      "Iteration 1726, loss = 1.98978035\n",
      "Iteration 1727, loss = 1.98970437\n",
      "Iteration 1728, loss = 1.98851754\n",
      "Iteration 1729, loss = 1.98874507\n",
      "Iteration 1730, loss = 1.98965577\n",
      "Iteration 1731, loss = 1.98804762\n",
      "Iteration 1732, loss = 1.98749304\n",
      "Iteration 1733, loss = 1.98855539\n",
      "Iteration 1734, loss = 1.98931259\n",
      "Iteration 1735, loss = 1.98628286\n",
      "Iteration 1736, loss = 1.98614005\n",
      "Iteration 1737, loss = 1.98603727\n",
      "Iteration 1738, loss = 1.98504478\n",
      "Iteration 1739, loss = 1.98521470\n",
      "Iteration 1740, loss = 1.98723163\n",
      "Iteration 1741, loss = 1.98417746\n",
      "Iteration 1742, loss = 1.98329705\n",
      "Iteration 1743, loss = 1.98289889\n",
      "Iteration 1744, loss = 1.98316293\n",
      "Iteration 1745, loss = 1.98221082\n",
      "Iteration 1746, loss = 1.98354738\n",
      "Iteration 1747, loss = 1.98293474\n",
      "Iteration 1748, loss = 1.98159773\n",
      "Iteration 1749, loss = 1.98222284\n",
      "Iteration 1750, loss = 1.98217859\n",
      "Iteration 1751, loss = 1.98202214\n",
      "Iteration 1752, loss = 1.98127673\n",
      "Iteration 1753, loss = 1.98147332\n",
      "Iteration 1754, loss = 1.98092895\n",
      "Iteration 1755, loss = 1.98197510\n",
      "Iteration 1756, loss = 1.97930038\n",
      "Iteration 1757, loss = 1.98030815\n",
      "Iteration 1758, loss = 1.98034827\n",
      "Iteration 1759, loss = 1.98153887\n",
      "Iteration 1760, loss = 1.97864258\n",
      "Iteration 1761, loss = 1.97915778\n",
      "Iteration 1762, loss = 1.97870601\n",
      "Iteration 1763, loss = 1.97736214\n",
      "Iteration 1764, loss = 1.97648721\n",
      "Iteration 1765, loss = 1.97507286\n",
      "Iteration 1766, loss = 1.97763160\n",
      "Iteration 1767, loss = 1.97562313\n",
      "Iteration 1768, loss = 1.97657564\n",
      "Iteration 1769, loss = 1.97562868\n",
      "Iteration 1770, loss = 1.97468063\n",
      "Iteration 1771, loss = 1.97471581\n",
      "Iteration 1772, loss = 1.97491769\n",
      "Iteration 1773, loss = 1.97519049\n",
      "Iteration 1774, loss = 1.97495690\n",
      "Iteration 1775, loss = 1.97446318\n",
      "Iteration 1776, loss = 1.97413178\n",
      "Iteration 1777, loss = 1.97313606\n",
      "Iteration 1778, loss = 1.97297958\n",
      "Iteration 1779, loss = 1.97244234\n",
      "Iteration 1780, loss = 1.97161865\n",
      "Iteration 1781, loss = 1.97238333\n",
      "Iteration 1782, loss = 1.97168117\n",
      "Iteration 1783, loss = 1.97134540\n",
      "Iteration 1784, loss = 1.97061711\n",
      "Iteration 1785, loss = 1.97134885\n",
      "Iteration 1786, loss = 1.97123319\n",
      "Iteration 1787, loss = 1.97083065\n",
      "Iteration 1788, loss = 1.97193995\n",
      "Iteration 1789, loss = 1.96936393\n",
      "Iteration 1790, loss = 1.96795841\n",
      "Iteration 1791, loss = 1.96993455\n",
      "Iteration 1792, loss = 1.96827298\n",
      "Iteration 1793, loss = 1.96890122\n",
      "Iteration 1794, loss = 1.96948660\n",
      "Iteration 1795, loss = 1.96791155\n",
      "Iteration 1796, loss = 1.96765185\n",
      "Iteration 1797, loss = 1.96605264\n",
      "Iteration 1798, loss = 1.96792519\n",
      "Iteration 1799, loss = 1.96829310\n",
      "Iteration 1800, loss = 1.96766289\n",
      "Iteration 1801, loss = 1.96658830\n",
      "Iteration 1802, loss = 1.96639653\n",
      "Iteration 1803, loss = 1.96610303\n",
      "Iteration 1804, loss = 1.96525749\n",
      "Iteration 1805, loss = 1.96625356\n",
      "Iteration 1806, loss = 1.96663171\n",
      "Iteration 1807, loss = 1.96645954\n",
      "Iteration 1808, loss = 1.96332675\n",
      "Iteration 1809, loss = 1.96188993\n",
      "Iteration 1810, loss = 1.96323301\n",
      "Iteration 1811, loss = 1.96398165\n",
      "Iteration 1812, loss = 1.96187233\n",
      "Iteration 1813, loss = 1.96222525\n",
      "Iteration 1814, loss = 1.96221661\n",
      "Iteration 1815, loss = 1.96308961\n",
      "Iteration 1816, loss = 1.96072149\n",
      "Iteration 1817, loss = 1.96200186\n",
      "Iteration 1818, loss = 1.96113990\n",
      "Iteration 1819, loss = 1.96081818\n",
      "Iteration 1820, loss = 1.96103393\n",
      "Iteration 1821, loss = 1.95996954\n",
      "Iteration 1822, loss = 1.95965004\n",
      "Iteration 1823, loss = 1.95882150\n",
      "Iteration 1824, loss = 1.95982858\n",
      "Iteration 1825, loss = 1.96043672\n",
      "Iteration 1826, loss = 1.95825598\n",
      "Iteration 1827, loss = 1.95924606\n",
      "Iteration 1828, loss = 1.95823178\n",
      "Iteration 1829, loss = 1.95679242\n",
      "Iteration 1830, loss = 1.95697576\n",
      "Iteration 1831, loss = 1.95875708\n",
      "Iteration 1832, loss = 1.95653164\n",
      "Iteration 1833, loss = 1.95601816\n",
      "Iteration 1834, loss = 1.95597746\n",
      "Iteration 1835, loss = 1.95623990\n",
      "Iteration 1836, loss = 1.95476180\n",
      "Iteration 1837, loss = 1.95500992\n",
      "Iteration 1838, loss = 1.95301473\n",
      "Iteration 1839, loss = 1.95351543\n",
      "Iteration 1840, loss = 1.95326431\n",
      "Iteration 1841, loss = 1.95675100\n",
      "Iteration 1842, loss = 1.95285333\n",
      "Iteration 1843, loss = 1.95242754\n",
      "Iteration 1844, loss = 1.95814207\n",
      "Iteration 1845, loss = 1.95486865\n",
      "Iteration 1846, loss = 1.95208244\n",
      "Iteration 1847, loss = 1.95036601\n",
      "Iteration 1848, loss = 1.95300365\n",
      "Iteration 1849, loss = 1.95173777\n",
      "Iteration 1850, loss = 1.95207045\n",
      "Iteration 1851, loss = 1.95099640\n",
      "Iteration 1852, loss = 1.94966412\n",
      "Iteration 1853, loss = 1.95078518\n",
      "Iteration 1854, loss = 1.94982672\n",
      "Iteration 1855, loss = 1.95105583\n",
      "Iteration 1856, loss = 1.95031796\n",
      "Iteration 1857, loss = 1.95050519\n",
      "Iteration 1858, loss = 1.94868491\n",
      "Iteration 1859, loss = 1.94757039\n",
      "Iteration 1860, loss = 1.94802956\n",
      "Iteration 1861, loss = 1.94704969\n",
      "Iteration 1862, loss = 1.94866784\n",
      "Iteration 1863, loss = 1.94832445\n",
      "Iteration 1864, loss = 1.94603650\n",
      "Iteration 1865, loss = 1.94613571\n",
      "Iteration 1866, loss = 1.94533431\n",
      "Iteration 1867, loss = 1.94530834\n",
      "Iteration 1868, loss = 1.94593418\n",
      "Iteration 1869, loss = 1.94619871\n",
      "Iteration 1870, loss = 1.94597431\n",
      "Iteration 1871, loss = 1.94553915\n",
      "Iteration 1872, loss = 1.94540138\n",
      "Iteration 1873, loss = 1.94479066\n",
      "Iteration 1874, loss = 1.94347779\n",
      "Iteration 1875, loss = 1.94280313\n",
      "Iteration 1876, loss = 1.94249068\n",
      "Iteration 1877, loss = 1.94257989\n",
      "Iteration 1878, loss = 1.94162537\n",
      "Iteration 1879, loss = 1.94108974\n",
      "Iteration 1880, loss = 1.94262966\n",
      "Iteration 1881, loss = 1.94214436\n",
      "Iteration 1882, loss = 1.94282344\n",
      "Iteration 1883, loss = 1.94191350\n",
      "Iteration 1884, loss = 1.93957774\n",
      "Iteration 1885, loss = 1.94035321\n",
      "Iteration 1886, loss = 1.94244687\n",
      "Iteration 1887, loss = 1.94167414\n",
      "Iteration 1888, loss = 1.94096870\n",
      "Iteration 1889, loss = 1.93996524\n",
      "Iteration 1890, loss = 1.94015067\n",
      "Iteration 1891, loss = 1.94051615\n",
      "Iteration 1892, loss = 1.93932638\n",
      "Iteration 1893, loss = 1.93999128\n",
      "Iteration 1894, loss = 1.93783281\n",
      "Iteration 1895, loss = 1.93715565\n",
      "Iteration 1896, loss = 1.93945969\n",
      "Iteration 1897, loss = 1.93803444\n",
      "Iteration 1898, loss = 1.93839168\n",
      "Iteration 1899, loss = 1.93558204\n",
      "Iteration 1900, loss = 1.93738643\n",
      "Iteration 1901, loss = 1.93663934\n",
      "Iteration 1902, loss = 1.93617349\n",
      "Iteration 1903, loss = 1.93614732\n",
      "Iteration 1904, loss = 1.93667191\n",
      "Iteration 1905, loss = 1.93631707\n",
      "Iteration 1906, loss = 1.93627289\n",
      "Iteration 1907, loss = 1.93422455\n",
      "Iteration 1908, loss = 1.93393101\n",
      "Iteration 1909, loss = 1.93612255\n",
      "Iteration 1910, loss = 1.93461234\n",
      "Iteration 1911, loss = 1.93485629\n",
      "Iteration 1912, loss = 1.93400136\n",
      "Iteration 1913, loss = 1.93272985\n",
      "Iteration 1914, loss = 1.93284246\n",
      "Iteration 1915, loss = 1.93325462\n",
      "Iteration 1916, loss = 1.93241662\n",
      "Iteration 1917, loss = 1.93308782\n",
      "Iteration 1918, loss = 1.93107008\n",
      "Iteration 1919, loss = 1.93148637\n",
      "Iteration 1920, loss = 1.93056437\n",
      "Iteration 1921, loss = 1.92928635\n",
      "Iteration 1922, loss = 1.92953993\n",
      "Iteration 1923, loss = 1.92975529\n",
      "Iteration 1924, loss = 1.93197318\n",
      "Iteration 1925, loss = 1.92700783\n",
      "Iteration 1926, loss = 1.92725475\n",
      "Iteration 1927, loss = 1.92890623\n",
      "Iteration 1928, loss = 1.92817900\n",
      "Iteration 1929, loss = 1.92848160\n",
      "Iteration 1930, loss = 1.92845584\n",
      "Iteration 1931, loss = 1.92999771\n",
      "Iteration 1932, loss = 1.92718279\n",
      "Iteration 1933, loss = 1.92661820\n",
      "Iteration 1934, loss = 1.92649211\n",
      "Iteration 1935, loss = 1.92724601\n",
      "Iteration 1936, loss = 1.92725991\n",
      "Iteration 1937, loss = 1.92818287\n",
      "Iteration 1938, loss = 1.92727685\n",
      "Iteration 1939, loss = 1.92427300\n",
      "Iteration 1940, loss = 1.93085226\n",
      "Iteration 1941, loss = 1.92759910\n",
      "Iteration 1942, loss = 1.92470759\n",
      "Iteration 1943, loss = 1.92380160\n",
      "Iteration 1944, loss = 1.92329690\n",
      "Iteration 1945, loss = 1.92339973\n",
      "Iteration 1946, loss = 1.92363466\n",
      "Iteration 1947, loss = 1.92231441\n",
      "Iteration 1948, loss = 1.92392732\n",
      "Iteration 1949, loss = 1.92312452\n",
      "Iteration 1950, loss = 1.92279527\n",
      "Iteration 1951, loss = 1.92226358\n",
      "Iteration 1952, loss = 1.92208669\n",
      "Iteration 1953, loss = 1.92123417\n",
      "Iteration 1954, loss = 1.92200772\n",
      "Iteration 1955, loss = 1.92196402\n",
      "Iteration 1956, loss = 1.92311214\n",
      "Iteration 1957, loss = 1.92419780\n",
      "Iteration 1958, loss = 1.92043874\n",
      "Iteration 1959, loss = 1.92027053\n",
      "Iteration 1960, loss = 1.91907935\n",
      "Iteration 1961, loss = 1.91849271\n",
      "Iteration 1962, loss = 1.91734494\n",
      "Iteration 1963, loss = 1.91877477\n",
      "Iteration 1964, loss = 1.92008843\n",
      "Iteration 1965, loss = 1.91781495\n",
      "Iteration 1966, loss = 1.91950942\n",
      "Iteration 1967, loss = 1.91741942\n",
      "Iteration 1968, loss = 1.91936037\n",
      "Iteration 1969, loss = 1.91900302\n",
      "Iteration 1970, loss = 1.91909367\n",
      "Iteration 1971, loss = 1.91689335\n",
      "Iteration 1972, loss = 1.91598288\n",
      "Iteration 1973, loss = 1.91655515\n",
      "Iteration 1974, loss = 1.91629942\n",
      "Iteration 1975, loss = 1.91600346\n",
      "Iteration 1976, loss = 1.91624916\n",
      "Iteration 1977, loss = 1.91654661\n",
      "Iteration 1978, loss = 1.91560453\n",
      "Iteration 1979, loss = 1.91419699\n",
      "Iteration 1980, loss = 1.91513938\n",
      "Iteration 1981, loss = 1.91493878\n",
      "Iteration 1982, loss = 1.91457375\n",
      "Iteration 1983, loss = 1.91411839\n",
      "Iteration 1984, loss = 1.91228717\n",
      "Iteration 1985, loss = 1.91135300\n",
      "Iteration 1986, loss = 1.91189855\n",
      "Iteration 1987, loss = 1.91088487\n",
      "Iteration 1988, loss = 1.91211043\n",
      "Iteration 1989, loss = 1.91213295\n",
      "Iteration 1990, loss = 1.91102840\n",
      "Iteration 1991, loss = 1.90999988\n",
      "Iteration 1992, loss = 1.90904138\n",
      "Iteration 1993, loss = 1.90958285\n",
      "Iteration 1994, loss = 1.91129104\n",
      "Iteration 1995, loss = 1.90960952\n",
      "Iteration 1996, loss = 1.90839876\n",
      "Iteration 1997, loss = 1.91086041\n",
      "Iteration 1998, loss = 1.91101074\n",
      "Iteration 1999, loss = 1.90905666\n",
      "Iteration 2000, loss = 1.90883400\n",
      "Iteration 2001, loss = 1.90836318\n",
      "Iteration 2002, loss = 1.90966034\n",
      "Iteration 2003, loss = 1.90888717\n",
      "Iteration 2004, loss = 1.90765497\n",
      "Iteration 2005, loss = 1.90548722\n",
      "Iteration 2006, loss = 1.90872588\n",
      "Iteration 2007, loss = 1.90679644\n",
      "Iteration 2008, loss = 1.90570218\n",
      "Iteration 2009, loss = 1.90492118\n",
      "Iteration 2010, loss = 1.90542733\n",
      "Iteration 2011, loss = 1.90671386\n",
      "Iteration 2012, loss = 1.90645630\n",
      "Iteration 2013, loss = 1.90568529\n",
      "Iteration 2014, loss = 1.90650275\n",
      "Iteration 2015, loss = 1.90609294\n",
      "Iteration 2016, loss = 1.90484716\n",
      "Iteration 2017, loss = 1.90420863\n",
      "Iteration 2018, loss = 1.90506195\n",
      "Iteration 2019, loss = 1.90495196\n",
      "Iteration 2020, loss = 1.90551215\n",
      "Iteration 2021, loss = 1.90329724\n",
      "Iteration 2022, loss = 1.90297516\n",
      "Iteration 2023, loss = 1.90283795\n",
      "Iteration 2024, loss = 1.90261153\n",
      "Iteration 2025, loss = 1.90120722\n",
      "Iteration 2026, loss = 1.90274985\n",
      "Iteration 2027, loss = 1.90196740\n",
      "Iteration 2028, loss = 1.90093183\n",
      "Iteration 2029, loss = 1.90022907\n",
      "Iteration 2030, loss = 1.90049311\n",
      "Iteration 2031, loss = 1.89994720\n",
      "Iteration 2032, loss = 1.90007575\n",
      "Iteration 2033, loss = 1.89948000\n",
      "Iteration 2034, loss = 1.90074026\n",
      "Iteration 2035, loss = 1.90066644\n",
      "Iteration 2036, loss = 1.89840192\n",
      "Iteration 2037, loss = 1.89913818\n",
      "Iteration 2038, loss = 1.89859610\n",
      "Iteration 2039, loss = 1.90122819\n",
      "Iteration 2040, loss = 1.89890143\n",
      "Iteration 2041, loss = 1.89811231\n",
      "Iteration 2042, loss = 1.89838113\n",
      "Iteration 2043, loss = 1.89748268\n",
      "Iteration 2044, loss = 1.89633012\n",
      "Iteration 2045, loss = 1.89694695\n",
      "Iteration 2046, loss = 1.89710984\n",
      "Iteration 2047, loss = 1.89719094\n",
      "Iteration 2048, loss = 1.89593812\n",
      "Iteration 2049, loss = 1.89545736\n",
      "Iteration 2050, loss = 1.89602631\n",
      "Iteration 2051, loss = 1.89420996\n",
      "Iteration 2052, loss = 1.89497845\n",
      "Iteration 2053, loss = 1.89524754\n",
      "Iteration 2054, loss = 1.89598165\n",
      "Iteration 2055, loss = 1.89462199\n",
      "Iteration 2056, loss = 1.89401397\n",
      "Iteration 2057, loss = 1.89323904\n",
      "Iteration 2058, loss = 1.89345410\n",
      "Iteration 2059, loss = 1.89299183\n",
      "Iteration 2060, loss = 1.89224443\n",
      "Iteration 2061, loss = 1.89141240\n",
      "Iteration 2062, loss = 1.89399713\n",
      "Iteration 2063, loss = 1.89145598\n",
      "Iteration 2064, loss = 1.89057380\n",
      "Iteration 2065, loss = 1.89037662\n",
      "Iteration 2066, loss = 1.89122143\n",
      "Iteration 2067, loss = 1.88995219\n",
      "Iteration 2068, loss = 1.89088089\n",
      "Iteration 2069, loss = 1.89264969\n",
      "Iteration 2070, loss = 1.89076767\n",
      "Iteration 2071, loss = 1.88906861\n",
      "Iteration 2072, loss = 1.88918814\n",
      "Iteration 2073, loss = 1.89017387\n",
      "Iteration 2074, loss = 1.88902312\n",
      "Iteration 2075, loss = 1.88790022\n",
      "Iteration 2076, loss = 1.88840365\n",
      "Iteration 2077, loss = 1.88913557\n",
      "Iteration 2078, loss = 1.88895797\n",
      "Iteration 2079, loss = 1.88800147\n",
      "Iteration 2080, loss = 1.88734319\n",
      "Iteration 2081, loss = 1.88690225\n",
      "Iteration 2082, loss = 1.88841399\n",
      "Iteration 2083, loss = 1.88632465\n",
      "Iteration 2084, loss = 1.88598053\n",
      "Iteration 2085, loss = 1.88515715\n",
      "Iteration 2086, loss = 1.88392108\n",
      "Iteration 2087, loss = 1.88492475\n",
      "Iteration 2088, loss = 1.88441269\n",
      "Iteration 2089, loss = 1.88426851\n",
      "Iteration 2090, loss = 1.88391401\n",
      "Iteration 2091, loss = 1.88532636\n",
      "Iteration 2092, loss = 1.88380665\n",
      "Iteration 2093, loss = 1.88428527\n",
      "Iteration 2094, loss = 1.88230693\n",
      "Iteration 2095, loss = 1.88318499\n",
      "Iteration 2096, loss = 1.88289139\n",
      "Iteration 2097, loss = 1.88375629\n",
      "Iteration 2098, loss = 1.88132877\n",
      "Iteration 2099, loss = 1.88152038\n",
      "Iteration 2100, loss = 1.88149017\n",
      "Iteration 2101, loss = 1.88220566\n",
      "Iteration 2102, loss = 1.88159083\n",
      "Iteration 2103, loss = 1.88093574\n",
      "Iteration 2104, loss = 1.88035787\n",
      "Iteration 2105, loss = 1.87948572\n",
      "Iteration 2106, loss = 1.87937042\n",
      "Iteration 2107, loss = 1.88082536\n",
      "Iteration 2108, loss = 1.88091545\n",
      "Iteration 2109, loss = 1.87945767\n",
      "Iteration 2110, loss = 1.88029632\n",
      "Iteration 2111, loss = 1.87895653\n",
      "Iteration 2112, loss = 1.87997095\n",
      "Iteration 2113, loss = 1.87892824\n",
      "Iteration 2114, loss = 1.87824718\n",
      "Iteration 2115, loss = 1.87802954\n",
      "Iteration 2116, loss = 1.87696787\n",
      "Iteration 2117, loss = 1.87816590\n",
      "Iteration 2118, loss = 1.87717878\n",
      "Iteration 2119, loss = 1.87732708\n",
      "Iteration 2120, loss = 1.87747683\n",
      "Iteration 2121, loss = 1.87620762\n",
      "Iteration 2122, loss = 1.87631254\n",
      "Iteration 2123, loss = 1.87567733\n",
      "Iteration 2124, loss = 1.87475877\n",
      "Iteration 2125, loss = 1.87493279\n",
      "Iteration 2126, loss = 1.87562767\n",
      "Iteration 2127, loss = 1.87444180\n",
      "Iteration 2128, loss = 1.87407569\n",
      "Iteration 2129, loss = 1.87447275\n",
      "Iteration 2130, loss = 1.87518330\n",
      "Iteration 2131, loss = 1.87420069\n",
      "Iteration 2132, loss = 1.87443854\n",
      "Iteration 2133, loss = 1.87440363\n",
      "Iteration 2134, loss = 1.87255955\n",
      "Iteration 2135, loss = 1.87475602\n",
      "Iteration 2136, loss = 1.87516589\n",
      "Iteration 2137, loss = 1.87303871\n",
      "Iteration 2138, loss = 1.87267133\n",
      "Iteration 2139, loss = 1.87029686\n",
      "Iteration 2140, loss = 1.87146854\n",
      "Iteration 2141, loss = 1.87238584\n",
      "Iteration 2142, loss = 1.87297598\n",
      "Iteration 2143, loss = 1.87215014\n",
      "Iteration 2144, loss = 1.87027901\n",
      "Iteration 2145, loss = 1.87040035\n",
      "Iteration 2146, loss = 1.86931335\n",
      "Iteration 2147, loss = 1.86913573\n",
      "Iteration 2148, loss = 1.87019313\n",
      "Iteration 2149, loss = 1.87078514\n",
      "Iteration 2150, loss = 1.86886527\n",
      "Iteration 2151, loss = 1.86890840\n",
      "Iteration 2152, loss = 1.86877700\n",
      "Iteration 2153, loss = 1.86862233\n",
      "Iteration 2154, loss = 1.86803631\n",
      "Iteration 2155, loss = 1.86669124\n",
      "Iteration 2156, loss = 1.86998560\n",
      "Iteration 2157, loss = 1.86630930\n",
      "Iteration 2158, loss = 1.86768574\n",
      "Iteration 2159, loss = 1.86849043\n",
      "Iteration 2160, loss = 1.86736150\n",
      "Iteration 2161, loss = 1.86465090\n",
      "Iteration 2162, loss = 1.86805032\n",
      "Iteration 2163, loss = 1.86663531\n",
      "Iteration 2164, loss = 1.86539276\n",
      "Iteration 2165, loss = 1.86611030\n",
      "Iteration 2166, loss = 1.86477261\n",
      "Iteration 2167, loss = 1.86568103\n",
      "Iteration 2168, loss = 1.86490442\n",
      "Iteration 2169, loss = 1.86656507\n",
      "Iteration 2170, loss = 1.86528783\n",
      "Iteration 2171, loss = 1.86432771\n",
      "Iteration 2172, loss = 1.86422314\n",
      "Iteration 2173, loss = 1.86348857\n",
      "Iteration 2174, loss = 1.86392047\n",
      "Iteration 2175, loss = 1.86434316\n",
      "Iteration 2176, loss = 1.86354174\n",
      "Iteration 2177, loss = 1.86331600\n",
      "Iteration 2178, loss = 1.86411623\n",
      "Iteration 2179, loss = 1.86351502\n",
      "Iteration 2180, loss = 1.86334258\n",
      "Iteration 2181, loss = 1.86119274\n",
      "Iteration 2182, loss = 1.86104714\n",
      "Iteration 2183, loss = 1.86186962\n",
      "Iteration 2184, loss = 1.86250437\n",
      "Iteration 2185, loss = 1.86171802\n",
      "Iteration 2186, loss = 1.85980369\n",
      "Iteration 2187, loss = 1.86032478\n",
      "Iteration 2188, loss = 1.86048332\n",
      "Iteration 2189, loss = 1.86004083\n",
      "Iteration 2190, loss = 1.85907394\n",
      "Iteration 2191, loss = 1.86020909\n",
      "Iteration 2192, loss = 1.85900052\n",
      "Iteration 2193, loss = 1.85764147\n",
      "Iteration 2194, loss = 1.85797193\n",
      "Iteration 2195, loss = 1.85872241\n",
      "Iteration 2196, loss = 1.85657287\n",
      "Iteration 2197, loss = 1.86034257\n",
      "Iteration 2198, loss = 1.85786837\n",
      "Iteration 2199, loss = 1.85645553\n",
      "Iteration 2200, loss = 1.85755388\n",
      "Iteration 2201, loss = 1.85585594\n",
      "Iteration 2202, loss = 1.85534683\n",
      "Iteration 2203, loss = 1.85526334\n",
      "Iteration 2204, loss = 1.85469439\n",
      "Iteration 2205, loss = 1.85489230\n",
      "Iteration 2206, loss = 1.85589627\n",
      "Iteration 2207, loss = 1.85480068\n",
      "Iteration 2208, loss = 1.85433558\n",
      "Iteration 2209, loss = 1.85681901\n",
      "Iteration 2210, loss = 1.85406193\n",
      "Iteration 2211, loss = 1.85537712\n",
      "Iteration 2212, loss = 1.85367769\n",
      "Iteration 2213, loss = 1.85416603\n",
      "Iteration 2214, loss = 1.85696950\n",
      "Iteration 2215, loss = 1.85756130\n",
      "Iteration 2216, loss = 1.85286243\n",
      "Iteration 2217, loss = 1.85204223\n",
      "Iteration 2218, loss = 1.85377263\n",
      "Iteration 2219, loss = 1.85769868\n",
      "Iteration 2220, loss = 1.85266839\n",
      "Iteration 2221, loss = 1.85382138\n",
      "Iteration 2222, loss = 1.85240667\n",
      "Iteration 2223, loss = 1.85079938\n",
      "Iteration 2224, loss = 1.85090374\n",
      "Iteration 2225, loss = 1.85130823\n",
      "Iteration 2226, loss = 1.85132058\n",
      "Iteration 2227, loss = 1.84991394\n",
      "Iteration 2228, loss = 1.85057844\n",
      "Iteration 2229, loss = 1.85035138\n",
      "Iteration 2230, loss = 1.84948734\n",
      "Iteration 2231, loss = 1.85075100\n",
      "Iteration 2232, loss = 1.84976688\n",
      "Iteration 2233, loss = 1.84905020\n",
      "Iteration 2234, loss = 1.84813481\n",
      "Iteration 2235, loss = 1.84864753\n",
      "Iteration 2236, loss = 1.84784435\n",
      "Iteration 2237, loss = 1.84706376\n",
      "Iteration 2238, loss = 1.84610407\n",
      "Iteration 2239, loss = 1.84850542\n",
      "Iteration 2240, loss = 1.84806340\n",
      "Iteration 2241, loss = 1.84666789\n",
      "Iteration 2242, loss = 1.84619113\n",
      "Iteration 2243, loss = 1.84594438\n",
      "Iteration 2244, loss = 1.84756292\n",
      "Iteration 2245, loss = 1.84944904\n",
      "Iteration 2246, loss = 1.84804148\n",
      "Iteration 2247, loss = 1.84455978\n",
      "Iteration 2248, loss = 1.84744425\n",
      "Iteration 2249, loss = 1.84752441\n",
      "Iteration 2250, loss = 1.84550989\n",
      "Iteration 2251, loss = 1.84525341\n",
      "Iteration 2252, loss = 1.84514437\n",
      "Iteration 2253, loss = 1.84414934\n",
      "Iteration 2254, loss = 1.84486519\n",
      "Iteration 2255, loss = 1.84431779\n",
      "Iteration 2256, loss = 1.84331398\n",
      "Iteration 2257, loss = 1.84258504\n",
      "Iteration 2258, loss = 1.84366541\n",
      "Iteration 2259, loss = 1.84395074\n",
      "Iteration 2260, loss = 1.84240772\n",
      "Iteration 2261, loss = 1.84436424\n",
      "Iteration 2262, loss = 1.84228248\n",
      "Iteration 2263, loss = 1.84017421\n",
      "Iteration 2264, loss = 1.84193068\n",
      "Iteration 2265, loss = 1.84226533\n",
      "Iteration 2266, loss = 1.84134082\n",
      "Iteration 2267, loss = 1.84106499\n",
      "Iteration 2268, loss = 1.84137148\n",
      "Iteration 2269, loss = 1.84086591\n",
      "Iteration 2270, loss = 1.84170502\n",
      "Iteration 2271, loss = 1.84050050\n",
      "Iteration 2272, loss = 1.84030217\n",
      "Iteration 2273, loss = 1.83885149\n",
      "Iteration 2274, loss = 1.84001880\n",
      "Iteration 2275, loss = 1.83950239\n",
      "Iteration 2276, loss = 1.83789772\n",
      "Iteration 2277, loss = 1.83835113\n",
      "Iteration 2278, loss = 1.83878046\n",
      "Iteration 2279, loss = 1.83887678\n",
      "Iteration 2280, loss = 1.83719093\n",
      "Iteration 2281, loss = 1.83876965\n",
      "Iteration 2282, loss = 1.83637564\n",
      "Iteration 2283, loss = 1.83633909\n",
      "Iteration 2284, loss = 1.83885202\n",
      "Iteration 2285, loss = 1.83812690\n",
      "Iteration 2286, loss = 1.83945850\n",
      "Iteration 2287, loss = 1.83704161\n",
      "Iteration 2288, loss = 1.83618596\n",
      "Iteration 2289, loss = 1.83716232\n",
      "Iteration 2290, loss = 1.83346947\n",
      "Iteration 2291, loss = 1.83465118\n",
      "Iteration 2292, loss = 1.83733305\n",
      "Iteration 2293, loss = 1.83472055\n",
      "Iteration 2294, loss = 1.83344875\n",
      "Iteration 2295, loss = 1.83478232\n",
      "Iteration 2296, loss = 1.83373173\n",
      "Iteration 2297, loss = 1.83595875\n",
      "Iteration 2298, loss = 1.83446352\n",
      "Iteration 2299, loss = 1.83296563\n",
      "Iteration 2300, loss = 1.83365984\n",
      "Iteration 2301, loss = 1.83389141\n",
      "Iteration 2302, loss = 1.83344485\n",
      "Iteration 2303, loss = 1.83299051\n",
      "Iteration 2304, loss = 1.83187269\n",
      "Iteration 2305, loss = 1.83290147\n",
      "Iteration 2306, loss = 1.83383437\n",
      "Iteration 2307, loss = 1.83286906\n",
      "Iteration 2308, loss = 1.83227305\n",
      "Iteration 2309, loss = 1.83206428\n",
      "Iteration 2310, loss = 1.83099306\n",
      "Iteration 2311, loss = 1.83148704\n",
      "Iteration 2312, loss = 1.83080535\n",
      "Iteration 2313, loss = 1.82956464\n",
      "Iteration 2314, loss = 1.82963425\n",
      "Iteration 2315, loss = 1.82997312\n",
      "Iteration 2316, loss = 1.82934297\n",
      "Iteration 2317, loss = 1.83080238\n",
      "Iteration 2318, loss = 1.82974881\n",
      "Iteration 2319, loss = 1.82814539\n",
      "Iteration 2320, loss = 1.82886886\n",
      "Iteration 2321, loss = 1.82896911\n",
      "Iteration 2322, loss = 1.82827630\n",
      "Iteration 2323, loss = 1.82799667\n",
      "Iteration 2324, loss = 1.82844254\n",
      "Iteration 2325, loss = 1.82696334\n",
      "Iteration 2326, loss = 1.82786204\n",
      "Iteration 2327, loss = 1.82811290\n",
      "Iteration 2328, loss = 1.82870162\n",
      "Iteration 2329, loss = 1.82726333\n",
      "Iteration 2330, loss = 1.82641271\n",
      "Iteration 2331, loss = 1.82569713\n",
      "Iteration 2332, loss = 1.82488754\n",
      "Iteration 2333, loss = 1.82703358\n",
      "Iteration 2334, loss = 1.82635594\n",
      "Iteration 2335, loss = 1.82612321\n",
      "Iteration 2336, loss = 1.82586627\n",
      "Iteration 2337, loss = 1.82554874\n",
      "Iteration 2338, loss = 1.82369098\n",
      "Iteration 2339, loss = 1.82455792\n",
      "Iteration 2340, loss = 1.82400302\n",
      "Iteration 2341, loss = 1.82375152\n",
      "Iteration 2342, loss = 1.82364478\n",
      "Iteration 2343, loss = 1.82576274\n",
      "Iteration 2344, loss = 1.82223795\n",
      "Iteration 2345, loss = 1.82450597\n",
      "Iteration 2346, loss = 1.82333100\n",
      "Iteration 2347, loss = 1.82406770\n",
      "Iteration 2348, loss = 1.82299598\n",
      "Iteration 2349, loss = 1.82613005\n",
      "Iteration 2350, loss = 1.82257680\n",
      "Iteration 2351, loss = 1.82256851\n",
      "Iteration 2352, loss = 1.82236768\n",
      "Iteration 2353, loss = 1.82250684\n",
      "Iteration 2354, loss = 1.82006896\n",
      "Iteration 2355, loss = 1.82167914\n",
      "Iteration 2356, loss = 1.82018771\n",
      "Iteration 2357, loss = 1.82174572\n",
      "Iteration 2358, loss = 1.82123183\n",
      "Iteration 2359, loss = 1.81965201\n",
      "Iteration 2360, loss = 1.82144296\n",
      "Iteration 2361, loss = 1.82171479\n",
      "Iteration 2362, loss = 1.81889646\n",
      "Iteration 2363, loss = 1.81876867\n",
      "Iteration 2364, loss = 1.82084825\n",
      "Iteration 2365, loss = 1.81762714\n",
      "Iteration 2366, loss = 1.81900193\n",
      "Iteration 2367, loss = 1.81891926\n",
      "Iteration 2368, loss = 1.81803604\n",
      "Iteration 2369, loss = 1.81696400\n",
      "Iteration 2370, loss = 1.81778558\n",
      "Iteration 2371, loss = 1.81902136\n",
      "Iteration 2372, loss = 1.81778043\n",
      "Iteration 2373, loss = 1.81755547\n",
      "Iteration 2374, loss = 1.81636243\n",
      "Iteration 2375, loss = 1.81791056\n",
      "Iteration 2376, loss = 1.81754775\n",
      "Iteration 2377, loss = 1.81673196\n",
      "Iteration 2378, loss = 1.81571121\n",
      "Iteration 2379, loss = 1.81486674\n",
      "Iteration 2380, loss = 1.81695228\n",
      "Iteration 2381, loss = 1.81703172\n",
      "Iteration 2382, loss = 1.81574274\n",
      "Iteration 2383, loss = 1.81552612\n",
      "Iteration 2384, loss = 1.81466049\n",
      "Iteration 2385, loss = 1.81563302\n",
      "Iteration 2386, loss = 1.81548982\n",
      "Iteration 2387, loss = 1.81490907\n",
      "Iteration 2388, loss = 1.81708734\n",
      "Iteration 2389, loss = 1.81426396\n",
      "Iteration 2390, loss = 1.81715970\n",
      "Iteration 2391, loss = 1.81507019\n",
      "Iteration 2392, loss = 1.81237201\n",
      "Iteration 2393, loss = 1.81172943\n",
      "Iteration 2394, loss = 1.81187163\n",
      "Iteration 2395, loss = 1.81230352\n",
      "Iteration 2396, loss = 1.81096113\n",
      "Iteration 2397, loss = 1.81083917\n",
      "Iteration 2398, loss = 1.81279053\n",
      "Iteration 2399, loss = 1.81238510\n",
      "Iteration 2400, loss = 1.80993833\n",
      "Iteration 2401, loss = 1.81118529\n",
      "Iteration 2402, loss = 1.80965859\n",
      "Iteration 2403, loss = 1.81185381\n",
      "Iteration 2404, loss = 1.81071193\n",
      "Iteration 2405, loss = 1.81251724\n",
      "Iteration 2406, loss = 1.81130001\n",
      "Iteration 2407, loss = 1.81042457\n",
      "Iteration 2408, loss = 1.81047545\n",
      "Iteration 2409, loss = 1.80854277\n",
      "Iteration 2410, loss = 1.80903103\n",
      "Iteration 2411, loss = 1.80841293\n",
      "Iteration 2412, loss = 1.80910525\n",
      "Iteration 2413, loss = 1.81137714\n",
      "Iteration 2414, loss = 1.80990195\n",
      "Iteration 2415, loss = 1.81092303\n",
      "Iteration 2416, loss = 1.81151675\n",
      "Iteration 2417, loss = 1.80921999\n",
      "Iteration 2418, loss = 1.80663886\n",
      "Iteration 2419, loss = 1.80598233\n",
      "Iteration 2420, loss = 1.80741214\n",
      "Iteration 2421, loss = 1.80649976\n",
      "Iteration 2422, loss = 1.80553857\n",
      "Iteration 2423, loss = 1.80525578\n",
      "Iteration 2424, loss = 1.80697512\n",
      "Iteration 2425, loss = 1.80751602\n",
      "Iteration 2426, loss = 1.80662293\n",
      "Iteration 2427, loss = 1.80576202\n",
      "Iteration 2428, loss = 1.80490084\n",
      "Iteration 2429, loss = 1.80565181\n",
      "Iteration 2430, loss = 1.80610050\n",
      "Iteration 2431, loss = 1.80358506\n",
      "Iteration 2432, loss = 1.80673596\n",
      "Iteration 2433, loss = 1.80768036\n",
      "Iteration 2434, loss = 1.80674729\n",
      "Iteration 2435, loss = 1.80478938\n",
      "Iteration 2436, loss = 1.80480639\n",
      "Iteration 2437, loss = 1.80378988\n",
      "Iteration 2438, loss = 1.80280500\n",
      "Iteration 2439, loss = 1.80297932\n",
      "Iteration 2440, loss = 1.80172232\n",
      "Iteration 2441, loss = 1.80141971\n",
      "Iteration 2442, loss = 1.80220080\n",
      "Iteration 2443, loss = 1.80243042\n",
      "Iteration 2444, loss = 1.80180131\n",
      "Iteration 2445, loss = 1.80393938\n",
      "Iteration 2446, loss = 1.80110636\n",
      "Iteration 2447, loss = 1.79984164\n",
      "Iteration 2448, loss = 1.80144505\n",
      "Iteration 2449, loss = 1.80123293\n",
      "Iteration 2450, loss = 1.79965438\n",
      "Iteration 2451, loss = 1.79873027\n",
      "Iteration 2452, loss = 1.79924465\n",
      "Iteration 2453, loss = 1.80077643\n",
      "Iteration 2454, loss = 1.80073152\n",
      "Iteration 2455, loss = 1.80205261\n",
      "Iteration 2456, loss = 1.80069295\n",
      "Iteration 2457, loss = 1.79958070\n",
      "Iteration 2458, loss = 1.79865521\n",
      "Iteration 2459, loss = 1.80028219\n",
      "Iteration 2460, loss = 1.79706691\n",
      "Iteration 2461, loss = 1.79959676\n",
      "Iteration 2462, loss = 1.79637043\n",
      "Iteration 2463, loss = 1.79969265\n",
      "Iteration 2464, loss = 1.79893589\n",
      "Iteration 2465, loss = 1.80099292\n",
      "Iteration 2466, loss = 1.79747089\n",
      "Iteration 2467, loss = 1.79506982\n",
      "Iteration 2468, loss = 1.79755779\n",
      "Iteration 2469, loss = 1.79552706\n",
      "Iteration 2470, loss = 1.79607739\n",
      "Iteration 2471, loss = 1.79638118\n",
      "Iteration 2472, loss = 1.79671134\n",
      "Iteration 2473, loss = 1.79664511\n",
      "Iteration 2474, loss = 1.79576047\n",
      "Iteration 2475, loss = 1.79625517\n",
      "Iteration 2476, loss = 1.79457879\n",
      "Iteration 2477, loss = 1.79522812\n",
      "Iteration 2478, loss = 1.79460580\n",
      "Iteration 2479, loss = 1.79479737\n",
      "Iteration 2480, loss = 1.79340053\n",
      "Iteration 2481, loss = 1.79342800\n",
      "Iteration 2482, loss = 1.79367159\n",
      "Iteration 2483, loss = 1.79466129\n",
      "Iteration 2484, loss = 1.79382973\n",
      "Iteration 2485, loss = 1.79310221\n",
      "Iteration 2486, loss = 1.79367901\n",
      "Iteration 2487, loss = 1.79270380\n",
      "Iteration 2488, loss = 1.79148726\n",
      "Iteration 2489, loss = 1.79230958\n",
      "Iteration 2490, loss = 1.79315620\n",
      "Iteration 2491, loss = 1.79429297\n",
      "Iteration 2492, loss = 1.79374016\n",
      "Iteration 2493, loss = 1.79345236\n",
      "Iteration 2494, loss = 1.79109382\n",
      "Iteration 2495, loss = 1.79582794\n",
      "Iteration 2496, loss = 1.79205605\n",
      "Iteration 2497, loss = 1.79439174\n",
      "Iteration 2498, loss = 1.79101018\n",
      "Iteration 2499, loss = 1.78882634\n",
      "Iteration 2500, loss = 1.78979221\n",
      "Iteration 2501, loss = 1.78976055\n",
      "Iteration 2502, loss = 1.78958199\n",
      "Iteration 2503, loss = 1.78980386\n",
      "Iteration 2504, loss = 1.79071322\n",
      "Iteration 2505, loss = 1.78813131\n",
      "Iteration 2506, loss = 1.79024171\n",
      "Iteration 2507, loss = 1.78705543\n",
      "Iteration 2508, loss = 1.79043999\n",
      "Iteration 2509, loss = 1.79009748\n",
      "Iteration 2510, loss = 1.78839440\n",
      "Iteration 2511, loss = 1.78948453\n",
      "Iteration 2512, loss = 1.78826862\n",
      "Iteration 2513, loss = 1.78906024\n",
      "Iteration 2514, loss = 1.78512962\n",
      "Iteration 2515, loss = 1.78791753\n",
      "Iteration 2516, loss = 1.78621724\n",
      "Iteration 2517, loss = 1.78830660\n",
      "Iteration 2518, loss = 1.78678166\n",
      "Iteration 2519, loss = 1.78627780\n",
      "Iteration 2520, loss = 1.78558781\n",
      "Iteration 2521, loss = 1.78408262\n",
      "Iteration 2522, loss = 1.78572523\n",
      "Iteration 2523, loss = 1.78368106\n",
      "Iteration 2524, loss = 1.78458916\n",
      "Iteration 2525, loss = 1.78483086\n",
      "Iteration 2526, loss = 1.78725996\n",
      "Iteration 2527, loss = 1.78513280\n",
      "Iteration 2528, loss = 1.78512698\n",
      "Iteration 2529, loss = 1.78518140\n",
      "Iteration 2530, loss = 1.78398825\n",
      "Iteration 2531, loss = 1.78271726\n",
      "Iteration 2532, loss = 1.78247507\n",
      "Iteration 2533, loss = 1.78330199\n",
      "Iteration 2534, loss = 1.78309228\n",
      "Iteration 2535, loss = 1.78340196\n",
      "Iteration 2536, loss = 1.78417823\n",
      "Iteration 2537, loss = 1.78365907\n",
      "Iteration 2538, loss = 1.78279812\n",
      "Iteration 2539, loss = 1.78276608\n",
      "Iteration 2540, loss = 1.78100108\n",
      "Iteration 2541, loss = 1.78031184\n",
      "Iteration 2542, loss = 1.78116696\n",
      "Iteration 2543, loss = 1.78076042\n",
      "Iteration 2544, loss = 1.78056012\n",
      "Iteration 2545, loss = 1.78013877\n",
      "Iteration 2546, loss = 1.77874466\n",
      "Iteration 2547, loss = 1.78191114\n",
      "Iteration 2548, loss = 1.78042360\n",
      "Iteration 2549, loss = 1.78263969\n",
      "Iteration 2550, loss = 1.78261413\n",
      "Iteration 2551, loss = 1.78081632\n",
      "Iteration 2552, loss = 1.77900008\n",
      "Iteration 2553, loss = 1.77893126\n",
      "Iteration 2554, loss = 1.77923409\n",
      "Iteration 2555, loss = 1.77876344\n",
      "Iteration 2556, loss = 1.77996594\n",
      "Iteration 2557, loss = 1.77892408\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23328649\n",
      "Iteration 2, loss = 4.13422395\n",
      "Iteration 3, loss = 4.03913573\n",
      "Iteration 4, loss = 3.94531111\n",
      "Iteration 5, loss = 3.84628359\n",
      "Iteration 6, loss = 3.74061035\n",
      "Iteration 7, loss = 3.63171137\n",
      "Iteration 8, loss = 3.52292014\n",
      "Iteration 9, loss = 3.42232204\n",
      "Iteration 10, loss = 3.33818478\n",
      "Iteration 11, loss = 3.27951878\n",
      "Iteration 12, loss = 3.24360384\n",
      "Iteration 13, loss = 3.23112926\n",
      "Iteration 14, loss = 3.22674444\n",
      "Iteration 15, loss = 3.22457720\n",
      "Iteration 16, loss = 3.22268491\n",
      "Iteration 17, loss = 3.22145057\n",
      "Iteration 18, loss = 3.21899276\n",
      "Iteration 19, loss = 3.21755170\n",
      "Iteration 20, loss = 3.21635366\n",
      "Iteration 21, loss = 3.21586586\n",
      "Iteration 22, loss = 3.21465437\n",
      "Iteration 23, loss = 3.21374629\n",
      "Iteration 24, loss = 3.21216333\n",
      "Iteration 25, loss = 3.21033705\n",
      "Iteration 26, loss = 3.20939229\n",
      "Iteration 27, loss = 3.20825707\n",
      "Iteration 28, loss = 3.20695374\n",
      "Iteration 29, loss = 3.20499344\n",
      "Iteration 30, loss = 3.20442723\n",
      "Iteration 31, loss = 3.20336866\n",
      "Iteration 32, loss = 3.20226345\n",
      "Iteration 33, loss = 3.20134831\n",
      "Iteration 34, loss = 3.20097320\n",
      "Iteration 35, loss = 3.19979109\n",
      "Iteration 36, loss = 3.19810409\n",
      "Iteration 37, loss = 3.19678246\n",
      "Iteration 38, loss = 3.19518931\n",
      "Iteration 39, loss = 3.19367966\n",
      "Iteration 40, loss = 3.19276223\n",
      "Iteration 41, loss = 3.19176561\n",
      "Iteration 42, loss = 3.19127018\n",
      "Iteration 43, loss = 3.19018557\n",
      "Iteration 44, loss = 3.18884506\n",
      "Iteration 45, loss = 3.18689865\n",
      "Iteration 46, loss = 3.18470665\n",
      "Iteration 47, loss = 3.18341725\n",
      "Iteration 48, loss = 3.18217058\n",
      "Iteration 49, loss = 3.18165101\n",
      "Iteration 50, loss = 3.18060952\n",
      "Iteration 51, loss = 3.17917815\n",
      "Iteration 52, loss = 3.17778538\n",
      "Iteration 53, loss = 3.17716693\n",
      "Iteration 54, loss = 3.17580643\n",
      "Iteration 55, loss = 3.17463217\n",
      "Iteration 56, loss = 3.17263772\n",
      "Iteration 57, loss = 3.17325402\n",
      "Iteration 58, loss = 3.17155407\n",
      "Iteration 59, loss = 3.16974427\n",
      "Iteration 60, loss = 3.16781958\n",
      "Iteration 61, loss = 3.16634562\n",
      "Iteration 62, loss = 3.16533730\n",
      "Iteration 63, loss = 3.16415204\n",
      "Iteration 64, loss = 3.16275178\n",
      "Iteration 65, loss = 3.16020354\n",
      "Iteration 66, loss = 3.15873236\n",
      "Iteration 67, loss = 3.15803280\n",
      "Iteration 68, loss = 3.15619733\n",
      "Iteration 69, loss = 3.15455529\n",
      "Iteration 70, loss = 3.15339303\n",
      "Iteration 71, loss = 3.15263762\n",
      "Iteration 72, loss = 3.15145659\n",
      "Iteration 73, loss = 3.15059915\n",
      "Iteration 74, loss = 3.14900400\n",
      "Iteration 75, loss = 3.14730842\n",
      "Iteration 76, loss = 3.14677145\n",
      "Iteration 77, loss = 3.14540383\n",
      "Iteration 78, loss = 3.14324136\n",
      "Iteration 79, loss = 3.14154018\n",
      "Iteration 80, loss = 3.14126462\n",
      "Iteration 81, loss = 3.13939501\n",
      "Iteration 82, loss = 3.13902561\n",
      "Iteration 83, loss = 3.13750154\n",
      "Iteration 84, loss = 3.13488467\n",
      "Iteration 85, loss = 3.13314307\n",
      "Iteration 86, loss = 3.13246144\n",
      "Iteration 87, loss = 3.13014526\n",
      "Iteration 88, loss = 3.12948907\n",
      "Iteration 89, loss = 3.12846575\n",
      "Iteration 90, loss = 3.12664686\n",
      "Iteration 91, loss = 3.12538228\n",
      "Iteration 92, loss = 3.12401054\n",
      "Iteration 93, loss = 3.12186549\n",
      "Iteration 94, loss = 3.11970473\n",
      "Iteration 95, loss = 3.11904300\n",
      "Iteration 96, loss = 3.11727017\n",
      "Iteration 97, loss = 3.11560391\n",
      "Iteration 98, loss = 3.11450498\n",
      "Iteration 99, loss = 3.11397981\n",
      "Iteration 100, loss = 3.11213590\n",
      "Iteration 101, loss = 3.11108003\n",
      "Iteration 102, loss = 3.10952982\n",
      "Iteration 103, loss = 3.10806596\n",
      "Iteration 104, loss = 3.10648253\n",
      "Iteration 105, loss = 3.10598410\n",
      "Iteration 106, loss = 3.10509869\n",
      "Iteration 107, loss = 3.10282473\n",
      "Iteration 108, loss = 3.10228934\n",
      "Iteration 109, loss = 3.10127135\n",
      "Iteration 110, loss = 3.09968114\n",
      "Iteration 111, loss = 3.09709344\n",
      "Iteration 112, loss = 3.09559808\n",
      "Iteration 113, loss = 3.09510579\n",
      "Iteration 114, loss = 3.09370925\n",
      "Iteration 115, loss = 3.09217059\n",
      "Iteration 116, loss = 3.09113000\n",
      "Iteration 117, loss = 3.08957474\n",
      "Iteration 118, loss = 3.08789879\n",
      "Iteration 119, loss = 3.08594115\n",
      "Iteration 120, loss = 3.08511770\n",
      "Iteration 121, loss = 3.08390900\n",
      "Iteration 122, loss = 3.08173486\n",
      "Iteration 123, loss = 3.08007893\n",
      "Iteration 124, loss = 3.07875551\n",
      "Iteration 125, loss = 3.07761253\n",
      "Iteration 126, loss = 3.07587629\n",
      "Iteration 127, loss = 3.07454447\n",
      "Iteration 128, loss = 3.07427709\n",
      "Iteration 129, loss = 3.07201834\n",
      "Iteration 130, loss = 3.06986700\n",
      "Iteration 131, loss = 3.06848120\n",
      "Iteration 132, loss = 3.06751452\n",
      "Iteration 133, loss = 3.06625684\n",
      "Iteration 134, loss = 3.06459297\n",
      "Iteration 135, loss = 3.06500599\n",
      "Iteration 136, loss = 3.06259246\n",
      "Iteration 137, loss = 3.06051540\n",
      "Iteration 138, loss = 3.06014866\n",
      "Iteration 139, loss = 3.05800144\n",
      "Iteration 140, loss = 3.05676320\n",
      "Iteration 141, loss = 3.05598128\n",
      "Iteration 142, loss = 3.05352803\n",
      "Iteration 143, loss = 3.05224855\n",
      "Iteration 144, loss = 3.05082494\n",
      "Iteration 145, loss = 3.04872890\n",
      "Iteration 146, loss = 3.04753821\n",
      "Iteration 147, loss = 3.04652908\n",
      "Iteration 148, loss = 3.04503143\n",
      "Iteration 149, loss = 3.04459914\n",
      "Iteration 150, loss = 3.04344672\n",
      "Iteration 151, loss = 3.04183434\n",
      "Iteration 152, loss = 3.03999404\n",
      "Iteration 153, loss = 3.03924906\n",
      "Iteration 154, loss = 3.03806762\n",
      "Iteration 155, loss = 3.03689227\n",
      "Iteration 156, loss = 3.03537023\n",
      "Iteration 157, loss = 3.03365748\n",
      "Iteration 158, loss = 3.03181533\n",
      "Iteration 159, loss = 3.03039435\n",
      "Iteration 160, loss = 3.02969316\n",
      "Iteration 161, loss = 3.02760725\n",
      "Iteration 162, loss = 3.02571481\n",
      "Iteration 163, loss = 3.02436082\n",
      "Iteration 164, loss = 3.02349253\n",
      "Iteration 165, loss = 3.02292492\n",
      "Iteration 166, loss = 3.02121899\n",
      "Iteration 167, loss = 3.02115079\n",
      "Iteration 168, loss = 3.01828971\n",
      "Iteration 169, loss = 3.01791402\n",
      "Iteration 170, loss = 3.01535645\n",
      "Iteration 171, loss = 3.01376929\n",
      "Iteration 172, loss = 3.01381047\n",
      "Iteration 173, loss = 3.01127742\n",
      "Iteration 174, loss = 3.01002363\n",
      "Iteration 175, loss = 3.01025182\n",
      "Iteration 176, loss = 3.00804274\n",
      "Iteration 177, loss = 3.00652905\n",
      "Iteration 178, loss = 3.00486635\n",
      "Iteration 179, loss = 3.00356959\n",
      "Iteration 180, loss = 3.00351453\n",
      "Iteration 181, loss = 3.00088539\n",
      "Iteration 182, loss = 3.00095852\n",
      "Iteration 183, loss = 2.99941314\n",
      "Iteration 184, loss = 2.99733025\n",
      "Iteration 185, loss = 2.99579913\n",
      "Iteration 186, loss = 2.99467659\n",
      "Iteration 187, loss = 2.99515756\n",
      "Iteration 188, loss = 2.99241859\n",
      "Iteration 189, loss = 2.98996108\n",
      "Iteration 190, loss = 2.98901476\n",
      "Iteration 191, loss = 2.98826893\n",
      "Iteration 192, loss = 2.98707866\n",
      "Iteration 193, loss = 2.98617010\n",
      "Iteration 194, loss = 2.98612898\n",
      "Iteration 195, loss = 2.98412447\n",
      "Iteration 196, loss = 2.98227619\n",
      "Iteration 197, loss = 2.98103901\n",
      "Iteration 198, loss = 2.97984560\n",
      "Iteration 199, loss = 2.97834148\n",
      "Iteration 200, loss = 2.97792862\n",
      "Iteration 201, loss = 2.97658694\n",
      "Iteration 202, loss = 2.97484982\n",
      "Iteration 203, loss = 2.97355399\n",
      "Iteration 204, loss = 2.97269118\n",
      "Iteration 205, loss = 2.97270029\n",
      "Iteration 206, loss = 2.97138121\n",
      "Iteration 207, loss = 2.96773887\n",
      "Iteration 208, loss = 2.96656565\n",
      "Iteration 209, loss = 2.96605392\n",
      "Iteration 210, loss = 2.96502888\n",
      "Iteration 211, loss = 2.96224516\n",
      "Iteration 212, loss = 2.96123127\n",
      "Iteration 213, loss = 2.96002052\n",
      "Iteration 214, loss = 2.95917447\n",
      "Iteration 215, loss = 2.95808990\n",
      "Iteration 216, loss = 2.95616881\n",
      "Iteration 217, loss = 2.95578247\n",
      "Iteration 218, loss = 2.95329620\n",
      "Iteration 219, loss = 2.95184605\n",
      "Iteration 220, loss = 2.95108226\n",
      "Iteration 221, loss = 2.95047393\n",
      "Iteration 222, loss = 2.94940418\n",
      "Iteration 223, loss = 2.94811497\n",
      "Iteration 224, loss = 2.94888238\n",
      "Iteration 225, loss = 2.94803394\n",
      "Iteration 226, loss = 2.94370169\n",
      "Iteration 227, loss = 2.94344045\n",
      "Iteration 228, loss = 2.94225607\n",
      "Iteration 229, loss = 2.93973087\n",
      "Iteration 230, loss = 2.93864859\n",
      "Iteration 231, loss = 2.93870734\n",
      "Iteration 232, loss = 2.93733434\n",
      "Iteration 233, loss = 2.93572244\n",
      "Iteration 234, loss = 2.93409289\n",
      "Iteration 235, loss = 2.93439299\n",
      "Iteration 236, loss = 2.93232717\n",
      "Iteration 237, loss = 2.93271445\n",
      "Iteration 238, loss = 2.93111680\n",
      "Iteration 239, loss = 2.92873299\n",
      "Iteration 240, loss = 2.92840678\n",
      "Iteration 241, loss = 2.92694400\n",
      "Iteration 242, loss = 2.92595452\n",
      "Iteration 243, loss = 2.92414714\n",
      "Iteration 244, loss = 2.92310543\n",
      "Iteration 245, loss = 2.92143403\n",
      "Iteration 246, loss = 2.92075795\n",
      "Iteration 247, loss = 2.91913512\n",
      "Iteration 248, loss = 2.91866336\n",
      "Iteration 249, loss = 2.91844090\n",
      "Iteration 250, loss = 2.91648139\n",
      "Iteration 251, loss = 2.91521391\n",
      "Iteration 252, loss = 2.91374526\n",
      "Iteration 253, loss = 2.91286126\n",
      "Iteration 254, loss = 2.91291700\n",
      "Iteration 255, loss = 2.91101646\n",
      "Iteration 256, loss = 2.90948506\n",
      "Iteration 257, loss = 2.90857604\n",
      "Iteration 258, loss = 2.90848443\n",
      "Iteration 259, loss = 2.90695129\n",
      "Iteration 260, loss = 2.90364712\n",
      "Iteration 261, loss = 2.90439205\n",
      "Iteration 262, loss = 2.90445177\n",
      "Iteration 263, loss = 2.90245817\n",
      "Iteration 264, loss = 2.90042160\n",
      "Iteration 265, loss = 2.89866415\n",
      "Iteration 266, loss = 2.89764170\n",
      "Iteration 267, loss = 2.89600727\n",
      "Iteration 268, loss = 2.89519203\n",
      "Iteration 269, loss = 2.89517897\n",
      "Iteration 270, loss = 2.89281862\n",
      "Iteration 271, loss = 2.89226601\n",
      "Iteration 272, loss = 2.89093304\n",
      "Iteration 273, loss = 2.88990818\n",
      "Iteration 274, loss = 2.89162824\n",
      "Iteration 275, loss = 2.88898748\n",
      "Iteration 276, loss = 2.88682853\n",
      "Iteration 277, loss = 2.88505165\n",
      "Iteration 278, loss = 2.88538049\n",
      "Iteration 279, loss = 2.88446997\n",
      "Iteration 280, loss = 2.88172498\n",
      "Iteration 281, loss = 2.88096687\n",
      "Iteration 282, loss = 2.87920411\n",
      "Iteration 283, loss = 2.87735571\n",
      "Iteration 284, loss = 2.87752978\n",
      "Iteration 285, loss = 2.87618680\n",
      "Iteration 286, loss = 2.87515617\n",
      "Iteration 287, loss = 2.87474435\n",
      "Iteration 288, loss = 2.87416728\n",
      "Iteration 289, loss = 2.87160457\n",
      "Iteration 290, loss = 2.87037349\n",
      "Iteration 291, loss = 2.87002790\n",
      "Iteration 292, loss = 2.86904724\n",
      "Iteration 293, loss = 2.86797710\n",
      "Iteration 294, loss = 2.86694526\n",
      "Iteration 295, loss = 2.86552759\n",
      "Iteration 296, loss = 2.86447150\n",
      "Iteration 297, loss = 2.86325371\n",
      "Iteration 298, loss = 2.86177518\n",
      "Iteration 299, loss = 2.86096310\n",
      "Iteration 300, loss = 2.86012163\n",
      "Iteration 301, loss = 2.86042986\n",
      "Iteration 302, loss = 2.85864000\n",
      "Iteration 303, loss = 2.85991025\n",
      "Iteration 304, loss = 2.85725925\n",
      "Iteration 305, loss = 2.85447270\n",
      "Iteration 306, loss = 2.85368795\n",
      "Iteration 307, loss = 2.85236682\n",
      "Iteration 308, loss = 2.85190028\n",
      "Iteration 309, loss = 2.85086872\n",
      "Iteration 310, loss = 2.84922603\n",
      "Iteration 311, loss = 2.85211407\n",
      "Iteration 312, loss = 2.85000809\n",
      "Iteration 313, loss = 2.84882776\n",
      "Iteration 314, loss = 2.84637368\n",
      "Iteration 315, loss = 2.84483164\n",
      "Iteration 316, loss = 2.84399687\n",
      "Iteration 317, loss = 2.84342840\n",
      "Iteration 318, loss = 2.84283445\n",
      "Iteration 319, loss = 2.84030680\n",
      "Iteration 320, loss = 2.83922098\n",
      "Iteration 321, loss = 2.83804941\n",
      "Iteration 322, loss = 2.83748970\n",
      "Iteration 323, loss = 2.83629611\n",
      "Iteration 324, loss = 2.83500983\n",
      "Iteration 325, loss = 2.83330668\n",
      "Iteration 326, loss = 2.83274236\n",
      "Iteration 327, loss = 2.83309883\n",
      "Iteration 328, loss = 2.83103565\n",
      "Iteration 329, loss = 2.82985257\n",
      "Iteration 330, loss = 2.82866564\n",
      "Iteration 331, loss = 2.82796818\n",
      "Iteration 332, loss = 2.82578605\n",
      "Iteration 333, loss = 2.82506761\n",
      "Iteration 334, loss = 2.82425708\n",
      "Iteration 335, loss = 2.82508006\n",
      "Iteration 336, loss = 2.82361155\n",
      "Iteration 337, loss = 2.82238488\n",
      "Iteration 338, loss = 2.82100486\n",
      "Iteration 339, loss = 2.81941768\n",
      "Iteration 340, loss = 2.81863029\n",
      "Iteration 341, loss = 2.81721425\n",
      "Iteration 342, loss = 2.81712022\n",
      "Iteration 343, loss = 2.81573677\n",
      "Iteration 344, loss = 2.81406910\n",
      "Iteration 345, loss = 2.81336937\n",
      "Iteration 346, loss = 2.81339391\n",
      "Iteration 347, loss = 2.81323112\n",
      "Iteration 348, loss = 2.81151768\n",
      "Iteration 349, loss = 2.81078848\n",
      "Iteration 350, loss = 2.80830819\n",
      "Iteration 351, loss = 2.80786302\n",
      "Iteration 352, loss = 2.80606693\n",
      "Iteration 353, loss = 2.80530352\n",
      "Iteration 354, loss = 2.80423748\n",
      "Iteration 355, loss = 2.80374453\n",
      "Iteration 356, loss = 2.80314814\n",
      "Iteration 357, loss = 2.80124550\n",
      "Iteration 358, loss = 2.80211178\n",
      "Iteration 359, loss = 2.79922602\n",
      "Iteration 360, loss = 2.79899235\n",
      "Iteration 361, loss = 2.79784057\n",
      "Iteration 362, loss = 2.79663106\n",
      "Iteration 363, loss = 2.79455549\n",
      "Iteration 364, loss = 2.79448397\n",
      "Iteration 365, loss = 2.79440471\n",
      "Iteration 366, loss = 2.79252554\n",
      "Iteration 367, loss = 2.79081678\n",
      "Iteration 368, loss = 2.79136607\n",
      "Iteration 369, loss = 2.78995389\n",
      "Iteration 370, loss = 2.78968692\n",
      "Iteration 371, loss = 2.78768131\n",
      "Iteration 372, loss = 2.78712051\n",
      "Iteration 373, loss = 2.78635357\n",
      "Iteration 374, loss = 2.78394331\n",
      "Iteration 375, loss = 2.78358502\n",
      "Iteration 376, loss = 2.78311299\n",
      "Iteration 377, loss = 2.78082487\n",
      "Iteration 378, loss = 2.78288562\n",
      "Iteration 379, loss = 2.78081216\n",
      "Iteration 380, loss = 2.77860749\n",
      "Iteration 381, loss = 2.77810354\n",
      "Iteration 382, loss = 2.77753652\n",
      "Iteration 383, loss = 2.77615270\n",
      "Iteration 384, loss = 2.77612470\n",
      "Iteration 385, loss = 2.77411720\n",
      "Iteration 386, loss = 2.77334797\n",
      "Iteration 387, loss = 2.77206349\n",
      "Iteration 388, loss = 2.77027029\n",
      "Iteration 389, loss = 2.76997738\n",
      "Iteration 390, loss = 2.77020498\n",
      "Iteration 391, loss = 2.76851551\n",
      "Iteration 392, loss = 2.76653743\n",
      "Iteration 393, loss = 2.76634615\n",
      "Iteration 394, loss = 2.76514242\n",
      "Iteration 395, loss = 2.76394783\n",
      "Iteration 396, loss = 2.76291513\n",
      "Iteration 397, loss = 2.76309387\n",
      "Iteration 398, loss = 2.76240025\n",
      "Iteration 399, loss = 2.76122357\n",
      "Iteration 400, loss = 2.76195496\n",
      "Iteration 401, loss = 2.76109995\n",
      "Iteration 402, loss = 2.75778231\n",
      "Iteration 403, loss = 2.75788494\n",
      "Iteration 404, loss = 2.75609972\n",
      "Iteration 405, loss = 2.75588756\n",
      "Iteration 406, loss = 2.75464061\n",
      "Iteration 407, loss = 2.75302884\n",
      "Iteration 408, loss = 2.75196547\n",
      "Iteration 409, loss = 2.75123013\n",
      "Iteration 410, loss = 2.74951654\n",
      "Iteration 411, loss = 2.74868106\n",
      "Iteration 412, loss = 2.74789791\n",
      "Iteration 413, loss = 2.74766528\n",
      "Iteration 414, loss = 2.74651511\n",
      "Iteration 415, loss = 2.74541055\n",
      "Iteration 416, loss = 2.74454723\n",
      "Iteration 417, loss = 2.74433278\n",
      "Iteration 418, loss = 2.74367255\n",
      "Iteration 419, loss = 2.74187018\n",
      "Iteration 420, loss = 2.74062501\n",
      "Iteration 421, loss = 2.74018901\n",
      "Iteration 422, loss = 2.73847366\n",
      "Iteration 423, loss = 2.73723343\n",
      "Iteration 424, loss = 2.73670130\n",
      "Iteration 425, loss = 2.73659587\n",
      "Iteration 426, loss = 2.73472506\n",
      "Iteration 427, loss = 2.73346824\n",
      "Iteration 428, loss = 2.73317495\n",
      "Iteration 429, loss = 2.73162364\n",
      "Iteration 430, loss = 2.73097140\n",
      "Iteration 431, loss = 2.72992581\n",
      "Iteration 432, loss = 2.72975054\n",
      "Iteration 433, loss = 2.72880279\n",
      "Iteration 434, loss = 2.72706323\n",
      "Iteration 435, loss = 2.72556990\n",
      "Iteration 436, loss = 2.72610770\n",
      "Iteration 437, loss = 2.72471362\n",
      "Iteration 438, loss = 2.72330976\n",
      "Iteration 439, loss = 2.72242116\n",
      "Iteration 440, loss = 2.72217941\n",
      "Iteration 441, loss = 2.72052705\n",
      "Iteration 442, loss = 2.72023463\n",
      "Iteration 443, loss = 2.72067917\n",
      "Iteration 444, loss = 2.71888940\n",
      "Iteration 445, loss = 2.71674549\n",
      "Iteration 446, loss = 2.71540082\n",
      "Iteration 447, loss = 2.71445218\n",
      "Iteration 448, loss = 2.71340071\n",
      "Iteration 449, loss = 2.71297372\n",
      "Iteration 450, loss = 2.71216094\n",
      "Iteration 451, loss = 2.71142789\n",
      "Iteration 452, loss = 2.71015179\n",
      "Iteration 453, loss = 2.70914268\n",
      "Iteration 454, loss = 2.70913456\n",
      "Iteration 455, loss = 2.70804699\n",
      "Iteration 456, loss = 2.70740944\n",
      "Iteration 457, loss = 2.70789914\n",
      "Iteration 458, loss = 2.70548474\n",
      "Iteration 459, loss = 2.70379434\n",
      "Iteration 460, loss = 2.70404324\n",
      "Iteration 461, loss = 2.70259520\n",
      "Iteration 462, loss = 2.70153676\n",
      "Iteration 463, loss = 2.70128733\n",
      "Iteration 464, loss = 2.69961594\n",
      "Iteration 465, loss = 2.69897416\n",
      "Iteration 466, loss = 2.69822160\n",
      "Iteration 467, loss = 2.69739329\n",
      "Iteration 468, loss = 2.69628072\n",
      "Iteration 469, loss = 2.69526093\n",
      "Iteration 470, loss = 2.69399522\n",
      "Iteration 471, loss = 2.69443071\n",
      "Iteration 472, loss = 2.69368858\n",
      "Iteration 473, loss = 2.69187312\n",
      "Iteration 474, loss = 2.69067749\n",
      "Iteration 475, loss = 2.68954816\n",
      "Iteration 476, loss = 2.68904606\n",
      "Iteration 477, loss = 2.68819155\n",
      "Iteration 478, loss = 2.68729693\n",
      "Iteration 479, loss = 2.68667338\n",
      "Iteration 480, loss = 2.68600558\n",
      "Iteration 481, loss = 2.68521078\n",
      "Iteration 482, loss = 2.68415840\n",
      "Iteration 483, loss = 2.68281803\n",
      "Iteration 484, loss = 2.68183021\n",
      "Iteration 485, loss = 2.68126198\n",
      "Iteration 486, loss = 2.68064560\n",
      "Iteration 487, loss = 2.68020164\n",
      "Iteration 488, loss = 2.67826568\n",
      "Iteration 489, loss = 2.67753015\n",
      "Iteration 490, loss = 2.67624822\n",
      "Iteration 491, loss = 2.67522241\n",
      "Iteration 492, loss = 2.67471477\n",
      "Iteration 493, loss = 2.67494302\n",
      "Iteration 494, loss = 2.67564284\n",
      "Iteration 495, loss = 2.67239119\n",
      "Iteration 496, loss = 2.67137021\n",
      "Iteration 497, loss = 2.67063944\n",
      "Iteration 498, loss = 2.67049066\n",
      "Iteration 499, loss = 2.66916264\n",
      "Iteration 500, loss = 2.66765082\n",
      "Iteration 501, loss = 2.66755702\n",
      "Iteration 502, loss = 2.66712246\n",
      "Iteration 503, loss = 2.66577189\n",
      "Iteration 504, loss = 2.66518728\n",
      "Iteration 505, loss = 2.66479615\n",
      "Iteration 506, loss = 2.66408623\n",
      "Iteration 507, loss = 2.66311104\n",
      "Iteration 508, loss = 2.66164859\n",
      "Iteration 509, loss = 2.66063175\n",
      "Iteration 510, loss = 2.66032129\n",
      "Iteration 511, loss = 2.65883423\n",
      "Iteration 512, loss = 2.66061821\n",
      "Iteration 513, loss = 2.65771725\n",
      "Iteration 514, loss = 2.65803273\n",
      "Iteration 515, loss = 2.65645355\n",
      "Iteration 516, loss = 2.65496869\n",
      "Iteration 517, loss = 2.65369658\n",
      "Iteration 518, loss = 2.65371622\n",
      "Iteration 519, loss = 2.65222781\n",
      "Iteration 520, loss = 2.65113788\n",
      "Iteration 521, loss = 2.65198424\n",
      "Iteration 522, loss = 2.65038611\n",
      "Iteration 523, loss = 2.64937437\n",
      "Iteration 524, loss = 2.64949110\n",
      "Iteration 525, loss = 2.64754218\n",
      "Iteration 526, loss = 2.64677154\n",
      "Iteration 527, loss = 2.64579084\n",
      "Iteration 528, loss = 2.64466999\n",
      "Iteration 529, loss = 2.64436912\n",
      "Iteration 530, loss = 2.64542337\n",
      "Iteration 531, loss = 2.64286595\n",
      "Iteration 532, loss = 2.64367724\n",
      "Iteration 533, loss = 2.64054495\n",
      "Iteration 534, loss = 2.63932878\n",
      "Iteration 535, loss = 2.63942150\n",
      "Iteration 536, loss = 2.63776649\n",
      "Iteration 537, loss = 2.63686396\n",
      "Iteration 538, loss = 2.63575976\n",
      "Iteration 539, loss = 2.63532714\n",
      "Iteration 540, loss = 2.63459157\n",
      "Iteration 541, loss = 2.63390175\n",
      "Iteration 542, loss = 2.63495620\n",
      "Iteration 543, loss = 2.63480068\n",
      "Iteration 544, loss = 2.63294462\n",
      "Iteration 545, loss = 2.63086799\n",
      "Iteration 546, loss = 2.63020020\n",
      "Iteration 547, loss = 2.62802976\n",
      "Iteration 548, loss = 2.62822968\n",
      "Iteration 549, loss = 2.62725393\n",
      "Iteration 550, loss = 2.62692632\n",
      "Iteration 551, loss = 2.62706611\n",
      "Iteration 552, loss = 2.62577688\n",
      "Iteration 553, loss = 2.62493510\n",
      "Iteration 554, loss = 2.62404999\n",
      "Iteration 555, loss = 2.62235896\n",
      "Iteration 556, loss = 2.62228187\n",
      "Iteration 557, loss = 2.62170404\n",
      "Iteration 558, loss = 2.62094203\n",
      "Iteration 559, loss = 2.62051237\n",
      "Iteration 560, loss = 2.61993396\n",
      "Iteration 561, loss = 2.61782090\n",
      "Iteration 562, loss = 2.62053463\n",
      "Iteration 563, loss = 2.61789175\n",
      "Iteration 564, loss = 2.61587998\n",
      "Iteration 565, loss = 2.61516021\n",
      "Iteration 566, loss = 2.61456124\n",
      "Iteration 567, loss = 2.61283650\n",
      "Iteration 568, loss = 2.61243231\n",
      "Iteration 569, loss = 2.61165753\n",
      "Iteration 570, loss = 2.61017917\n",
      "Iteration 571, loss = 2.61054429\n",
      "Iteration 572, loss = 2.61056557\n",
      "Iteration 573, loss = 2.61043298\n",
      "Iteration 574, loss = 2.60805028\n",
      "Iteration 575, loss = 2.60756603\n",
      "Iteration 576, loss = 2.60660339\n",
      "Iteration 577, loss = 2.60405342\n",
      "Iteration 578, loss = 2.60504081\n",
      "Iteration 579, loss = 2.60385512\n",
      "Iteration 580, loss = 2.60258471\n",
      "Iteration 581, loss = 2.60285809\n",
      "Iteration 582, loss = 2.60157949\n",
      "Iteration 583, loss = 2.60116353\n",
      "Iteration 584, loss = 2.60127440\n",
      "Iteration 585, loss = 2.59928198\n",
      "Iteration 586, loss = 2.59858195\n",
      "Iteration 587, loss = 2.59893419\n",
      "Iteration 588, loss = 2.59641946\n",
      "Iteration 589, loss = 2.59967365\n",
      "Iteration 590, loss = 2.59675766\n",
      "Iteration 591, loss = 2.59535908\n",
      "Iteration 592, loss = 2.59357299\n",
      "Iteration 593, loss = 2.59319024\n",
      "Iteration 594, loss = 2.59206238\n",
      "Iteration 595, loss = 2.59071515\n",
      "Iteration 596, loss = 2.59017608\n",
      "Iteration 597, loss = 2.58999844\n",
      "Iteration 598, loss = 2.58979819\n",
      "Iteration 599, loss = 2.58733205\n",
      "Iteration 600, loss = 2.58856769\n",
      "Iteration 601, loss = 2.58796523\n",
      "Iteration 602, loss = 2.58603035\n",
      "Iteration 603, loss = 2.58589086\n",
      "Iteration 604, loss = 2.58447149\n",
      "Iteration 605, loss = 2.58273128\n",
      "Iteration 606, loss = 2.58421649\n",
      "Iteration 607, loss = 2.58227837\n",
      "Iteration 608, loss = 2.58213774\n",
      "Iteration 609, loss = 2.58155298\n",
      "Iteration 610, loss = 2.58012448\n",
      "Iteration 611, loss = 2.57926395\n",
      "Iteration 612, loss = 2.57854139\n",
      "Iteration 613, loss = 2.57660757\n",
      "Iteration 614, loss = 2.57739204\n",
      "Iteration 615, loss = 2.57745151\n",
      "Iteration 616, loss = 2.57513502\n",
      "Iteration 617, loss = 2.57554604\n",
      "Iteration 618, loss = 2.57340405\n",
      "Iteration 619, loss = 2.57316044\n",
      "Iteration 620, loss = 2.57354592\n",
      "Iteration 621, loss = 2.57278748\n",
      "Iteration 622, loss = 2.57118560\n",
      "Iteration 623, loss = 2.57109473\n",
      "Iteration 624, loss = 2.57096490\n",
      "Iteration 625, loss = 2.56930178\n",
      "Iteration 626, loss = 2.56802702\n",
      "Iteration 627, loss = 2.56778493\n",
      "Iteration 628, loss = 2.56776246\n",
      "Iteration 629, loss = 2.56684383\n",
      "Iteration 630, loss = 2.56590220\n",
      "Iteration 631, loss = 2.56491231\n",
      "Iteration 632, loss = 2.56386190\n",
      "Iteration 633, loss = 2.56283867\n",
      "Iteration 634, loss = 2.56179682\n",
      "Iteration 635, loss = 2.56118833\n",
      "Iteration 636, loss = 2.56106988\n",
      "Iteration 637, loss = 2.56073276\n",
      "Iteration 638, loss = 2.56003406\n",
      "Iteration 639, loss = 2.55921115\n",
      "Iteration 640, loss = 2.55789902\n",
      "Iteration 641, loss = 2.55670002\n",
      "Iteration 642, loss = 2.55584304\n",
      "Iteration 643, loss = 2.55537570\n",
      "Iteration 644, loss = 2.55558883\n",
      "Iteration 645, loss = 2.55501775\n",
      "Iteration 646, loss = 2.55383038\n",
      "Iteration 647, loss = 2.55381304\n",
      "Iteration 648, loss = 2.55176021\n",
      "Iteration 649, loss = 2.55159062\n",
      "Iteration 650, loss = 2.55074137\n",
      "Iteration 651, loss = 2.55131516\n",
      "Iteration 652, loss = 2.55028588\n",
      "Iteration 653, loss = 2.54814517\n",
      "Iteration 654, loss = 2.54673597\n",
      "Iteration 655, loss = 2.54655175\n",
      "Iteration 656, loss = 2.54592078\n",
      "Iteration 657, loss = 2.54491118\n",
      "Iteration 658, loss = 2.54397415\n",
      "Iteration 659, loss = 2.54432931\n",
      "Iteration 660, loss = 2.54359413\n",
      "Iteration 661, loss = 2.54378469\n",
      "Iteration 662, loss = 2.54355033\n",
      "Iteration 663, loss = 2.54232869\n",
      "Iteration 664, loss = 2.54148787\n",
      "Iteration 665, loss = 2.54050578\n",
      "Iteration 666, loss = 2.54177561\n",
      "Iteration 667, loss = 2.53946203\n",
      "Iteration 668, loss = 2.53770103\n",
      "Iteration 669, loss = 2.53871944\n",
      "Iteration 670, loss = 2.53814993\n",
      "Iteration 671, loss = 2.53617350\n",
      "Iteration 672, loss = 2.53428414\n",
      "Iteration 673, loss = 2.53547132\n",
      "Iteration 674, loss = 2.53469352\n",
      "Iteration 675, loss = 2.53275776\n",
      "Iteration 676, loss = 2.53206200\n",
      "Iteration 677, loss = 2.53257310\n",
      "Iteration 678, loss = 2.53167736\n",
      "Iteration 679, loss = 2.53063953\n",
      "Iteration 680, loss = 2.53018545\n",
      "Iteration 681, loss = 2.52881768\n",
      "Iteration 682, loss = 2.52870857\n",
      "Iteration 683, loss = 2.52904755\n",
      "Iteration 684, loss = 2.52804855\n",
      "Iteration 685, loss = 2.52644409\n",
      "Iteration 686, loss = 2.52544652\n",
      "Iteration 687, loss = 2.52600295\n",
      "Iteration 688, loss = 2.52555086\n",
      "Iteration 689, loss = 2.52479329\n",
      "Iteration 690, loss = 2.52338593\n",
      "Iteration 691, loss = 2.52143852\n",
      "Iteration 692, loss = 2.52225973\n",
      "Iteration 693, loss = 2.52128087\n",
      "Iteration 694, loss = 2.51998338\n",
      "Iteration 695, loss = 2.51953921\n",
      "Iteration 696, loss = 2.51896429\n",
      "Iteration 697, loss = 2.51808512\n",
      "Iteration 698, loss = 2.51561060\n",
      "Iteration 699, loss = 2.51581379\n",
      "Iteration 700, loss = 2.51495072\n",
      "Iteration 701, loss = 2.51395877\n",
      "Iteration 702, loss = 2.51371360\n",
      "Iteration 703, loss = 2.51289958\n",
      "Iteration 704, loss = 2.51361922\n",
      "Iteration 705, loss = 2.51385551\n",
      "Iteration 706, loss = 2.51163927\n",
      "Iteration 707, loss = 2.51172668\n",
      "Iteration 708, loss = 2.51069615\n",
      "Iteration 709, loss = 2.51126288\n",
      "Iteration 710, loss = 2.51071233\n",
      "Iteration 711, loss = 2.50918297\n",
      "Iteration 712, loss = 2.50707983\n",
      "Iteration 713, loss = 2.50650375\n",
      "Iteration 714, loss = 2.50693587\n",
      "Iteration 715, loss = 2.50608624\n",
      "Iteration 716, loss = 2.50500688\n",
      "Iteration 717, loss = 2.50388973\n",
      "Iteration 718, loss = 2.50258827\n",
      "Iteration 719, loss = 2.50186705\n",
      "Iteration 720, loss = 2.50282688\n",
      "Iteration 721, loss = 2.50130447\n",
      "Iteration 722, loss = 2.50101721\n",
      "Iteration 723, loss = 2.49954300\n",
      "Iteration 724, loss = 2.49964946\n",
      "Iteration 725, loss = 2.49866181\n",
      "Iteration 726, loss = 2.49840541\n",
      "Iteration 727, loss = 2.49898057\n",
      "Iteration 728, loss = 2.49646543\n",
      "Iteration 729, loss = 2.49672398\n",
      "Iteration 730, loss = 2.49698502\n",
      "Iteration 731, loss = 2.49597871\n",
      "Iteration 732, loss = 2.49475988\n",
      "Iteration 733, loss = 2.49342347\n",
      "Iteration 734, loss = 2.49273828\n",
      "Iteration 735, loss = 2.49211484\n",
      "Iteration 736, loss = 2.49190205\n",
      "Iteration 737, loss = 2.48959803\n",
      "Iteration 738, loss = 2.48929219\n",
      "Iteration 739, loss = 2.48732089\n",
      "Iteration 740, loss = 2.48822406\n",
      "Iteration 741, loss = 2.48799083\n",
      "Iteration 742, loss = 2.48634174\n",
      "Iteration 743, loss = 2.48577094\n",
      "Iteration 744, loss = 2.48686481\n",
      "Iteration 745, loss = 2.48569253\n",
      "Iteration 746, loss = 2.48397021\n",
      "Iteration 747, loss = 2.48403313\n",
      "Iteration 748, loss = 2.48287380\n",
      "Iteration 749, loss = 2.48252438\n",
      "Iteration 750, loss = 2.48289218\n",
      "Iteration 751, loss = 2.48219593\n",
      "Iteration 752, loss = 2.48200188\n",
      "Iteration 753, loss = 2.47993908\n",
      "Iteration 754, loss = 2.47875642\n",
      "Iteration 755, loss = 2.47792254\n",
      "Iteration 756, loss = 2.47721500\n",
      "Iteration 757, loss = 2.47805507\n",
      "Iteration 758, loss = 2.47618183\n",
      "Iteration 759, loss = 2.47795921\n",
      "Iteration 760, loss = 2.47684897\n",
      "Iteration 761, loss = 2.47472881\n",
      "Iteration 762, loss = 2.47413752\n",
      "Iteration 763, loss = 2.47391774\n",
      "Iteration 764, loss = 2.47403586\n",
      "Iteration 765, loss = 2.47171348\n",
      "Iteration 766, loss = 2.47245113\n",
      "Iteration 767, loss = 2.47102025\n",
      "Iteration 768, loss = 2.47068240\n",
      "Iteration 769, loss = 2.47235813\n",
      "Iteration 770, loss = 2.46837165\n",
      "Iteration 771, loss = 2.47081400\n",
      "Iteration 772, loss = 2.47000118\n",
      "Iteration 773, loss = 2.46731265\n",
      "Iteration 774, loss = 2.46969827\n",
      "Iteration 775, loss = 2.46752257\n",
      "Iteration 776, loss = 2.46482464\n",
      "Iteration 777, loss = 2.46452813\n",
      "Iteration 778, loss = 2.46367017\n",
      "Iteration 779, loss = 2.46251665\n",
      "Iteration 780, loss = 2.46207886\n",
      "Iteration 781, loss = 2.46098266\n",
      "Iteration 782, loss = 2.46088275\n",
      "Iteration 783, loss = 2.46065168\n",
      "Iteration 784, loss = 2.45921377\n",
      "Iteration 785, loss = 2.45852633\n",
      "Iteration 786, loss = 2.45866803\n",
      "Iteration 787, loss = 2.45814294\n",
      "Iteration 788, loss = 2.45797938\n",
      "Iteration 789, loss = 2.45673847\n",
      "Iteration 790, loss = 2.45687981\n",
      "Iteration 791, loss = 2.45613831\n",
      "Iteration 792, loss = 2.45442227\n",
      "Iteration 793, loss = 2.45434748\n",
      "Iteration 794, loss = 2.45424920\n",
      "Iteration 795, loss = 2.45369572\n",
      "Iteration 796, loss = 2.45335863\n",
      "Iteration 797, loss = 2.45071272\n",
      "Iteration 798, loss = 2.45113744\n",
      "Iteration 799, loss = 2.44919798\n",
      "Iteration 800, loss = 2.45127239\n",
      "Iteration 801, loss = 2.44920179\n",
      "Iteration 802, loss = 2.44894818\n",
      "Iteration 803, loss = 2.44767382\n",
      "Iteration 804, loss = 2.44684359\n",
      "Iteration 805, loss = 2.44720562\n",
      "Iteration 806, loss = 2.44604021\n",
      "Iteration 807, loss = 2.44689591\n",
      "Iteration 808, loss = 2.44547404\n",
      "Iteration 809, loss = 2.44371751\n",
      "Iteration 810, loss = 2.44316233\n",
      "Iteration 811, loss = 2.44436454\n",
      "Iteration 812, loss = 2.44185407\n",
      "Iteration 813, loss = 2.44280143\n",
      "Iteration 814, loss = 2.44144949\n",
      "Iteration 815, loss = 2.44062830\n",
      "Iteration 816, loss = 2.43949665\n",
      "Iteration 817, loss = 2.43984952\n",
      "Iteration 818, loss = 2.43806792\n",
      "Iteration 819, loss = 2.43718379\n",
      "Iteration 820, loss = 2.43685376\n",
      "Iteration 821, loss = 2.43658553\n",
      "Iteration 822, loss = 2.43568918\n",
      "Iteration 823, loss = 2.43660336\n",
      "Iteration 824, loss = 2.43589929\n",
      "Iteration 825, loss = 2.43340055\n",
      "Iteration 826, loss = 2.43456176\n",
      "Iteration 827, loss = 2.43308632\n",
      "Iteration 828, loss = 2.43319802\n",
      "Iteration 829, loss = 2.43224501\n",
      "Iteration 830, loss = 2.43018255\n",
      "Iteration 831, loss = 2.43070414\n",
      "Iteration 832, loss = 2.43121086\n",
      "Iteration 833, loss = 2.42985636\n",
      "Iteration 834, loss = 2.42920854\n",
      "Iteration 835, loss = 2.43001892\n",
      "Iteration 836, loss = 2.42818299\n",
      "Iteration 837, loss = 2.42759953\n",
      "Iteration 838, loss = 2.42641372\n",
      "Iteration 839, loss = 2.42605746\n",
      "Iteration 840, loss = 2.42588598\n",
      "Iteration 841, loss = 2.42448207\n",
      "Iteration 842, loss = 2.42278467\n",
      "Iteration 843, loss = 2.42577069\n",
      "Iteration 844, loss = 2.42482419\n",
      "Iteration 845, loss = 2.42161245\n",
      "Iteration 846, loss = 2.42219728\n",
      "Iteration 847, loss = 2.42099744\n",
      "Iteration 848, loss = 2.42039158\n",
      "Iteration 849, loss = 2.42048412\n",
      "Iteration 850, loss = 2.42060141\n",
      "Iteration 851, loss = 2.41930418\n",
      "Iteration 852, loss = 2.41808508\n",
      "Iteration 853, loss = 2.41653549\n",
      "Iteration 854, loss = 2.41582788\n",
      "Iteration 855, loss = 2.41499231\n",
      "Iteration 856, loss = 2.41512338\n",
      "Iteration 857, loss = 2.41388587\n",
      "Iteration 858, loss = 2.41403192\n",
      "Iteration 859, loss = 2.41467695\n",
      "Iteration 860, loss = 2.41297851\n",
      "Iteration 861, loss = 2.41237144\n",
      "Iteration 862, loss = 2.41129459\n",
      "Iteration 863, loss = 2.41526656\n",
      "Iteration 864, loss = 2.41264355\n",
      "Iteration 865, loss = 2.40981949\n",
      "Iteration 866, loss = 2.40994467\n",
      "Iteration 867, loss = 2.40954562\n",
      "Iteration 868, loss = 2.41005189\n",
      "Iteration 869, loss = 2.40761514\n",
      "Iteration 870, loss = 2.40724071\n",
      "Iteration 871, loss = 2.40733362\n",
      "Iteration 872, loss = 2.40614691\n",
      "Iteration 873, loss = 2.40565650\n",
      "Iteration 874, loss = 2.40530040\n",
      "Iteration 875, loss = 2.40588298\n",
      "Iteration 876, loss = 2.40465346\n",
      "Iteration 877, loss = 2.40314834\n",
      "Iteration 878, loss = 2.40206508\n",
      "Iteration 879, loss = 2.40227790\n",
      "Iteration 880, loss = 2.40252445\n",
      "Iteration 881, loss = 2.40152334\n",
      "Iteration 882, loss = 2.40043908\n",
      "Iteration 883, loss = 2.40108166\n",
      "Iteration 884, loss = 2.40117578\n",
      "Iteration 885, loss = 2.39809909\n",
      "Iteration 886, loss = 2.39940090\n",
      "Iteration 887, loss = 2.39734687\n",
      "Iteration 888, loss = 2.39680785\n",
      "Iteration 889, loss = 2.39793298\n",
      "Iteration 890, loss = 2.39692978\n",
      "Iteration 891, loss = 2.39645786\n",
      "Iteration 892, loss = 2.39575257\n",
      "Iteration 893, loss = 2.39434395\n",
      "Iteration 894, loss = 2.39671541\n",
      "Iteration 895, loss = 2.39394039\n",
      "Iteration 896, loss = 2.39488406\n",
      "Iteration 897, loss = 2.39394811\n",
      "Iteration 898, loss = 2.39198794\n",
      "Iteration 899, loss = 2.39114690\n",
      "Iteration 900, loss = 2.38934460\n",
      "Iteration 901, loss = 2.38936322\n",
      "Iteration 902, loss = 2.38906178\n",
      "Iteration 903, loss = 2.38988736\n",
      "Iteration 904, loss = 2.38838435\n",
      "Iteration 905, loss = 2.38693630\n",
      "Iteration 906, loss = 2.38799257\n",
      "Iteration 907, loss = 2.38701086\n",
      "Iteration 908, loss = 2.38576680\n",
      "Iteration 909, loss = 2.38620646\n",
      "Iteration 910, loss = 2.38657738\n",
      "Iteration 911, loss = 2.38598830\n",
      "Iteration 912, loss = 2.38403520\n",
      "Iteration 913, loss = 2.38358338\n",
      "Iteration 914, loss = 2.38217351\n",
      "Iteration 915, loss = 2.38567971\n",
      "Iteration 916, loss = 2.38355956\n",
      "Iteration 917, loss = 2.38217639\n",
      "Iteration 918, loss = 2.37999702\n",
      "Iteration 919, loss = 2.37788131\n",
      "Iteration 920, loss = 2.37879877\n",
      "Iteration 921, loss = 2.37954226\n",
      "Iteration 922, loss = 2.37841321\n",
      "Iteration 923, loss = 2.37794216\n",
      "Iteration 924, loss = 2.37765812\n",
      "Iteration 925, loss = 2.37670675\n",
      "Iteration 926, loss = 2.37421741\n",
      "Iteration 927, loss = 2.37498419\n",
      "Iteration 928, loss = 2.37502363\n",
      "Iteration 929, loss = 2.37581489\n",
      "Iteration 930, loss = 2.37414645\n",
      "Iteration 931, loss = 2.37207015\n",
      "Iteration 932, loss = 2.37332319\n",
      "Iteration 933, loss = 2.37237439\n",
      "Iteration 934, loss = 2.37131890\n",
      "Iteration 935, loss = 2.37122022\n",
      "Iteration 936, loss = 2.36946599\n",
      "Iteration 937, loss = 2.36894269\n",
      "Iteration 938, loss = 2.36801321\n",
      "Iteration 939, loss = 2.36976517\n",
      "Iteration 940, loss = 2.36885704\n",
      "Iteration 941, loss = 2.36674235\n",
      "Iteration 942, loss = 2.36722089\n",
      "Iteration 943, loss = 2.36556257\n",
      "Iteration 944, loss = 2.36656908\n",
      "Iteration 945, loss = 2.36439821\n",
      "Iteration 946, loss = 2.36574367\n",
      "Iteration 947, loss = 2.36420620\n",
      "Iteration 948, loss = 2.36423288\n",
      "Iteration 949, loss = 2.36398025\n",
      "Iteration 950, loss = 2.36494233\n",
      "Iteration 951, loss = 2.36332952\n",
      "Iteration 952, loss = 2.36281650\n",
      "Iteration 953, loss = 2.36032837\n",
      "Iteration 954, loss = 2.36073494\n",
      "Iteration 955, loss = 2.36018689\n",
      "Iteration 956, loss = 2.35966645\n",
      "Iteration 957, loss = 2.35811173\n",
      "Iteration 958, loss = 2.35781069\n",
      "Iteration 959, loss = 2.35968068\n",
      "Iteration 960, loss = 2.35657855\n",
      "Iteration 961, loss = 2.35965249\n",
      "Iteration 962, loss = 2.35655632\n",
      "Iteration 963, loss = 2.35515567\n",
      "Iteration 964, loss = 2.35511631\n",
      "Iteration 965, loss = 2.35494235\n",
      "Iteration 966, loss = 2.35442708\n",
      "Iteration 967, loss = 2.35347332\n",
      "Iteration 968, loss = 2.35227947\n",
      "Iteration 969, loss = 2.35221841\n",
      "Iteration 970, loss = 2.35242081\n",
      "Iteration 971, loss = 2.35090371\n",
      "Iteration 972, loss = 2.34985429\n",
      "Iteration 973, loss = 2.34924081\n",
      "Iteration 974, loss = 2.34986155\n",
      "Iteration 975, loss = 2.34894067\n",
      "Iteration 976, loss = 2.34833015\n",
      "Iteration 977, loss = 2.34821374\n",
      "Iteration 978, loss = 2.34814319\n",
      "Iteration 979, loss = 2.34696191\n",
      "Iteration 980, loss = 2.34598217\n",
      "Iteration 981, loss = 2.34661497\n",
      "Iteration 982, loss = 2.34820285\n",
      "Iteration 983, loss = 2.34441913\n",
      "Iteration 984, loss = 2.34569999\n",
      "Iteration 985, loss = 2.34427231\n",
      "Iteration 986, loss = 2.34389288\n",
      "Iteration 987, loss = 2.34377959\n",
      "Iteration 988, loss = 2.34264527\n",
      "Iteration 989, loss = 2.34241780\n",
      "Iteration 990, loss = 2.34149952\n",
      "Iteration 991, loss = 2.34287989\n",
      "Iteration 992, loss = 2.34114010\n",
      "Iteration 993, loss = 2.34058249\n",
      "Iteration 994, loss = 2.33991450\n",
      "Iteration 995, loss = 2.33907520\n",
      "Iteration 996, loss = 2.34029292\n",
      "Iteration 997, loss = 2.34101701\n",
      "Iteration 998, loss = 2.34180716\n",
      "Iteration 999, loss = 2.33922977\n",
      "Iteration 1000, loss = 2.33770327\n",
      "Iteration 1001, loss = 2.33595397\n",
      "Iteration 1002, loss = 2.33577742\n",
      "Iteration 1003, loss = 2.33490610\n",
      "Iteration 1004, loss = 2.33457645\n",
      "Iteration 1005, loss = 2.33427400\n",
      "Iteration 1006, loss = 2.33389388\n",
      "Iteration 1007, loss = 2.33422178\n",
      "Iteration 1008, loss = 2.33296620\n",
      "Iteration 1009, loss = 2.33177831\n",
      "Iteration 1010, loss = 2.33229520\n",
      "Iteration 1011, loss = 2.33019350\n",
      "Iteration 1012, loss = 2.33157253\n",
      "Iteration 1013, loss = 2.33041719\n",
      "Iteration 1014, loss = 2.33234627\n",
      "Iteration 1015, loss = 2.32981642\n",
      "Iteration 1016, loss = 2.32839984\n",
      "Iteration 1017, loss = 2.32743753\n",
      "Iteration 1018, loss = 2.32779695\n",
      "Iteration 1019, loss = 2.32645071\n",
      "Iteration 1020, loss = 2.32521462\n",
      "Iteration 1021, loss = 2.32581971\n",
      "Iteration 1022, loss = 2.32563854\n",
      "Iteration 1023, loss = 2.32576607\n",
      "Iteration 1024, loss = 2.32502939\n",
      "Iteration 1025, loss = 2.32514822\n",
      "Iteration 1026, loss = 2.32681892\n",
      "Iteration 1027, loss = 2.32501849\n",
      "Iteration 1028, loss = 2.32341492\n",
      "Iteration 1029, loss = 2.32264927\n",
      "Iteration 1030, loss = 2.32120231\n",
      "Iteration 1031, loss = 2.32048011\n",
      "Iteration 1032, loss = 2.31945576\n",
      "Iteration 1033, loss = 2.32040736\n",
      "Iteration 1034, loss = 2.31884061\n",
      "Iteration 1035, loss = 2.31834575\n",
      "Iteration 1036, loss = 2.31809310\n",
      "Iteration 1037, loss = 2.31764827\n",
      "Iteration 1038, loss = 2.31777882\n",
      "Iteration 1039, loss = 2.31735677\n",
      "Iteration 1040, loss = 2.31767682\n",
      "Iteration 1041, loss = 2.31788643\n",
      "Iteration 1042, loss = 2.31912800\n",
      "Iteration 1043, loss = 2.31605928\n",
      "Iteration 1044, loss = 2.31612804\n",
      "Iteration 1045, loss = 2.31458763\n",
      "Iteration 1046, loss = 2.31448215\n",
      "Iteration 1047, loss = 2.31284719\n",
      "Iteration 1048, loss = 2.31301822\n",
      "Iteration 1049, loss = 2.31362473\n",
      "Iteration 1050, loss = 2.31303184\n",
      "Iteration 1051, loss = 2.31211404\n",
      "Iteration 1052, loss = 2.31182796\n",
      "Iteration 1053, loss = 2.31079800\n",
      "Iteration 1054, loss = 2.31060142\n",
      "Iteration 1055, loss = 2.31039170\n",
      "Iteration 1056, loss = 2.30887576\n",
      "Iteration 1057, loss = 2.30883357\n",
      "Iteration 1058, loss = 2.30970845\n",
      "Iteration 1059, loss = 2.30851061\n",
      "Iteration 1060, loss = 2.30841923\n",
      "Iteration 1061, loss = 2.30900308\n",
      "Iteration 1062, loss = 2.30533092\n",
      "Iteration 1063, loss = 2.30667114\n",
      "Iteration 1064, loss = 2.30492047\n",
      "Iteration 1065, loss = 2.30488670\n",
      "Iteration 1066, loss = 2.30455604\n",
      "Iteration 1067, loss = 2.30530055\n",
      "Iteration 1068, loss = 2.30312267\n",
      "Iteration 1069, loss = 2.30279579\n",
      "Iteration 1070, loss = 2.30358359\n",
      "Iteration 1071, loss = 2.30190969\n",
      "Iteration 1072, loss = 2.30259924\n",
      "Iteration 1073, loss = 2.30169017\n",
      "Iteration 1074, loss = 2.30129382\n",
      "Iteration 1075, loss = 2.30095226\n",
      "Iteration 1076, loss = 2.30133420\n",
      "Iteration 1077, loss = 2.30197822\n",
      "Iteration 1078, loss = 2.29720701\n",
      "Iteration 1079, loss = 2.30034244\n",
      "Iteration 1080, loss = 2.29938634\n",
      "Iteration 1081, loss = 2.29740229\n",
      "Iteration 1082, loss = 2.29684302\n",
      "Iteration 1083, loss = 2.29704522\n",
      "Iteration 1084, loss = 2.29723944\n",
      "Iteration 1085, loss = 2.29654561\n",
      "Iteration 1086, loss = 2.29514538\n",
      "Iteration 1087, loss = 2.29323631\n",
      "Iteration 1088, loss = 2.29296019\n",
      "Iteration 1089, loss = 2.29211606\n",
      "Iteration 1090, loss = 2.29289471\n",
      "Iteration 1091, loss = 2.29230307\n",
      "Iteration 1092, loss = 2.29190362\n",
      "Iteration 1093, loss = 2.29165348\n",
      "Iteration 1094, loss = 2.29302328\n",
      "Iteration 1095, loss = 2.29224442\n",
      "Iteration 1096, loss = 2.29359872\n",
      "Iteration 1097, loss = 2.29010394\n",
      "Iteration 1098, loss = 2.28905804\n",
      "Iteration 1099, loss = 2.28779318\n",
      "Iteration 1100, loss = 2.28726153\n",
      "Iteration 1101, loss = 2.28854662\n",
      "Iteration 1102, loss = 2.28613264\n",
      "Iteration 1103, loss = 2.28743174\n",
      "Iteration 1104, loss = 2.28693918\n",
      "Iteration 1105, loss = 2.28620340\n",
      "Iteration 1106, loss = 2.28678285\n",
      "Iteration 1107, loss = 2.28544372\n",
      "Iteration 1108, loss = 2.28450819\n",
      "Iteration 1109, loss = 2.28555358\n",
      "Iteration 1110, loss = 2.28336807\n",
      "Iteration 1111, loss = 2.28590870\n",
      "Iteration 1112, loss = 2.28573367\n",
      "Iteration 1113, loss = 2.28305584\n",
      "Iteration 1114, loss = 2.28327539\n",
      "Iteration 1115, loss = 2.28203265\n",
      "Iteration 1116, loss = 2.27910840\n",
      "Iteration 1117, loss = 2.28290943\n",
      "Iteration 1118, loss = 2.28116182\n",
      "Iteration 1119, loss = 2.27926521\n",
      "Iteration 1120, loss = 2.27818851\n",
      "Iteration 1121, loss = 2.27779672\n",
      "Iteration 1122, loss = 2.27949580\n",
      "Iteration 1123, loss = 2.27773559\n",
      "Iteration 1124, loss = 2.27807313\n",
      "Iteration 1125, loss = 2.27810154\n",
      "Iteration 1126, loss = 2.27927545\n",
      "Iteration 1127, loss = 2.27737305\n",
      "Iteration 1128, loss = 2.27513670\n",
      "Iteration 1129, loss = 2.27507478\n",
      "Iteration 1130, loss = 2.27568502\n",
      "Iteration 1131, loss = 2.27451834\n",
      "Iteration 1132, loss = 2.27297517\n",
      "Iteration 1133, loss = 2.27376113\n",
      "Iteration 1134, loss = 2.27267272\n",
      "Iteration 1135, loss = 2.27129755\n",
      "Iteration 1136, loss = 2.27093723\n",
      "Iteration 1137, loss = 2.27099769\n",
      "Iteration 1138, loss = 2.27120116\n",
      "Iteration 1139, loss = 2.26959695\n",
      "Iteration 1140, loss = 2.26936853\n",
      "Iteration 1141, loss = 2.26886291\n",
      "Iteration 1142, loss = 2.26862428\n",
      "Iteration 1143, loss = 2.26796493\n",
      "Iteration 1144, loss = 2.26788466\n",
      "Iteration 1145, loss = 2.26807389\n",
      "Iteration 1146, loss = 2.26742842\n",
      "Iteration 1147, loss = 2.26686242\n",
      "Iteration 1148, loss = 2.26516583\n",
      "Iteration 1149, loss = 2.26545705\n",
      "Iteration 1150, loss = 2.26605978\n",
      "Iteration 1151, loss = 2.26521286\n",
      "Iteration 1152, loss = 2.26416496\n",
      "Iteration 1153, loss = 2.26462421\n",
      "Iteration 1154, loss = 2.26394281\n",
      "Iteration 1155, loss = 2.26397222\n",
      "Iteration 1156, loss = 2.26306760\n",
      "Iteration 1157, loss = 2.26200945\n",
      "Iteration 1158, loss = 2.26193537\n",
      "Iteration 1159, loss = 2.26114435\n",
      "Iteration 1160, loss = 2.26008291\n",
      "Iteration 1161, loss = 2.26331299\n",
      "Iteration 1162, loss = 2.25984000\n",
      "Iteration 1163, loss = 2.26120048\n",
      "Iteration 1164, loss = 2.25803650\n",
      "Iteration 1165, loss = 2.25951745\n",
      "Iteration 1166, loss = 2.25771428\n",
      "Iteration 1167, loss = 2.25813415\n",
      "Iteration 1168, loss = 2.25690857\n",
      "Iteration 1169, loss = 2.25605095\n",
      "Iteration 1170, loss = 2.25685847\n",
      "Iteration 1171, loss = 2.25551406\n",
      "Iteration 1172, loss = 2.25463151\n",
      "Iteration 1173, loss = 2.25554953\n",
      "Iteration 1174, loss = 2.25426608\n",
      "Iteration 1175, loss = 2.25566714\n",
      "Iteration 1176, loss = 2.25471122\n",
      "Iteration 1177, loss = 2.25340792\n",
      "Iteration 1178, loss = 2.25238616\n",
      "Iteration 1179, loss = 2.25154708\n",
      "Iteration 1180, loss = 2.25060420\n",
      "Iteration 1181, loss = 2.25078471\n",
      "Iteration 1182, loss = 2.24945952\n",
      "Iteration 1183, loss = 2.25088400\n",
      "Iteration 1184, loss = 2.25042568\n",
      "Iteration 1185, loss = 2.24948716\n",
      "Iteration 1186, loss = 2.24940860\n",
      "Iteration 1187, loss = 2.24933953\n",
      "Iteration 1188, loss = 2.24791586\n",
      "Iteration 1189, loss = 2.24915601\n",
      "Iteration 1190, loss = 2.24817298\n",
      "Iteration 1191, loss = 2.24748689\n",
      "Iteration 1192, loss = 2.24701220\n",
      "Iteration 1193, loss = 2.24630640\n",
      "Iteration 1194, loss = 2.24675890\n",
      "Iteration 1195, loss = 2.24663007\n",
      "Iteration 1196, loss = 2.24554596\n",
      "Iteration 1197, loss = 2.24366431\n",
      "Iteration 1198, loss = 2.24403403\n",
      "Iteration 1199, loss = 2.24486693\n",
      "Iteration 1200, loss = 2.24504974\n",
      "Iteration 1201, loss = 2.24550846\n",
      "Iteration 1202, loss = 2.24439613\n",
      "Iteration 1203, loss = 2.24351360\n",
      "Iteration 1204, loss = 2.24175831\n",
      "Iteration 1205, loss = 2.24160395\n",
      "Iteration 1206, loss = 2.24114350\n",
      "Iteration 1207, loss = 2.24040421\n",
      "Iteration 1208, loss = 2.24092332\n",
      "Iteration 1209, loss = 2.24143660\n",
      "Iteration 1210, loss = 2.23958438\n",
      "Iteration 1211, loss = 2.23797020\n",
      "Iteration 1212, loss = 2.23811234\n",
      "Iteration 1213, loss = 2.23892547\n",
      "Iteration 1214, loss = 2.23810854\n",
      "Iteration 1215, loss = 2.23684601\n",
      "Iteration 1216, loss = 2.23721564\n",
      "Iteration 1217, loss = 2.23698444\n",
      "Iteration 1218, loss = 2.23584468\n",
      "Iteration 1219, loss = 2.23560901\n",
      "Iteration 1220, loss = 2.23514229\n",
      "Iteration 1221, loss = 2.23571077\n",
      "Iteration 1222, loss = 2.23429926\n",
      "Iteration 1223, loss = 2.23345067\n",
      "Iteration 1224, loss = 2.23322602\n",
      "Iteration 1225, loss = 2.23445889\n",
      "Iteration 1226, loss = 2.23397921\n",
      "Iteration 1227, loss = 2.23283365\n",
      "Iteration 1228, loss = 2.23140888\n",
      "Iteration 1229, loss = 2.23110455\n",
      "Iteration 1230, loss = 2.23190672\n",
      "Iteration 1231, loss = 2.23000847\n",
      "Iteration 1232, loss = 2.22994003\n",
      "Iteration 1233, loss = 2.22907373\n",
      "Iteration 1234, loss = 2.22865850\n",
      "Iteration 1235, loss = 2.22842928\n",
      "Iteration 1236, loss = 2.22731729\n",
      "Iteration 1237, loss = 2.22714025\n",
      "Iteration 1238, loss = 2.22754826\n",
      "Iteration 1239, loss = 2.22632008\n",
      "Iteration 1240, loss = 2.22646545\n",
      "Iteration 1241, loss = 2.22687037\n",
      "Iteration 1242, loss = 2.22542093\n",
      "Iteration 1243, loss = 2.22529033\n",
      "Iteration 1244, loss = 2.22587895\n",
      "Iteration 1245, loss = 2.22530688\n",
      "Iteration 1246, loss = 2.22508467\n",
      "Iteration 1247, loss = 2.22354514\n",
      "Iteration 1248, loss = 2.22323649\n",
      "Iteration 1249, loss = 2.22208946\n",
      "Iteration 1250, loss = 2.22340760\n",
      "Iteration 1251, loss = 2.22219310\n",
      "Iteration 1252, loss = 2.22298820\n",
      "Iteration 1253, loss = 2.22230555\n",
      "Iteration 1254, loss = 2.22120817\n",
      "Iteration 1255, loss = 2.22093190\n",
      "Iteration 1256, loss = 2.22086317\n",
      "Iteration 1257, loss = 2.22093290\n",
      "Iteration 1258, loss = 2.22080885\n",
      "Iteration 1259, loss = 2.22111154\n",
      "Iteration 1260, loss = 2.22016443\n",
      "Iteration 1261, loss = 2.21895929\n",
      "Iteration 1262, loss = 2.21821243\n",
      "Iteration 1263, loss = 2.21773167\n",
      "Iteration 1264, loss = 2.21711213\n",
      "Iteration 1265, loss = 2.21648596\n",
      "Iteration 1266, loss = 2.21538239\n",
      "Iteration 1267, loss = 2.21597782\n",
      "Iteration 1268, loss = 2.21625215\n",
      "Iteration 1269, loss = 2.21649700\n",
      "Iteration 1270, loss = 2.21435276\n",
      "Iteration 1271, loss = 2.21366005\n",
      "Iteration 1272, loss = 2.21375715\n",
      "Iteration 1273, loss = 2.21276807\n",
      "Iteration 1274, loss = 2.21337870\n",
      "Iteration 1275, loss = 2.21293221\n",
      "Iteration 1276, loss = 2.21193366\n",
      "Iteration 1277, loss = 2.21224399\n",
      "Iteration 1278, loss = 2.21155918\n",
      "Iteration 1279, loss = 2.21250029\n",
      "Iteration 1280, loss = 2.21131867\n",
      "Iteration 1281, loss = 2.21098249\n",
      "Iteration 1282, loss = 2.21150892\n",
      "Iteration 1283, loss = 2.20997347\n",
      "Iteration 1284, loss = 2.20960004\n",
      "Iteration 1285, loss = 2.20927179\n",
      "Iteration 1286, loss = 2.20852495\n",
      "Iteration 1287, loss = 2.20967563\n",
      "Iteration 1288, loss = 2.20796191\n",
      "Iteration 1289, loss = 2.20776825\n",
      "Iteration 1290, loss = 2.20551700\n",
      "Iteration 1291, loss = 2.20528508\n",
      "Iteration 1292, loss = 2.20568711\n",
      "Iteration 1293, loss = 2.20639225\n",
      "Iteration 1294, loss = 2.20609224\n",
      "Iteration 1295, loss = 2.20616645\n",
      "Iteration 1296, loss = 2.20569447\n",
      "Iteration 1297, loss = 2.20383372\n",
      "Iteration 1298, loss = 2.20361017\n",
      "Iteration 1299, loss = 2.20363169\n",
      "Iteration 1300, loss = 2.20222183\n",
      "Iteration 1301, loss = 2.20134754\n",
      "Iteration 1302, loss = 2.20146383\n",
      "Iteration 1303, loss = 2.20087353\n",
      "Iteration 1304, loss = 2.20168359\n",
      "Iteration 1305, loss = 2.20138055\n",
      "Iteration 1306, loss = 2.20073680\n",
      "Iteration 1307, loss = 2.19908775\n",
      "Iteration 1308, loss = 2.20043173\n",
      "Iteration 1309, loss = 2.19908237\n",
      "Iteration 1310, loss = 2.19935672\n",
      "Iteration 1311, loss = 2.19948117\n",
      "Iteration 1312, loss = 2.20005903\n",
      "Iteration 1313, loss = 2.19967889\n",
      "Iteration 1314, loss = 2.19802029\n",
      "Iteration 1315, loss = 2.19630683\n",
      "Iteration 1316, loss = 2.19627858\n",
      "Iteration 1317, loss = 2.19611419\n",
      "Iteration 1318, loss = 2.19627930\n",
      "Iteration 1319, loss = 2.19511433\n",
      "Iteration 1320, loss = 2.19443403\n",
      "Iteration 1321, loss = 2.19657639\n",
      "Iteration 1322, loss = 2.19547553\n",
      "Iteration 1323, loss = 2.19426393\n",
      "Iteration 1324, loss = 2.19487729\n",
      "Iteration 1325, loss = 2.19292240\n",
      "Iteration 1326, loss = 2.19310293\n",
      "Iteration 1327, loss = 2.19282220\n",
      "Iteration 1328, loss = 2.19211481\n",
      "Iteration 1329, loss = 2.19214129\n",
      "Iteration 1330, loss = 2.19063071\n",
      "Iteration 1331, loss = 2.19112459\n",
      "Iteration 1332, loss = 2.19181987\n",
      "Iteration 1333, loss = 2.19053710\n",
      "Iteration 1334, loss = 2.18992796\n",
      "Iteration 1335, loss = 2.18972732\n",
      "Iteration 1336, loss = 2.18887254\n",
      "Iteration 1337, loss = 2.18970666\n",
      "Iteration 1338, loss = 2.18863667\n",
      "Iteration 1339, loss = 2.18644283\n",
      "Iteration 1340, loss = 2.18735718\n",
      "Iteration 1341, loss = 2.18636231\n",
      "Iteration 1342, loss = 2.18633450\n",
      "Iteration 1343, loss = 2.18716330\n",
      "Iteration 1344, loss = 2.18672346\n",
      "Iteration 1345, loss = 2.18838785\n",
      "Iteration 1346, loss = 2.18467723\n",
      "Iteration 1347, loss = 2.18454829\n",
      "Iteration 1348, loss = 2.18449538\n",
      "Iteration 1349, loss = 2.18377456\n",
      "Iteration 1350, loss = 2.18472248\n",
      "Iteration 1351, loss = 2.18401790\n",
      "Iteration 1352, loss = 2.18305635\n",
      "Iteration 1353, loss = 2.18236245\n",
      "Iteration 1354, loss = 2.18203110\n",
      "Iteration 1355, loss = 2.18203376\n",
      "Iteration 1356, loss = 2.18196913\n",
      "Iteration 1357, loss = 2.18249463\n",
      "Iteration 1358, loss = 2.18004066\n",
      "Iteration 1359, loss = 2.18051575\n",
      "Iteration 1360, loss = 2.17982880\n",
      "Iteration 1361, loss = 2.17967661\n",
      "Iteration 1362, loss = 2.17950881\n",
      "Iteration 1363, loss = 2.17844815\n",
      "Iteration 1364, loss = 2.17860099\n",
      "Iteration 1365, loss = 2.17735052\n",
      "Iteration 1366, loss = 2.17773590\n",
      "Iteration 1367, loss = 2.17903810\n",
      "Iteration 1368, loss = 2.17695941\n",
      "Iteration 1369, loss = 2.17705926\n",
      "Iteration 1370, loss = 2.17676105\n",
      "Iteration 1371, loss = 2.17915388\n",
      "Iteration 1372, loss = 2.17551836\n",
      "Iteration 1373, loss = 2.17487761\n",
      "Iteration 1374, loss = 2.17464137\n",
      "Iteration 1375, loss = 2.17488013\n",
      "Iteration 1376, loss = 2.17381961\n",
      "Iteration 1377, loss = 2.17543891\n",
      "Iteration 1378, loss = 2.17264899\n",
      "Iteration 1379, loss = 2.17253465\n",
      "Iteration 1380, loss = 2.17251302\n",
      "Iteration 1381, loss = 2.17297238\n",
      "Iteration 1382, loss = 2.17324976\n",
      "Iteration 1383, loss = 2.17124027\n",
      "Iteration 1384, loss = 2.17144056\n",
      "Iteration 1385, loss = 2.17004199\n",
      "Iteration 1386, loss = 2.17025742\n",
      "Iteration 1387, loss = 2.16961641\n",
      "Iteration 1388, loss = 2.16900267\n",
      "Iteration 1389, loss = 2.16880149\n",
      "Iteration 1390, loss = 2.16925757\n",
      "Iteration 1391, loss = 2.16900116\n",
      "Iteration 1392, loss = 2.16758334\n",
      "Iteration 1393, loss = 2.16645580\n",
      "Iteration 1394, loss = 2.16716621\n",
      "Iteration 1395, loss = 2.16726520\n",
      "Iteration 1396, loss = 2.16556446\n",
      "Iteration 1397, loss = 2.16596019\n",
      "Iteration 1398, loss = 2.16525999\n",
      "Iteration 1399, loss = 2.16640274\n",
      "Iteration 1400, loss = 2.16490963\n",
      "Iteration 1401, loss = 2.16437120\n",
      "Iteration 1402, loss = 2.16496657\n",
      "Iteration 1403, loss = 2.16446158\n",
      "Iteration 1404, loss = 2.16470320\n",
      "Iteration 1405, loss = 2.16513170\n",
      "Iteration 1406, loss = 2.16270154\n",
      "Iteration 1407, loss = 2.16305547\n",
      "Iteration 1408, loss = 2.16285946\n",
      "Iteration 1409, loss = 2.16184342\n",
      "Iteration 1410, loss = 2.16318912\n",
      "Iteration 1411, loss = 2.16292741\n",
      "Iteration 1412, loss = 2.16100229\n",
      "Iteration 1413, loss = 2.16043658\n",
      "Iteration 1414, loss = 2.16005285\n",
      "Iteration 1415, loss = 2.16073165\n",
      "Iteration 1416, loss = 2.15953782\n",
      "Iteration 1417, loss = 2.15910696\n",
      "Iteration 1418, loss = 2.16010069\n",
      "Iteration 1419, loss = 2.15882258\n",
      "Iteration 1420, loss = 2.15924810\n",
      "Iteration 1421, loss = 2.15729333\n",
      "Iteration 1422, loss = 2.15790113\n",
      "Iteration 1423, loss = 2.15709153\n",
      "Iteration 1424, loss = 2.15571355\n",
      "Iteration 1425, loss = 2.15722376\n",
      "Iteration 1426, loss = 2.15565726\n",
      "Iteration 1427, loss = 2.15496341\n",
      "Iteration 1428, loss = 2.15405327\n",
      "Iteration 1429, loss = 2.15493392\n",
      "Iteration 1430, loss = 2.15599336\n",
      "Iteration 1431, loss = 2.15419789\n",
      "Iteration 1432, loss = 2.15589910\n",
      "Iteration 1433, loss = 2.15565986\n",
      "Iteration 1434, loss = 2.15272963\n",
      "Iteration 1435, loss = 2.15539736\n",
      "Iteration 1436, loss = 2.15172299\n",
      "Iteration 1437, loss = 2.15462406\n",
      "Iteration 1438, loss = 2.15105558\n",
      "Iteration 1439, loss = 2.15196468\n",
      "Iteration 1440, loss = 2.15178393\n",
      "Iteration 1441, loss = 2.15157132\n",
      "Iteration 1442, loss = 2.15037355\n",
      "Iteration 1443, loss = 2.15200666\n",
      "Iteration 1444, loss = 2.15024964\n",
      "Iteration 1445, loss = 2.14911947\n",
      "Iteration 1446, loss = 2.15031550\n",
      "Iteration 1447, loss = 2.15081503\n",
      "Iteration 1448, loss = 2.15100850\n",
      "Iteration 1449, loss = 2.15002323\n",
      "Iteration 1450, loss = 2.14780823\n",
      "Iteration 1451, loss = 2.14614664\n",
      "Iteration 1452, loss = 2.14666364\n",
      "Iteration 1453, loss = 2.14648242\n",
      "Iteration 1454, loss = 2.14668875\n",
      "Iteration 1455, loss = 2.14614969\n",
      "Iteration 1456, loss = 2.14544530\n",
      "Iteration 1457, loss = 2.14486954\n",
      "Iteration 1458, loss = 2.14442205\n",
      "Iteration 1459, loss = 2.14433880\n",
      "Iteration 1460, loss = 2.14504202\n",
      "Iteration 1461, loss = 2.14429577\n",
      "Iteration 1462, loss = 2.14367197\n",
      "Iteration 1463, loss = 2.14411582\n",
      "Iteration 1464, loss = 2.14346195\n",
      "Iteration 1465, loss = 2.14485639\n",
      "Iteration 1466, loss = 2.14483628\n",
      "Iteration 1467, loss = 2.14282613\n",
      "Iteration 1468, loss = 2.14221798\n",
      "Iteration 1469, loss = 2.14295479\n",
      "Iteration 1470, loss = 2.14142086\n",
      "Iteration 1471, loss = 2.13940216\n",
      "Iteration 1472, loss = 2.14024761\n",
      "Iteration 1473, loss = 2.13958850\n",
      "Iteration 1474, loss = 2.14339402\n",
      "Iteration 1475, loss = 2.14033057\n",
      "Iteration 1476, loss = 2.13989485\n",
      "Iteration 1477, loss = 2.13879199\n",
      "Iteration 1478, loss = 2.13872399\n",
      "Iteration 1479, loss = 2.13759727\n",
      "Iteration 1480, loss = 2.13772304\n",
      "Iteration 1481, loss = 2.13634615\n",
      "Iteration 1482, loss = 2.13665075\n",
      "Iteration 1483, loss = 2.13640490\n",
      "Iteration 1484, loss = 2.13447949\n",
      "Iteration 1485, loss = 2.13584875\n",
      "Iteration 1486, loss = 2.13719693\n",
      "Iteration 1487, loss = 2.13414178\n",
      "Iteration 1488, loss = 2.13437908\n",
      "Iteration 1489, loss = 2.13513125\n",
      "Iteration 1490, loss = 2.13473067\n",
      "Iteration 1491, loss = 2.13190255\n",
      "Iteration 1492, loss = 2.13191543\n",
      "Iteration 1493, loss = 2.13495229\n",
      "Iteration 1494, loss = 2.13295090\n",
      "Iteration 1495, loss = 2.13292443\n",
      "Iteration 1496, loss = 2.13256912\n",
      "Iteration 1497, loss = 2.13289605\n",
      "Iteration 1498, loss = 2.13343298\n",
      "Iteration 1499, loss = 2.13232419\n",
      "Iteration 1500, loss = 2.13168642\n",
      "Iteration 1501, loss = 2.13094106\n",
      "Iteration 1502, loss = 2.13109670\n",
      "Iteration 1503, loss = 2.13008292\n",
      "Iteration 1504, loss = 2.12989473\n",
      "Iteration 1505, loss = 2.12868319\n",
      "Iteration 1506, loss = 2.12971106\n",
      "Iteration 1507, loss = 2.12800663\n",
      "Iteration 1508, loss = 2.12695448\n",
      "Iteration 1509, loss = 2.12731386\n",
      "Iteration 1510, loss = 2.12726570\n",
      "Iteration 1511, loss = 2.12766486\n",
      "Iteration 1512, loss = 2.12761854\n",
      "Iteration 1513, loss = 2.12731857\n",
      "Iteration 1514, loss = 2.12633687\n",
      "Iteration 1515, loss = 2.12521116\n",
      "Iteration 1516, loss = 2.12593142\n",
      "Iteration 1517, loss = 2.12692537\n",
      "Iteration 1518, loss = 2.12440262\n",
      "Iteration 1519, loss = 2.12566889\n",
      "Iteration 1520, loss = 2.12664099\n",
      "Iteration 1521, loss = 2.12629409\n",
      "Iteration 1522, loss = 2.12443915\n",
      "Iteration 1523, loss = 2.12488813\n",
      "Iteration 1524, loss = 2.12407594\n",
      "Iteration 1525, loss = 2.12364382\n",
      "Iteration 1526, loss = 2.12261956\n",
      "Iteration 1527, loss = 2.12297357\n",
      "Iteration 1528, loss = 2.12237220\n",
      "Iteration 1529, loss = 2.12290033\n",
      "Iteration 1530, loss = 2.12229269\n",
      "Iteration 1531, loss = 2.12031667\n",
      "Iteration 1532, loss = 2.12045532\n",
      "Iteration 1533, loss = 2.12004811\n",
      "Iteration 1534, loss = 2.12002860\n",
      "Iteration 1535, loss = 2.11953075\n",
      "Iteration 1536, loss = 2.11905097\n",
      "Iteration 1537, loss = 2.11823445\n",
      "Iteration 1538, loss = 2.11983234\n",
      "Iteration 1539, loss = 2.11852392\n",
      "Iteration 1540, loss = 2.11906738\n",
      "Iteration 1541, loss = 2.11829259\n",
      "Iteration 1542, loss = 2.11826498\n",
      "Iteration 1543, loss = 2.11782189\n",
      "Iteration 1544, loss = 2.11628226\n",
      "Iteration 1545, loss = 2.11607107\n",
      "Iteration 1546, loss = 2.11705016\n",
      "Iteration 1547, loss = 2.11608404\n",
      "Iteration 1548, loss = 2.11605833\n",
      "Iteration 1549, loss = 2.11609548\n",
      "Iteration 1550, loss = 2.11521894\n",
      "Iteration 1551, loss = 2.11490638\n",
      "Iteration 1552, loss = 2.11346850\n",
      "Iteration 1553, loss = 2.11379131\n",
      "Iteration 1554, loss = 2.11414813\n",
      "Iteration 1555, loss = 2.11389255\n",
      "Iteration 1556, loss = 2.11307648\n",
      "Iteration 1557, loss = 2.11218882\n",
      "Iteration 1558, loss = 2.11274520\n",
      "Iteration 1559, loss = 2.11255955\n",
      "Iteration 1560, loss = 2.11245336\n",
      "Iteration 1561, loss = 2.11299404\n",
      "Iteration 1562, loss = 2.11082092\n",
      "Iteration 1563, loss = 2.11023621\n",
      "Iteration 1564, loss = 2.10917999\n",
      "Iteration 1565, loss = 2.11220751\n",
      "Iteration 1566, loss = 2.10948316\n",
      "Iteration 1567, loss = 2.11017758\n",
      "Iteration 1568, loss = 2.10964917\n",
      "Iteration 1569, loss = 2.10959084\n",
      "Iteration 1570, loss = 2.10981368\n",
      "Iteration 1571, loss = 2.11009581\n",
      "Iteration 1572, loss = 2.11026487\n",
      "Iteration 1573, loss = 2.10911825\n",
      "Iteration 1574, loss = 2.10876057\n",
      "Iteration 1575, loss = 2.10662834\n",
      "Iteration 1576, loss = 2.10793058\n",
      "Iteration 1577, loss = 2.10696044\n",
      "Iteration 1578, loss = 2.10879852\n",
      "Iteration 1579, loss = 2.10746917\n",
      "Iteration 1580, loss = 2.10814712\n",
      "Iteration 1581, loss = 2.10548683\n",
      "Iteration 1582, loss = 2.10375776\n",
      "Iteration 1583, loss = 2.10517729\n",
      "Iteration 1584, loss = 2.10397210\n",
      "Iteration 1585, loss = 2.10284112\n",
      "Iteration 1586, loss = 2.10289800\n",
      "Iteration 1587, loss = 2.10268646\n",
      "Iteration 1588, loss = 2.10132413\n",
      "Iteration 1589, loss = 2.10263396\n",
      "Iteration 1590, loss = 2.10405356\n",
      "Iteration 1591, loss = 2.10250564\n",
      "Iteration 1592, loss = 2.10110893\n",
      "Iteration 1593, loss = 2.10107691\n",
      "Iteration 1594, loss = 2.10128689\n",
      "Iteration 1595, loss = 2.10055181\n",
      "Iteration 1596, loss = 2.10153835\n",
      "Iteration 1597, loss = 2.10085689\n",
      "Iteration 1598, loss = 2.10233176\n",
      "Iteration 1599, loss = 2.10095503\n",
      "Iteration 1600, loss = 2.09820861\n",
      "Iteration 1601, loss = 2.09874224\n",
      "Iteration 1602, loss = 2.09904959\n",
      "Iteration 1603, loss = 2.09814137\n",
      "Iteration 1604, loss = 2.09846837\n",
      "Iteration 1605, loss = 2.09779352\n",
      "Iteration 1606, loss = 2.09700793\n",
      "Iteration 1607, loss = 2.09918598\n",
      "Iteration 1608, loss = 2.09733978\n",
      "Iteration 1609, loss = 2.09803888\n",
      "Iteration 1610, loss = 2.09674237\n",
      "Iteration 1611, loss = 2.09531457\n",
      "Iteration 1612, loss = 2.09548511\n",
      "Iteration 1613, loss = 2.09519976\n",
      "Iteration 1614, loss = 2.09431647\n",
      "Iteration 1615, loss = 2.09560715\n",
      "Iteration 1616, loss = 2.09772015\n",
      "Iteration 1617, loss = 2.09776004\n",
      "Iteration 1618, loss = 2.09473493\n",
      "Iteration 1619, loss = 2.09374613\n",
      "Iteration 1620, loss = 2.09485273\n",
      "Iteration 1621, loss = 2.09485978\n",
      "Iteration 1622, loss = 2.09538518\n",
      "Iteration 1623, loss = 2.09256169\n",
      "Iteration 1624, loss = 2.09314522\n",
      "Iteration 1625, loss = 2.09280988\n",
      "Iteration 1626, loss = 2.09177656\n",
      "Iteration 1627, loss = 2.09294855\n",
      "Iteration 1628, loss = 2.09210272\n",
      "Iteration 1629, loss = 2.09000428\n",
      "Iteration 1630, loss = 2.09039586\n",
      "Iteration 1631, loss = 2.09128619\n",
      "Iteration 1632, loss = 2.09135788\n",
      "Iteration 1633, loss = 2.09086256\n",
      "Iteration 1634, loss = 2.09149282\n",
      "Iteration 1635, loss = 2.08969343\n",
      "Iteration 1636, loss = 2.09048593\n",
      "Iteration 1637, loss = 2.08979951\n",
      "Iteration 1638, loss = 2.08973563\n",
      "Iteration 1639, loss = 2.08891340\n",
      "Iteration 1640, loss = 2.08687853\n",
      "Iteration 1641, loss = 2.08729182\n",
      "Iteration 1642, loss = 2.08848699\n",
      "Iteration 1643, loss = 2.08762025\n",
      "Iteration 1644, loss = 2.08728909\n",
      "Iteration 1645, loss = 2.08782530\n",
      "Iteration 1646, loss = 2.08650954\n",
      "Iteration 1647, loss = 2.08598028\n",
      "Iteration 1648, loss = 2.08523992\n",
      "Iteration 1649, loss = 2.08500209\n",
      "Iteration 1650, loss = 2.08386690\n",
      "Iteration 1651, loss = 2.08339248\n",
      "Iteration 1652, loss = 2.08340247\n",
      "Iteration 1653, loss = 2.08535306\n",
      "Iteration 1654, loss = 2.08480902\n",
      "Iteration 1655, loss = 2.08588724\n",
      "Iteration 1656, loss = 2.08320380\n",
      "Iteration 1657, loss = 2.08542941\n",
      "Iteration 1658, loss = 2.08468187\n",
      "Iteration 1659, loss = 2.08450326\n",
      "Iteration 1660, loss = 2.08470763\n",
      "Iteration 1661, loss = 2.08242500\n",
      "Iteration 1662, loss = 2.08139824\n",
      "Iteration 1663, loss = 2.08186763\n",
      "Iteration 1664, loss = 2.08121093\n",
      "Iteration 1665, loss = 2.07932636\n",
      "Iteration 1666, loss = 2.08065259\n",
      "Iteration 1667, loss = 2.08178801\n",
      "Iteration 1668, loss = 2.07932119\n",
      "Iteration 1669, loss = 2.07812633\n",
      "Iteration 1670, loss = 2.07771369\n",
      "Iteration 1671, loss = 2.07765425\n",
      "Iteration 1672, loss = 2.07820454\n",
      "Iteration 1673, loss = 2.07766976\n",
      "Iteration 1674, loss = 2.07559457\n",
      "Iteration 1675, loss = 2.07698214\n",
      "Iteration 1676, loss = 2.07625987\n",
      "Iteration 1677, loss = 2.07695822\n",
      "Iteration 1678, loss = 2.07655634\n",
      "Iteration 1679, loss = 2.07626383\n",
      "Iteration 1680, loss = 2.07807961\n",
      "Iteration 1681, loss = 2.07627465\n",
      "Iteration 1682, loss = 2.07522798\n",
      "Iteration 1683, loss = 2.07548311\n",
      "Iteration 1684, loss = 2.07384524\n",
      "Iteration 1685, loss = 2.07463717\n",
      "Iteration 1686, loss = 2.07306821\n",
      "Iteration 1687, loss = 2.07386548\n",
      "Iteration 1688, loss = 2.07494506\n",
      "Iteration 1689, loss = 2.07324299\n",
      "Iteration 1690, loss = 2.07345987\n",
      "Iteration 1691, loss = 2.07329084\n",
      "Iteration 1692, loss = 2.07298189\n",
      "Iteration 1693, loss = 2.07069660\n",
      "Iteration 1694, loss = 2.07424809\n",
      "Iteration 1695, loss = 2.07208326\n",
      "Iteration 1696, loss = 2.07279035\n",
      "Iteration 1697, loss = 2.07355554\n",
      "Iteration 1698, loss = 2.07473687\n",
      "Iteration 1699, loss = 2.07264298\n",
      "Iteration 1700, loss = 2.07078396\n",
      "Iteration 1701, loss = 2.07126767\n",
      "Iteration 1702, loss = 2.06954539\n",
      "Iteration 1703, loss = 2.06934373\n",
      "Iteration 1704, loss = 2.06835850\n",
      "Iteration 1705, loss = 2.06911159\n",
      "Iteration 1706, loss = 2.06977558\n",
      "Iteration 1707, loss = 2.06973784\n",
      "Iteration 1708, loss = 2.06829392\n",
      "Iteration 1709, loss = 2.06678165\n",
      "Iteration 1710, loss = 2.06686968\n",
      "Iteration 1711, loss = 2.06744771\n",
      "Iteration 1712, loss = 2.06968481\n",
      "Iteration 1713, loss = 2.06881071\n",
      "Iteration 1714, loss = 2.06752125\n",
      "Iteration 1715, loss = 2.06616833\n",
      "Iteration 1716, loss = 2.06497558\n",
      "Iteration 1717, loss = 2.06519309\n",
      "Iteration 1718, loss = 2.06412375\n",
      "Iteration 1719, loss = 2.06650993\n",
      "Iteration 1720, loss = 2.06491993\n",
      "Iteration 1721, loss = 2.06400319\n",
      "Iteration 1722, loss = 2.06259083\n",
      "Iteration 1723, loss = 2.06613874\n",
      "Iteration 1724, loss = 2.06337937\n",
      "Iteration 1725, loss = 2.06473740\n",
      "Iteration 1726, loss = 2.06181681\n",
      "Iteration 1727, loss = 2.06498924\n",
      "Iteration 1728, loss = 2.06416738\n",
      "Iteration 1729, loss = 2.06120923\n",
      "Iteration 1730, loss = 2.06533842\n",
      "Iteration 1731, loss = 2.06298679\n",
      "Iteration 1732, loss = 2.06299962\n",
      "Iteration 1733, loss = 2.06275004\n",
      "Iteration 1734, loss = 2.06075095\n",
      "Iteration 1735, loss = 2.06194635\n",
      "Iteration 1736, loss = 2.06022511\n",
      "Iteration 1737, loss = 2.06053772\n",
      "Iteration 1738, loss = 2.06034873\n",
      "Iteration 1739, loss = 2.05916411\n",
      "Iteration 1740, loss = 2.06016707\n",
      "Iteration 1741, loss = 2.06088719\n",
      "Iteration 1742, loss = 2.05738673\n",
      "Iteration 1743, loss = 2.05800933\n",
      "Iteration 1744, loss = 2.05798748\n",
      "Iteration 1745, loss = 2.05829260\n",
      "Iteration 1746, loss = 2.05696759\n",
      "Iteration 1747, loss = 2.05645936\n",
      "Iteration 1748, loss = 2.05679850\n",
      "Iteration 1749, loss = 2.05928355\n",
      "Iteration 1750, loss = 2.05715335\n",
      "Iteration 1751, loss = 2.05540530\n",
      "Iteration 1752, loss = 2.05513902\n",
      "Iteration 1753, loss = 2.06036850\n",
      "Iteration 1754, loss = 2.05554471\n",
      "Iteration 1755, loss = 2.05641825\n",
      "Iteration 1756, loss = 2.05541242\n",
      "Iteration 1757, loss = 2.05526475\n",
      "Iteration 1758, loss = 2.05460874\n",
      "Iteration 1759, loss = 2.05420245\n",
      "Iteration 1760, loss = 2.05411868\n",
      "Iteration 1761, loss = 2.05531336\n",
      "Iteration 1762, loss = 2.05255666\n",
      "Iteration 1763, loss = 2.05371668\n",
      "Iteration 1764, loss = 2.05407569\n",
      "Iteration 1765, loss = 2.05299894\n",
      "Iteration 1766, loss = 2.05381277\n",
      "Iteration 1767, loss = 2.05275122\n",
      "Iteration 1768, loss = 2.05245055\n",
      "Iteration 1769, loss = 2.05245860\n",
      "Iteration 1770, loss = 2.05112051\n",
      "Iteration 1771, loss = 2.05059011\n",
      "Iteration 1772, loss = 2.05038968\n",
      "Iteration 1773, loss = 2.05158765\n",
      "Iteration 1774, loss = 2.05004305\n",
      "Iteration 1775, loss = 2.04832146\n",
      "Iteration 1776, loss = 2.04934179\n",
      "Iteration 1777, loss = 2.05024110\n",
      "Iteration 1778, loss = 2.04873904\n",
      "Iteration 1779, loss = 2.04804162\n",
      "Iteration 1780, loss = 2.04859848\n",
      "Iteration 1781, loss = 2.04850330\n",
      "Iteration 1782, loss = 2.04873402\n",
      "Iteration 1783, loss = 2.04772631\n",
      "Iteration 1784, loss = 2.04716451\n",
      "Iteration 1785, loss = 2.04790477\n",
      "Iteration 1786, loss = 2.04813699\n",
      "Iteration 1787, loss = 2.04781407\n",
      "Iteration 1788, loss = 2.04788463\n",
      "Iteration 1789, loss = 2.04629824\n",
      "Iteration 1790, loss = 2.04649164\n",
      "Iteration 1791, loss = 2.04399229\n",
      "Iteration 1792, loss = 2.04687515\n",
      "Iteration 1793, loss = 2.04650655\n",
      "Iteration 1794, loss = 2.04703012\n",
      "Iteration 1795, loss = 2.04615004\n",
      "Iteration 1796, loss = 2.04629040\n",
      "Iteration 1797, loss = 2.04501823\n",
      "Iteration 1798, loss = 2.04457286\n",
      "Iteration 1799, loss = 2.04402123\n",
      "Iteration 1800, loss = 2.04323611\n",
      "Iteration 1801, loss = 2.04363630\n",
      "Iteration 1802, loss = 2.04243181\n",
      "Iteration 1803, loss = 2.04281206\n",
      "Iteration 1804, loss = 2.04292586\n",
      "Iteration 1805, loss = 2.04095702\n",
      "Iteration 1806, loss = 2.04151694\n",
      "Iteration 1807, loss = 2.04100735\n",
      "Iteration 1808, loss = 2.04112634\n",
      "Iteration 1809, loss = 2.04054625\n",
      "Iteration 1810, loss = 2.04046888\n",
      "Iteration 1811, loss = 2.03991018\n",
      "Iteration 1812, loss = 2.03903873\n",
      "Iteration 1813, loss = 2.03971681\n",
      "Iteration 1814, loss = 2.03871019\n",
      "Iteration 1815, loss = 2.03984564\n",
      "Iteration 1816, loss = 2.03999555\n",
      "Iteration 1817, loss = 2.03814292\n",
      "Iteration 1818, loss = 2.03819399\n",
      "Iteration 1819, loss = 2.03844771\n",
      "Iteration 1820, loss = 2.04057686\n",
      "Iteration 1821, loss = 2.03985840\n",
      "Iteration 1822, loss = 2.03762390\n",
      "Iteration 1823, loss = 2.03803938\n",
      "Iteration 1824, loss = 2.03868364\n",
      "Iteration 1825, loss = 2.03771932\n",
      "Iteration 1826, loss = 2.03913497\n",
      "Iteration 1827, loss = 2.03981353\n",
      "Iteration 1828, loss = 2.03788367\n",
      "Iteration 1829, loss = 2.03557395\n",
      "Iteration 1830, loss = 2.03495730\n",
      "Iteration 1831, loss = 2.03587019\n",
      "Iteration 1832, loss = 2.03505496\n",
      "Iteration 1833, loss = 2.03577331\n",
      "Iteration 1834, loss = 2.03441678\n",
      "Iteration 1835, loss = 2.03214815\n",
      "Iteration 1836, loss = 2.03314464\n",
      "Iteration 1837, loss = 2.03336990\n",
      "Iteration 1838, loss = 2.03402190\n",
      "Iteration 1839, loss = 2.03591329\n",
      "Iteration 1840, loss = 2.03221221\n",
      "Iteration 1841, loss = 2.03189034\n",
      "Iteration 1842, loss = 2.03177421\n",
      "Iteration 1843, loss = 2.03246851\n",
      "Iteration 1844, loss = 2.03227156\n",
      "Iteration 1845, loss = 2.03154469\n",
      "Iteration 1846, loss = 2.03039537\n",
      "Iteration 1847, loss = 2.02991026\n",
      "Iteration 1848, loss = 2.03175406\n",
      "Iteration 1849, loss = 2.03032886\n",
      "Iteration 1850, loss = 2.03019519\n",
      "Iteration 1851, loss = 2.02975410\n",
      "Iteration 1852, loss = 2.02950663\n",
      "Iteration 1853, loss = 2.03110915\n",
      "Iteration 1854, loss = 2.02993044\n",
      "Iteration 1855, loss = 2.02886440\n",
      "Iteration 1856, loss = 2.02976792\n",
      "Iteration 1857, loss = 2.02910470\n",
      "Iteration 1858, loss = 2.03001020\n",
      "Iteration 1859, loss = 2.03074514\n",
      "Iteration 1860, loss = 2.03030338\n",
      "Iteration 1861, loss = 2.02977233\n",
      "Iteration 1862, loss = 2.02824961\n",
      "Iteration 1863, loss = 2.02546246\n",
      "Iteration 1864, loss = 2.02656107\n",
      "Iteration 1865, loss = 2.02720363\n",
      "Iteration 1866, loss = 2.02659221\n",
      "Iteration 1867, loss = 2.02611797\n",
      "Iteration 1868, loss = 2.02614210\n",
      "Iteration 1869, loss = 2.02606382\n",
      "Iteration 1870, loss = 2.02710909\n",
      "Iteration 1871, loss = 2.02735653\n",
      "Iteration 1872, loss = 2.02666244\n",
      "Iteration 1873, loss = 2.02734815\n",
      "Iteration 1874, loss = 2.02397911\n",
      "Iteration 1875, loss = 2.02334585\n",
      "Iteration 1876, loss = 2.02330180\n",
      "Iteration 1877, loss = 2.02384852\n",
      "Iteration 1878, loss = 2.02361644\n",
      "Iteration 1879, loss = 2.02304872\n",
      "Iteration 1880, loss = 2.02254506\n",
      "Iteration 1881, loss = 2.02416991\n",
      "Iteration 1882, loss = 2.02329045\n",
      "Iteration 1883, loss = 2.02462213\n",
      "Iteration 1884, loss = 2.02275169\n",
      "Iteration 1885, loss = 2.02256502\n",
      "Iteration 1886, loss = 2.02159139\n",
      "Iteration 1887, loss = 2.02365924\n",
      "Iteration 1888, loss = 2.02209770\n",
      "Iteration 1889, loss = 2.01989798\n",
      "Iteration 1890, loss = 2.01951132\n",
      "Iteration 1891, loss = 2.02011080\n",
      "Iteration 1892, loss = 2.01970981\n",
      "Iteration 1893, loss = 2.02040106\n",
      "Iteration 1894, loss = 2.01997907\n",
      "Iteration 1895, loss = 2.01892868\n",
      "Iteration 1896, loss = 2.01816421\n",
      "Iteration 1897, loss = 2.01854195\n",
      "Iteration 1898, loss = 2.01966190\n",
      "Iteration 1899, loss = 2.01841684\n",
      "Iteration 1900, loss = 2.01730804\n",
      "Iteration 1901, loss = 2.01757048\n",
      "Iteration 1902, loss = 2.01692039\n",
      "Iteration 1903, loss = 2.01725291\n",
      "Iteration 1904, loss = 2.01499875\n",
      "Iteration 1905, loss = 2.01823086\n",
      "Iteration 1906, loss = 2.01632995\n",
      "Iteration 1907, loss = 2.01533650\n",
      "Iteration 1908, loss = 2.01771235\n",
      "Iteration 1909, loss = 2.01711488\n",
      "Iteration 1910, loss = 2.01733349\n",
      "Iteration 1911, loss = 2.01491556\n",
      "Iteration 1912, loss = 2.01396846\n",
      "Iteration 1913, loss = 2.01551810\n",
      "Iteration 1914, loss = 2.01586722\n",
      "Iteration 1915, loss = 2.01392889\n",
      "Iteration 1916, loss = 2.01550613\n",
      "Iteration 1917, loss = 2.01674529\n",
      "Iteration 1918, loss = 2.01545864\n",
      "Iteration 1919, loss = 2.01399206\n",
      "Iteration 1920, loss = 2.01374187\n",
      "Iteration 1921, loss = 2.01413227\n",
      "Iteration 1922, loss = 2.01240533\n",
      "Iteration 1923, loss = 2.01145055\n",
      "Iteration 1924, loss = 2.01239280\n",
      "Iteration 1925, loss = 2.01234897\n",
      "Iteration 1926, loss = 2.01023729\n",
      "Iteration 1927, loss = 2.01158344\n",
      "Iteration 1928, loss = 2.01125183\n",
      "Iteration 1929, loss = 2.01292001\n",
      "Iteration 1930, loss = 2.01143562\n",
      "Iteration 1931, loss = 2.00959789\n",
      "Iteration 1932, loss = 2.01123673\n",
      "Iteration 1933, loss = 2.01009000\n",
      "Iteration 1934, loss = 2.00941375\n",
      "Iteration 1935, loss = 2.01015818\n",
      "Iteration 1936, loss = 2.00905549\n",
      "Iteration 1937, loss = 2.01015130\n",
      "Iteration 1938, loss = 2.00850991\n",
      "Iteration 1939, loss = 2.00950682\n",
      "Iteration 1940, loss = 2.00936806\n",
      "Iteration 1941, loss = 2.00998460\n",
      "Iteration 1942, loss = 2.00846091\n",
      "Iteration 1943, loss = 2.00843573\n",
      "Iteration 1944, loss = 2.00900320\n",
      "Iteration 1945, loss = 2.00709652\n",
      "Iteration 1946, loss = 2.00700437\n",
      "Iteration 1947, loss = 2.00620267\n",
      "Iteration 1948, loss = 2.00700707\n",
      "Iteration 1949, loss = 2.00594159\n",
      "Iteration 1950, loss = 2.00716155\n",
      "Iteration 1951, loss = 2.00566823\n",
      "Iteration 1952, loss = 2.00638477\n",
      "Iteration 1953, loss = 2.00540514\n",
      "Iteration 1954, loss = 2.00725130\n",
      "Iteration 1955, loss = 2.00527618\n",
      "Iteration 1956, loss = 2.00637034\n",
      "Iteration 1957, loss = 2.00460894\n",
      "Iteration 1958, loss = 2.00512826\n",
      "Iteration 1959, loss = 2.00436493\n",
      "Iteration 1960, loss = 2.00437545\n",
      "Iteration 1961, loss = 2.00362322\n",
      "Iteration 1962, loss = 2.00275496\n",
      "Iteration 1963, loss = 2.00332494\n",
      "Iteration 1964, loss = 2.00219435\n",
      "Iteration 1965, loss = 2.00276283\n",
      "Iteration 1966, loss = 2.00325148\n",
      "Iteration 1967, loss = 2.00465040\n",
      "Iteration 1968, loss = 2.00392097\n",
      "Iteration 1969, loss = 2.00164549\n",
      "Iteration 1970, loss = 2.00139370\n",
      "Iteration 1971, loss = 2.00220844\n",
      "Iteration 1972, loss = 2.00215249\n",
      "Iteration 1973, loss = 2.00267896\n",
      "Iteration 1974, loss = 2.00202451\n",
      "Iteration 1975, loss = 2.00036282\n",
      "Iteration 1976, loss = 1.99974354\n",
      "Iteration 1977, loss = 1.99931897\n",
      "Iteration 1978, loss = 1.99900688\n",
      "Iteration 1979, loss = 1.99934071\n",
      "Iteration 1980, loss = 1.99961340\n",
      "Iteration 1981, loss = 2.00000454\n",
      "Iteration 1982, loss = 1.99865207\n",
      "Iteration 1983, loss = 1.99799502\n",
      "Iteration 1984, loss = 2.00011737\n",
      "Iteration 1985, loss = 1.99977954\n",
      "Iteration 1986, loss = 1.99765535\n",
      "Iteration 1987, loss = 1.99829779\n",
      "Iteration 1988, loss = 1.99895913\n",
      "Iteration 1989, loss = 1.99764923\n",
      "Iteration 1990, loss = 1.99830310\n",
      "Iteration 1991, loss = 1.99647229\n",
      "Iteration 1992, loss = 1.99932836\n",
      "Iteration 1993, loss = 1.99584783\n",
      "Iteration 1994, loss = 1.99662382\n",
      "Iteration 1995, loss = 1.99519440\n",
      "Iteration 1996, loss = 1.99494858\n",
      "Iteration 1997, loss = 1.99701320\n",
      "Iteration 1998, loss = 1.99609256\n",
      "Iteration 1999, loss = 1.99675049\n",
      "Iteration 2000, loss = 1.99546296\n",
      "Iteration 2001, loss = 1.99602246\n",
      "Iteration 2002, loss = 1.99481194\n",
      "Iteration 2003, loss = 1.99361232\n",
      "Iteration 2004, loss = 1.99269772\n",
      "Iteration 2005, loss = 1.99372989\n",
      "Iteration 2006, loss = 1.99304601\n",
      "Iteration 2007, loss = 1.99284724\n",
      "Iteration 2008, loss = 1.99186991\n",
      "Iteration 2009, loss = 1.99334653\n",
      "Iteration 2010, loss = 1.99296016\n",
      "Iteration 2011, loss = 1.99315388\n",
      "Iteration 2012, loss = 1.99287266\n",
      "Iteration 2013, loss = 1.99159538\n",
      "Iteration 2014, loss = 1.99102044\n",
      "Iteration 2015, loss = 1.99079471\n",
      "Iteration 2016, loss = 1.99156854\n",
      "Iteration 2017, loss = 1.99091495\n",
      "Iteration 2018, loss = 1.99158783\n",
      "Iteration 2019, loss = 1.99127923\n",
      "Iteration 2020, loss = 1.99079875\n",
      "Iteration 2021, loss = 1.99054498\n",
      "Iteration 2022, loss = 1.98904228\n",
      "Iteration 2023, loss = 1.98943638\n",
      "Iteration 2024, loss = 1.98818274\n",
      "Iteration 2025, loss = 1.98801614\n",
      "Iteration 2026, loss = 1.98722699\n",
      "Iteration 2027, loss = 1.98733402\n",
      "Iteration 2028, loss = 1.98851600\n",
      "Iteration 2029, loss = 1.98762325\n",
      "Iteration 2030, loss = 1.98877567\n",
      "Iteration 2031, loss = 1.98750588\n",
      "Iteration 2032, loss = 1.98945895\n",
      "Iteration 2033, loss = 1.98744301\n",
      "Iteration 2034, loss = 1.98729089\n",
      "Iteration 2035, loss = 1.98704148\n",
      "Iteration 2036, loss = 1.98523227\n",
      "Iteration 2037, loss = 1.98548056\n",
      "Iteration 2038, loss = 1.98519403\n",
      "Iteration 2039, loss = 1.98431572\n",
      "Iteration 2040, loss = 1.98617445\n",
      "Iteration 2041, loss = 1.98587093\n",
      "Iteration 2042, loss = 1.98584591\n",
      "Iteration 2043, loss = 1.98435170\n",
      "Iteration 2044, loss = 1.98349555\n",
      "Iteration 2045, loss = 1.98464102\n",
      "Iteration 2046, loss = 1.98436292\n",
      "Iteration 2047, loss = 1.98381771\n",
      "Iteration 2048, loss = 1.98319902\n",
      "Iteration 2049, loss = 1.98356495\n",
      "Iteration 2050, loss = 1.98328497\n",
      "Iteration 2051, loss = 1.98149137\n",
      "Iteration 2052, loss = 1.98243631\n",
      "Iteration 2053, loss = 1.98241929\n",
      "Iteration 2054, loss = 1.98248022\n",
      "Iteration 2055, loss = 1.98108525\n",
      "Iteration 2056, loss = 1.98183736\n",
      "Iteration 2057, loss = 1.98232044\n",
      "Iteration 2058, loss = 1.98284937\n",
      "Iteration 2059, loss = 1.98229012\n",
      "Iteration 2060, loss = 1.98151152\n",
      "Iteration 2061, loss = 1.97967025\n",
      "Iteration 2062, loss = 1.98146806\n",
      "Iteration 2063, loss = 1.98102318\n",
      "Iteration 2064, loss = 1.97905853\n",
      "Iteration 2065, loss = 1.98278999\n",
      "Iteration 2066, loss = 1.98088014\n",
      "Iteration 2067, loss = 1.97842790\n",
      "Iteration 2068, loss = 1.98026299\n",
      "Iteration 2069, loss = 1.97886019\n",
      "Iteration 2070, loss = 1.97790236\n",
      "Iteration 2071, loss = 1.98007791\n",
      "Iteration 2072, loss = 1.97821542\n",
      "Iteration 2073, loss = 1.97875198\n",
      "Iteration 2074, loss = 1.97672410\n",
      "Iteration 2075, loss = 1.97766438\n",
      "Iteration 2076, loss = 1.97788106\n",
      "Iteration 2077, loss = 1.97714113\n",
      "Iteration 2078, loss = 1.97795411\n",
      "Iteration 2079, loss = 1.97685242\n",
      "Iteration 2080, loss = 1.97679768\n",
      "Iteration 2081, loss = 1.97679160\n",
      "Iteration 2082, loss = 1.97538985\n",
      "Iteration 2083, loss = 1.97560839\n",
      "Iteration 2084, loss = 1.97502862\n",
      "Iteration 2085, loss = 1.97461260\n",
      "Iteration 2086, loss = 1.97506094\n",
      "Iteration 2087, loss = 1.97438110\n",
      "Iteration 2088, loss = 1.97354530\n",
      "Iteration 2089, loss = 1.97337255\n",
      "Iteration 2090, loss = 1.97355664\n",
      "Iteration 2091, loss = 1.97614687\n",
      "Iteration 2092, loss = 1.97419739\n",
      "Iteration 2093, loss = 1.97391311\n",
      "Iteration 2094, loss = 1.97428011\n",
      "Iteration 2095, loss = 1.97482472\n",
      "Iteration 2096, loss = 1.97406812\n",
      "Iteration 2097, loss = 1.97364044\n",
      "Iteration 2098, loss = 1.97184235\n",
      "Iteration 2099, loss = 1.97338934\n",
      "Iteration 2100, loss = 1.97238088\n",
      "Iteration 2101, loss = 1.97293773\n",
      "Iteration 2102, loss = 1.97159978\n",
      "Iteration 2103, loss = 1.97085359\n",
      "Iteration 2104, loss = 1.97295190\n",
      "Iteration 2105, loss = 1.97041023\n",
      "Iteration 2106, loss = 1.97215735\n",
      "Iteration 2107, loss = 1.97091616\n",
      "Iteration 2108, loss = 1.97033291\n",
      "Iteration 2109, loss = 1.97320115\n",
      "Iteration 2110, loss = 1.97130148\n",
      "Iteration 2111, loss = 1.97105172\n",
      "Iteration 2112, loss = 1.97227009\n",
      "Iteration 2113, loss = 1.97127612\n",
      "Iteration 2114, loss = 1.97145004\n",
      "Iteration 2115, loss = 1.96983846\n",
      "Iteration 2116, loss = 1.96865670\n",
      "Iteration 2117, loss = 1.96856532\n",
      "Iteration 2118, loss = 1.96759122\n",
      "Iteration 2119, loss = 1.96760677\n",
      "Iteration 2120, loss = 1.96765642\n",
      "Iteration 2121, loss = 1.96791249\n",
      "Iteration 2122, loss = 1.96860917\n",
      "Iteration 2123, loss = 1.96628260\n",
      "Iteration 2124, loss = 1.96708569\n",
      "Iteration 2125, loss = 1.96705861\n",
      "Iteration 2126, loss = 1.96601146\n",
      "Iteration 2127, loss = 1.96709997\n",
      "Iteration 2128, loss = 1.96608847\n",
      "Iteration 2129, loss = 1.96767131\n",
      "Iteration 2130, loss = 1.96663886\n",
      "Iteration 2131, loss = 1.96434847\n",
      "Iteration 2132, loss = 1.96883311\n",
      "Iteration 2133, loss = 1.96446784\n",
      "Iteration 2134, loss = 1.96597332\n",
      "Iteration 2135, loss = 1.96664151\n",
      "Iteration 2136, loss = 1.96746608\n",
      "Iteration 2137, loss = 1.96442024\n",
      "Iteration 2138, loss = 1.96481236\n",
      "Iteration 2139, loss = 1.96378947\n",
      "Iteration 2140, loss = 1.96324596\n",
      "Iteration 2141, loss = 1.96299807\n",
      "Iteration 2142, loss = 1.96423925\n",
      "Iteration 2143, loss = 1.96285177\n",
      "Iteration 2144, loss = 1.96206438\n",
      "Iteration 2145, loss = 1.96427041\n",
      "Iteration 2146, loss = 1.96275071\n",
      "Iteration 2147, loss = 1.96092582\n",
      "Iteration 2148, loss = 1.96096549\n",
      "Iteration 2149, loss = 1.96140464\n",
      "Iteration 2150, loss = 1.96189496\n",
      "Iteration 2151, loss = 1.96320852\n",
      "Iteration 2152, loss = 1.96065589\n",
      "Iteration 2153, loss = 1.96132847\n",
      "Iteration 2154, loss = 1.96153112\n",
      "Iteration 2155, loss = 1.95954144\n",
      "Iteration 2156, loss = 1.96286665\n",
      "Iteration 2157, loss = 1.96083165\n",
      "Iteration 2158, loss = 1.96139679\n",
      "Iteration 2159, loss = 1.95888077\n",
      "Iteration 2160, loss = 1.96328180\n",
      "Iteration 2161, loss = 1.95675621\n",
      "Iteration 2162, loss = 1.96250067\n",
      "Iteration 2163, loss = 1.96179972\n",
      "Iteration 2164, loss = 1.95885502\n",
      "Iteration 2165, loss = 1.95872411\n",
      "Iteration 2166, loss = 1.95905498\n",
      "Iteration 2167, loss = 1.96006534\n",
      "Iteration 2168, loss = 1.96021044\n",
      "Iteration 2169, loss = 1.96004943\n",
      "Iteration 2170, loss = 1.95842340\n",
      "Iteration 2171, loss = 1.95656132\n",
      "Iteration 2172, loss = 1.95809783\n",
      "Iteration 2173, loss = 1.95776593\n",
      "Iteration 2174, loss = 1.95633857\n",
      "Iteration 2175, loss = 1.95696889\n",
      "Iteration 2176, loss = 1.95512236\n",
      "Iteration 2177, loss = 1.95552099\n",
      "Iteration 2178, loss = 1.95451644\n",
      "Iteration 2179, loss = 1.95525357\n",
      "Iteration 2180, loss = 1.95553325\n",
      "Iteration 2181, loss = 1.95522656\n",
      "Iteration 2182, loss = 1.95783538\n",
      "Iteration 2183, loss = 1.95704069\n",
      "Iteration 2184, loss = 1.95539768\n",
      "Iteration 2185, loss = 1.95489400\n",
      "Iteration 2186, loss = 1.95449775\n",
      "Iteration 2187, loss = 1.95274605\n",
      "Iteration 2188, loss = 1.95247060\n",
      "Iteration 2189, loss = 1.95291226\n",
      "Iteration 2190, loss = 1.95323754\n",
      "Iteration 2191, loss = 1.95350961\n",
      "Iteration 2192, loss = 1.95359498\n",
      "Iteration 2193, loss = 1.95258847\n",
      "Iteration 2194, loss = 1.95279667\n",
      "Iteration 2195, loss = 1.95258516\n",
      "Iteration 2196, loss = 1.95072560\n",
      "Iteration 2197, loss = 1.95281520\n",
      "Iteration 2198, loss = 1.95374143\n",
      "Iteration 2199, loss = 1.95230071\n",
      "Iteration 2200, loss = 1.95096642\n",
      "Iteration 2201, loss = 1.95320611\n",
      "Iteration 2202, loss = 1.95238515\n",
      "Iteration 2203, loss = 1.94967931\n",
      "Iteration 2204, loss = 1.95045487\n",
      "Iteration 2205, loss = 1.94928837\n",
      "Iteration 2206, loss = 1.95140934\n",
      "Iteration 2207, loss = 1.95088291\n",
      "Iteration 2208, loss = 1.95000209\n",
      "Iteration 2209, loss = 1.94970343\n",
      "Iteration 2210, loss = 1.94870698\n",
      "Iteration 2211, loss = 1.94848362\n",
      "Iteration 2212, loss = 1.94984332\n",
      "Iteration 2213, loss = 1.94760136\n",
      "Iteration 2214, loss = 1.94678148\n",
      "Iteration 2215, loss = 1.94692123\n",
      "Iteration 2216, loss = 1.94702635\n",
      "Iteration 2217, loss = 1.94814723\n",
      "Iteration 2218, loss = 1.94798399\n",
      "Iteration 2219, loss = 1.94659962\n",
      "Iteration 2220, loss = 1.94739380\n",
      "Iteration 2221, loss = 1.94788192\n",
      "Iteration 2222, loss = 1.94683000\n",
      "Iteration 2223, loss = 1.94636989\n",
      "Iteration 2224, loss = 1.94564325\n",
      "Iteration 2225, loss = 1.94554337\n",
      "Iteration 2226, loss = 1.94695993\n",
      "Iteration 2227, loss = 1.94711144\n",
      "Iteration 2228, loss = 1.94514076\n",
      "Iteration 2229, loss = 1.94662630\n",
      "Iteration 2230, loss = 1.94740976\n",
      "Iteration 2231, loss = 1.94543118\n",
      "Iteration 2232, loss = 1.94471871\n",
      "Iteration 2233, loss = 1.94485530\n",
      "Iteration 2234, loss = 1.94573468\n",
      "Iteration 2235, loss = 1.94628091\n",
      "Iteration 2236, loss = 1.94580582\n",
      "Iteration 2237, loss = 1.94537131\n",
      "Iteration 2238, loss = 1.94916268\n",
      "Iteration 2239, loss = 1.94492931\n",
      "Iteration 2240, loss = 1.94876081\n",
      "Iteration 2241, loss = 1.94421262\n",
      "Iteration 2242, loss = 1.94349030\n",
      "Iteration 2243, loss = 1.94310715\n",
      "Iteration 2244, loss = 1.94237961\n",
      "Iteration 2245, loss = 1.94180201\n",
      "Iteration 2246, loss = 1.94252020\n",
      "Iteration 2247, loss = 1.94119795\n",
      "Iteration 2248, loss = 1.94150237\n",
      "Iteration 2249, loss = 1.94028624\n",
      "Iteration 2250, loss = 1.93990924\n",
      "Iteration 2251, loss = 1.93970393\n",
      "Iteration 2252, loss = 1.93985466\n",
      "Iteration 2253, loss = 1.94085429\n",
      "Iteration 2254, loss = 1.94077302\n",
      "Iteration 2255, loss = 1.93929437\n",
      "Iteration 2256, loss = 1.94069166\n",
      "Iteration 2257, loss = 1.93980201\n",
      "Iteration 2258, loss = 1.93940800\n",
      "Iteration 2259, loss = 1.93808536\n",
      "Iteration 2260, loss = 1.93970926\n",
      "Iteration 2261, loss = 1.93812710\n",
      "Iteration 2262, loss = 1.93868729\n",
      "Iteration 2263, loss = 1.93766240\n",
      "Iteration 2264, loss = 1.93779433\n",
      "Iteration 2265, loss = 1.93881976\n",
      "Iteration 2266, loss = 1.93683180\n",
      "Iteration 2267, loss = 1.93689413\n",
      "Iteration 2268, loss = 1.93615949\n",
      "Iteration 2269, loss = 1.93791775\n",
      "Iteration 2270, loss = 1.93888336\n",
      "Iteration 2271, loss = 1.93585613\n",
      "Iteration 2272, loss = 1.93648679\n",
      "Iteration 2273, loss = 1.93742249\n",
      "Iteration 2274, loss = 1.93839261\n",
      "Iteration 2275, loss = 1.93668641\n",
      "Iteration 2276, loss = 1.93580215\n",
      "Iteration 2277, loss = 1.93485351\n",
      "Iteration 2278, loss = 1.93597829\n",
      "Iteration 2279, loss = 1.93339908\n",
      "Iteration 2280, loss = 1.93679811\n",
      "Iteration 2281, loss = 1.93575689\n",
      "Iteration 2282, loss = 1.93740239\n",
      "Iteration 2283, loss = 1.93423808\n",
      "Iteration 2284, loss = 1.93374041\n",
      "Iteration 2285, loss = 1.93404386\n",
      "Iteration 2286, loss = 1.93308691\n",
      "Iteration 2287, loss = 1.93324347\n",
      "Iteration 2288, loss = 1.93216138\n",
      "Iteration 2289, loss = 1.93152350\n",
      "Iteration 2290, loss = 1.93353093\n",
      "Iteration 2291, loss = 1.93462369\n",
      "Iteration 2292, loss = 1.93298953\n",
      "Iteration 2293, loss = 1.93247548\n",
      "Iteration 2294, loss = 1.93209756\n",
      "Iteration 2295, loss = 1.93198049\n",
      "Iteration 2296, loss = 1.93211125\n",
      "Iteration 2297, loss = 1.93127351\n",
      "Iteration 2298, loss = 1.92911947\n",
      "Iteration 2299, loss = 1.93074400\n",
      "Iteration 2300, loss = 1.93144886\n",
      "Iteration 2301, loss = 1.92982409\n",
      "Iteration 2302, loss = 1.92920108\n",
      "Iteration 2303, loss = 1.92769047\n",
      "Iteration 2304, loss = 1.93003776\n",
      "Iteration 2305, loss = 1.92799656\n",
      "Iteration 2306, loss = 1.92892897\n",
      "Iteration 2307, loss = 1.92919552\n",
      "Iteration 2308, loss = 1.93027041\n",
      "Iteration 2309, loss = 1.92949478\n",
      "Iteration 2310, loss = 1.92838178\n",
      "Iteration 2311, loss = 1.93081829\n",
      "Iteration 2312, loss = 1.92897073\n",
      "Iteration 2313, loss = 1.93247858\n",
      "Iteration 2314, loss = 1.92875682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23088732\n",
      "Iteration 2, loss = 4.13327633\n",
      "Iteration 3, loss = 4.03763925\n",
      "Iteration 4, loss = 3.94082767\n",
      "Iteration 5, loss = 3.84193862\n",
      "Iteration 6, loss = 3.73791823\n",
      "Iteration 7, loss = 3.63357123\n",
      "Iteration 8, loss = 3.52989763\n",
      "Iteration 9, loss = 3.43306177\n",
      "Iteration 10, loss = 3.34735195\n",
      "Iteration 11, loss = 3.28575712\n",
      "Iteration 12, loss = 3.24495965\n",
      "Iteration 13, loss = 3.22853139\n",
      "Iteration 14, loss = 3.22522492\n",
      "Iteration 15, loss = 3.22494082\n",
      "Iteration 16, loss = 3.22245748\n",
      "Iteration 17, loss = 3.21944687\n",
      "Iteration 18, loss = 3.21779462\n",
      "Iteration 19, loss = 3.21658938\n",
      "Iteration 20, loss = 3.21516702\n",
      "Iteration 21, loss = 3.21466976\n",
      "Iteration 22, loss = 3.21391483\n",
      "Iteration 23, loss = 3.21286453\n",
      "Iteration 24, loss = 3.21140718\n",
      "Iteration 25, loss = 3.21047081\n",
      "Iteration 26, loss = 3.20923272\n",
      "Iteration 27, loss = 3.20755530\n",
      "Iteration 28, loss = 3.20653919\n",
      "Iteration 29, loss = 3.20538920\n",
      "Iteration 30, loss = 3.20381719\n",
      "Iteration 31, loss = 3.20321963\n",
      "Iteration 32, loss = 3.20234097\n",
      "Iteration 33, loss = 3.20152536\n",
      "Iteration 34, loss = 3.20074152\n",
      "Iteration 35, loss = 3.19964741\n",
      "Iteration 36, loss = 3.19848306\n",
      "Iteration 37, loss = 3.19673303\n",
      "Iteration 38, loss = 3.19557927\n",
      "Iteration 39, loss = 3.19422233\n",
      "Iteration 40, loss = 3.19308471\n",
      "Iteration 41, loss = 3.19158820\n",
      "Iteration 42, loss = 3.19076295\n",
      "Iteration 43, loss = 3.18908292\n",
      "Iteration 44, loss = 3.18875942\n",
      "Iteration 45, loss = 3.18663597\n",
      "Iteration 46, loss = 3.18605226\n",
      "Iteration 47, loss = 3.18519168\n",
      "Iteration 48, loss = 3.18369445\n",
      "Iteration 49, loss = 3.18338984\n",
      "Iteration 50, loss = 3.18172538\n",
      "Iteration 51, loss = 3.18056264\n",
      "Iteration 52, loss = 3.17885528\n",
      "Iteration 53, loss = 3.17737896\n",
      "Iteration 54, loss = 3.17569319\n",
      "Iteration 55, loss = 3.17344763\n",
      "Iteration 56, loss = 3.17281016\n",
      "Iteration 57, loss = 3.17139841\n",
      "Iteration 58, loss = 3.16980741\n",
      "Iteration 59, loss = 3.16887497\n",
      "Iteration 60, loss = 3.16761020\n",
      "Iteration 61, loss = 3.16586612\n",
      "Iteration 62, loss = 3.16527294\n",
      "Iteration 63, loss = 3.16423134\n",
      "Iteration 64, loss = 3.16363683\n",
      "Iteration 65, loss = 3.16217158\n",
      "Iteration 66, loss = 3.15992392\n",
      "Iteration 67, loss = 3.15794448\n",
      "Iteration 68, loss = 3.15657201\n",
      "Iteration 69, loss = 3.15565697\n",
      "Iteration 70, loss = 3.15425615\n",
      "Iteration 71, loss = 3.15321705\n",
      "Iteration 72, loss = 3.15097744\n",
      "Iteration 73, loss = 3.14982358\n",
      "Iteration 74, loss = 3.14844354\n",
      "Iteration 75, loss = 3.14687806\n",
      "Iteration 76, loss = 3.14537984\n",
      "Iteration 77, loss = 3.14476772\n",
      "Iteration 78, loss = 3.14309648\n",
      "Iteration 79, loss = 3.14151811\n",
      "Iteration 80, loss = 3.14030801\n",
      "Iteration 81, loss = 3.13782900\n",
      "Iteration 82, loss = 3.13698903\n",
      "Iteration 83, loss = 3.13516780\n",
      "Iteration 84, loss = 3.13445726\n",
      "Iteration 85, loss = 3.13295156\n",
      "Iteration 86, loss = 3.13176678\n",
      "Iteration 87, loss = 3.13046109\n",
      "Iteration 88, loss = 3.12973225\n",
      "Iteration 89, loss = 3.12766392\n",
      "Iteration 90, loss = 3.12641883\n",
      "Iteration 91, loss = 3.12459359\n",
      "Iteration 92, loss = 3.12281923\n",
      "Iteration 93, loss = 3.12226685\n",
      "Iteration 94, loss = 3.12072024\n",
      "Iteration 95, loss = 3.11892386\n",
      "Iteration 96, loss = 3.11730936\n",
      "Iteration 97, loss = 3.11633992\n",
      "Iteration 98, loss = 3.11414361\n",
      "Iteration 99, loss = 3.11296094\n",
      "Iteration 100, loss = 3.11204362\n",
      "Iteration 101, loss = 3.11036874\n",
      "Iteration 102, loss = 3.10797295\n",
      "Iteration 103, loss = 3.10730086\n",
      "Iteration 104, loss = 3.10578384\n",
      "Iteration 105, loss = 3.10525165\n",
      "Iteration 106, loss = 3.10332561\n",
      "Iteration 107, loss = 3.10130618\n",
      "Iteration 108, loss = 3.10003519\n",
      "Iteration 109, loss = 3.09904359\n",
      "Iteration 110, loss = 3.09764838\n",
      "Iteration 111, loss = 3.09561446\n",
      "Iteration 112, loss = 3.09402843\n",
      "Iteration 113, loss = 3.09280097\n",
      "Iteration 114, loss = 3.09113563\n",
      "Iteration 115, loss = 3.08994037\n",
      "Iteration 116, loss = 3.08877193\n",
      "Iteration 117, loss = 3.08740900\n",
      "Iteration 118, loss = 3.08620383\n",
      "Iteration 119, loss = 3.08473115\n",
      "Iteration 120, loss = 3.08322938\n",
      "Iteration 121, loss = 3.08153838\n",
      "Iteration 122, loss = 3.08037310\n",
      "Iteration 123, loss = 3.08020896\n",
      "Iteration 124, loss = 3.07839005\n",
      "Iteration 125, loss = 3.07665498\n",
      "Iteration 126, loss = 3.07578188\n",
      "Iteration 127, loss = 3.07450980\n",
      "Iteration 128, loss = 3.07152583\n",
      "Iteration 129, loss = 3.07003315\n",
      "Iteration 130, loss = 3.06899657\n",
      "Iteration 131, loss = 3.06903890\n",
      "Iteration 132, loss = 3.06851582\n",
      "Iteration 133, loss = 3.06623072\n",
      "Iteration 134, loss = 3.06426067\n",
      "Iteration 135, loss = 3.06226605\n",
      "Iteration 136, loss = 3.06125548\n",
      "Iteration 137, loss = 3.05998077\n",
      "Iteration 138, loss = 3.05860064\n",
      "Iteration 139, loss = 3.05741621\n",
      "Iteration 140, loss = 3.05513985\n",
      "Iteration 141, loss = 3.05407880\n",
      "Iteration 142, loss = 3.05229905\n",
      "Iteration 143, loss = 3.05032938\n",
      "Iteration 144, loss = 3.04975148\n",
      "Iteration 145, loss = 3.04851316\n",
      "Iteration 146, loss = 3.04737808\n",
      "Iteration 147, loss = 3.04570921\n",
      "Iteration 148, loss = 3.04399967\n",
      "Iteration 149, loss = 3.04310776\n",
      "Iteration 150, loss = 3.04256437\n",
      "Iteration 151, loss = 3.03972160\n",
      "Iteration 152, loss = 3.03852201\n",
      "Iteration 153, loss = 3.03774262\n",
      "Iteration 154, loss = 3.03622228\n",
      "Iteration 155, loss = 3.03489504\n",
      "Iteration 156, loss = 3.03298161\n",
      "Iteration 157, loss = 3.03376718\n",
      "Iteration 158, loss = 3.03123823\n",
      "Iteration 159, loss = 3.02856001\n",
      "Iteration 160, loss = 3.02686952\n",
      "Iteration 161, loss = 3.02613780\n",
      "Iteration 162, loss = 3.02482006\n",
      "Iteration 163, loss = 3.02337585\n",
      "Iteration 164, loss = 3.02154666\n",
      "Iteration 165, loss = 3.02027567\n",
      "Iteration 166, loss = 3.01955213\n",
      "Iteration 167, loss = 3.01731619\n",
      "Iteration 168, loss = 3.01691124\n",
      "Iteration 169, loss = 3.01507223\n",
      "Iteration 170, loss = 3.01380562\n",
      "Iteration 171, loss = 3.01291512\n",
      "Iteration 172, loss = 3.01133279\n",
      "Iteration 173, loss = 3.00969140\n",
      "Iteration 174, loss = 3.00848652\n",
      "Iteration 175, loss = 3.00792570\n",
      "Iteration 176, loss = 3.00614654\n",
      "Iteration 177, loss = 3.00422892\n",
      "Iteration 178, loss = 3.00324634\n",
      "Iteration 179, loss = 3.00220534\n",
      "Iteration 180, loss = 3.00134334\n",
      "Iteration 181, loss = 2.99885746\n",
      "Iteration 182, loss = 2.99813162\n",
      "Iteration 183, loss = 2.99660140\n",
      "Iteration 184, loss = 2.99545360\n",
      "Iteration 185, loss = 2.99516142\n",
      "Iteration 186, loss = 2.99364691\n",
      "Iteration 187, loss = 2.99245578\n",
      "Iteration 188, loss = 2.99176984\n",
      "Iteration 189, loss = 2.98909821\n",
      "Iteration 190, loss = 2.98786790\n",
      "Iteration 191, loss = 2.98693040\n",
      "Iteration 192, loss = 2.98438949\n",
      "Iteration 193, loss = 2.98345655\n",
      "Iteration 194, loss = 2.98182039\n",
      "Iteration 195, loss = 2.98188096\n",
      "Iteration 196, loss = 2.97912568\n",
      "Iteration 197, loss = 2.97886380\n",
      "Iteration 198, loss = 2.97650522\n",
      "Iteration 199, loss = 2.97527673\n",
      "Iteration 200, loss = 2.97430160\n",
      "Iteration 201, loss = 2.97350863\n",
      "Iteration 202, loss = 2.97198155\n",
      "Iteration 203, loss = 2.97101643\n",
      "Iteration 204, loss = 2.96949617\n",
      "Iteration 205, loss = 2.96748950\n",
      "Iteration 206, loss = 2.96672516\n",
      "Iteration 207, loss = 2.96510182\n",
      "Iteration 208, loss = 2.96326435\n",
      "Iteration 209, loss = 2.96187058\n",
      "Iteration 210, loss = 2.96111284\n",
      "Iteration 211, loss = 2.96047257\n",
      "Iteration 212, loss = 2.95895242\n",
      "Iteration 213, loss = 2.95696398\n",
      "Iteration 214, loss = 2.95539604\n",
      "Iteration 215, loss = 2.95412420\n",
      "Iteration 216, loss = 2.95239098\n",
      "Iteration 217, loss = 2.95150836\n",
      "Iteration 218, loss = 2.95012468\n",
      "Iteration 219, loss = 2.94878238\n",
      "Iteration 220, loss = 2.94928804\n",
      "Iteration 221, loss = 2.94727123\n",
      "Iteration 222, loss = 2.94559866\n",
      "Iteration 223, loss = 2.94437889\n",
      "Iteration 224, loss = 2.94325149\n",
      "Iteration 225, loss = 2.94166458\n",
      "Iteration 226, loss = 2.94018078\n",
      "Iteration 227, loss = 2.93873462\n",
      "Iteration 228, loss = 2.93759826\n",
      "Iteration 229, loss = 2.93688020\n",
      "Iteration 230, loss = 2.93672956\n",
      "Iteration 231, loss = 2.93545328\n",
      "Iteration 232, loss = 2.93266396\n",
      "Iteration 233, loss = 2.93162914\n",
      "Iteration 234, loss = 2.93089138\n",
      "Iteration 235, loss = 2.92812131\n",
      "Iteration 236, loss = 2.92702250\n",
      "Iteration 237, loss = 2.92590908\n",
      "Iteration 238, loss = 2.92503225\n",
      "Iteration 239, loss = 2.92354768\n",
      "Iteration 240, loss = 2.92263285\n",
      "Iteration 241, loss = 2.92110994\n",
      "Iteration 242, loss = 2.91934222\n",
      "Iteration 243, loss = 2.91893720\n",
      "Iteration 244, loss = 2.91696623\n",
      "Iteration 245, loss = 2.91547665\n",
      "Iteration 246, loss = 2.91783328\n",
      "Iteration 247, loss = 2.91514398\n",
      "Iteration 248, loss = 2.91412558\n",
      "Iteration 249, loss = 2.91224367\n",
      "Iteration 250, loss = 2.91057715\n",
      "Iteration 251, loss = 2.90851777\n",
      "Iteration 252, loss = 2.90834119\n",
      "Iteration 253, loss = 2.90598889\n",
      "Iteration 254, loss = 2.90485345\n",
      "Iteration 255, loss = 2.90608574\n",
      "Iteration 256, loss = 2.90429082\n",
      "Iteration 257, loss = 2.90222450\n",
      "Iteration 258, loss = 2.90076047\n",
      "Iteration 259, loss = 2.90062400\n",
      "Iteration 260, loss = 2.89863071\n",
      "Iteration 261, loss = 2.89705783\n",
      "Iteration 262, loss = 2.89503439\n",
      "Iteration 263, loss = 2.89472297\n",
      "Iteration 264, loss = 2.89305290\n",
      "Iteration 265, loss = 2.89238567\n",
      "Iteration 266, loss = 2.89055586\n",
      "Iteration 267, loss = 2.88906965\n",
      "Iteration 268, loss = 2.88798009\n",
      "Iteration 269, loss = 2.88610657\n",
      "Iteration 270, loss = 2.88708779\n",
      "Iteration 271, loss = 2.88582699\n",
      "Iteration 272, loss = 2.88316217\n",
      "Iteration 273, loss = 2.88222830\n",
      "Iteration 274, loss = 2.88053531\n",
      "Iteration 275, loss = 2.88005720\n",
      "Iteration 276, loss = 2.87834725\n",
      "Iteration 277, loss = 2.87739876\n",
      "Iteration 278, loss = 2.87725122\n",
      "Iteration 279, loss = 2.87472691\n",
      "Iteration 280, loss = 2.87370287\n",
      "Iteration 281, loss = 2.87564090\n",
      "Iteration 282, loss = 2.87296766\n",
      "Iteration 283, loss = 2.87065176\n",
      "Iteration 284, loss = 2.87071039\n",
      "Iteration 285, loss = 2.86740541\n",
      "Iteration 286, loss = 2.86629415\n",
      "Iteration 287, loss = 2.86547534\n",
      "Iteration 288, loss = 2.86476837\n",
      "Iteration 289, loss = 2.86318100\n",
      "Iteration 290, loss = 2.86164263\n",
      "Iteration 291, loss = 2.86010405\n",
      "Iteration 292, loss = 2.85908965\n",
      "Iteration 293, loss = 2.85829782\n",
      "Iteration 294, loss = 2.85725531\n",
      "Iteration 295, loss = 2.85596420\n",
      "Iteration 296, loss = 2.85449362\n",
      "Iteration 297, loss = 2.85364140\n",
      "Iteration 298, loss = 2.85292772\n",
      "Iteration 299, loss = 2.85351429\n",
      "Iteration 300, loss = 2.84983768\n",
      "Iteration 301, loss = 2.84893437\n",
      "Iteration 302, loss = 2.84784426\n",
      "Iteration 303, loss = 2.84778897\n",
      "Iteration 304, loss = 2.84662023\n",
      "Iteration 305, loss = 2.84536567\n",
      "Iteration 306, loss = 2.84491490\n",
      "Iteration 307, loss = 2.84269673\n",
      "Iteration 308, loss = 2.84076504\n",
      "Iteration 309, loss = 2.83912309\n",
      "Iteration 310, loss = 2.83875463\n",
      "Iteration 311, loss = 2.83949396\n",
      "Iteration 312, loss = 2.83738030\n",
      "Iteration 313, loss = 2.83468431\n",
      "Iteration 314, loss = 2.83417063\n",
      "Iteration 315, loss = 2.83259311\n",
      "Iteration 316, loss = 2.83157290\n",
      "Iteration 317, loss = 2.83377627\n",
      "Iteration 318, loss = 2.83248218\n",
      "Iteration 319, loss = 2.82868222\n",
      "Iteration 320, loss = 2.82848711\n",
      "Iteration 321, loss = 2.82662156\n",
      "Iteration 322, loss = 2.82492011\n",
      "Iteration 323, loss = 2.82444446\n",
      "Iteration 324, loss = 2.82257274\n",
      "Iteration 325, loss = 2.82146811\n",
      "Iteration 326, loss = 2.82084798\n",
      "Iteration 327, loss = 2.81976742\n",
      "Iteration 328, loss = 2.82067020\n",
      "Iteration 329, loss = 2.82357158\n",
      "Iteration 330, loss = 2.81899944\n",
      "Iteration 331, loss = 2.81827055\n",
      "Iteration 332, loss = 2.81453577\n",
      "Iteration 333, loss = 2.81743061\n",
      "Iteration 334, loss = 2.81283481\n",
      "Iteration 335, loss = 2.81227501\n",
      "Iteration 336, loss = 2.81156204\n",
      "Iteration 337, loss = 2.80807419\n",
      "Iteration 338, loss = 2.80753037\n",
      "Iteration 339, loss = 2.80931197\n",
      "Iteration 340, loss = 2.80726637\n",
      "Iteration 341, loss = 2.80475429\n",
      "Iteration 342, loss = 2.80426879\n",
      "Iteration 343, loss = 2.80188420\n",
      "Iteration 344, loss = 2.79974491\n",
      "Iteration 345, loss = 2.80028775\n",
      "Iteration 346, loss = 2.79845057\n",
      "Iteration 347, loss = 2.79691519\n",
      "Iteration 348, loss = 2.79754859\n",
      "Iteration 349, loss = 2.79513412\n",
      "Iteration 350, loss = 2.79487690\n",
      "Iteration 351, loss = 2.79503264\n",
      "Iteration 352, loss = 2.79160680\n",
      "Iteration 353, loss = 2.79101613\n",
      "Iteration 354, loss = 2.79013327\n",
      "Iteration 355, loss = 2.78967258\n",
      "Iteration 356, loss = 2.78819244\n",
      "Iteration 357, loss = 2.78725015\n",
      "Iteration 358, loss = 2.78704248\n",
      "Iteration 359, loss = 2.78566034\n",
      "Iteration 360, loss = 2.78408889\n",
      "Iteration 361, loss = 2.78228192\n",
      "Iteration 362, loss = 2.78217438\n",
      "Iteration 363, loss = 2.78047435\n",
      "Iteration 364, loss = 2.77931327\n",
      "Iteration 365, loss = 2.77878670\n",
      "Iteration 366, loss = 2.77798318\n",
      "Iteration 367, loss = 2.77815249\n",
      "Iteration 368, loss = 2.77440399\n",
      "Iteration 369, loss = 2.77362027\n",
      "Iteration 370, loss = 2.77342493\n",
      "Iteration 371, loss = 2.77250172\n",
      "Iteration 372, loss = 2.77175972\n",
      "Iteration 373, loss = 2.77057926\n",
      "Iteration 374, loss = 2.76917179\n",
      "Iteration 375, loss = 2.76824085\n",
      "Iteration 376, loss = 2.76670062\n",
      "Iteration 377, loss = 2.76724870\n",
      "Iteration 378, loss = 2.76663669\n",
      "Iteration 379, loss = 2.76438390\n",
      "Iteration 380, loss = 2.76306145\n",
      "Iteration 381, loss = 2.76161493\n",
      "Iteration 382, loss = 2.76164858\n",
      "Iteration 383, loss = 2.76048473\n",
      "Iteration 384, loss = 2.75883227\n",
      "Iteration 385, loss = 2.75879741\n",
      "Iteration 386, loss = 2.75659356\n",
      "Iteration 387, loss = 2.75675881\n",
      "Iteration 388, loss = 2.75531883\n",
      "Iteration 389, loss = 2.75341633\n",
      "Iteration 390, loss = 2.75336023\n",
      "Iteration 391, loss = 2.75144088\n",
      "Iteration 392, loss = 2.75066079\n",
      "Iteration 393, loss = 2.74990439\n",
      "Iteration 394, loss = 2.74830804\n",
      "Iteration 395, loss = 2.74809053\n",
      "Iteration 396, loss = 2.74730853\n",
      "Iteration 397, loss = 2.74465640\n",
      "Iteration 398, loss = 2.74482509\n",
      "Iteration 399, loss = 2.74358769\n",
      "Iteration 400, loss = 2.74308361\n",
      "Iteration 401, loss = 2.74208756\n",
      "Iteration 402, loss = 2.74075804\n",
      "Iteration 403, loss = 2.74037311\n",
      "Iteration 404, loss = 2.73876617\n",
      "Iteration 405, loss = 2.73754552\n",
      "Iteration 406, loss = 2.73693085\n",
      "Iteration 407, loss = 2.73588347\n",
      "Iteration 408, loss = 2.73479322\n",
      "Iteration 409, loss = 2.73301054\n",
      "Iteration 410, loss = 2.73330428\n",
      "Iteration 411, loss = 2.73132889\n",
      "Iteration 412, loss = 2.73075462\n",
      "Iteration 413, loss = 2.73038838\n",
      "Iteration 414, loss = 2.73094620\n",
      "Iteration 415, loss = 2.72854650\n",
      "Iteration 416, loss = 2.72741620\n",
      "Iteration 417, loss = 2.72643160\n",
      "Iteration 418, loss = 2.72493926\n",
      "Iteration 419, loss = 2.72351845\n",
      "Iteration 420, loss = 2.72395564\n",
      "Iteration 421, loss = 2.72312564\n",
      "Iteration 422, loss = 2.72155979\n",
      "Iteration 423, loss = 2.71939027\n",
      "Iteration 424, loss = 2.71856315\n",
      "Iteration 425, loss = 2.71797139\n",
      "Iteration 426, loss = 2.71688041\n",
      "Iteration 427, loss = 2.71716946\n",
      "Iteration 428, loss = 2.71627294\n",
      "Iteration 429, loss = 2.71535052\n",
      "Iteration 430, loss = 2.71399172\n",
      "Iteration 431, loss = 2.71300994\n",
      "Iteration 432, loss = 2.71237272\n",
      "Iteration 433, loss = 2.71038530\n",
      "Iteration 434, loss = 2.70958988\n",
      "Iteration 435, loss = 2.70839684\n",
      "Iteration 436, loss = 2.70767147\n",
      "Iteration 437, loss = 2.70784269\n",
      "Iteration 438, loss = 2.70561212\n",
      "Iteration 439, loss = 2.70512685\n",
      "Iteration 440, loss = 2.70421585\n",
      "Iteration 441, loss = 2.70329057\n",
      "Iteration 442, loss = 2.70120989\n",
      "Iteration 443, loss = 2.70084111\n",
      "Iteration 444, loss = 2.70066873\n",
      "Iteration 445, loss = 2.69930757\n",
      "Iteration 446, loss = 2.69807021\n",
      "Iteration 447, loss = 2.69765422\n",
      "Iteration 448, loss = 2.69627421\n",
      "Iteration 449, loss = 2.69523315\n",
      "Iteration 450, loss = 2.69517559\n",
      "Iteration 451, loss = 2.69334984\n",
      "Iteration 452, loss = 2.69217733\n",
      "Iteration 453, loss = 2.69122370\n",
      "Iteration 454, loss = 2.69076218\n",
      "Iteration 455, loss = 2.68992144\n",
      "Iteration 456, loss = 2.68889577\n",
      "Iteration 457, loss = 2.68872262\n",
      "Iteration 458, loss = 2.68764999\n",
      "Iteration 459, loss = 2.68652287\n",
      "Iteration 460, loss = 2.68601913\n",
      "Iteration 461, loss = 2.68573410\n",
      "Iteration 462, loss = 2.68431392\n",
      "Iteration 463, loss = 2.68458067\n",
      "Iteration 464, loss = 2.68267243\n",
      "Iteration 465, loss = 2.68107106\n",
      "Iteration 466, loss = 2.68024390\n",
      "Iteration 467, loss = 2.67947840\n",
      "Iteration 468, loss = 2.67819974\n",
      "Iteration 469, loss = 2.67730350\n",
      "Iteration 470, loss = 2.67596479\n",
      "Iteration 471, loss = 2.67510372\n",
      "Iteration 472, loss = 2.67476586\n",
      "Iteration 473, loss = 2.67449134\n",
      "Iteration 474, loss = 2.67314012\n",
      "Iteration 475, loss = 2.67264696\n",
      "Iteration 476, loss = 2.67078696\n",
      "Iteration 477, loss = 2.66929507\n",
      "Iteration 478, loss = 2.66807683\n",
      "Iteration 479, loss = 2.66802253\n",
      "Iteration 480, loss = 2.66679541\n",
      "Iteration 481, loss = 2.66578067\n",
      "Iteration 482, loss = 2.66530987\n",
      "Iteration 483, loss = 2.66498840\n",
      "Iteration 484, loss = 2.66245574\n",
      "Iteration 485, loss = 2.66205542\n",
      "Iteration 486, loss = 2.66159923\n",
      "Iteration 487, loss = 2.66229539\n",
      "Iteration 488, loss = 2.66005268\n",
      "Iteration 489, loss = 2.65777417\n",
      "Iteration 490, loss = 2.65751307\n",
      "Iteration 491, loss = 2.65674647\n",
      "Iteration 492, loss = 2.65615887\n",
      "Iteration 493, loss = 2.65508744\n",
      "Iteration 494, loss = 2.65450619\n",
      "Iteration 495, loss = 2.65360413\n",
      "Iteration 496, loss = 2.65414377\n",
      "Iteration 497, loss = 2.65227548\n",
      "Iteration 498, loss = 2.65195778\n",
      "Iteration 499, loss = 2.65133894\n",
      "Iteration 500, loss = 2.64931884\n",
      "Iteration 501, loss = 2.64776981\n",
      "Iteration 502, loss = 2.64665210\n",
      "Iteration 503, loss = 2.64602504\n",
      "Iteration 504, loss = 2.64506899\n",
      "Iteration 505, loss = 2.64561266\n",
      "Iteration 506, loss = 2.64465542\n",
      "Iteration 507, loss = 2.64327710\n",
      "Iteration 508, loss = 2.64179505\n",
      "Iteration 509, loss = 2.64134605\n",
      "Iteration 510, loss = 2.64096196\n",
      "Iteration 511, loss = 2.64089609\n",
      "Iteration 512, loss = 2.63894763\n",
      "Iteration 513, loss = 2.63680887\n",
      "Iteration 514, loss = 2.63652920\n",
      "Iteration 515, loss = 2.63679713\n",
      "Iteration 516, loss = 2.63569471\n",
      "Iteration 517, loss = 2.63492932\n",
      "Iteration 518, loss = 2.63275193\n",
      "Iteration 519, loss = 2.63239135\n",
      "Iteration 520, loss = 2.63243888\n",
      "Iteration 521, loss = 2.63138778\n",
      "Iteration 522, loss = 2.63000916\n",
      "Iteration 523, loss = 2.62830173\n",
      "Iteration 524, loss = 2.62775516\n",
      "Iteration 525, loss = 2.62639326\n",
      "Iteration 526, loss = 2.62762465\n",
      "Iteration 527, loss = 2.62759617\n",
      "Iteration 528, loss = 2.62502405\n",
      "Iteration 529, loss = 2.62515932\n",
      "Iteration 530, loss = 2.62478357\n",
      "Iteration 531, loss = 2.62190670\n",
      "Iteration 532, loss = 2.62136080\n",
      "Iteration 533, loss = 2.62098971\n",
      "Iteration 534, loss = 2.62034868\n",
      "Iteration 535, loss = 2.61942469\n",
      "Iteration 536, loss = 2.61739349\n",
      "Iteration 537, loss = 2.61627840\n",
      "Iteration 538, loss = 2.61624156\n",
      "Iteration 539, loss = 2.61723875\n",
      "Iteration 540, loss = 2.61472034\n",
      "Iteration 541, loss = 2.61344364\n",
      "Iteration 542, loss = 2.61238057\n",
      "Iteration 543, loss = 2.61273093\n",
      "Iteration 544, loss = 2.61141447\n",
      "Iteration 545, loss = 2.61132144\n",
      "Iteration 546, loss = 2.61103000\n",
      "Iteration 547, loss = 2.60804701\n",
      "Iteration 548, loss = 2.60762188\n",
      "Iteration 549, loss = 2.60762341\n",
      "Iteration 550, loss = 2.60662690\n",
      "Iteration 551, loss = 2.60670656\n",
      "Iteration 552, loss = 2.60615658\n",
      "Iteration 553, loss = 2.60498265\n",
      "Iteration 554, loss = 2.60429454\n",
      "Iteration 555, loss = 2.60224633\n",
      "Iteration 556, loss = 2.60096694\n",
      "Iteration 557, loss = 2.59916986\n",
      "Iteration 558, loss = 2.59937980\n",
      "Iteration 559, loss = 2.59867187\n",
      "Iteration 560, loss = 2.59823616\n",
      "Iteration 561, loss = 2.59668352\n",
      "Iteration 562, loss = 2.59699902\n",
      "Iteration 563, loss = 2.59623895\n",
      "Iteration 564, loss = 2.59544816\n",
      "Iteration 565, loss = 2.59420752\n",
      "Iteration 566, loss = 2.59311839\n",
      "Iteration 567, loss = 2.59222672\n",
      "Iteration 568, loss = 2.59224044\n",
      "Iteration 569, loss = 2.59150651\n",
      "Iteration 570, loss = 2.59021073\n",
      "Iteration 571, loss = 2.59467738\n",
      "Iteration 572, loss = 2.59134007\n",
      "Iteration 573, loss = 2.58766201\n",
      "Iteration 574, loss = 2.58861939\n",
      "Iteration 575, loss = 2.58667712\n",
      "Iteration 576, loss = 2.58575220\n",
      "Iteration 577, loss = 2.58516741\n",
      "Iteration 578, loss = 2.58577860\n",
      "Iteration 579, loss = 2.58226476\n",
      "Iteration 580, loss = 2.58235997\n",
      "Iteration 581, loss = 2.58188499\n",
      "Iteration 582, loss = 2.58040985\n",
      "Iteration 583, loss = 2.57947921\n",
      "Iteration 584, loss = 2.57827783\n",
      "Iteration 585, loss = 2.57905177\n",
      "Iteration 586, loss = 2.57782786\n",
      "Iteration 587, loss = 2.57726942\n",
      "Iteration 588, loss = 2.57603494\n",
      "Iteration 589, loss = 2.57538312\n",
      "Iteration 590, loss = 2.57426702\n",
      "Iteration 591, loss = 2.57426096\n",
      "Iteration 592, loss = 2.57235644\n",
      "Iteration 593, loss = 2.57170639\n",
      "Iteration 594, loss = 2.57018345\n",
      "Iteration 595, loss = 2.57000909\n",
      "Iteration 596, loss = 2.56931105\n",
      "Iteration 597, loss = 2.56905953\n",
      "Iteration 598, loss = 2.56854799\n",
      "Iteration 599, loss = 2.56790892\n",
      "Iteration 600, loss = 2.56825244\n",
      "Iteration 601, loss = 2.56649881\n",
      "Iteration 602, loss = 2.56406774\n",
      "Iteration 603, loss = 2.56414338\n",
      "Iteration 604, loss = 2.56321085\n",
      "Iteration 605, loss = 2.56223585\n",
      "Iteration 606, loss = 2.56242805\n",
      "Iteration 607, loss = 2.56026277\n",
      "Iteration 608, loss = 2.56073650\n",
      "Iteration 609, loss = 2.55841960\n",
      "Iteration 610, loss = 2.55946674\n",
      "Iteration 611, loss = 2.56061822\n",
      "Iteration 612, loss = 2.55888932\n",
      "Iteration 613, loss = 2.55604376\n",
      "Iteration 614, loss = 2.55756918\n",
      "Iteration 615, loss = 2.55549829\n",
      "Iteration 616, loss = 2.55642280\n",
      "Iteration 617, loss = 2.55399857\n",
      "Iteration 618, loss = 2.55313223\n",
      "Iteration 619, loss = 2.55249662\n",
      "Iteration 620, loss = 2.55090691\n",
      "Iteration 621, loss = 2.54967871\n",
      "Iteration 622, loss = 2.55026951\n",
      "Iteration 623, loss = 2.54936545\n",
      "Iteration 624, loss = 2.54838147\n",
      "Iteration 625, loss = 2.54880204\n",
      "Iteration 626, loss = 2.54698840\n",
      "Iteration 627, loss = 2.54655192\n",
      "Iteration 628, loss = 2.54666891\n",
      "Iteration 629, loss = 2.54563841\n",
      "Iteration 630, loss = 2.54490728\n",
      "Iteration 631, loss = 2.54337072\n",
      "Iteration 632, loss = 2.54300891\n",
      "Iteration 633, loss = 2.54202751\n",
      "Iteration 634, loss = 2.54085510\n",
      "Iteration 635, loss = 2.53961609\n",
      "Iteration 636, loss = 2.54116990\n",
      "Iteration 637, loss = 2.53906720\n",
      "Iteration 638, loss = 2.53840881\n",
      "Iteration 639, loss = 2.53894139\n",
      "Iteration 640, loss = 2.53742130\n",
      "Iteration 641, loss = 2.53578055\n",
      "Iteration 642, loss = 2.53439141\n",
      "Iteration 643, loss = 2.53409783\n",
      "Iteration 644, loss = 2.53290360\n",
      "Iteration 645, loss = 2.53326676\n",
      "Iteration 646, loss = 2.53278140\n",
      "Iteration 647, loss = 2.53299825\n",
      "Iteration 648, loss = 2.52968371\n",
      "Iteration 649, loss = 2.53057042\n",
      "Iteration 650, loss = 2.53023171\n",
      "Iteration 651, loss = 2.52794778\n",
      "Iteration 652, loss = 2.52709483\n",
      "Iteration 653, loss = 2.52827318\n",
      "Iteration 654, loss = 2.52684928\n",
      "Iteration 655, loss = 2.52554159\n",
      "Iteration 656, loss = 2.52482228\n",
      "Iteration 657, loss = 2.52462999\n",
      "Iteration 658, loss = 2.52301997\n",
      "Iteration 659, loss = 2.52265597\n",
      "Iteration 660, loss = 2.52184875\n",
      "Iteration 661, loss = 2.52042221\n",
      "Iteration 662, loss = 2.52029996\n",
      "Iteration 663, loss = 2.52064345\n",
      "Iteration 664, loss = 2.51899714\n",
      "Iteration 665, loss = 2.51897344\n",
      "Iteration 666, loss = 2.51872609\n",
      "Iteration 667, loss = 2.51688285\n",
      "Iteration 668, loss = 2.51529608\n",
      "Iteration 669, loss = 2.51573471\n",
      "Iteration 670, loss = 2.51446706\n",
      "Iteration 671, loss = 2.51513887\n",
      "Iteration 672, loss = 2.51281992\n",
      "Iteration 673, loss = 2.51192155\n",
      "Iteration 674, loss = 2.51133825\n",
      "Iteration 675, loss = 2.51091351\n",
      "Iteration 676, loss = 2.51103574\n",
      "Iteration 677, loss = 2.51027705\n",
      "Iteration 678, loss = 2.50922051\n",
      "Iteration 679, loss = 2.50854651\n",
      "Iteration 680, loss = 2.50753673\n",
      "Iteration 681, loss = 2.50736233\n",
      "Iteration 682, loss = 2.50735151\n",
      "Iteration 683, loss = 2.50583870\n",
      "Iteration 684, loss = 2.50507654\n",
      "Iteration 685, loss = 2.50394796\n",
      "Iteration 686, loss = 2.50717238\n",
      "Iteration 687, loss = 2.50281376\n",
      "Iteration 688, loss = 2.50154398\n",
      "Iteration 689, loss = 2.50204665\n",
      "Iteration 690, loss = 2.49982421\n",
      "Iteration 691, loss = 2.50007770\n",
      "Iteration 692, loss = 2.49928460\n",
      "Iteration 693, loss = 2.49818466\n",
      "Iteration 694, loss = 2.49773375\n",
      "Iteration 695, loss = 2.49605963\n",
      "Iteration 696, loss = 2.49596764\n",
      "Iteration 697, loss = 2.49575502\n",
      "Iteration 698, loss = 2.49530632\n",
      "Iteration 699, loss = 2.49453912\n",
      "Iteration 700, loss = 2.49296952\n",
      "Iteration 701, loss = 2.49381716\n",
      "Iteration 702, loss = 2.49245823\n",
      "Iteration 703, loss = 2.49218049\n",
      "Iteration 704, loss = 2.49154595\n",
      "Iteration 705, loss = 2.49160335\n",
      "Iteration 706, loss = 2.49151930\n",
      "Iteration 707, loss = 2.48876023\n",
      "Iteration 708, loss = 2.48808522\n",
      "Iteration 709, loss = 2.48759660\n",
      "Iteration 710, loss = 2.48685253\n",
      "Iteration 711, loss = 2.48621519\n",
      "Iteration 712, loss = 2.48669498\n",
      "Iteration 713, loss = 2.48412041\n",
      "Iteration 714, loss = 2.48331675\n",
      "Iteration 715, loss = 2.48350995\n",
      "Iteration 716, loss = 2.48272611\n",
      "Iteration 717, loss = 2.48148174\n",
      "Iteration 718, loss = 2.48064881\n",
      "Iteration 719, loss = 2.47907271\n",
      "Iteration 720, loss = 2.47915623\n",
      "Iteration 721, loss = 2.48113290\n",
      "Iteration 722, loss = 2.47863989\n",
      "Iteration 723, loss = 2.47773875\n",
      "Iteration 724, loss = 2.47803702\n",
      "Iteration 725, loss = 2.47575692\n",
      "Iteration 726, loss = 2.47613582\n",
      "Iteration 727, loss = 2.47553704\n",
      "Iteration 728, loss = 2.47465984\n",
      "Iteration 729, loss = 2.47380791\n",
      "Iteration 730, loss = 2.47222460\n",
      "Iteration 731, loss = 2.47386009\n",
      "Iteration 732, loss = 2.47356989\n",
      "Iteration 733, loss = 2.47240393\n",
      "Iteration 734, loss = 2.47080413\n",
      "Iteration 735, loss = 2.46951219\n",
      "Iteration 736, loss = 2.46918254\n",
      "Iteration 737, loss = 2.46828267\n",
      "Iteration 738, loss = 2.46670859\n",
      "Iteration 739, loss = 2.46610542\n",
      "Iteration 740, loss = 2.46831002\n",
      "Iteration 741, loss = 2.46709868\n",
      "Iteration 742, loss = 2.46547667\n",
      "Iteration 743, loss = 2.46617115\n",
      "Iteration 744, loss = 2.46446003\n",
      "Iteration 745, loss = 2.46330671\n",
      "Iteration 746, loss = 2.46361008\n",
      "Iteration 747, loss = 2.46169540\n",
      "Iteration 748, loss = 2.46093588\n",
      "Iteration 749, loss = 2.45961720\n",
      "Iteration 750, loss = 2.45970685\n",
      "Iteration 751, loss = 2.45818989\n",
      "Iteration 752, loss = 2.45793549\n",
      "Iteration 753, loss = 2.45757847\n",
      "Iteration 754, loss = 2.45751928\n",
      "Iteration 755, loss = 2.45705542\n",
      "Iteration 756, loss = 2.45564770\n",
      "Iteration 757, loss = 2.45665066\n",
      "Iteration 758, loss = 2.45433299\n",
      "Iteration 759, loss = 2.45334656\n",
      "Iteration 760, loss = 2.45374330\n",
      "Iteration 761, loss = 2.45236069\n",
      "Iteration 762, loss = 2.45199563\n",
      "Iteration 763, loss = 2.45079540\n",
      "Iteration 764, loss = 2.44981750\n",
      "Iteration 765, loss = 2.45020532\n",
      "Iteration 766, loss = 2.44970878\n",
      "Iteration 767, loss = 2.44917441\n",
      "Iteration 768, loss = 2.44865464\n",
      "Iteration 769, loss = 2.44822251\n",
      "Iteration 770, loss = 2.44554282\n",
      "Iteration 771, loss = 2.44626670\n",
      "Iteration 772, loss = 2.44985684\n",
      "Iteration 773, loss = 2.44492170\n",
      "Iteration 774, loss = 2.44686529\n",
      "Iteration 775, loss = 2.44365301\n",
      "Iteration 776, loss = 2.44282227\n",
      "Iteration 777, loss = 2.44318859\n",
      "Iteration 778, loss = 2.44191851\n",
      "Iteration 779, loss = 2.44062592\n",
      "Iteration 780, loss = 2.44063562\n",
      "Iteration 781, loss = 2.44004207\n",
      "Iteration 782, loss = 2.43914342\n",
      "Iteration 783, loss = 2.43738701\n",
      "Iteration 784, loss = 2.43797601\n",
      "Iteration 785, loss = 2.43656021\n",
      "Iteration 786, loss = 2.43610514\n",
      "Iteration 787, loss = 2.43634757\n",
      "Iteration 788, loss = 2.43550728\n",
      "Iteration 789, loss = 2.43606972\n",
      "Iteration 790, loss = 2.43281023\n",
      "Iteration 791, loss = 2.43169054\n",
      "Iteration 792, loss = 2.43134332\n",
      "Iteration 793, loss = 2.43152959\n",
      "Iteration 794, loss = 2.43084437\n",
      "Iteration 795, loss = 2.42956782\n",
      "Iteration 796, loss = 2.43015145\n",
      "Iteration 797, loss = 2.43099881\n",
      "Iteration 798, loss = 2.42863457\n",
      "Iteration 799, loss = 2.42752403\n",
      "Iteration 800, loss = 2.42719041\n",
      "Iteration 801, loss = 2.42827786\n",
      "Iteration 802, loss = 2.42670042\n",
      "Iteration 803, loss = 2.42549898\n",
      "Iteration 804, loss = 2.42446762\n",
      "Iteration 805, loss = 2.42554771\n",
      "Iteration 806, loss = 2.42501175\n",
      "Iteration 807, loss = 2.42318201\n",
      "Iteration 808, loss = 2.42220989\n",
      "Iteration 809, loss = 2.42311207\n",
      "Iteration 810, loss = 2.42008516\n",
      "Iteration 811, loss = 2.42139160\n",
      "Iteration 812, loss = 2.41920126\n",
      "Iteration 813, loss = 2.41826143\n",
      "Iteration 814, loss = 2.41853091\n",
      "Iteration 815, loss = 2.41813449\n",
      "Iteration 816, loss = 2.41729691\n",
      "Iteration 817, loss = 2.41722399\n",
      "Iteration 818, loss = 2.41731922\n",
      "Iteration 819, loss = 2.41610005\n",
      "Iteration 820, loss = 2.41470977\n",
      "Iteration 821, loss = 2.41581835\n",
      "Iteration 822, loss = 2.41349439\n",
      "Iteration 823, loss = 2.41284873\n",
      "Iteration 824, loss = 2.41319537\n",
      "Iteration 825, loss = 2.41252096\n",
      "Iteration 826, loss = 2.41259358\n",
      "Iteration 827, loss = 2.41115255\n",
      "Iteration 828, loss = 2.41005822\n",
      "Iteration 829, loss = 2.41019917\n",
      "Iteration 830, loss = 2.40920940\n",
      "Iteration 831, loss = 2.40831769\n",
      "Iteration 832, loss = 2.40773356\n",
      "Iteration 833, loss = 2.40739078\n",
      "Iteration 834, loss = 2.40698129\n",
      "Iteration 835, loss = 2.40645395\n",
      "Iteration 836, loss = 2.40480198\n",
      "Iteration 837, loss = 2.40408029\n",
      "Iteration 838, loss = 2.40469339\n",
      "Iteration 839, loss = 2.40426499\n",
      "Iteration 840, loss = 2.40283055\n",
      "Iteration 841, loss = 2.40112637\n",
      "Iteration 842, loss = 2.40064197\n",
      "Iteration 843, loss = 2.40076007\n",
      "Iteration 844, loss = 2.39990058\n",
      "Iteration 845, loss = 2.39919819\n",
      "Iteration 846, loss = 2.40002993\n",
      "Iteration 847, loss = 2.39869886\n",
      "Iteration 848, loss = 2.39777291\n",
      "Iteration 849, loss = 2.39675191\n",
      "Iteration 850, loss = 2.39696350\n",
      "Iteration 851, loss = 2.39811168\n",
      "Iteration 852, loss = 2.39534507\n",
      "Iteration 853, loss = 2.39480560\n",
      "Iteration 854, loss = 2.39300578\n",
      "Iteration 855, loss = 2.39427667\n",
      "Iteration 856, loss = 2.39353055\n",
      "Iteration 857, loss = 2.39242714\n",
      "Iteration 858, loss = 2.39211602\n",
      "Iteration 859, loss = 2.39215009\n",
      "Iteration 860, loss = 2.39139088\n",
      "Iteration 861, loss = 2.39074917\n",
      "Iteration 862, loss = 2.38943565\n",
      "Iteration 863, loss = 2.38776040\n",
      "Iteration 864, loss = 2.38754381\n",
      "Iteration 865, loss = 2.38803149\n",
      "Iteration 866, loss = 2.38714793\n",
      "Iteration 867, loss = 2.38629344\n",
      "Iteration 868, loss = 2.38688927\n",
      "Iteration 869, loss = 2.38571327\n",
      "Iteration 870, loss = 2.38488526\n",
      "Iteration 871, loss = 2.38423538\n",
      "Iteration 872, loss = 2.38296771\n",
      "Iteration 873, loss = 2.38364167\n",
      "Iteration 874, loss = 2.38248105\n",
      "Iteration 875, loss = 2.38302102\n",
      "Iteration 876, loss = 2.38291737\n",
      "Iteration 877, loss = 2.38246439\n",
      "Iteration 878, loss = 2.38099943\n",
      "Iteration 879, loss = 2.37968800\n",
      "Iteration 880, loss = 2.37957301\n",
      "Iteration 881, loss = 2.38022451\n",
      "Iteration 882, loss = 2.37894568\n",
      "Iteration 883, loss = 2.37730339\n",
      "Iteration 884, loss = 2.37813673\n",
      "Iteration 885, loss = 2.37645662\n",
      "Iteration 886, loss = 2.37665270\n",
      "Iteration 887, loss = 2.37577533\n",
      "Iteration 888, loss = 2.37454610\n",
      "Iteration 889, loss = 2.37494258\n",
      "Iteration 890, loss = 2.37410108\n",
      "Iteration 891, loss = 2.37321868\n",
      "Iteration 892, loss = 2.37303153\n",
      "Iteration 893, loss = 2.37103638\n",
      "Iteration 894, loss = 2.37119943\n",
      "Iteration 895, loss = 2.37030118\n",
      "Iteration 896, loss = 2.37059038\n",
      "Iteration 897, loss = 2.36982344\n",
      "Iteration 898, loss = 2.36895160\n",
      "Iteration 899, loss = 2.36766207\n",
      "Iteration 900, loss = 2.36739826\n",
      "Iteration 901, loss = 2.36979641\n",
      "Iteration 902, loss = 2.36760621\n",
      "Iteration 903, loss = 2.36572118\n",
      "Iteration 904, loss = 2.36451098\n",
      "Iteration 905, loss = 2.36359735\n",
      "Iteration 906, loss = 2.36423749\n",
      "Iteration 907, loss = 2.36566376\n",
      "Iteration 908, loss = 2.36451453\n",
      "Iteration 909, loss = 2.36222737\n",
      "Iteration 910, loss = 2.36266319\n",
      "Iteration 911, loss = 2.36110587\n",
      "Iteration 912, loss = 2.36060728\n",
      "Iteration 913, loss = 2.35937015\n",
      "Iteration 914, loss = 2.35988870\n",
      "Iteration 915, loss = 2.35892545\n",
      "Iteration 916, loss = 2.35781331\n",
      "Iteration 917, loss = 2.35924016\n",
      "Iteration 918, loss = 2.36094752\n",
      "Iteration 919, loss = 2.35857084\n",
      "Iteration 920, loss = 2.35697561\n",
      "Iteration 921, loss = 2.35575422\n",
      "Iteration 922, loss = 2.35414567\n",
      "Iteration 923, loss = 2.35325018\n",
      "Iteration 924, loss = 2.35340165\n",
      "Iteration 925, loss = 2.35432767\n",
      "Iteration 926, loss = 2.35330428\n",
      "Iteration 927, loss = 2.35328024\n",
      "Iteration 928, loss = 2.35155868\n",
      "Iteration 929, loss = 2.34918119\n",
      "Iteration 930, loss = 2.34963323\n",
      "Iteration 931, loss = 2.34928627\n",
      "Iteration 932, loss = 2.34867253\n",
      "Iteration 933, loss = 2.34792552\n",
      "Iteration 934, loss = 2.34743670\n",
      "Iteration 935, loss = 2.34607369\n",
      "Iteration 936, loss = 2.34731746\n",
      "Iteration 937, loss = 2.34678392\n",
      "Iteration 938, loss = 2.34606714\n",
      "Iteration 939, loss = 2.34502209\n",
      "Iteration 940, loss = 2.34528102\n",
      "Iteration 941, loss = 2.34640289\n",
      "Iteration 942, loss = 2.34366309\n",
      "Iteration 943, loss = 2.34340900\n",
      "Iteration 944, loss = 2.34243259\n",
      "Iteration 945, loss = 2.34109445\n",
      "Iteration 946, loss = 2.34138726\n",
      "Iteration 947, loss = 2.34027310\n",
      "Iteration 948, loss = 2.33899883\n",
      "Iteration 949, loss = 2.33791223\n",
      "Iteration 950, loss = 2.33815034\n",
      "Iteration 951, loss = 2.33970329\n",
      "Iteration 952, loss = 2.33855151\n",
      "Iteration 953, loss = 2.33791085\n",
      "Iteration 954, loss = 2.33663679\n",
      "Iteration 955, loss = 2.33637703\n",
      "Iteration 956, loss = 2.33568075\n",
      "Iteration 957, loss = 2.33456512\n",
      "Iteration 958, loss = 2.33413039\n",
      "Iteration 959, loss = 2.33419341\n",
      "Iteration 960, loss = 2.33407103\n",
      "Iteration 961, loss = 2.33274638\n",
      "Iteration 962, loss = 2.33310880\n",
      "Iteration 963, loss = 2.33245755\n",
      "Iteration 964, loss = 2.33193714\n",
      "Iteration 965, loss = 2.33027025\n",
      "Iteration 966, loss = 2.33097740\n",
      "Iteration 967, loss = 2.32994334\n",
      "Iteration 968, loss = 2.32864720\n",
      "Iteration 969, loss = 2.32874283\n",
      "Iteration 970, loss = 2.32904403\n",
      "Iteration 971, loss = 2.32714258\n",
      "Iteration 972, loss = 2.32762892\n",
      "Iteration 973, loss = 2.32821277\n",
      "Iteration 974, loss = 2.32665109\n",
      "Iteration 975, loss = 2.32679961\n",
      "Iteration 976, loss = 2.32472722\n",
      "Iteration 977, loss = 2.32303034\n",
      "Iteration 978, loss = 2.32414172\n",
      "Iteration 979, loss = 2.32297066\n",
      "Iteration 980, loss = 2.32439048\n",
      "Iteration 981, loss = 2.32408084\n",
      "Iteration 982, loss = 2.32320606\n",
      "Iteration 983, loss = 2.32157491\n",
      "Iteration 984, loss = 2.32064589\n",
      "Iteration 985, loss = 2.32036399\n",
      "Iteration 986, loss = 2.32067175\n",
      "Iteration 987, loss = 2.31799722\n",
      "Iteration 988, loss = 2.31912210\n",
      "Iteration 989, loss = 2.31935423\n",
      "Iteration 990, loss = 2.31882215\n",
      "Iteration 991, loss = 2.31848503\n",
      "Iteration 992, loss = 2.31747064\n",
      "Iteration 993, loss = 2.31683924\n",
      "Iteration 994, loss = 2.31625602\n",
      "Iteration 995, loss = 2.31521188\n",
      "Iteration 996, loss = 2.31470873\n",
      "Iteration 997, loss = 2.31523262\n",
      "Iteration 998, loss = 2.31478545\n",
      "Iteration 999, loss = 2.31449514\n",
      "Iteration 1000, loss = 2.31247169\n",
      "Iteration 1001, loss = 2.31160692\n",
      "Iteration 1002, loss = 2.31095019\n",
      "Iteration 1003, loss = 2.31105245\n",
      "Iteration 1004, loss = 2.31236607\n",
      "Iteration 1005, loss = 2.30958992\n",
      "Iteration 1006, loss = 2.30896939\n",
      "Iteration 1007, loss = 2.30871025\n",
      "Iteration 1008, loss = 2.30868875\n",
      "Iteration 1009, loss = 2.30871474\n",
      "Iteration 1010, loss = 2.30641639\n",
      "Iteration 1011, loss = 2.30607244\n",
      "Iteration 1012, loss = 2.30583833\n",
      "Iteration 1013, loss = 2.30542627\n",
      "Iteration 1014, loss = 2.30437478\n",
      "Iteration 1015, loss = 2.30383020\n",
      "Iteration 1016, loss = 2.30416593\n",
      "Iteration 1017, loss = 2.30253343\n",
      "Iteration 1018, loss = 2.30275834\n",
      "Iteration 1019, loss = 2.30214332\n",
      "Iteration 1020, loss = 2.30173195\n",
      "Iteration 1021, loss = 2.30163949\n",
      "Iteration 1022, loss = 2.30243804\n",
      "Iteration 1023, loss = 2.29954775\n",
      "Iteration 1024, loss = 2.30162062\n",
      "Iteration 1025, loss = 2.30041564\n",
      "Iteration 1026, loss = 2.30001456\n",
      "Iteration 1027, loss = 2.29864955\n",
      "Iteration 1028, loss = 2.30054625\n",
      "Iteration 1029, loss = 2.29930175\n",
      "Iteration 1030, loss = 2.29549467\n",
      "Iteration 1031, loss = 2.29676998\n",
      "Iteration 1032, loss = 2.29685633\n",
      "Iteration 1033, loss = 2.29482946\n",
      "Iteration 1034, loss = 2.29561560\n",
      "Iteration 1035, loss = 2.29432856\n",
      "Iteration 1036, loss = 2.29282013\n",
      "Iteration 1037, loss = 2.29237358\n",
      "Iteration 1038, loss = 2.29337241\n",
      "Iteration 1039, loss = 2.29270847\n",
      "Iteration 1040, loss = 2.29130008\n",
      "Iteration 1041, loss = 2.29156593\n",
      "Iteration 1042, loss = 2.29129422\n",
      "Iteration 1043, loss = 2.29031144\n",
      "Iteration 1044, loss = 2.28890709\n",
      "Iteration 1045, loss = 2.28828666\n",
      "Iteration 1046, loss = 2.28818980\n",
      "Iteration 1047, loss = 2.28824384\n",
      "Iteration 1048, loss = 2.28661972\n",
      "Iteration 1049, loss = 2.28863719\n",
      "Iteration 1050, loss = 2.28774546\n",
      "Iteration 1051, loss = 2.28604784\n",
      "Iteration 1052, loss = 2.28612688\n",
      "Iteration 1053, loss = 2.28535994\n",
      "Iteration 1054, loss = 2.28580456\n",
      "Iteration 1055, loss = 2.28489247\n",
      "Iteration 1056, loss = 2.28283114\n",
      "Iteration 1057, loss = 2.28257756\n",
      "Iteration 1058, loss = 2.28173438\n",
      "Iteration 1059, loss = 2.28157675\n",
      "Iteration 1060, loss = 2.28077771\n",
      "Iteration 1061, loss = 2.28103641\n",
      "Iteration 1062, loss = 2.27956342\n",
      "Iteration 1063, loss = 2.27954107\n",
      "Iteration 1064, loss = 2.27918633\n",
      "Iteration 1065, loss = 2.27850138\n",
      "Iteration 1066, loss = 2.27779065\n",
      "Iteration 1067, loss = 2.27771892\n",
      "Iteration 1068, loss = 2.27642388\n",
      "Iteration 1069, loss = 2.27615242\n",
      "Iteration 1070, loss = 2.27616299\n",
      "Iteration 1071, loss = 2.27663735\n",
      "Iteration 1072, loss = 2.27627278\n",
      "Iteration 1073, loss = 2.27518837\n",
      "Iteration 1074, loss = 2.27388564\n",
      "Iteration 1075, loss = 2.27352372\n",
      "Iteration 1076, loss = 2.27285154\n",
      "Iteration 1077, loss = 2.27149180\n",
      "Iteration 1078, loss = 2.27395353\n",
      "Iteration 1079, loss = 2.27114658\n",
      "Iteration 1080, loss = 2.27155637\n",
      "Iteration 1081, loss = 2.27027854\n",
      "Iteration 1082, loss = 2.27047827\n",
      "Iteration 1083, loss = 2.27034207\n",
      "Iteration 1084, loss = 2.26970444\n",
      "Iteration 1085, loss = 2.27100426\n",
      "Iteration 1086, loss = 2.27030784\n",
      "Iteration 1087, loss = 2.26906888\n",
      "Iteration 1088, loss = 2.26789497\n",
      "Iteration 1089, loss = 2.26681241\n",
      "Iteration 1090, loss = 2.26640467\n",
      "Iteration 1091, loss = 2.26445306\n",
      "Iteration 1092, loss = 2.26780024\n",
      "Iteration 1093, loss = 2.26607493\n",
      "Iteration 1094, loss = 2.26437432\n",
      "Iteration 1095, loss = 2.26487011\n",
      "Iteration 1096, loss = 2.26605203\n",
      "Iteration 1097, loss = 2.26321846\n",
      "Iteration 1098, loss = 2.26147012\n",
      "Iteration 1099, loss = 2.26105677\n",
      "Iteration 1100, loss = 2.26128242\n",
      "Iteration 1101, loss = 2.26072700\n",
      "Iteration 1102, loss = 2.26114467\n",
      "Iteration 1103, loss = 2.25938245\n",
      "Iteration 1104, loss = 2.26320100\n",
      "Iteration 1105, loss = 2.26040953\n",
      "Iteration 1106, loss = 2.25910027\n",
      "Iteration 1107, loss = 2.25809142\n",
      "Iteration 1108, loss = 2.25719518\n",
      "Iteration 1109, loss = 2.25638931\n",
      "Iteration 1110, loss = 2.25632506\n",
      "Iteration 1111, loss = 2.25560192\n",
      "Iteration 1112, loss = 2.25416053\n",
      "Iteration 1113, loss = 2.25476879\n",
      "Iteration 1114, loss = 2.25637032\n",
      "Iteration 1115, loss = 2.25509942\n",
      "Iteration 1116, loss = 2.25453659\n",
      "Iteration 1117, loss = 2.25207360\n",
      "Iteration 1118, loss = 2.25318363\n",
      "Iteration 1119, loss = 2.25263303\n",
      "Iteration 1120, loss = 2.25173845\n",
      "Iteration 1121, loss = 2.25442040\n",
      "Iteration 1122, loss = 2.25279451\n",
      "Iteration 1123, loss = 2.25065587\n",
      "Iteration 1124, loss = 2.24943982\n",
      "Iteration 1125, loss = 2.24927174\n",
      "Iteration 1126, loss = 2.25104134\n",
      "Iteration 1127, loss = 2.25041508\n",
      "Iteration 1128, loss = 2.24803619\n",
      "Iteration 1129, loss = 2.24686862\n",
      "Iteration 1130, loss = 2.24693521\n",
      "Iteration 1131, loss = 2.24666734\n",
      "Iteration 1132, loss = 2.24470204\n",
      "Iteration 1133, loss = 2.24396408\n",
      "Iteration 1134, loss = 2.24422702\n",
      "Iteration 1135, loss = 2.24310494\n",
      "Iteration 1136, loss = 2.24316232\n",
      "Iteration 1137, loss = 2.24271162\n",
      "Iteration 1138, loss = 2.24389206\n",
      "Iteration 1139, loss = 2.24326652\n",
      "Iteration 1140, loss = 2.24244350\n",
      "Iteration 1141, loss = 2.24180195\n",
      "Iteration 1142, loss = 2.24212318\n",
      "Iteration 1143, loss = 2.24024050\n",
      "Iteration 1144, loss = 2.24086646\n",
      "Iteration 1145, loss = 2.24085421\n",
      "Iteration 1146, loss = 2.24014583\n",
      "Iteration 1147, loss = 2.24040454\n",
      "Iteration 1148, loss = 2.23800877\n",
      "Iteration 1149, loss = 2.23837995\n",
      "Iteration 1150, loss = 2.23656016\n",
      "Iteration 1151, loss = 2.23655924\n",
      "Iteration 1152, loss = 2.23718578\n",
      "Iteration 1153, loss = 2.23589725\n",
      "Iteration 1154, loss = 2.23491912\n",
      "Iteration 1155, loss = 2.23478699\n",
      "Iteration 1156, loss = 2.23333248\n",
      "Iteration 1157, loss = 2.23519029\n",
      "Iteration 1158, loss = 2.23493007\n",
      "Iteration 1159, loss = 2.23450932\n",
      "Iteration 1160, loss = 2.23288250\n",
      "Iteration 1161, loss = 2.23053242\n",
      "Iteration 1162, loss = 2.23195517\n",
      "Iteration 1163, loss = 2.23085998\n",
      "Iteration 1164, loss = 2.23052557\n",
      "Iteration 1165, loss = 2.22854879\n",
      "Iteration 1166, loss = 2.22856147\n",
      "Iteration 1167, loss = 2.22806535\n",
      "Iteration 1168, loss = 2.22916728\n",
      "Iteration 1169, loss = 2.22759522\n",
      "Iteration 1170, loss = 2.22758313\n",
      "Iteration 1171, loss = 2.22728323\n",
      "Iteration 1172, loss = 2.22567422\n",
      "Iteration 1173, loss = 2.22735401\n",
      "Iteration 1174, loss = 2.22616113\n",
      "Iteration 1175, loss = 2.22644563\n",
      "Iteration 1176, loss = 2.22529873\n",
      "Iteration 1177, loss = 2.22501117\n",
      "Iteration 1178, loss = 2.22511274\n",
      "Iteration 1179, loss = 2.22422986\n",
      "Iteration 1180, loss = 2.22193342\n",
      "Iteration 1181, loss = 2.22327039\n",
      "Iteration 1182, loss = 2.22201424\n",
      "Iteration 1183, loss = 2.22259093\n",
      "Iteration 1184, loss = 2.22101161\n",
      "Iteration 1185, loss = 2.22099012\n",
      "Iteration 1186, loss = 2.22149091\n",
      "Iteration 1187, loss = 2.22060756\n",
      "Iteration 1188, loss = 2.21878307\n",
      "Iteration 1189, loss = 2.22009221\n",
      "Iteration 1190, loss = 2.21902770\n",
      "Iteration 1191, loss = 2.22034936\n",
      "Iteration 1192, loss = 2.22104345\n",
      "Iteration 1193, loss = 2.21604163\n",
      "Iteration 1194, loss = 2.21931815\n",
      "Iteration 1195, loss = 2.21730880\n",
      "Iteration 1196, loss = 2.21533466\n",
      "Iteration 1197, loss = 2.21488589\n",
      "Iteration 1198, loss = 2.21371704\n",
      "Iteration 1199, loss = 2.21532654\n",
      "Iteration 1200, loss = 2.21377855\n",
      "Iteration 1201, loss = 2.21266116\n",
      "Iteration 1202, loss = 2.21263548\n",
      "Iteration 1203, loss = 2.21295202\n",
      "Iteration 1204, loss = 2.21301123\n",
      "Iteration 1205, loss = 2.21124644\n",
      "Iteration 1206, loss = 2.21127317\n",
      "Iteration 1207, loss = 2.20980314\n",
      "Iteration 1208, loss = 2.21106413\n",
      "Iteration 1209, loss = 2.20969570\n",
      "Iteration 1210, loss = 2.20844721\n",
      "Iteration 1211, loss = 2.20998531\n",
      "Iteration 1212, loss = 2.20918894\n",
      "Iteration 1213, loss = 2.20938068\n",
      "Iteration 1214, loss = 2.20684561\n",
      "Iteration 1215, loss = 2.20808921\n",
      "Iteration 1216, loss = 2.20820681\n",
      "Iteration 1217, loss = 2.20602954\n",
      "Iteration 1218, loss = 2.20744053\n",
      "Iteration 1219, loss = 2.20613396\n",
      "Iteration 1220, loss = 2.20508045\n",
      "Iteration 1221, loss = 2.20610203\n",
      "Iteration 1222, loss = 2.20534520\n",
      "Iteration 1223, loss = 2.20390290\n",
      "Iteration 1224, loss = 2.20443987\n",
      "Iteration 1225, loss = 2.20386511\n",
      "Iteration 1226, loss = 2.20199168\n",
      "Iteration 1227, loss = 2.20118300\n",
      "Iteration 1228, loss = 2.20066772\n",
      "Iteration 1229, loss = 2.19968296\n",
      "Iteration 1230, loss = 2.19992457\n",
      "Iteration 1231, loss = 2.19873770\n",
      "Iteration 1232, loss = 2.19906734\n",
      "Iteration 1233, loss = 2.20025361\n",
      "Iteration 1234, loss = 2.19889368\n",
      "Iteration 1235, loss = 2.19852432\n",
      "Iteration 1236, loss = 2.19781531\n",
      "Iteration 1237, loss = 2.19827385\n",
      "Iteration 1238, loss = 2.19813809\n",
      "Iteration 1239, loss = 2.19803302\n",
      "Iteration 1240, loss = 2.19737946\n",
      "Iteration 1241, loss = 2.19631363\n",
      "Iteration 1242, loss = 2.19581530\n",
      "Iteration 1243, loss = 2.19485935\n",
      "Iteration 1244, loss = 2.19472544\n",
      "Iteration 1245, loss = 2.19475610\n",
      "Iteration 1246, loss = 2.19469784\n",
      "Iteration 1247, loss = 2.19367889\n",
      "Iteration 1248, loss = 2.19405700\n",
      "Iteration 1249, loss = 2.19137290\n",
      "Iteration 1250, loss = 2.19211656\n",
      "Iteration 1251, loss = 2.19149077\n",
      "Iteration 1252, loss = 2.19253563\n",
      "Iteration 1253, loss = 2.19090219\n",
      "Iteration 1254, loss = 2.19090240\n",
      "Iteration 1255, loss = 2.19018385\n",
      "Iteration 1256, loss = 2.18861655\n",
      "Iteration 1257, loss = 2.18777163\n",
      "Iteration 1258, loss = 2.18701440\n",
      "Iteration 1259, loss = 2.18930140\n",
      "Iteration 1260, loss = 2.18631263\n",
      "Iteration 1261, loss = 2.18715337\n",
      "Iteration 1262, loss = 2.18649228\n",
      "Iteration 1263, loss = 2.18770695\n",
      "Iteration 1264, loss = 2.18506221\n",
      "Iteration 1265, loss = 2.18719038\n",
      "Iteration 1266, loss = 2.18453053\n",
      "Iteration 1267, loss = 2.18468399\n",
      "Iteration 1268, loss = 2.18445427\n",
      "Iteration 1269, loss = 2.18499223\n",
      "Iteration 1270, loss = 2.18623189\n",
      "Iteration 1271, loss = 2.18247826\n",
      "Iteration 1272, loss = 2.18145291\n",
      "Iteration 1273, loss = 2.18181392\n",
      "Iteration 1274, loss = 2.18114429\n",
      "Iteration 1275, loss = 2.18380626\n",
      "Iteration 1276, loss = 2.17991412\n",
      "Iteration 1277, loss = 2.17957245\n",
      "Iteration 1278, loss = 2.18068466\n",
      "Iteration 1279, loss = 2.17987285\n",
      "Iteration 1280, loss = 2.17910755\n",
      "Iteration 1281, loss = 2.17689528\n",
      "Iteration 1282, loss = 2.17682232\n",
      "Iteration 1283, loss = 2.17750639\n",
      "Iteration 1284, loss = 2.17683572\n",
      "Iteration 1285, loss = 2.17702164\n",
      "Iteration 1286, loss = 2.17747834\n",
      "Iteration 1287, loss = 2.17596515\n",
      "Iteration 1288, loss = 2.17617328\n",
      "Iteration 1289, loss = 2.17479538\n",
      "Iteration 1290, loss = 2.17595179\n",
      "Iteration 1291, loss = 2.17756727\n",
      "Iteration 1292, loss = 2.17593960\n",
      "Iteration 1293, loss = 2.17279903\n",
      "Iteration 1294, loss = 2.17335486\n",
      "Iteration 1295, loss = 2.17205513\n",
      "Iteration 1296, loss = 2.17368735\n",
      "Iteration 1297, loss = 2.17368375\n",
      "Iteration 1298, loss = 2.17093041\n",
      "Iteration 1299, loss = 2.16982781\n",
      "Iteration 1300, loss = 2.16986536\n",
      "Iteration 1301, loss = 2.16969186\n",
      "Iteration 1302, loss = 2.16816154\n",
      "Iteration 1303, loss = 2.16891339\n",
      "Iteration 1304, loss = 2.16994817\n",
      "Iteration 1305, loss = 2.16979164\n",
      "Iteration 1306, loss = 2.16790613\n",
      "Iteration 1307, loss = 2.16753806\n",
      "Iteration 1308, loss = 2.16695346\n",
      "Iteration 1309, loss = 2.16653806\n",
      "Iteration 1310, loss = 2.16542518\n",
      "Iteration 1311, loss = 2.16561263\n",
      "Iteration 1312, loss = 2.16512852\n",
      "Iteration 1313, loss = 2.16616543\n",
      "Iteration 1314, loss = 2.16705266\n",
      "Iteration 1315, loss = 2.16569672\n",
      "Iteration 1316, loss = 2.16349105\n",
      "Iteration 1317, loss = 2.16444660\n",
      "Iteration 1318, loss = 2.16370828\n",
      "Iteration 1319, loss = 2.16199288\n",
      "Iteration 1320, loss = 2.16229947\n",
      "Iteration 1321, loss = 2.16232664\n",
      "Iteration 1322, loss = 2.16127288\n",
      "Iteration 1323, loss = 2.16083410\n",
      "Iteration 1324, loss = 2.16070643\n",
      "Iteration 1325, loss = 2.15957024\n",
      "Iteration 1326, loss = 2.15996150\n",
      "Iteration 1327, loss = 2.15925636\n",
      "Iteration 1328, loss = 2.15913385\n",
      "Iteration 1329, loss = 2.15812047\n",
      "Iteration 1330, loss = 2.16030980\n",
      "Iteration 1331, loss = 2.15668982\n",
      "Iteration 1332, loss = 2.15857633\n",
      "Iteration 1333, loss = 2.15674331\n",
      "Iteration 1334, loss = 2.15555442\n",
      "Iteration 1335, loss = 2.15717761\n",
      "Iteration 1336, loss = 2.15654014\n",
      "Iteration 1337, loss = 2.15474535\n",
      "Iteration 1338, loss = 2.15327489\n",
      "Iteration 1339, loss = 2.15496421\n",
      "Iteration 1340, loss = 2.15436954\n",
      "Iteration 1341, loss = 2.15242391\n",
      "Iteration 1342, loss = 2.15314044\n",
      "Iteration 1343, loss = 2.15250369\n",
      "Iteration 1344, loss = 2.15168506\n",
      "Iteration 1345, loss = 2.15120879\n",
      "Iteration 1346, loss = 2.15238672\n",
      "Iteration 1347, loss = 2.15055031\n",
      "Iteration 1348, loss = 2.15264120\n",
      "Iteration 1349, loss = 2.15072877\n",
      "Iteration 1350, loss = 2.15000550\n",
      "Iteration 1351, loss = 2.14870914\n",
      "Iteration 1352, loss = 2.14973167\n",
      "Iteration 1353, loss = 2.14996678\n",
      "Iteration 1354, loss = 2.14969029\n",
      "Iteration 1355, loss = 2.14761814\n",
      "Iteration 1356, loss = 2.14773794\n",
      "Iteration 1357, loss = 2.14776219\n",
      "Iteration 1358, loss = 2.14644303\n",
      "Iteration 1359, loss = 2.14479070\n",
      "Iteration 1360, loss = 2.14654685\n",
      "Iteration 1361, loss = 2.14500714\n",
      "Iteration 1362, loss = 2.14445782\n",
      "Iteration 1363, loss = 2.14314349\n",
      "Iteration 1364, loss = 2.14553776\n",
      "Iteration 1365, loss = 2.14352795\n",
      "Iteration 1366, loss = 2.14439817\n",
      "Iteration 1367, loss = 2.14123533\n",
      "Iteration 1368, loss = 2.14187020\n",
      "Iteration 1369, loss = 2.14150006\n",
      "Iteration 1370, loss = 2.14247622\n",
      "Iteration 1371, loss = 2.14203880\n",
      "Iteration 1372, loss = 2.14036254\n",
      "Iteration 1373, loss = 2.14151106\n",
      "Iteration 1374, loss = 2.14114127\n",
      "Iteration 1375, loss = 2.14051085\n",
      "Iteration 1376, loss = 2.13899530\n",
      "Iteration 1377, loss = 2.13963218\n",
      "Iteration 1378, loss = 2.13936821\n",
      "Iteration 1379, loss = 2.13798350\n",
      "Iteration 1380, loss = 2.13660694\n",
      "Iteration 1381, loss = 2.13727190\n",
      "Iteration 1382, loss = 2.13858669\n",
      "Iteration 1383, loss = 2.13784162\n",
      "Iteration 1384, loss = 2.13673725\n",
      "Iteration 1385, loss = 2.13626167\n",
      "Iteration 1386, loss = 2.13587955\n",
      "Iteration 1387, loss = 2.13524432\n",
      "Iteration 1388, loss = 2.13400979\n",
      "Iteration 1389, loss = 2.13358268\n",
      "Iteration 1390, loss = 2.13384434\n",
      "Iteration 1391, loss = 2.13349708\n",
      "Iteration 1392, loss = 2.13344714\n",
      "Iteration 1393, loss = 2.13108392\n",
      "Iteration 1394, loss = 2.13249856\n",
      "Iteration 1395, loss = 2.13026163\n",
      "Iteration 1396, loss = 2.12925122\n",
      "Iteration 1397, loss = 2.13085690\n",
      "Iteration 1398, loss = 2.13287477\n",
      "Iteration 1399, loss = 2.13045614\n",
      "Iteration 1400, loss = 2.12888954\n",
      "Iteration 1401, loss = 2.13075640\n",
      "Iteration 1402, loss = 2.12825818\n",
      "Iteration 1403, loss = 2.12751431\n",
      "Iteration 1404, loss = 2.12768326\n",
      "Iteration 1405, loss = 2.12737667\n",
      "Iteration 1406, loss = 2.12716204\n",
      "Iteration 1407, loss = 2.12712541\n",
      "Iteration 1408, loss = 2.12668980\n",
      "Iteration 1409, loss = 2.12530046\n",
      "Iteration 1410, loss = 2.12503417\n",
      "Iteration 1411, loss = 2.12558728\n",
      "Iteration 1412, loss = 2.12557689\n",
      "Iteration 1413, loss = 2.12414499\n",
      "Iteration 1414, loss = 2.12621204\n",
      "Iteration 1415, loss = 2.12614519\n",
      "Iteration 1416, loss = 2.12450910\n",
      "Iteration 1417, loss = 2.12269513\n",
      "Iteration 1418, loss = 2.12247638\n",
      "Iteration 1419, loss = 2.12147678\n",
      "Iteration 1420, loss = 2.12235669\n",
      "Iteration 1421, loss = 2.12046113\n",
      "Iteration 1422, loss = 2.11996911\n",
      "Iteration 1423, loss = 2.12052144\n",
      "Iteration 1424, loss = 2.12126515\n",
      "Iteration 1425, loss = 2.12157998\n",
      "Iteration 1426, loss = 2.12097352\n",
      "Iteration 1427, loss = 2.12095761\n",
      "Iteration 1428, loss = 2.11871167\n",
      "Iteration 1429, loss = 2.11837299\n",
      "Iteration 1430, loss = 2.11897859\n",
      "Iteration 1431, loss = 2.11745780\n",
      "Iteration 1432, loss = 2.11626696\n",
      "Iteration 1433, loss = 2.11595980\n",
      "Iteration 1434, loss = 2.11517555\n",
      "Iteration 1435, loss = 2.11582045\n",
      "Iteration 1436, loss = 2.11585481\n",
      "Iteration 1437, loss = 2.11593381\n",
      "Iteration 1438, loss = 2.11712389\n",
      "Iteration 1439, loss = 2.11547094\n",
      "Iteration 1440, loss = 2.11870668\n",
      "Iteration 1441, loss = 2.11519615\n",
      "Iteration 1442, loss = 2.11479611\n",
      "Iteration 1443, loss = 2.11327483\n",
      "Iteration 1444, loss = 2.11251658\n",
      "Iteration 1445, loss = 2.11283859\n",
      "Iteration 1446, loss = 2.11461039\n",
      "Iteration 1447, loss = 2.11320193\n",
      "Iteration 1448, loss = 2.11080873\n",
      "Iteration 1449, loss = 2.11419914\n",
      "Iteration 1450, loss = 2.11051684\n",
      "Iteration 1451, loss = 2.11017987\n",
      "Iteration 1452, loss = 2.10938043\n",
      "Iteration 1453, loss = 2.10940687\n",
      "Iteration 1454, loss = 2.10942419\n",
      "Iteration 1455, loss = 2.10688894\n",
      "Iteration 1456, loss = 2.10762004\n",
      "Iteration 1457, loss = 2.10872430\n",
      "Iteration 1458, loss = 2.10868293\n",
      "Iteration 1459, loss = 2.10776595\n",
      "Iteration 1460, loss = 2.10609110\n",
      "Iteration 1461, loss = 2.10719956\n",
      "Iteration 1462, loss = 2.10628656\n",
      "Iteration 1463, loss = 2.10576484\n",
      "Iteration 1464, loss = 2.10608649\n",
      "Iteration 1465, loss = 2.10510814\n",
      "Iteration 1466, loss = 2.10414230\n",
      "Iteration 1467, loss = 2.10316010\n",
      "Iteration 1468, loss = 2.10367364\n",
      "Iteration 1469, loss = 2.10301189\n",
      "Iteration 1470, loss = 2.10236037\n",
      "Iteration 1471, loss = 2.10121994\n",
      "Iteration 1472, loss = 2.10313680\n",
      "Iteration 1473, loss = 2.10309623\n",
      "Iteration 1474, loss = 2.10213948\n",
      "Iteration 1475, loss = 2.10206080\n",
      "Iteration 1476, loss = 2.10205334\n",
      "Iteration 1477, loss = 2.10100776\n",
      "Iteration 1478, loss = 2.09868569\n",
      "Iteration 1479, loss = 2.09943139\n",
      "Iteration 1480, loss = 2.09947657\n",
      "Iteration 1481, loss = 2.09822510\n",
      "Iteration 1482, loss = 2.09990723\n",
      "Iteration 1483, loss = 2.10023540\n",
      "Iteration 1484, loss = 2.09649270\n",
      "Iteration 1485, loss = 2.09817681\n",
      "Iteration 1486, loss = 2.09620442\n",
      "Iteration 1487, loss = 2.09745169\n",
      "Iteration 1488, loss = 2.09669816\n",
      "Iteration 1489, loss = 2.09696316\n",
      "Iteration 1490, loss = 2.09495499\n",
      "Iteration 1491, loss = 2.09443603\n",
      "Iteration 1492, loss = 2.09342913\n",
      "Iteration 1493, loss = 2.09532891\n",
      "Iteration 1494, loss = 2.09543652\n",
      "Iteration 1495, loss = 2.09283210\n",
      "Iteration 1496, loss = 2.09330350\n",
      "Iteration 1497, loss = 2.09341878\n",
      "Iteration 1498, loss = 2.09251496\n",
      "Iteration 1499, loss = 2.09137478\n",
      "Iteration 1500, loss = 2.09246366\n",
      "Iteration 1501, loss = 2.09299923\n",
      "Iteration 1502, loss = 2.09419410\n",
      "Iteration 1503, loss = 2.09142275\n",
      "Iteration 1504, loss = 2.09034756\n",
      "Iteration 1505, loss = 2.08898643\n",
      "Iteration 1506, loss = 2.08957708\n",
      "Iteration 1507, loss = 2.08942315\n",
      "Iteration 1508, loss = 2.08855868\n",
      "Iteration 1509, loss = 2.08825700\n",
      "Iteration 1510, loss = 2.08780651\n",
      "Iteration 1511, loss = 2.08721024\n",
      "Iteration 1512, loss = 2.08838086\n",
      "Iteration 1513, loss = 2.08772689\n",
      "Iteration 1514, loss = 2.08577725\n",
      "Iteration 1515, loss = 2.08628430\n",
      "Iteration 1516, loss = 2.08486089\n",
      "Iteration 1517, loss = 2.08427606\n",
      "Iteration 1518, loss = 2.08627979\n",
      "Iteration 1519, loss = 2.08387861\n",
      "Iteration 1520, loss = 2.08670841\n",
      "Iteration 1521, loss = 2.08635586\n",
      "Iteration 1522, loss = 2.08330927\n",
      "Iteration 1523, loss = 2.08371103\n",
      "Iteration 1524, loss = 2.08302350\n",
      "Iteration 1525, loss = 2.08333925\n",
      "Iteration 1526, loss = 2.08200022\n",
      "Iteration 1527, loss = 2.08155718\n",
      "Iteration 1528, loss = 2.08091936\n",
      "Iteration 1529, loss = 2.08105231\n",
      "Iteration 1530, loss = 2.08035561\n",
      "Iteration 1531, loss = 2.08020112\n",
      "Iteration 1532, loss = 2.08065288\n",
      "Iteration 1533, loss = 2.07897836\n",
      "Iteration 1534, loss = 2.07913841\n",
      "Iteration 1535, loss = 2.07945959\n",
      "Iteration 1536, loss = 2.07952210\n",
      "Iteration 1537, loss = 2.07901299\n",
      "Iteration 1538, loss = 2.07873450\n",
      "Iteration 1539, loss = 2.07730323\n",
      "Iteration 1540, loss = 2.07752561\n",
      "Iteration 1541, loss = 2.07721013\n",
      "Iteration 1542, loss = 2.07655219\n",
      "Iteration 1543, loss = 2.07500594\n",
      "Iteration 1544, loss = 2.07597764\n",
      "Iteration 1545, loss = 2.07476564\n",
      "Iteration 1546, loss = 2.07613461\n",
      "Iteration 1547, loss = 2.07433144\n",
      "Iteration 1548, loss = 2.07582519\n",
      "Iteration 1549, loss = 2.07521962\n",
      "Iteration 1550, loss = 2.07405381\n",
      "Iteration 1551, loss = 2.07479871\n",
      "Iteration 1552, loss = 2.07361285\n",
      "Iteration 1553, loss = 2.07499549\n",
      "Iteration 1554, loss = 2.07358115\n",
      "Iteration 1555, loss = 2.07212052\n",
      "Iteration 1556, loss = 2.07268875\n",
      "Iteration 1557, loss = 2.07047000\n",
      "Iteration 1558, loss = 2.07375929\n",
      "Iteration 1559, loss = 2.07424853\n",
      "Iteration 1560, loss = 2.07041117\n",
      "Iteration 1561, loss = 2.06815853\n",
      "Iteration 1562, loss = 2.07017725\n",
      "Iteration 1563, loss = 2.06939822\n",
      "Iteration 1564, loss = 2.07112233\n",
      "Iteration 1565, loss = 2.06958103\n",
      "Iteration 1566, loss = 2.07108780\n",
      "Iteration 1567, loss = 2.06888086\n",
      "Iteration 1568, loss = 2.06802033\n",
      "Iteration 1569, loss = 2.06798764\n",
      "Iteration 1570, loss = 2.06826551\n",
      "Iteration 1571, loss = 2.06720283\n",
      "Iteration 1572, loss = 2.06623461\n",
      "Iteration 1573, loss = 2.06738954\n",
      "Iteration 1574, loss = 2.06628436\n",
      "Iteration 1575, loss = 2.06597887\n",
      "Iteration 1576, loss = 2.06676084\n",
      "Iteration 1577, loss = 2.06520304\n",
      "Iteration 1578, loss = 2.06484540\n",
      "Iteration 1579, loss = 2.06483208\n",
      "Iteration 1580, loss = 2.06487448\n",
      "Iteration 1581, loss = 2.06404129\n",
      "Iteration 1582, loss = 2.06473695\n",
      "Iteration 1583, loss = 2.06383400\n",
      "Iteration 1584, loss = 2.06224613\n",
      "Iteration 1585, loss = 2.06381117\n",
      "Iteration 1586, loss = 2.06253027\n",
      "Iteration 1587, loss = 2.06167170\n",
      "Iteration 1588, loss = 2.06068406\n",
      "Iteration 1589, loss = 2.06204143\n",
      "Iteration 1590, loss = 2.06082752\n",
      "Iteration 1591, loss = 2.05933273\n",
      "Iteration 1592, loss = 2.05978627\n",
      "Iteration 1593, loss = 2.05910164\n",
      "Iteration 1594, loss = 2.05947072\n",
      "Iteration 1595, loss = 2.06318816\n",
      "Iteration 1596, loss = 2.06135832\n",
      "Iteration 1597, loss = 2.05857819\n",
      "Iteration 1598, loss = 2.05892849\n",
      "Iteration 1599, loss = 2.05901280\n",
      "Iteration 1600, loss = 2.05680114\n",
      "Iteration 1601, loss = 2.05709759\n",
      "Iteration 1602, loss = 2.05576939\n",
      "Iteration 1603, loss = 2.05572753\n",
      "Iteration 1604, loss = 2.05732903\n",
      "Iteration 1605, loss = 2.05450934\n",
      "Iteration 1606, loss = 2.05488825\n",
      "Iteration 1607, loss = 2.05463485\n",
      "Iteration 1608, loss = 2.05514606\n",
      "Iteration 1609, loss = 2.05396442\n",
      "Iteration 1610, loss = 2.05272575\n",
      "Iteration 1611, loss = 2.05254782\n",
      "Iteration 1612, loss = 2.05362343\n",
      "Iteration 1613, loss = 2.05351872\n",
      "Iteration 1614, loss = 2.05242699\n",
      "Iteration 1615, loss = 2.05170763\n",
      "Iteration 1616, loss = 2.05206302\n",
      "Iteration 1617, loss = 2.05013708\n",
      "Iteration 1618, loss = 2.04994643\n",
      "Iteration 1619, loss = 2.05015004\n",
      "Iteration 1620, loss = 2.04932589\n",
      "Iteration 1621, loss = 2.05117888\n",
      "Iteration 1622, loss = 2.04975076\n",
      "Iteration 1623, loss = 2.04910795\n",
      "Iteration 1624, loss = 2.04818517\n",
      "Iteration 1625, loss = 2.04790227\n",
      "Iteration 1626, loss = 2.04775444\n",
      "Iteration 1627, loss = 2.04658980\n",
      "Iteration 1628, loss = 2.04706097\n",
      "Iteration 1629, loss = 2.04665188\n",
      "Iteration 1630, loss = 2.04670985\n",
      "Iteration 1631, loss = 2.04562205\n",
      "Iteration 1632, loss = 2.04445558\n",
      "Iteration 1633, loss = 2.04405532\n",
      "Iteration 1634, loss = 2.04510826\n",
      "Iteration 1635, loss = 2.04421928\n",
      "Iteration 1636, loss = 2.04797722\n",
      "Iteration 1637, loss = 2.04727840\n",
      "Iteration 1638, loss = 2.04658017\n",
      "Iteration 1639, loss = 2.04481898\n",
      "Iteration 1640, loss = 2.04385895\n",
      "Iteration 1641, loss = 2.04220333\n",
      "Iteration 1642, loss = 2.04233006\n",
      "Iteration 1643, loss = 2.04413425\n",
      "Iteration 1644, loss = 2.04346888\n",
      "Iteration 1645, loss = 2.04106561\n",
      "Iteration 1646, loss = 2.04279074\n",
      "Iteration 1647, loss = 2.04285249\n",
      "Iteration 1648, loss = 2.04092244\n",
      "Iteration 1649, loss = 2.03977264\n",
      "Iteration 1650, loss = 2.03959991\n",
      "Iteration 1651, loss = 2.04014306\n",
      "Iteration 1652, loss = 2.03917510\n",
      "Iteration 1653, loss = 2.03916466\n",
      "Iteration 1654, loss = 2.04051865\n",
      "Iteration 1655, loss = 2.03974076\n",
      "Iteration 1656, loss = 2.03889626\n",
      "Iteration 1657, loss = 2.03821111\n",
      "Iteration 1658, loss = 2.03697787\n",
      "Iteration 1659, loss = 2.03668663\n",
      "Iteration 1660, loss = 2.03690384\n",
      "Iteration 1661, loss = 2.03702899\n",
      "Iteration 1662, loss = 2.03688892\n",
      "Iteration 1663, loss = 2.03813829\n",
      "Iteration 1664, loss = 2.03516936\n",
      "Iteration 1665, loss = 2.03524694\n",
      "Iteration 1666, loss = 2.03542088\n",
      "Iteration 1667, loss = 2.03805235\n",
      "Iteration 1668, loss = 2.03561910\n",
      "Iteration 1669, loss = 2.03425258\n",
      "Iteration 1670, loss = 2.03464113\n",
      "Iteration 1671, loss = 2.03304345\n",
      "Iteration 1672, loss = 2.03235459\n",
      "Iteration 1673, loss = 2.03325390\n",
      "Iteration 1674, loss = 2.03139496\n",
      "Iteration 1675, loss = 2.03204076\n",
      "Iteration 1676, loss = 2.03149299\n",
      "Iteration 1677, loss = 2.03056779\n",
      "Iteration 1678, loss = 2.02997880\n",
      "Iteration 1679, loss = 2.03048490\n",
      "Iteration 1680, loss = 2.03098696\n",
      "Iteration 1681, loss = 2.03036630\n",
      "Iteration 1682, loss = 2.02981545\n",
      "Iteration 1683, loss = 2.03189348\n",
      "Iteration 1684, loss = 2.03118334\n",
      "Iteration 1685, loss = 2.03025334\n",
      "Iteration 1686, loss = 2.02734297\n",
      "Iteration 1687, loss = 2.02778260\n",
      "Iteration 1688, loss = 2.02677557\n",
      "Iteration 1689, loss = 2.02792277\n",
      "Iteration 1690, loss = 2.02804244\n",
      "Iteration 1691, loss = 2.02771897\n",
      "Iteration 1692, loss = 2.02678001\n",
      "Iteration 1693, loss = 2.02764541\n",
      "Iteration 1694, loss = 2.02815004\n",
      "Iteration 1695, loss = 2.02700136\n",
      "Iteration 1696, loss = 2.02727203\n",
      "Iteration 1697, loss = 2.02622786\n",
      "Iteration 1698, loss = 2.02425996\n",
      "Iteration 1699, loss = 2.02393929\n",
      "Iteration 1700, loss = 2.02516359\n",
      "Iteration 1701, loss = 2.02408291\n",
      "Iteration 1702, loss = 2.02256678\n",
      "Iteration 1703, loss = 2.02328345\n",
      "Iteration 1704, loss = 2.02347169\n",
      "Iteration 1705, loss = 2.02182073\n",
      "Iteration 1706, loss = 2.02274025\n",
      "Iteration 1707, loss = 2.02505143\n",
      "Iteration 1708, loss = 2.02491779\n",
      "Iteration 1709, loss = 2.02298499\n",
      "Iteration 1710, loss = 2.02122812\n",
      "Iteration 1711, loss = 2.02065363\n",
      "Iteration 1712, loss = 2.02147864\n",
      "Iteration 1713, loss = 2.02501117\n",
      "Iteration 1714, loss = 2.02101632\n",
      "Iteration 1715, loss = 2.02068848\n",
      "Iteration 1716, loss = 2.02037327\n",
      "Iteration 1717, loss = 2.02073863\n",
      "Iteration 1718, loss = 2.01800912\n",
      "Iteration 1719, loss = 2.01791683\n",
      "Iteration 1720, loss = 2.01672124\n",
      "Iteration 1721, loss = 2.01684650\n",
      "Iteration 1722, loss = 2.01712002\n",
      "Iteration 1723, loss = 2.01624850\n",
      "Iteration 1724, loss = 2.01529261\n",
      "Iteration 1725, loss = 2.01491689\n",
      "Iteration 1726, loss = 2.01468323\n",
      "Iteration 1727, loss = 2.01468110\n",
      "Iteration 1728, loss = 2.01390636\n",
      "Iteration 1729, loss = 2.01317692\n",
      "Iteration 1730, loss = 2.01461902\n",
      "Iteration 1731, loss = 2.01480291\n",
      "Iteration 1732, loss = 2.01313666\n",
      "Iteration 1733, loss = 2.01237513\n",
      "Iteration 1734, loss = 2.01416476\n",
      "Iteration 1735, loss = 2.01334505\n",
      "Iteration 1736, loss = 2.01219734\n",
      "Iteration 1737, loss = 2.01458117\n",
      "Iteration 1738, loss = 2.01421604\n",
      "Iteration 1739, loss = 2.01379622\n",
      "Iteration 1740, loss = 2.01242369\n",
      "Iteration 1741, loss = 2.01091950\n",
      "Iteration 1742, loss = 2.01068346\n",
      "Iteration 1743, loss = 2.01044403\n",
      "Iteration 1744, loss = 2.01222247\n",
      "Iteration 1745, loss = 2.01030052\n",
      "Iteration 1746, loss = 2.00957782\n",
      "Iteration 1747, loss = 2.01021014\n",
      "Iteration 1748, loss = 2.01004437\n",
      "Iteration 1749, loss = 2.00972161\n",
      "Iteration 1750, loss = 2.01025924\n",
      "Iteration 1751, loss = 2.00759169\n",
      "Iteration 1752, loss = 2.00774423\n",
      "Iteration 1753, loss = 2.00735887\n",
      "Iteration 1754, loss = 2.00724121\n",
      "Iteration 1755, loss = 2.00647059\n",
      "Iteration 1756, loss = 2.00761358\n",
      "Iteration 1757, loss = 2.00640799\n",
      "Iteration 1758, loss = 2.00795038\n",
      "Iteration 1759, loss = 2.00459004\n",
      "Iteration 1760, loss = 2.00470992\n",
      "Iteration 1761, loss = 2.00698089\n",
      "Iteration 1762, loss = 2.00815720\n",
      "Iteration 1763, loss = 2.00511198\n",
      "Iteration 1764, loss = 2.00417069\n",
      "Iteration 1765, loss = 2.00378594\n",
      "Iteration 1766, loss = 2.00277702\n",
      "Iteration 1767, loss = 2.00249392\n",
      "Iteration 1768, loss = 2.00222429\n",
      "Iteration 1769, loss = 2.00273452\n",
      "Iteration 1770, loss = 2.00163272\n",
      "Iteration 1771, loss = 2.00214534\n",
      "Iteration 1772, loss = 2.00086303\n",
      "Iteration 1773, loss = 2.00109887\n",
      "Iteration 1774, loss = 1.99886317\n",
      "Iteration 1775, loss = 1.99994801\n",
      "Iteration 1776, loss = 2.00057648\n",
      "Iteration 1777, loss = 1.99960177\n",
      "Iteration 1778, loss = 1.99851786\n",
      "Iteration 1779, loss = 1.99905135\n",
      "Iteration 1780, loss = 1.99873802\n",
      "Iteration 1781, loss = 1.99717374\n",
      "Iteration 1782, loss = 1.99831240\n",
      "Iteration 1783, loss = 1.99847118\n",
      "Iteration 1784, loss = 1.99696309\n",
      "Iteration 1785, loss = 1.99713095\n",
      "Iteration 1786, loss = 1.99758149\n",
      "Iteration 1787, loss = 1.99762020\n",
      "Iteration 1788, loss = 1.99607369\n",
      "Iteration 1789, loss = 1.99801187\n",
      "Iteration 1790, loss = 1.99519605\n",
      "Iteration 1791, loss = 1.99525785\n",
      "Iteration 1792, loss = 1.99355338\n",
      "Iteration 1793, loss = 1.99651625\n",
      "Iteration 1794, loss = 1.99582226\n",
      "Iteration 1795, loss = 1.99349571\n",
      "Iteration 1796, loss = 1.99453134\n",
      "Iteration 1797, loss = 1.99439927\n",
      "Iteration 1798, loss = 1.99338903\n",
      "Iteration 1799, loss = 1.99348975\n",
      "Iteration 1800, loss = 1.99377208\n",
      "Iteration 1801, loss = 1.99334022\n",
      "Iteration 1802, loss = 1.99490350\n",
      "Iteration 1803, loss = 1.99303037\n",
      "Iteration 1804, loss = 1.99197967\n",
      "Iteration 1805, loss = 1.99304420\n",
      "Iteration 1806, loss = 1.99312841\n",
      "Iteration 1807, loss = 1.99179890\n",
      "Iteration 1808, loss = 1.98952919\n",
      "Iteration 1809, loss = 1.99057368\n",
      "Iteration 1810, loss = 1.99110292\n",
      "Iteration 1811, loss = 1.99151289\n",
      "Iteration 1812, loss = 1.98972951\n",
      "Iteration 1813, loss = 1.99050353\n",
      "Iteration 1814, loss = 1.98872373\n",
      "Iteration 1815, loss = 1.98827069\n",
      "Iteration 1816, loss = 1.98959476\n",
      "Iteration 1817, loss = 1.98868260\n",
      "Iteration 1818, loss = 1.98665537\n",
      "Iteration 1819, loss = 1.98838121\n",
      "Iteration 1820, loss = 1.98756755\n",
      "Iteration 1821, loss = 1.98580406\n",
      "Iteration 1822, loss = 1.98577290\n",
      "Iteration 1823, loss = 1.98629696\n",
      "Iteration 1824, loss = 1.98596331\n",
      "Iteration 1825, loss = 1.98713840\n",
      "Iteration 1826, loss = 1.98683669\n",
      "Iteration 1827, loss = 1.98784111\n",
      "Iteration 1828, loss = 1.98690940\n",
      "Iteration 1829, loss = 1.98347988\n",
      "Iteration 1830, loss = 1.98406611\n",
      "Iteration 1831, loss = 1.98242102\n",
      "Iteration 1832, loss = 1.98128968\n",
      "Iteration 1833, loss = 1.98196815\n",
      "Iteration 1834, loss = 1.98255296\n",
      "Iteration 1835, loss = 1.98365625\n",
      "Iteration 1836, loss = 1.98249446\n",
      "Iteration 1837, loss = 1.98460509\n",
      "Iteration 1838, loss = 1.98357340\n",
      "Iteration 1839, loss = 1.98292516\n",
      "Iteration 1840, loss = 1.98132647\n",
      "Iteration 1841, loss = 1.98276739\n",
      "Iteration 1842, loss = 1.98160483\n",
      "Iteration 1843, loss = 1.98032454\n",
      "Iteration 1844, loss = 1.98076667\n",
      "Iteration 1845, loss = 1.97938154\n",
      "Iteration 1846, loss = 1.97971458\n",
      "Iteration 1847, loss = 1.97758539\n",
      "Iteration 1848, loss = 1.97900130\n",
      "Iteration 1849, loss = 1.97914993\n",
      "Iteration 1850, loss = 1.97920970\n",
      "Iteration 1851, loss = 1.98118637\n",
      "Iteration 1852, loss = 1.97815658\n",
      "Iteration 1853, loss = 1.97829218\n",
      "Iteration 1854, loss = 1.97749977\n",
      "Iteration 1855, loss = 1.97604192\n",
      "Iteration 1856, loss = 1.97786464\n",
      "Iteration 1857, loss = 1.97656339\n",
      "Iteration 1858, loss = 1.97691984\n",
      "Iteration 1859, loss = 1.97539542\n",
      "Iteration 1860, loss = 1.97587000\n",
      "Iteration 1861, loss = 1.97794403\n",
      "Iteration 1862, loss = 1.97546512\n",
      "Iteration 1863, loss = 1.97623658\n",
      "Iteration 1864, loss = 1.97543897\n",
      "Iteration 1865, loss = 1.97471241\n",
      "Iteration 1866, loss = 1.97368441\n",
      "Iteration 1867, loss = 1.97312410\n",
      "Iteration 1868, loss = 1.97437633\n",
      "Iteration 1869, loss = 1.97351051\n",
      "Iteration 1870, loss = 1.97368683\n",
      "Iteration 1871, loss = 1.97314073\n",
      "Iteration 1872, loss = 1.97349206\n",
      "Iteration 1873, loss = 1.97283780\n",
      "Iteration 1874, loss = 1.97368257\n",
      "Iteration 1875, loss = 1.97254839\n",
      "Iteration 1876, loss = 1.97234279\n",
      "Iteration 1877, loss = 1.97034498\n",
      "Iteration 1878, loss = 1.97207397\n",
      "Iteration 1879, loss = 1.97266510\n",
      "Iteration 1880, loss = 1.96845121\n",
      "Iteration 1881, loss = 1.96881082\n",
      "Iteration 1882, loss = 1.96956154\n",
      "Iteration 1883, loss = 1.96909315\n",
      "Iteration 1884, loss = 1.96763111\n",
      "Iteration 1885, loss = 1.97114408\n",
      "Iteration 1886, loss = 1.97155276\n",
      "Iteration 1887, loss = 1.97278052\n",
      "Iteration 1888, loss = 1.96937810\n",
      "Iteration 1889, loss = 1.96703041\n",
      "Iteration 1890, loss = 1.96775991\n",
      "Iteration 1891, loss = 1.96697184\n",
      "Iteration 1892, loss = 1.96547204\n",
      "Iteration 1893, loss = 1.96907323\n",
      "Iteration 1894, loss = 1.96740849\n",
      "Iteration 1895, loss = 1.96523883\n",
      "Iteration 1896, loss = 1.96595871\n",
      "Iteration 1897, loss = 1.96709583\n",
      "Iteration 1898, loss = 1.96523751\n",
      "Iteration 1899, loss = 1.96506274\n",
      "Iteration 1900, loss = 1.96374410\n",
      "Iteration 1901, loss = 1.96519589\n",
      "Iteration 1902, loss = 1.96283430\n",
      "Iteration 1903, loss = 1.96342819\n",
      "Iteration 1904, loss = 1.96482314\n",
      "Iteration 1905, loss = 1.96467249\n",
      "Iteration 1906, loss = 1.96353840\n",
      "Iteration 1907, loss = 1.96136925\n",
      "Iteration 1908, loss = 1.96286945\n",
      "Iteration 1909, loss = 1.96294601\n",
      "Iteration 1910, loss = 1.96267722\n",
      "Iteration 1911, loss = 1.96211244\n",
      "Iteration 1912, loss = 1.96096251\n",
      "Iteration 1913, loss = 1.96076405\n",
      "Iteration 1914, loss = 1.96093589\n",
      "Iteration 1915, loss = 1.96106619\n",
      "Iteration 1916, loss = 1.95940779\n",
      "Iteration 1917, loss = 1.96041216\n",
      "Iteration 1918, loss = 1.95966305\n",
      "Iteration 1919, loss = 1.96047751\n",
      "Iteration 1920, loss = 1.95993384\n",
      "Iteration 1921, loss = 1.96053931\n",
      "Iteration 1922, loss = 1.95795387\n",
      "Iteration 1923, loss = 1.95706400\n",
      "Iteration 1924, loss = 1.95586286\n",
      "Iteration 1925, loss = 1.95757118\n",
      "Iteration 1926, loss = 1.95657536\n",
      "Iteration 1927, loss = 1.96071164\n",
      "Iteration 1928, loss = 1.96065848\n",
      "Iteration 1929, loss = 1.95850062\n",
      "Iteration 1930, loss = 1.95639310\n",
      "Iteration 1931, loss = 1.95516924\n",
      "Iteration 1932, loss = 1.95628258\n",
      "Iteration 1933, loss = 1.95829831\n",
      "Iteration 1934, loss = 1.95774941\n",
      "Iteration 1935, loss = 1.95584950\n",
      "Iteration 1936, loss = 1.95564875\n",
      "Iteration 1937, loss = 1.95432946\n",
      "Iteration 1938, loss = 1.95404613\n",
      "Iteration 1939, loss = 1.95452784\n",
      "Iteration 1940, loss = 1.95534189\n",
      "Iteration 1941, loss = 1.95379820\n",
      "Iteration 1942, loss = 1.95340776\n",
      "Iteration 1943, loss = 1.95257393\n",
      "Iteration 1944, loss = 1.95366047\n",
      "Iteration 1945, loss = 1.95337378\n",
      "Iteration 1946, loss = 1.95206324\n",
      "Iteration 1947, loss = 1.95378226\n",
      "Iteration 1948, loss = 1.95301026\n",
      "Iteration 1949, loss = 1.95030580\n",
      "Iteration 1950, loss = 1.95200703\n",
      "Iteration 1951, loss = 1.94866256\n",
      "Iteration 1952, loss = 1.95027491\n",
      "Iteration 1953, loss = 1.95073656\n",
      "Iteration 1954, loss = 1.95137420\n",
      "Iteration 1955, loss = 1.94922491\n",
      "Iteration 1956, loss = 1.94917748\n",
      "Iteration 1957, loss = 1.94937807\n",
      "Iteration 1958, loss = 1.94971688\n",
      "Iteration 1959, loss = 1.94874548\n",
      "Iteration 1960, loss = 1.94908959\n",
      "Iteration 1961, loss = 1.94808290\n",
      "Iteration 1962, loss = 1.95055084\n",
      "Iteration 1963, loss = 1.94815304\n",
      "Iteration 1964, loss = 1.94715286\n",
      "Iteration 1965, loss = 1.94752555\n",
      "Iteration 1966, loss = 1.94594186\n",
      "Iteration 1967, loss = 1.94535809\n",
      "Iteration 1968, loss = 1.94585450\n",
      "Iteration 1969, loss = 1.94585054\n",
      "Iteration 1970, loss = 1.94578504\n",
      "Iteration 1971, loss = 1.94449259\n",
      "Iteration 1972, loss = 1.94448878\n",
      "Iteration 1973, loss = 1.94444745\n",
      "Iteration 1974, loss = 1.94596575\n",
      "Iteration 1975, loss = 1.94552722\n",
      "Iteration 1976, loss = 1.94415852\n",
      "Iteration 1977, loss = 1.94242638\n",
      "Iteration 1978, loss = 1.94207175\n",
      "Iteration 1979, loss = 1.94245226\n",
      "Iteration 1980, loss = 1.94150104\n",
      "Iteration 1981, loss = 1.94165040\n",
      "Iteration 1982, loss = 1.94216170\n",
      "Iteration 1983, loss = 1.94144658\n",
      "Iteration 1984, loss = 1.94483875\n",
      "Iteration 1985, loss = 1.94329872\n",
      "Iteration 1986, loss = 1.94171971\n",
      "Iteration 1987, loss = 1.94139155\n",
      "Iteration 1988, loss = 1.93992112\n",
      "Iteration 1989, loss = 1.94026387\n",
      "Iteration 1990, loss = 1.93990587\n",
      "Iteration 1991, loss = 1.94003980\n",
      "Iteration 1992, loss = 1.94134433\n",
      "Iteration 1993, loss = 1.93936935\n",
      "Iteration 1994, loss = 1.93892867\n",
      "Iteration 1995, loss = 1.93754807\n",
      "Iteration 1996, loss = 1.93986133\n",
      "Iteration 1997, loss = 1.93879680\n",
      "Iteration 1998, loss = 1.93911339\n",
      "Iteration 1999, loss = 1.94016194\n",
      "Iteration 2000, loss = 1.93859477\n",
      "Iteration 2001, loss = 1.93909210\n",
      "Iteration 2002, loss = 1.93642384\n",
      "Iteration 2003, loss = 1.93573230\n",
      "Iteration 2004, loss = 1.93750612\n",
      "Iteration 2005, loss = 1.93497286\n",
      "Iteration 2006, loss = 1.93764121\n",
      "Iteration 2007, loss = 1.93602339\n",
      "Iteration 2008, loss = 1.93456487\n",
      "Iteration 2009, loss = 1.93445136\n",
      "Iteration 2010, loss = 1.93456358\n",
      "Iteration 2011, loss = 1.93610782\n",
      "Iteration 2012, loss = 1.93534087\n",
      "Iteration 2013, loss = 1.93365447\n",
      "Iteration 2014, loss = 1.93478010\n",
      "Iteration 2015, loss = 1.93382383\n",
      "Iteration 2016, loss = 1.93511804\n",
      "Iteration 2017, loss = 1.93436613\n",
      "Iteration 2018, loss = 1.93371766\n",
      "Iteration 2019, loss = 1.93372289\n",
      "Iteration 2020, loss = 1.93244660\n",
      "Iteration 2021, loss = 1.93251960\n",
      "Iteration 2022, loss = 1.93229326\n",
      "Iteration 2023, loss = 1.93140756\n",
      "Iteration 2024, loss = 1.93343091\n",
      "Iteration 2025, loss = 1.93321569\n",
      "Iteration 2026, loss = 1.92972526\n",
      "Iteration 2027, loss = 1.93189364\n",
      "Iteration 2028, loss = 1.93150002\n",
      "Iteration 2029, loss = 1.93324623\n",
      "Iteration 2030, loss = 1.93138785\n",
      "Iteration 2031, loss = 1.92876208\n",
      "Iteration 2032, loss = 1.92965147\n",
      "Iteration 2033, loss = 1.92882183\n",
      "Iteration 2034, loss = 1.92710927\n",
      "Iteration 2035, loss = 1.92797139\n",
      "Iteration 2036, loss = 1.92754669\n",
      "Iteration 2037, loss = 1.92762356\n",
      "Iteration 2038, loss = 1.92587354\n",
      "Iteration 2039, loss = 1.92883299\n",
      "Iteration 2040, loss = 1.92805961\n",
      "Iteration 2041, loss = 1.92715460\n",
      "Iteration 2042, loss = 1.92559766\n",
      "Iteration 2043, loss = 1.92515208\n",
      "Iteration 2044, loss = 1.92675161\n",
      "Iteration 2045, loss = 1.92618543\n",
      "Iteration 2046, loss = 1.92624309\n",
      "Iteration 2047, loss = 1.92589253\n",
      "Iteration 2048, loss = 1.92515742\n",
      "Iteration 2049, loss = 1.92351902\n",
      "Iteration 2050, loss = 1.92830540\n",
      "Iteration 2051, loss = 1.92472848\n",
      "Iteration 2052, loss = 1.92365383\n",
      "Iteration 2053, loss = 1.92397149\n",
      "Iteration 2054, loss = 1.92383272\n",
      "Iteration 2055, loss = 1.92371658\n",
      "Iteration 2056, loss = 1.92382366\n",
      "Iteration 2057, loss = 1.92295672\n",
      "Iteration 2058, loss = 1.92419193\n",
      "Iteration 2059, loss = 1.92217926\n",
      "Iteration 2060, loss = 1.92284913\n",
      "Iteration 2061, loss = 1.92262873\n",
      "Iteration 2062, loss = 1.92153189\n",
      "Iteration 2063, loss = 1.92101488\n",
      "Iteration 2064, loss = 1.92034355\n",
      "Iteration 2065, loss = 1.92158809\n",
      "Iteration 2066, loss = 1.92165581\n",
      "Iteration 2067, loss = 1.92214522\n",
      "Iteration 2068, loss = 1.92248422\n",
      "Iteration 2069, loss = 1.92004686\n",
      "Iteration 2070, loss = 1.91945962\n",
      "Iteration 2071, loss = 1.92085100\n",
      "Iteration 2072, loss = 1.91939121\n",
      "Iteration 2073, loss = 1.92011062\n",
      "Iteration 2074, loss = 1.91865248\n",
      "Iteration 2075, loss = 1.91682028\n",
      "Iteration 2076, loss = 1.91773231\n",
      "Iteration 2077, loss = 1.91704565\n",
      "Iteration 2078, loss = 1.91705378\n",
      "Iteration 2079, loss = 1.91776245\n",
      "Iteration 2080, loss = 1.91810628\n",
      "Iteration 2081, loss = 1.91654960\n",
      "Iteration 2082, loss = 1.91663277\n",
      "Iteration 2083, loss = 1.91627862\n",
      "Iteration 2084, loss = 1.91664158\n",
      "Iteration 2085, loss = 1.91739256\n",
      "Iteration 2086, loss = 1.91675653\n",
      "Iteration 2087, loss = 1.91652054\n",
      "Iteration 2088, loss = 1.91507454\n",
      "Iteration 2089, loss = 1.91468321\n",
      "Iteration 2090, loss = 1.91587346\n",
      "Iteration 2091, loss = 1.91509217\n",
      "Iteration 2092, loss = 1.91423574\n",
      "Iteration 2093, loss = 1.91357122\n",
      "Iteration 2094, loss = 1.91404943\n",
      "Iteration 2095, loss = 1.91375598\n",
      "Iteration 2096, loss = 1.91441399\n",
      "Iteration 2097, loss = 1.91386108\n",
      "Iteration 2098, loss = 1.91295676\n",
      "Iteration 2099, loss = 1.91277198\n",
      "Iteration 2100, loss = 1.91191024\n",
      "Iteration 2101, loss = 1.91224390\n",
      "Iteration 2102, loss = 1.91148404\n",
      "Iteration 2103, loss = 1.91094583\n",
      "Iteration 2104, loss = 1.91069529\n",
      "Iteration 2105, loss = 1.91067027\n",
      "Iteration 2106, loss = 1.91065658\n",
      "Iteration 2107, loss = 1.91017142\n",
      "Iteration 2108, loss = 1.90990595\n",
      "Iteration 2109, loss = 1.91177871\n",
      "Iteration 2110, loss = 1.91161812\n",
      "Iteration 2111, loss = 1.91061794\n",
      "Iteration 2112, loss = 1.91003613\n",
      "Iteration 2113, loss = 1.90972614\n",
      "Iteration 2114, loss = 1.91345312\n",
      "Iteration 2115, loss = 1.91158044\n",
      "Iteration 2116, loss = 1.91025290\n",
      "Iteration 2117, loss = 1.91058685\n",
      "Iteration 2118, loss = 1.90897401\n",
      "Iteration 2119, loss = 1.90911524\n",
      "Iteration 2120, loss = 1.90829943\n",
      "Iteration 2121, loss = 1.90672552\n",
      "Iteration 2122, loss = 1.90587707\n",
      "Iteration 2123, loss = 1.90612257\n",
      "Iteration 2124, loss = 1.90650083\n",
      "Iteration 2125, loss = 1.90650009\n",
      "Iteration 2126, loss = 1.90589861\n",
      "Iteration 2127, loss = 1.90712445\n",
      "Iteration 2128, loss = 1.90488041\n",
      "Iteration 2129, loss = 1.90629519\n",
      "Iteration 2130, loss = 1.90514427\n",
      "Iteration 2131, loss = 1.90581738\n",
      "Iteration 2132, loss = 1.90497194\n",
      "Iteration 2133, loss = 1.90319855\n",
      "Iteration 2134, loss = 1.90420041\n",
      "Iteration 2135, loss = 1.90342677\n",
      "Iteration 2136, loss = 1.90435090\n",
      "Iteration 2137, loss = 1.90320314\n",
      "Iteration 2138, loss = 1.90212882\n",
      "Iteration 2139, loss = 1.90281475\n",
      "Iteration 2140, loss = 1.90266982\n",
      "Iteration 2141, loss = 1.90194933\n",
      "Iteration 2142, loss = 1.90237074\n",
      "Iteration 2143, loss = 1.90277827\n",
      "Iteration 2144, loss = 1.90292274\n",
      "Iteration 2145, loss = 1.90190789\n",
      "Iteration 2146, loss = 1.90510660\n",
      "Iteration 2147, loss = 1.90073940\n",
      "Iteration 2148, loss = 1.90266052\n",
      "Iteration 2149, loss = 1.90041765\n",
      "Iteration 2150, loss = 1.90055025\n",
      "Iteration 2151, loss = 1.90042955\n",
      "Iteration 2152, loss = 1.90099373\n",
      "Iteration 2153, loss = 1.89962510\n",
      "Iteration 2154, loss = 1.89915293\n",
      "Iteration 2155, loss = 1.90007135\n",
      "Iteration 2156, loss = 1.90024181\n",
      "Iteration 2157, loss = 1.90049925\n",
      "Iteration 2158, loss = 1.90127817\n",
      "Iteration 2159, loss = 1.89969132\n",
      "Iteration 2160, loss = 1.89844228\n",
      "Iteration 2161, loss = 1.89971675\n",
      "Iteration 2162, loss = 1.89930527\n",
      "Iteration 2163, loss = 1.89894183\n",
      "Iteration 2164, loss = 1.89906122\n",
      "Iteration 2165, loss = 1.90098723\n",
      "Iteration 2166, loss = 1.89828228\n",
      "Iteration 2167, loss = 1.89656171\n",
      "Iteration 2168, loss = 1.89698581\n",
      "Iteration 2169, loss = 1.89534778\n",
      "Iteration 2170, loss = 1.89535007\n",
      "Iteration 2171, loss = 1.89580732\n",
      "Iteration 2172, loss = 1.89593960\n",
      "Iteration 2173, loss = 1.89481180\n",
      "Iteration 2174, loss = 1.89363499\n",
      "Iteration 2175, loss = 1.89332712\n",
      "Iteration 2176, loss = 1.89449195\n",
      "Iteration 2177, loss = 1.89444273\n",
      "Iteration 2178, loss = 1.89252209\n",
      "Iteration 2179, loss = 1.89286018\n",
      "Iteration 2180, loss = 1.89229343\n",
      "Iteration 2181, loss = 1.89242041\n",
      "Iteration 2182, loss = 1.89321370\n",
      "Iteration 2183, loss = 1.89324382\n",
      "Iteration 2184, loss = 1.89177335\n",
      "Iteration 2185, loss = 1.89051007\n",
      "Iteration 2186, loss = 1.89180732\n",
      "Iteration 2187, loss = 1.89166552\n",
      "Iteration 2188, loss = 1.89059666\n",
      "Iteration 2189, loss = 1.89101825\n",
      "Iteration 2190, loss = 1.89058882\n",
      "Iteration 2191, loss = 1.89068482\n",
      "Iteration 2192, loss = 1.89151946\n",
      "Iteration 2193, loss = 1.88996223\n",
      "Iteration 2194, loss = 1.88967960\n",
      "Iteration 2195, loss = 1.88844418\n",
      "Iteration 2196, loss = 1.88913319\n",
      "Iteration 2197, loss = 1.88894161\n",
      "Iteration 2198, loss = 1.88871666\n",
      "Iteration 2199, loss = 1.88782927\n",
      "Iteration 2200, loss = 1.89003109\n",
      "Iteration 2201, loss = 1.89034073\n",
      "Iteration 2202, loss = 1.88933130\n",
      "Iteration 2203, loss = 1.88812200\n",
      "Iteration 2204, loss = 1.88646915\n",
      "Iteration 2205, loss = 1.88770875\n",
      "Iteration 2206, loss = 1.88966545\n",
      "Iteration 2207, loss = 1.88816303\n",
      "Iteration 2208, loss = 1.88718174\n",
      "Iteration 2209, loss = 1.88699918\n",
      "Iteration 2210, loss = 1.88703382\n",
      "Iteration 2211, loss = 1.88557910\n",
      "Iteration 2212, loss = 1.88736052\n",
      "Iteration 2213, loss = 1.88760701\n",
      "Iteration 2214, loss = 1.88660694\n",
      "Iteration 2215, loss = 1.88521359\n",
      "Iteration 2216, loss = 1.88447039\n",
      "Iteration 2217, loss = 1.88662482\n",
      "Iteration 2218, loss = 1.88417988\n",
      "Iteration 2219, loss = 1.88318701\n",
      "Iteration 2220, loss = 1.88306033\n",
      "Iteration 2221, loss = 1.88366845\n",
      "Iteration 2222, loss = 1.88249183\n",
      "Iteration 2223, loss = 1.88197406\n",
      "Iteration 2224, loss = 1.88171755\n",
      "Iteration 2225, loss = 1.88360417\n",
      "Iteration 2226, loss = 1.88374882\n",
      "Iteration 2227, loss = 1.88479393\n",
      "Iteration 2228, loss = 1.88185706\n",
      "Iteration 2229, loss = 1.88161820\n",
      "Iteration 2230, loss = 1.88130079\n",
      "Iteration 2231, loss = 1.88099687\n",
      "Iteration 2232, loss = 1.88050197\n",
      "Iteration 2233, loss = 1.88202518\n",
      "Iteration 2234, loss = 1.88244063\n",
      "Iteration 2235, loss = 1.88337521\n",
      "Iteration 2236, loss = 1.88023183\n",
      "Iteration 2237, loss = 1.88118055\n",
      "Iteration 2238, loss = 1.88085776\n",
      "Iteration 2239, loss = 1.88089767\n",
      "Iteration 2240, loss = 1.88593752\n",
      "Iteration 2241, loss = 1.88130543\n",
      "Iteration 2242, loss = 1.87854217\n",
      "Iteration 2243, loss = 1.87919651\n",
      "Iteration 2244, loss = 1.87882668\n",
      "Iteration 2245, loss = 1.87719895\n",
      "Iteration 2246, loss = 1.87731819\n",
      "Iteration 2247, loss = 1.87808043\n",
      "Iteration 2248, loss = 1.87683500\n",
      "Iteration 2249, loss = 1.87765027\n",
      "Iteration 2250, loss = 1.87759581\n",
      "Iteration 2251, loss = 1.87638145\n",
      "Iteration 2252, loss = 1.87694219\n",
      "Iteration 2253, loss = 1.87574543\n",
      "Iteration 2254, loss = 1.87625557\n",
      "Iteration 2255, loss = 1.87763441\n",
      "Iteration 2256, loss = 1.87592921\n",
      "Iteration 2257, loss = 1.87466030\n",
      "Iteration 2258, loss = 1.87500222\n",
      "Iteration 2259, loss = 1.87619044\n",
      "Iteration 2260, loss = 1.87471928\n",
      "Iteration 2261, loss = 1.87392540\n",
      "Iteration 2262, loss = 1.87444244\n",
      "Iteration 2263, loss = 1.87428368\n",
      "Iteration 2264, loss = 1.87525956\n",
      "Iteration 2265, loss = 1.87696877\n",
      "Iteration 2266, loss = 1.87503132\n",
      "Iteration 2267, loss = 1.87539900\n",
      "Iteration 2268, loss = 1.87411108\n",
      "Iteration 2269, loss = 1.87267299\n",
      "Iteration 2270, loss = 1.87180586\n",
      "Iteration 2271, loss = 1.87066601\n",
      "Iteration 2272, loss = 1.87152884\n",
      "Iteration 2273, loss = 1.87361382\n",
      "Iteration 2274, loss = 1.87388581\n",
      "Iteration 2275, loss = 1.87259546\n",
      "Iteration 2276, loss = 1.87077882\n",
      "Iteration 2277, loss = 1.87106252\n",
      "Iteration 2278, loss = 1.87081199\n",
      "Iteration 2279, loss = 1.87131961\n",
      "Iteration 2280, loss = 1.87086943\n",
      "Iteration 2281, loss = 1.87143306\n",
      "Iteration 2282, loss = 1.87073330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23330620\n",
      "Iteration 2, loss = 4.13373462\n",
      "Iteration 3, loss = 4.03829350\n",
      "Iteration 4, loss = 3.94121080\n",
      "Iteration 5, loss = 3.84097042\n",
      "Iteration 6, loss = 3.73730427\n",
      "Iteration 7, loss = 3.62970616\n",
      "Iteration 8, loss = 3.52418772\n",
      "Iteration 9, loss = 3.42693606\n",
      "Iteration 10, loss = 3.34627031\n",
      "Iteration 11, loss = 3.28205658\n",
      "Iteration 12, loss = 3.24651229\n",
      "Iteration 13, loss = 3.23440384\n",
      "Iteration 14, loss = 3.22895460\n",
      "Iteration 15, loss = 3.22767327\n",
      "Iteration 16, loss = 3.22618301\n",
      "Iteration 17, loss = 3.22347621\n",
      "Iteration 18, loss = 3.22246326\n",
      "Iteration 19, loss = 3.22172177\n",
      "Iteration 20, loss = 3.22040804\n",
      "Iteration 21, loss = 3.21971427\n",
      "Iteration 22, loss = 3.21829724\n",
      "Iteration 23, loss = 3.21790279\n",
      "Iteration 24, loss = 3.21675239\n",
      "Iteration 25, loss = 3.21498132\n",
      "Iteration 26, loss = 3.21397783\n",
      "Iteration 27, loss = 3.21328436\n",
      "Iteration 28, loss = 3.21192829\n",
      "Iteration 29, loss = 3.21128303\n",
      "Iteration 30, loss = 3.20978076\n",
      "Iteration 31, loss = 3.20850835\n",
      "Iteration 32, loss = 3.20781230\n",
      "Iteration 33, loss = 3.20650663\n",
      "Iteration 34, loss = 3.20548624\n",
      "Iteration 35, loss = 3.20460080\n",
      "Iteration 36, loss = 3.20335254\n",
      "Iteration 37, loss = 3.20196082\n",
      "Iteration 38, loss = 3.20138357\n",
      "Iteration 39, loss = 3.20013807\n",
      "Iteration 40, loss = 3.19916022\n",
      "Iteration 41, loss = 3.19760556\n",
      "Iteration 42, loss = 3.19650258\n",
      "Iteration 43, loss = 3.19529643\n",
      "Iteration 44, loss = 3.19430800\n",
      "Iteration 45, loss = 3.19322966\n",
      "Iteration 46, loss = 3.19184583\n",
      "Iteration 47, loss = 3.19126021\n",
      "Iteration 48, loss = 3.19090941\n",
      "Iteration 49, loss = 3.18873800\n",
      "Iteration 50, loss = 3.18595692\n",
      "Iteration 51, loss = 3.18606107\n",
      "Iteration 52, loss = 3.18436092\n",
      "Iteration 53, loss = 3.18197747\n",
      "Iteration 54, loss = 3.18152567\n",
      "Iteration 55, loss = 3.17974685\n",
      "Iteration 56, loss = 3.17814542\n",
      "Iteration 57, loss = 3.17733456\n",
      "Iteration 58, loss = 3.17612389\n",
      "Iteration 59, loss = 3.17501799\n",
      "Iteration 60, loss = 3.17349407\n",
      "Iteration 61, loss = 3.17180045\n",
      "Iteration 62, loss = 3.17049583\n",
      "Iteration 63, loss = 3.16931154\n",
      "Iteration 64, loss = 3.16751714\n",
      "Iteration 65, loss = 3.16596039\n",
      "Iteration 66, loss = 3.16418751\n",
      "Iteration 67, loss = 3.16386110\n",
      "Iteration 68, loss = 3.16190923\n",
      "Iteration 69, loss = 3.16068385\n",
      "Iteration 70, loss = 3.15928203\n",
      "Iteration 71, loss = 3.15785412\n",
      "Iteration 72, loss = 3.15826720\n",
      "Iteration 73, loss = 3.15530177\n",
      "Iteration 74, loss = 3.15355350\n",
      "Iteration 75, loss = 3.15269353\n",
      "Iteration 76, loss = 3.15099141\n",
      "Iteration 77, loss = 3.14967966\n",
      "Iteration 78, loss = 3.14795658\n",
      "Iteration 79, loss = 3.14656646\n",
      "Iteration 80, loss = 3.14540850\n",
      "Iteration 81, loss = 3.14348613\n",
      "Iteration 82, loss = 3.14264970\n",
      "Iteration 83, loss = 3.14141419\n",
      "Iteration 84, loss = 3.14052818\n",
      "Iteration 85, loss = 3.13898856\n",
      "Iteration 86, loss = 3.13836714\n",
      "Iteration 87, loss = 3.13607268\n",
      "Iteration 88, loss = 3.13461434\n",
      "Iteration 89, loss = 3.13204259\n",
      "Iteration 90, loss = 3.13093105\n",
      "Iteration 91, loss = 3.12994270\n",
      "Iteration 92, loss = 3.12818087\n",
      "Iteration 93, loss = 3.12611190\n",
      "Iteration 94, loss = 3.12444312\n",
      "Iteration 95, loss = 3.12369762\n",
      "Iteration 96, loss = 3.12187758\n",
      "Iteration 97, loss = 3.12037017\n",
      "Iteration 98, loss = 3.11895389\n",
      "Iteration 99, loss = 3.11745314\n",
      "Iteration 100, loss = 3.11645697\n",
      "Iteration 101, loss = 3.11557607\n",
      "Iteration 102, loss = 3.11426543\n",
      "Iteration 103, loss = 3.11243376\n",
      "Iteration 104, loss = 3.11026011\n",
      "Iteration 105, loss = 3.10894750\n",
      "Iteration 106, loss = 3.10957850\n",
      "Iteration 107, loss = 3.10700658\n",
      "Iteration 108, loss = 3.10534650\n",
      "Iteration 109, loss = 3.10380452\n",
      "Iteration 110, loss = 3.10208973\n",
      "Iteration 111, loss = 3.10064912\n",
      "Iteration 112, loss = 3.09881887\n",
      "Iteration 113, loss = 3.09864020\n",
      "Iteration 114, loss = 3.09683770\n",
      "Iteration 115, loss = 3.09516259\n",
      "Iteration 116, loss = 3.09432439\n",
      "Iteration 117, loss = 3.09314219\n",
      "Iteration 118, loss = 3.09082139\n",
      "Iteration 119, loss = 3.08843970\n",
      "Iteration 120, loss = 3.08753585\n",
      "Iteration 121, loss = 3.08636316\n",
      "Iteration 122, loss = 3.08448829\n",
      "Iteration 123, loss = 3.08321584\n",
      "Iteration 124, loss = 3.08242543\n",
      "Iteration 125, loss = 3.08100222\n",
      "Iteration 126, loss = 3.07893331\n",
      "Iteration 127, loss = 3.07769269\n",
      "Iteration 128, loss = 3.07601995\n",
      "Iteration 129, loss = 3.07507620\n",
      "Iteration 130, loss = 3.07317242\n",
      "Iteration 131, loss = 3.07275669\n",
      "Iteration 132, loss = 3.07109776\n",
      "Iteration 133, loss = 3.06841939\n",
      "Iteration 134, loss = 3.06733558\n",
      "Iteration 135, loss = 3.06677836\n",
      "Iteration 136, loss = 3.06503685\n",
      "Iteration 137, loss = 3.06330692\n",
      "Iteration 138, loss = 3.06263311\n",
      "Iteration 139, loss = 3.06001819\n",
      "Iteration 140, loss = 3.05966287\n",
      "Iteration 141, loss = 3.05750363\n",
      "Iteration 142, loss = 3.05586912\n",
      "Iteration 143, loss = 3.05463444\n",
      "Iteration 144, loss = 3.05388766\n",
      "Iteration 145, loss = 3.05222378\n",
      "Iteration 146, loss = 3.05010773\n",
      "Iteration 147, loss = 3.04820604\n",
      "Iteration 148, loss = 3.04831946\n",
      "Iteration 149, loss = 3.04629305\n",
      "Iteration 150, loss = 3.04475972\n",
      "Iteration 151, loss = 3.04322499\n",
      "Iteration 152, loss = 3.04184072\n",
      "Iteration 153, loss = 3.04060748\n",
      "Iteration 154, loss = 3.03929738\n",
      "Iteration 155, loss = 3.03677623\n",
      "Iteration 156, loss = 3.03551841\n",
      "Iteration 157, loss = 3.03440307\n",
      "Iteration 158, loss = 3.03247924\n",
      "Iteration 159, loss = 3.03115954\n",
      "Iteration 160, loss = 3.02963634\n",
      "Iteration 161, loss = 3.02901041\n",
      "Iteration 162, loss = 3.02641733\n",
      "Iteration 163, loss = 3.02631927\n",
      "Iteration 164, loss = 3.02553160\n",
      "Iteration 165, loss = 3.02206068\n",
      "Iteration 166, loss = 3.02093275\n",
      "Iteration 167, loss = 3.02071815\n",
      "Iteration 168, loss = 3.01848176\n",
      "Iteration 169, loss = 3.01650163\n",
      "Iteration 170, loss = 3.01560691\n",
      "Iteration 171, loss = 3.01318349\n",
      "Iteration 172, loss = 3.01277285\n",
      "Iteration 173, loss = 3.01079552\n",
      "Iteration 174, loss = 3.00950496\n",
      "Iteration 175, loss = 3.00773531\n",
      "Iteration 176, loss = 3.00600992\n",
      "Iteration 177, loss = 3.00427571\n",
      "Iteration 178, loss = 3.00355481\n",
      "Iteration 179, loss = 3.00173770\n",
      "Iteration 180, loss = 3.00097629\n",
      "Iteration 181, loss = 3.00115132\n",
      "Iteration 182, loss = 2.99913154\n",
      "Iteration 183, loss = 2.99692407\n",
      "Iteration 184, loss = 2.99504963\n",
      "Iteration 185, loss = 2.99269190\n",
      "Iteration 186, loss = 2.99266974\n",
      "Iteration 187, loss = 2.99076108\n",
      "Iteration 188, loss = 2.98919292\n",
      "Iteration 189, loss = 2.98813489\n",
      "Iteration 190, loss = 2.98599414\n",
      "Iteration 191, loss = 2.98494069\n",
      "Iteration 192, loss = 2.98473777\n",
      "Iteration 193, loss = 2.98327180\n",
      "Iteration 194, loss = 2.98180842\n",
      "Iteration 195, loss = 2.98010024\n",
      "Iteration 196, loss = 2.97920591\n",
      "Iteration 197, loss = 2.97758139\n",
      "Iteration 198, loss = 2.97502863\n",
      "Iteration 199, loss = 2.97440376\n",
      "Iteration 200, loss = 2.97298653\n",
      "Iteration 201, loss = 2.97049624\n",
      "Iteration 202, loss = 2.96840151\n",
      "Iteration 203, loss = 2.96770657\n",
      "Iteration 204, loss = 2.96636552\n",
      "Iteration 205, loss = 2.96465966\n",
      "Iteration 206, loss = 2.96415434\n",
      "Iteration 207, loss = 2.96225964\n",
      "Iteration 208, loss = 2.96146819\n",
      "Iteration 209, loss = 2.96056529\n",
      "Iteration 210, loss = 2.95765245\n",
      "Iteration 211, loss = 2.95872314\n",
      "Iteration 212, loss = 2.95857895\n",
      "Iteration 213, loss = 2.95522294\n",
      "Iteration 214, loss = 2.95385281\n",
      "Iteration 215, loss = 2.95238883\n",
      "Iteration 216, loss = 2.95126531\n",
      "Iteration 217, loss = 2.94934865\n",
      "Iteration 218, loss = 2.94787962\n",
      "Iteration 219, loss = 2.94703339\n",
      "Iteration 220, loss = 2.94573502\n",
      "Iteration 221, loss = 2.94480613\n",
      "Iteration 222, loss = 2.94239082\n",
      "Iteration 223, loss = 2.94055940\n",
      "Iteration 224, loss = 2.93957133\n",
      "Iteration 225, loss = 2.93957203\n",
      "Iteration 226, loss = 2.93816289\n",
      "Iteration 227, loss = 2.93650122\n",
      "Iteration 228, loss = 2.93532108\n",
      "Iteration 229, loss = 2.93416689\n",
      "Iteration 230, loss = 2.93294272\n",
      "Iteration 231, loss = 2.93161563\n",
      "Iteration 232, loss = 2.93039299\n",
      "Iteration 233, loss = 2.92828706\n",
      "Iteration 234, loss = 2.92857103\n",
      "Iteration 235, loss = 2.92699931\n",
      "Iteration 236, loss = 2.92539228\n",
      "Iteration 237, loss = 2.92365992\n",
      "Iteration 238, loss = 2.92277749\n",
      "Iteration 239, loss = 2.92073825\n",
      "Iteration 240, loss = 2.91974869\n",
      "Iteration 241, loss = 2.91904062\n",
      "Iteration 242, loss = 2.91741003\n",
      "Iteration 243, loss = 2.91868719\n",
      "Iteration 244, loss = 2.91545299\n",
      "Iteration 245, loss = 2.91397673\n",
      "Iteration 246, loss = 2.91317695\n",
      "Iteration 247, loss = 2.91013055\n",
      "Iteration 248, loss = 2.90941783\n",
      "Iteration 249, loss = 2.90755588\n",
      "Iteration 250, loss = 2.90682956\n",
      "Iteration 251, loss = 2.90627436\n",
      "Iteration 252, loss = 2.90443390\n",
      "Iteration 253, loss = 2.90266438\n",
      "Iteration 254, loss = 2.90117908\n",
      "Iteration 255, loss = 2.90047681\n",
      "Iteration 256, loss = 2.89905104\n",
      "Iteration 257, loss = 2.89700031\n",
      "Iteration 258, loss = 2.89607863\n",
      "Iteration 259, loss = 2.89548606\n",
      "Iteration 260, loss = 2.89386010\n",
      "Iteration 261, loss = 2.89244646\n",
      "Iteration 262, loss = 2.89251134\n",
      "Iteration 263, loss = 2.89131032\n",
      "Iteration 264, loss = 2.88940139\n",
      "Iteration 265, loss = 2.88871724\n",
      "Iteration 266, loss = 2.88717098\n",
      "Iteration 267, loss = 2.88498539\n",
      "Iteration 268, loss = 2.88355000\n",
      "Iteration 269, loss = 2.88342192\n",
      "Iteration 270, loss = 2.88264500\n",
      "Iteration 271, loss = 2.88019801\n",
      "Iteration 272, loss = 2.87937476\n",
      "Iteration 273, loss = 2.87815255\n",
      "Iteration 274, loss = 2.87662914\n",
      "Iteration 275, loss = 2.87472280\n",
      "Iteration 276, loss = 2.87530490\n",
      "Iteration 277, loss = 2.87278285\n",
      "Iteration 278, loss = 2.87133357\n",
      "Iteration 279, loss = 2.87479469\n",
      "Iteration 280, loss = 2.87122486\n",
      "Iteration 281, loss = 2.86861633\n",
      "Iteration 282, loss = 2.86788565\n",
      "Iteration 283, loss = 2.86666688\n",
      "Iteration 284, loss = 2.86454234\n",
      "Iteration 285, loss = 2.86360518\n",
      "Iteration 286, loss = 2.86310688\n",
      "Iteration 287, loss = 2.86193288\n",
      "Iteration 288, loss = 2.86208652\n",
      "Iteration 289, loss = 2.86052509\n",
      "Iteration 290, loss = 2.85808908\n",
      "Iteration 291, loss = 2.85629472\n",
      "Iteration 292, loss = 2.85506078\n",
      "Iteration 293, loss = 2.85430381\n",
      "Iteration 294, loss = 2.85401187\n",
      "Iteration 295, loss = 2.85192925\n",
      "Iteration 296, loss = 2.85059484\n",
      "Iteration 297, loss = 2.85037158\n",
      "Iteration 298, loss = 2.84840772\n",
      "Iteration 299, loss = 2.84778042\n",
      "Iteration 300, loss = 2.84792752\n",
      "Iteration 301, loss = 2.84548697\n",
      "Iteration 302, loss = 2.84368655\n",
      "Iteration 303, loss = 2.84250804\n",
      "Iteration 304, loss = 2.84257854\n",
      "Iteration 305, loss = 2.84250771\n",
      "Iteration 306, loss = 2.84008239\n",
      "Iteration 307, loss = 2.83770467\n",
      "Iteration 308, loss = 2.83703022\n",
      "Iteration 309, loss = 2.83631243\n",
      "Iteration 310, loss = 2.83456407\n",
      "Iteration 311, loss = 2.83383842\n",
      "Iteration 312, loss = 2.83255893\n",
      "Iteration 313, loss = 2.83150739\n",
      "Iteration 314, loss = 2.83001165\n",
      "Iteration 315, loss = 2.82992460\n",
      "Iteration 316, loss = 2.82831647\n",
      "Iteration 317, loss = 2.82720975\n",
      "Iteration 318, loss = 2.82598723\n",
      "Iteration 319, loss = 2.82459446\n",
      "Iteration 320, loss = 2.82199085\n",
      "Iteration 321, loss = 2.82154441\n",
      "Iteration 322, loss = 2.82259323\n",
      "Iteration 323, loss = 2.82060572\n",
      "Iteration 324, loss = 2.81837471\n",
      "Iteration 325, loss = 2.81978781\n",
      "Iteration 326, loss = 2.81673625\n",
      "Iteration 327, loss = 2.81635988\n",
      "Iteration 328, loss = 2.81397304\n",
      "Iteration 329, loss = 2.81267997\n",
      "Iteration 330, loss = 2.81282924\n",
      "Iteration 331, loss = 2.81287646\n",
      "Iteration 332, loss = 2.81117614\n",
      "Iteration 333, loss = 2.80971455\n",
      "Iteration 334, loss = 2.80967839\n",
      "Iteration 335, loss = 2.80615691\n",
      "Iteration 336, loss = 2.80506900\n",
      "Iteration 337, loss = 2.80436587\n",
      "Iteration 338, loss = 2.80322772\n",
      "Iteration 339, loss = 2.80153641\n",
      "Iteration 340, loss = 2.80108929\n",
      "Iteration 341, loss = 2.80009939\n",
      "Iteration 342, loss = 2.79917791\n",
      "Iteration 343, loss = 2.79729729\n",
      "Iteration 344, loss = 2.79807143\n",
      "Iteration 345, loss = 2.79650244\n",
      "Iteration 346, loss = 2.79439328\n",
      "Iteration 347, loss = 2.79392072\n",
      "Iteration 348, loss = 2.79312378\n",
      "Iteration 349, loss = 2.79126662\n",
      "Iteration 350, loss = 2.78982999\n",
      "Iteration 351, loss = 2.78975042\n",
      "Iteration 352, loss = 2.78887224\n",
      "Iteration 353, loss = 2.78797047\n",
      "Iteration 354, loss = 2.78608536\n",
      "Iteration 355, loss = 2.78504819\n",
      "Iteration 356, loss = 2.78302530\n",
      "Iteration 357, loss = 2.78272405\n",
      "Iteration 358, loss = 2.78136574\n",
      "Iteration 359, loss = 2.78048559\n",
      "Iteration 360, loss = 2.77959690\n",
      "Iteration 361, loss = 2.77898964\n",
      "Iteration 362, loss = 2.77745117\n",
      "Iteration 363, loss = 2.77649035\n",
      "Iteration 364, loss = 2.77548625\n",
      "Iteration 365, loss = 2.77471037\n",
      "Iteration 366, loss = 2.77384597\n",
      "Iteration 367, loss = 2.77300394\n",
      "Iteration 368, loss = 2.77097862\n",
      "Iteration 369, loss = 2.77101934\n",
      "Iteration 370, loss = 2.76881133\n",
      "Iteration 371, loss = 2.76881222\n",
      "Iteration 372, loss = 2.76710314\n",
      "Iteration 373, loss = 2.76785715\n",
      "Iteration 374, loss = 2.76587923\n",
      "Iteration 375, loss = 2.76461764\n",
      "Iteration 376, loss = 2.76340400\n",
      "Iteration 377, loss = 2.76224964\n",
      "Iteration 378, loss = 2.76057005\n",
      "Iteration 379, loss = 2.75952756\n",
      "Iteration 380, loss = 2.75888472\n",
      "Iteration 381, loss = 2.75839114\n",
      "Iteration 382, loss = 2.75712279\n",
      "Iteration 383, loss = 2.75497165\n",
      "Iteration 384, loss = 2.75392386\n",
      "Iteration 385, loss = 2.75387583\n",
      "Iteration 386, loss = 2.75364016\n",
      "Iteration 387, loss = 2.75139416\n",
      "Iteration 388, loss = 2.75054488\n",
      "Iteration 389, loss = 2.75065435\n",
      "Iteration 390, loss = 2.74928006\n",
      "Iteration 391, loss = 2.74864001\n",
      "Iteration 392, loss = 2.74799948\n",
      "Iteration 393, loss = 2.74602971\n",
      "Iteration 394, loss = 2.74546407\n",
      "Iteration 395, loss = 2.74378400\n",
      "Iteration 396, loss = 2.74721317\n",
      "Iteration 397, loss = 2.74432155\n",
      "Iteration 398, loss = 2.74250281\n",
      "Iteration 399, loss = 2.74114852\n",
      "Iteration 400, loss = 2.73887951\n",
      "Iteration 401, loss = 2.73787750\n",
      "Iteration 402, loss = 2.73898370\n",
      "Iteration 403, loss = 2.73610931\n",
      "Iteration 404, loss = 2.73538393\n",
      "Iteration 405, loss = 2.73520731\n",
      "Iteration 406, loss = 2.73414322\n",
      "Iteration 407, loss = 2.73211554\n",
      "Iteration 408, loss = 2.73354923\n",
      "Iteration 409, loss = 2.73106089\n",
      "Iteration 410, loss = 2.73058562\n",
      "Iteration 411, loss = 2.72942691\n",
      "Iteration 412, loss = 2.72793588\n",
      "Iteration 413, loss = 2.72702002\n",
      "Iteration 414, loss = 2.72612879\n",
      "Iteration 415, loss = 2.72532845\n",
      "Iteration 416, loss = 2.72444044\n",
      "Iteration 417, loss = 2.72164763\n",
      "Iteration 418, loss = 2.72116859\n",
      "Iteration 419, loss = 2.72114982\n",
      "Iteration 420, loss = 2.71973707\n",
      "Iteration 421, loss = 2.71855318\n",
      "Iteration 422, loss = 2.71841206\n",
      "Iteration 423, loss = 2.71704403\n",
      "Iteration 424, loss = 2.71583210\n",
      "Iteration 425, loss = 2.71515987\n",
      "Iteration 426, loss = 2.71458297\n",
      "Iteration 427, loss = 2.71282998\n",
      "Iteration 428, loss = 2.71144031\n",
      "Iteration 429, loss = 2.71093345\n",
      "Iteration 430, loss = 2.71126284\n",
      "Iteration 431, loss = 2.70980012\n",
      "Iteration 432, loss = 2.70731442\n",
      "Iteration 433, loss = 2.70725042\n",
      "Iteration 434, loss = 2.70657533\n",
      "Iteration 435, loss = 2.70588367\n",
      "Iteration 436, loss = 2.70482144\n",
      "Iteration 437, loss = 2.70310730\n",
      "Iteration 438, loss = 2.70250530\n",
      "Iteration 439, loss = 2.70214080\n",
      "Iteration 440, loss = 2.70025261\n",
      "Iteration 441, loss = 2.70049792\n",
      "Iteration 442, loss = 2.69933928\n",
      "Iteration 443, loss = 2.69745811\n",
      "Iteration 444, loss = 2.69931135\n",
      "Iteration 445, loss = 2.69920657\n",
      "Iteration 446, loss = 2.69479941\n",
      "Iteration 447, loss = 2.69656292\n",
      "Iteration 448, loss = 2.69565159\n",
      "Iteration 449, loss = 2.69376387\n",
      "Iteration 450, loss = 2.69197922\n",
      "Iteration 451, loss = 2.69187921\n",
      "Iteration 452, loss = 2.69071945\n",
      "Iteration 453, loss = 2.69036161\n",
      "Iteration 454, loss = 2.68943662\n",
      "Iteration 455, loss = 2.68761214\n",
      "Iteration 456, loss = 2.68706142\n",
      "Iteration 457, loss = 2.68505166\n",
      "Iteration 458, loss = 2.68498722\n",
      "Iteration 459, loss = 2.68338332\n",
      "Iteration 460, loss = 2.68208815\n",
      "Iteration 461, loss = 2.68127720\n",
      "Iteration 462, loss = 2.68188311\n",
      "Iteration 463, loss = 2.67885852\n",
      "Iteration 464, loss = 2.67756068\n",
      "Iteration 465, loss = 2.67665136\n",
      "Iteration 466, loss = 2.67634368\n",
      "Iteration 467, loss = 2.67582708\n",
      "Iteration 468, loss = 2.67469512\n",
      "Iteration 469, loss = 2.67346878\n",
      "Iteration 470, loss = 2.67311053\n",
      "Iteration 471, loss = 2.67142249\n",
      "Iteration 472, loss = 2.67043956\n",
      "Iteration 473, loss = 2.66971549\n",
      "Iteration 474, loss = 2.66904485\n",
      "Iteration 475, loss = 2.66750324\n",
      "Iteration 476, loss = 2.66852039\n",
      "Iteration 477, loss = 2.66712562\n",
      "Iteration 478, loss = 2.66561377\n",
      "Iteration 479, loss = 2.66448798\n",
      "Iteration 480, loss = 2.66428646\n",
      "Iteration 481, loss = 2.66464578\n",
      "Iteration 482, loss = 2.66240516\n",
      "Iteration 483, loss = 2.66233686\n",
      "Iteration 484, loss = 2.66057998\n",
      "Iteration 485, loss = 2.66029499\n",
      "Iteration 486, loss = 2.65935027\n",
      "Iteration 487, loss = 2.65830061\n",
      "Iteration 488, loss = 2.65722718\n",
      "Iteration 489, loss = 2.65543549\n",
      "Iteration 490, loss = 2.65534906\n",
      "Iteration 491, loss = 2.65447067\n",
      "Iteration 492, loss = 2.65412863\n",
      "Iteration 493, loss = 2.65245903\n",
      "Iteration 494, loss = 2.65134809\n",
      "Iteration 495, loss = 2.65190775\n",
      "Iteration 496, loss = 2.65073480\n",
      "Iteration 497, loss = 2.64931916\n",
      "Iteration 498, loss = 2.64815018\n",
      "Iteration 499, loss = 2.64789650\n",
      "Iteration 500, loss = 2.64684753\n",
      "Iteration 501, loss = 2.64512441\n",
      "Iteration 502, loss = 2.64442809\n",
      "Iteration 503, loss = 2.64324640\n",
      "Iteration 504, loss = 2.64325981\n",
      "Iteration 505, loss = 2.64302827\n",
      "Iteration 506, loss = 2.64168041\n",
      "Iteration 507, loss = 2.64057507\n",
      "Iteration 508, loss = 2.63992188\n",
      "Iteration 509, loss = 2.63926699\n",
      "Iteration 510, loss = 2.64064337\n",
      "Iteration 511, loss = 2.63851045\n",
      "Iteration 512, loss = 2.63711560\n",
      "Iteration 513, loss = 2.63580083\n",
      "Iteration 514, loss = 2.63601798\n",
      "Iteration 515, loss = 2.63563060\n",
      "Iteration 516, loss = 2.63335457\n",
      "Iteration 517, loss = 2.63396505\n",
      "Iteration 518, loss = 2.63246664\n",
      "Iteration 519, loss = 2.63098249\n",
      "Iteration 520, loss = 2.63036090\n",
      "Iteration 521, loss = 2.63057328\n",
      "Iteration 522, loss = 2.62783847\n",
      "Iteration 523, loss = 2.62757608\n",
      "Iteration 524, loss = 2.62684816\n",
      "Iteration 525, loss = 2.62587642\n",
      "Iteration 526, loss = 2.62601217\n",
      "Iteration 527, loss = 2.62451188\n",
      "Iteration 528, loss = 2.62295121\n",
      "Iteration 529, loss = 2.62253512\n",
      "Iteration 530, loss = 2.62203580\n",
      "Iteration 531, loss = 2.62278130\n",
      "Iteration 532, loss = 2.62160991\n",
      "Iteration 533, loss = 2.62178323\n",
      "Iteration 534, loss = 2.62048608\n",
      "Iteration 535, loss = 2.61842797\n",
      "Iteration 536, loss = 2.61776256\n",
      "Iteration 537, loss = 2.61624148\n",
      "Iteration 538, loss = 2.61660369\n",
      "Iteration 539, loss = 2.61613595\n",
      "Iteration 540, loss = 2.61479581\n",
      "Iteration 541, loss = 2.61345328\n",
      "Iteration 542, loss = 2.61286982\n",
      "Iteration 543, loss = 2.61227133\n",
      "Iteration 544, loss = 2.61031805\n",
      "Iteration 545, loss = 2.60898142\n",
      "Iteration 546, loss = 2.60955997\n",
      "Iteration 547, loss = 2.60843213\n",
      "Iteration 548, loss = 2.60802495\n",
      "Iteration 549, loss = 2.60753494\n",
      "Iteration 550, loss = 2.60649315\n",
      "Iteration 551, loss = 2.60479698\n",
      "Iteration 552, loss = 2.60461296\n",
      "Iteration 553, loss = 2.60301542\n",
      "Iteration 554, loss = 2.60267385\n",
      "Iteration 555, loss = 2.60122921\n",
      "Iteration 556, loss = 2.60151960\n",
      "Iteration 557, loss = 2.60217837\n",
      "Iteration 558, loss = 2.59977598\n",
      "Iteration 559, loss = 2.60072988\n",
      "Iteration 560, loss = 2.59966011\n",
      "Iteration 561, loss = 2.59628315\n",
      "Iteration 562, loss = 2.59619659\n",
      "Iteration 563, loss = 2.59575091\n",
      "Iteration 564, loss = 2.59491701\n",
      "Iteration 565, loss = 2.59389058\n",
      "Iteration 566, loss = 2.59316792\n",
      "Iteration 567, loss = 2.59349683\n",
      "Iteration 568, loss = 2.59177996\n",
      "Iteration 569, loss = 2.59109704\n",
      "Iteration 570, loss = 2.59000941\n",
      "Iteration 571, loss = 2.58912644\n",
      "Iteration 572, loss = 2.58825331\n",
      "Iteration 573, loss = 2.58945428\n",
      "Iteration 574, loss = 2.58892582\n",
      "Iteration 575, loss = 2.58927199\n",
      "Iteration 576, loss = 2.58611027\n",
      "Iteration 577, loss = 2.58442657\n",
      "Iteration 578, loss = 2.58594042\n",
      "Iteration 579, loss = 2.58380401\n",
      "Iteration 580, loss = 2.58422563\n",
      "Iteration 581, loss = 2.58321918\n",
      "Iteration 582, loss = 2.58130495\n",
      "Iteration 583, loss = 2.58017375\n",
      "Iteration 584, loss = 2.58032391\n",
      "Iteration 585, loss = 2.57922829\n",
      "Iteration 586, loss = 2.57790494\n",
      "Iteration 587, loss = 2.57742027\n",
      "Iteration 588, loss = 2.57591816\n",
      "Iteration 589, loss = 2.57576697\n",
      "Iteration 590, loss = 2.57770337\n",
      "Iteration 591, loss = 2.57419702\n",
      "Iteration 592, loss = 2.57483157\n",
      "Iteration 593, loss = 2.57294865\n",
      "Iteration 594, loss = 2.57609945\n",
      "Iteration 595, loss = 2.57343871\n",
      "Iteration 596, loss = 2.57063103\n",
      "Iteration 597, loss = 2.56987635\n",
      "Iteration 598, loss = 2.56968155\n",
      "Iteration 599, loss = 2.56822518\n",
      "Iteration 600, loss = 2.56768457\n",
      "Iteration 601, loss = 2.56699474\n",
      "Iteration 602, loss = 2.56638982\n",
      "Iteration 603, loss = 2.56498301\n",
      "Iteration 604, loss = 2.56511064\n",
      "Iteration 605, loss = 2.56488032\n",
      "Iteration 606, loss = 2.56228636\n",
      "Iteration 607, loss = 2.56281534\n",
      "Iteration 608, loss = 2.56135160\n",
      "Iteration 609, loss = 2.56043256\n",
      "Iteration 610, loss = 2.56030698\n",
      "Iteration 611, loss = 2.56005670\n",
      "Iteration 612, loss = 2.56038328\n",
      "Iteration 613, loss = 2.55985218\n",
      "Iteration 614, loss = 2.55781498\n",
      "Iteration 615, loss = 2.55733801\n",
      "Iteration 616, loss = 2.55817511\n",
      "Iteration 617, loss = 2.55613711\n",
      "Iteration 618, loss = 2.55640622\n",
      "Iteration 619, loss = 2.55506424\n",
      "Iteration 620, loss = 2.55364896\n",
      "Iteration 621, loss = 2.55229889\n",
      "Iteration 622, loss = 2.55263573\n",
      "Iteration 623, loss = 2.55025256\n",
      "Iteration 624, loss = 2.55058359\n",
      "Iteration 625, loss = 2.55062171\n",
      "Iteration 626, loss = 2.54975937\n",
      "Iteration 627, loss = 2.54913511\n",
      "Iteration 628, loss = 2.54719460\n",
      "Iteration 629, loss = 2.54643700\n",
      "Iteration 630, loss = 2.54596781\n",
      "Iteration 631, loss = 2.54458411\n",
      "Iteration 632, loss = 2.54374854\n",
      "Iteration 633, loss = 2.54308236\n",
      "Iteration 634, loss = 2.54246895\n",
      "Iteration 635, loss = 2.54252856\n",
      "Iteration 636, loss = 2.54089087\n",
      "Iteration 637, loss = 2.54076084\n",
      "Iteration 638, loss = 2.53977695\n",
      "Iteration 639, loss = 2.54079450\n",
      "Iteration 640, loss = 2.54079934\n",
      "Iteration 641, loss = 2.53849006\n",
      "Iteration 642, loss = 2.53668738\n",
      "Iteration 643, loss = 2.53560619\n",
      "Iteration 644, loss = 2.53634073\n",
      "Iteration 645, loss = 2.53552013\n",
      "Iteration 646, loss = 2.53493996\n",
      "Iteration 647, loss = 2.53447302\n",
      "Iteration 648, loss = 2.53278071\n",
      "Iteration 649, loss = 2.53295481\n",
      "Iteration 650, loss = 2.53245959\n",
      "Iteration 651, loss = 2.53160139\n",
      "Iteration 652, loss = 2.53036789\n",
      "Iteration 653, loss = 2.52963997\n",
      "Iteration 654, loss = 2.52967933\n",
      "Iteration 655, loss = 2.52858748\n",
      "Iteration 656, loss = 2.52857346\n",
      "Iteration 657, loss = 2.52904419\n",
      "Iteration 658, loss = 2.52694569\n",
      "Iteration 659, loss = 2.52691402\n",
      "Iteration 660, loss = 2.52589106\n",
      "Iteration 661, loss = 2.52571359\n",
      "Iteration 662, loss = 2.52335547\n",
      "Iteration 663, loss = 2.52210722\n",
      "Iteration 664, loss = 2.52209644\n",
      "Iteration 665, loss = 2.52052750\n",
      "Iteration 666, loss = 2.52013736\n",
      "Iteration 667, loss = 2.52296356\n",
      "Iteration 668, loss = 2.52003087\n",
      "Iteration 669, loss = 2.51754923\n",
      "Iteration 670, loss = 2.51738183\n",
      "Iteration 671, loss = 2.51715522\n",
      "Iteration 672, loss = 2.51662695\n",
      "Iteration 673, loss = 2.51488359\n",
      "Iteration 674, loss = 2.51476696\n",
      "Iteration 675, loss = 2.51366067\n",
      "Iteration 676, loss = 2.51339652\n",
      "Iteration 677, loss = 2.51417123\n",
      "Iteration 678, loss = 2.51303033\n",
      "Iteration 679, loss = 2.51191458\n",
      "Iteration 680, loss = 2.51282430\n",
      "Iteration 681, loss = 2.51026669\n",
      "Iteration 682, loss = 2.51005813\n",
      "Iteration 683, loss = 2.50930122\n",
      "Iteration 684, loss = 2.50770972\n",
      "Iteration 685, loss = 2.50698407\n",
      "Iteration 686, loss = 2.50710709\n",
      "Iteration 687, loss = 2.50827029\n",
      "Iteration 688, loss = 2.50668710\n",
      "Iteration 689, loss = 2.50507515\n",
      "Iteration 690, loss = 2.50646376\n",
      "Iteration 691, loss = 2.50358894\n",
      "Iteration 692, loss = 2.50429144\n",
      "Iteration 693, loss = 2.50232013\n",
      "Iteration 694, loss = 2.50310476\n",
      "Iteration 695, loss = 2.50134464\n",
      "Iteration 696, loss = 2.49962861\n",
      "Iteration 697, loss = 2.49900975\n",
      "Iteration 698, loss = 2.49858232\n",
      "Iteration 699, loss = 2.49805549\n",
      "Iteration 700, loss = 2.49813388\n",
      "Iteration 701, loss = 2.49768199\n",
      "Iteration 702, loss = 2.49659139\n",
      "Iteration 703, loss = 2.49539977\n",
      "Iteration 704, loss = 2.49419889\n",
      "Iteration 705, loss = 2.49320831\n",
      "Iteration 706, loss = 2.49407081\n",
      "Iteration 707, loss = 2.49262256\n",
      "Iteration 708, loss = 2.49314820\n",
      "Iteration 709, loss = 2.49149408\n",
      "Iteration 710, loss = 2.49131152\n",
      "Iteration 711, loss = 2.49090631\n",
      "Iteration 712, loss = 2.48917274\n",
      "Iteration 713, loss = 2.48983722\n",
      "Iteration 714, loss = 2.48835949\n",
      "Iteration 715, loss = 2.48753969\n",
      "Iteration 716, loss = 2.48710723\n",
      "Iteration 717, loss = 2.48585533\n",
      "Iteration 718, loss = 2.48491049\n",
      "Iteration 719, loss = 2.48363682\n",
      "Iteration 720, loss = 2.48516356\n",
      "Iteration 721, loss = 2.48413211\n",
      "Iteration 722, loss = 2.48341574\n",
      "Iteration 723, loss = 2.48314004\n",
      "Iteration 724, loss = 2.48127639\n",
      "Iteration 725, loss = 2.48043046\n",
      "Iteration 726, loss = 2.48009597\n",
      "Iteration 727, loss = 2.47896553\n",
      "Iteration 728, loss = 2.48028452\n",
      "Iteration 729, loss = 2.47887421\n",
      "Iteration 730, loss = 2.47837308\n",
      "Iteration 731, loss = 2.47762366\n",
      "Iteration 732, loss = 2.47625807\n",
      "Iteration 733, loss = 2.47511474\n",
      "Iteration 734, loss = 2.47361224\n",
      "Iteration 735, loss = 2.47590049\n",
      "Iteration 736, loss = 2.47602011\n",
      "Iteration 737, loss = 2.47467394\n",
      "Iteration 738, loss = 2.47359234\n",
      "Iteration 739, loss = 2.47120331\n",
      "Iteration 740, loss = 2.47065809\n",
      "Iteration 741, loss = 2.46992137\n",
      "Iteration 742, loss = 2.46956661\n",
      "Iteration 743, loss = 2.46823295\n",
      "Iteration 744, loss = 2.46977392\n",
      "Iteration 745, loss = 2.46938486\n",
      "Iteration 746, loss = 2.46732689\n",
      "Iteration 747, loss = 2.46803299\n",
      "Iteration 748, loss = 2.46751828\n",
      "Iteration 749, loss = 2.46695119\n",
      "Iteration 750, loss = 2.46582040\n",
      "Iteration 751, loss = 2.46584124\n",
      "Iteration 752, loss = 2.46398236\n",
      "Iteration 753, loss = 2.46404536\n",
      "Iteration 754, loss = 2.46487085\n",
      "Iteration 755, loss = 2.46456076\n",
      "Iteration 756, loss = 2.46233027\n",
      "Iteration 757, loss = 2.46146456\n",
      "Iteration 758, loss = 2.45984984\n",
      "Iteration 759, loss = 2.45955026\n",
      "Iteration 760, loss = 2.45903072\n",
      "Iteration 761, loss = 2.45792333\n",
      "Iteration 762, loss = 2.45735725\n",
      "Iteration 763, loss = 2.45622031\n",
      "Iteration 764, loss = 2.45557436\n",
      "Iteration 765, loss = 2.45542958\n",
      "Iteration 766, loss = 2.45571561\n",
      "Iteration 767, loss = 2.45585321\n",
      "Iteration 768, loss = 2.45405821\n",
      "Iteration 769, loss = 2.45303797\n",
      "Iteration 770, loss = 2.45226899\n",
      "Iteration 771, loss = 2.45077344\n",
      "Iteration 772, loss = 2.45184150\n",
      "Iteration 773, loss = 2.45104960\n",
      "Iteration 774, loss = 2.45154734\n",
      "Iteration 775, loss = 2.45100420\n",
      "Iteration 776, loss = 2.44924970\n",
      "Iteration 777, loss = 2.44728392\n",
      "Iteration 778, loss = 2.44691542\n",
      "Iteration 779, loss = 2.44796158\n",
      "Iteration 780, loss = 2.44641136\n",
      "Iteration 781, loss = 2.44601527\n",
      "Iteration 782, loss = 2.44500413\n",
      "Iteration 783, loss = 2.44686099\n",
      "Iteration 784, loss = 2.44552370\n",
      "Iteration 785, loss = 2.44300343\n",
      "Iteration 786, loss = 2.44271530\n",
      "Iteration 787, loss = 2.44303087\n",
      "Iteration 788, loss = 2.44212175\n",
      "Iteration 789, loss = 2.44106119\n",
      "Iteration 790, loss = 2.44215099\n",
      "Iteration 791, loss = 2.43971902\n",
      "Iteration 792, loss = 2.43944841\n",
      "Iteration 793, loss = 2.44103611\n",
      "Iteration 794, loss = 2.43775353\n",
      "Iteration 795, loss = 2.43640856\n",
      "Iteration 796, loss = 2.43655571\n",
      "Iteration 797, loss = 2.43696194\n",
      "Iteration 798, loss = 2.43725203\n",
      "Iteration 799, loss = 2.43478996\n",
      "Iteration 800, loss = 2.43463959\n",
      "Iteration 801, loss = 2.43336282\n",
      "Iteration 802, loss = 2.43283239\n",
      "Iteration 803, loss = 2.43165781\n",
      "Iteration 804, loss = 2.43134846\n",
      "Iteration 805, loss = 2.43172701\n",
      "Iteration 806, loss = 2.42975992\n",
      "Iteration 807, loss = 2.42846168\n",
      "Iteration 808, loss = 2.43041884\n",
      "Iteration 809, loss = 2.42959226\n",
      "Iteration 810, loss = 2.42986975\n",
      "Iteration 811, loss = 2.42873129\n",
      "Iteration 812, loss = 2.42653283\n",
      "Iteration 813, loss = 2.42772892\n",
      "Iteration 814, loss = 2.42750182\n",
      "Iteration 815, loss = 2.42442368\n",
      "Iteration 816, loss = 2.42474830\n",
      "Iteration 817, loss = 2.42509266\n",
      "Iteration 818, loss = 2.42393859\n",
      "Iteration 819, loss = 2.42209936\n",
      "Iteration 820, loss = 2.42179779\n",
      "Iteration 821, loss = 2.42251606\n",
      "Iteration 822, loss = 2.42244985\n",
      "Iteration 823, loss = 2.42104981\n",
      "Iteration 824, loss = 2.41960107\n",
      "Iteration 825, loss = 2.41928406\n",
      "Iteration 826, loss = 2.41799116\n",
      "Iteration 827, loss = 2.41790045\n",
      "Iteration 828, loss = 2.41712584\n",
      "Iteration 829, loss = 2.41626989\n",
      "Iteration 830, loss = 2.41727146\n",
      "Iteration 831, loss = 2.41851593\n",
      "Iteration 832, loss = 2.41606768\n",
      "Iteration 833, loss = 2.41746980\n",
      "Iteration 834, loss = 2.41488516\n",
      "Iteration 835, loss = 2.41237433\n",
      "Iteration 836, loss = 2.41241274\n",
      "Iteration 837, loss = 2.41180172\n",
      "Iteration 838, loss = 2.41135601\n",
      "Iteration 839, loss = 2.41147086\n",
      "Iteration 840, loss = 2.41107847\n",
      "Iteration 841, loss = 2.41014469\n",
      "Iteration 842, loss = 2.40890697\n",
      "Iteration 843, loss = 2.41010103\n",
      "Iteration 844, loss = 2.40887393\n",
      "Iteration 845, loss = 2.40798640\n",
      "Iteration 846, loss = 2.40676252\n",
      "Iteration 847, loss = 2.40721574\n",
      "Iteration 848, loss = 2.40539100\n",
      "Iteration 849, loss = 2.40576668\n",
      "Iteration 850, loss = 2.40704141\n",
      "Iteration 851, loss = 2.40476769\n",
      "Iteration 852, loss = 2.40443179\n",
      "Iteration 853, loss = 2.40437511\n",
      "Iteration 854, loss = 2.40249445\n",
      "Iteration 855, loss = 2.40192280\n",
      "Iteration 856, loss = 2.40159261\n",
      "Iteration 857, loss = 2.40149915\n",
      "Iteration 858, loss = 2.39982446\n",
      "Iteration 859, loss = 2.40035375\n",
      "Iteration 860, loss = 2.39966901\n",
      "Iteration 861, loss = 2.39951005\n",
      "Iteration 862, loss = 2.39831389\n",
      "Iteration 863, loss = 2.39751322\n",
      "Iteration 864, loss = 2.39824799\n",
      "Iteration 865, loss = 2.39602594\n",
      "Iteration 866, loss = 2.39851367\n",
      "Iteration 867, loss = 2.39819329\n",
      "Iteration 868, loss = 2.39666234\n",
      "Iteration 869, loss = 2.39494335\n",
      "Iteration 870, loss = 2.39341218\n",
      "Iteration 871, loss = 2.39267652\n",
      "Iteration 872, loss = 2.39212807\n",
      "Iteration 873, loss = 2.39195163\n",
      "Iteration 874, loss = 2.39145609\n",
      "Iteration 875, loss = 2.38942168\n",
      "Iteration 876, loss = 2.39001450\n",
      "Iteration 877, loss = 2.38889053\n",
      "Iteration 878, loss = 2.38988993\n",
      "Iteration 879, loss = 2.38927887\n",
      "Iteration 880, loss = 2.38772174\n",
      "Iteration 881, loss = 2.38751328\n",
      "Iteration 882, loss = 2.38740384\n",
      "Iteration 883, loss = 2.38657366\n",
      "Iteration 884, loss = 2.38530718\n",
      "Iteration 885, loss = 2.38509059\n",
      "Iteration 886, loss = 2.38500463\n",
      "Iteration 887, loss = 2.38654832\n",
      "Iteration 888, loss = 2.38357416\n",
      "Iteration 889, loss = 2.38366266\n",
      "Iteration 890, loss = 2.38177687\n",
      "Iteration 891, loss = 2.38217215\n",
      "Iteration 892, loss = 2.38089099\n",
      "Iteration 893, loss = 2.38022230\n",
      "Iteration 894, loss = 2.38084733\n",
      "Iteration 895, loss = 2.37921216\n",
      "Iteration 896, loss = 2.37787529\n",
      "Iteration 897, loss = 2.37819634\n",
      "Iteration 898, loss = 2.37735184\n",
      "Iteration 899, loss = 2.37710578\n",
      "Iteration 900, loss = 2.37683737\n",
      "Iteration 901, loss = 2.37495532\n",
      "Iteration 902, loss = 2.37493257\n",
      "Iteration 903, loss = 2.37610217\n",
      "Iteration 904, loss = 2.37469184\n",
      "Iteration 905, loss = 2.37549454\n",
      "Iteration 906, loss = 2.37316119\n",
      "Iteration 907, loss = 2.37187444\n",
      "Iteration 908, loss = 2.37140651\n",
      "Iteration 909, loss = 2.37075786\n",
      "Iteration 910, loss = 2.37041572\n",
      "Iteration 911, loss = 2.37037550\n",
      "Iteration 912, loss = 2.36945619\n",
      "Iteration 913, loss = 2.37006894\n",
      "Iteration 914, loss = 2.36876053\n",
      "Iteration 915, loss = 2.36951365\n",
      "Iteration 916, loss = 2.36850826\n",
      "Iteration 917, loss = 2.36730233\n",
      "Iteration 918, loss = 2.36669754\n",
      "Iteration 919, loss = 2.36731010\n",
      "Iteration 920, loss = 2.36669079\n",
      "Iteration 921, loss = 2.36628830\n",
      "Iteration 922, loss = 2.36539280\n",
      "Iteration 923, loss = 2.36428334\n",
      "Iteration 924, loss = 2.36445056\n",
      "Iteration 925, loss = 2.36370189\n",
      "Iteration 926, loss = 2.36413745\n",
      "Iteration 927, loss = 2.36276060\n",
      "Iteration 928, loss = 2.36160301\n",
      "Iteration 929, loss = 2.36132990\n",
      "Iteration 930, loss = 2.36147361\n",
      "Iteration 931, loss = 2.35951507\n",
      "Iteration 932, loss = 2.35852952\n",
      "Iteration 933, loss = 2.36049927\n",
      "Iteration 934, loss = 2.36047896\n",
      "Iteration 935, loss = 2.35726721\n",
      "Iteration 936, loss = 2.35737466\n",
      "Iteration 937, loss = 2.35721419\n",
      "Iteration 938, loss = 2.35644652\n",
      "Iteration 939, loss = 2.35559087\n",
      "Iteration 940, loss = 2.35699997\n",
      "Iteration 941, loss = 2.35509636\n",
      "Iteration 942, loss = 2.35356210\n",
      "Iteration 943, loss = 2.35333998\n",
      "Iteration 944, loss = 2.35386526\n",
      "Iteration 945, loss = 2.35166754\n",
      "Iteration 946, loss = 2.35023889\n",
      "Iteration 947, loss = 2.35094647\n",
      "Iteration 948, loss = 2.35119531\n",
      "Iteration 949, loss = 2.34992751\n",
      "Iteration 950, loss = 2.34937356\n",
      "Iteration 951, loss = 2.34928989\n",
      "Iteration 952, loss = 2.34866076\n",
      "Iteration 953, loss = 2.34770888\n",
      "Iteration 954, loss = 2.34745059\n",
      "Iteration 955, loss = 2.34907207\n",
      "Iteration 956, loss = 2.34662650\n",
      "Iteration 957, loss = 2.34729584\n",
      "Iteration 958, loss = 2.34670137\n",
      "Iteration 959, loss = 2.34593957\n",
      "Iteration 960, loss = 2.34510206\n",
      "Iteration 961, loss = 2.34345596\n",
      "Iteration 962, loss = 2.34249582\n",
      "Iteration 963, loss = 2.34353318\n",
      "Iteration 964, loss = 2.34323385\n",
      "Iteration 965, loss = 2.34253493\n",
      "Iteration 966, loss = 2.34274280\n",
      "Iteration 967, loss = 2.34143283\n",
      "Iteration 968, loss = 2.34019619\n",
      "Iteration 969, loss = 2.34045595\n",
      "Iteration 970, loss = 2.34097573\n",
      "Iteration 971, loss = 2.34120730\n",
      "Iteration 972, loss = 2.33836639\n",
      "Iteration 973, loss = 2.34000068\n",
      "Iteration 974, loss = 2.33800494\n",
      "Iteration 975, loss = 2.33752918\n",
      "Iteration 976, loss = 2.33599093\n",
      "Iteration 977, loss = 2.33499349\n",
      "Iteration 978, loss = 2.33538078\n",
      "Iteration 979, loss = 2.33435293\n",
      "Iteration 980, loss = 2.33387344\n",
      "Iteration 981, loss = 2.33270915\n",
      "Iteration 982, loss = 2.33240261\n",
      "Iteration 983, loss = 2.33312621\n",
      "Iteration 984, loss = 2.33118963\n",
      "Iteration 985, loss = 2.33193632\n",
      "Iteration 986, loss = 2.33053830\n",
      "Iteration 987, loss = 2.32885693\n",
      "Iteration 988, loss = 2.32987310\n",
      "Iteration 989, loss = 2.32966599\n",
      "Iteration 990, loss = 2.33037399\n",
      "Iteration 991, loss = 2.32992153\n",
      "Iteration 992, loss = 2.32981041\n",
      "Iteration 993, loss = 2.32745381\n",
      "Iteration 994, loss = 2.33049057\n",
      "Iteration 995, loss = 2.32750159\n",
      "Iteration 996, loss = 2.32675357\n",
      "Iteration 997, loss = 2.32503239\n",
      "Iteration 998, loss = 2.32503882\n",
      "Iteration 999, loss = 2.32359896\n",
      "Iteration 1000, loss = 2.32270098\n",
      "Iteration 1001, loss = 2.32195835\n",
      "Iteration 1002, loss = 2.32225768\n",
      "Iteration 1003, loss = 2.32222422\n",
      "Iteration 1004, loss = 2.32051914\n",
      "Iteration 1005, loss = 2.32156875\n",
      "Iteration 1006, loss = 2.32066205\n",
      "Iteration 1007, loss = 2.31887730\n",
      "Iteration 1008, loss = 2.31895726\n",
      "Iteration 1009, loss = 2.31872741\n",
      "Iteration 1010, loss = 2.31836733\n",
      "Iteration 1011, loss = 2.31841493\n",
      "Iteration 1012, loss = 2.31847903\n",
      "Iteration 1013, loss = 2.31814955\n",
      "Iteration 1014, loss = 2.31673135\n",
      "Iteration 1015, loss = 2.31613715\n",
      "Iteration 1016, loss = 2.31674245\n",
      "Iteration 1017, loss = 2.31481019\n",
      "Iteration 1018, loss = 2.31399004\n",
      "Iteration 1019, loss = 2.31448577\n",
      "Iteration 1020, loss = 2.31272131\n",
      "Iteration 1021, loss = 2.31240884\n",
      "Iteration 1022, loss = 2.31345887\n",
      "Iteration 1023, loss = 2.31180429\n",
      "Iteration 1024, loss = 2.31206212\n",
      "Iteration 1025, loss = 2.31090155\n",
      "Iteration 1026, loss = 2.30958893\n",
      "Iteration 1027, loss = 2.30919903\n",
      "Iteration 1028, loss = 2.30861923\n",
      "Iteration 1029, loss = 2.30956269\n",
      "Iteration 1030, loss = 2.31050384\n",
      "Iteration 1031, loss = 2.30986498\n",
      "Iteration 1032, loss = 2.30895983\n",
      "Iteration 1033, loss = 2.30665514\n",
      "Iteration 1034, loss = 2.30829796\n",
      "Iteration 1035, loss = 2.30500871\n",
      "Iteration 1036, loss = 2.30820096\n",
      "Iteration 1037, loss = 2.30549693\n",
      "Iteration 1038, loss = 2.30439410\n",
      "Iteration 1039, loss = 2.30384316\n",
      "Iteration 1040, loss = 2.30411542\n",
      "Iteration 1041, loss = 2.30262779\n",
      "Iteration 1042, loss = 2.30224949\n",
      "Iteration 1043, loss = 2.30149889\n",
      "Iteration 1044, loss = 2.30099105\n",
      "Iteration 1045, loss = 2.30053697\n",
      "Iteration 1046, loss = 2.29997362\n",
      "Iteration 1047, loss = 2.30005041\n",
      "Iteration 1048, loss = 2.29983518\n",
      "Iteration 1049, loss = 2.29897042\n",
      "Iteration 1050, loss = 2.29911280\n",
      "Iteration 1051, loss = 2.29933836\n",
      "Iteration 1052, loss = 2.29933477\n",
      "Iteration 1053, loss = 2.29730929\n",
      "Iteration 1054, loss = 2.29713024\n",
      "Iteration 1055, loss = 2.29684901\n",
      "Iteration 1056, loss = 2.29621032\n",
      "Iteration 1057, loss = 2.29502977\n",
      "Iteration 1058, loss = 2.29376321\n",
      "Iteration 1059, loss = 2.29244152\n",
      "Iteration 1060, loss = 2.29445268\n",
      "Iteration 1061, loss = 2.29238516\n",
      "Iteration 1062, loss = 2.29211104\n",
      "Iteration 1063, loss = 2.29222733\n",
      "Iteration 1064, loss = 2.29258214\n",
      "Iteration 1065, loss = 2.29406596\n",
      "Iteration 1066, loss = 2.29253226\n",
      "Iteration 1067, loss = 2.28991872\n",
      "Iteration 1068, loss = 2.29029065\n",
      "Iteration 1069, loss = 2.28938265\n",
      "Iteration 1070, loss = 2.29000452\n",
      "Iteration 1071, loss = 2.28824299\n",
      "Iteration 1072, loss = 2.28846839\n",
      "Iteration 1073, loss = 2.28697293\n",
      "Iteration 1074, loss = 2.28592624\n",
      "Iteration 1075, loss = 2.28624162\n",
      "Iteration 1076, loss = 2.28627383\n",
      "Iteration 1077, loss = 2.28542372\n",
      "Iteration 1078, loss = 2.28674553\n",
      "Iteration 1079, loss = 2.28677263\n",
      "Iteration 1080, loss = 2.28436695\n",
      "Iteration 1081, loss = 2.28490316\n",
      "Iteration 1082, loss = 2.28424186\n",
      "Iteration 1083, loss = 2.28483607\n",
      "Iteration 1084, loss = 2.28228874\n",
      "Iteration 1085, loss = 2.28169105\n",
      "Iteration 1086, loss = 2.28205947\n",
      "Iteration 1087, loss = 2.28075537\n",
      "Iteration 1088, loss = 2.27936575\n",
      "Iteration 1089, loss = 2.28037810\n",
      "Iteration 1090, loss = 2.28195197\n",
      "Iteration 1091, loss = 2.27912852\n",
      "Iteration 1092, loss = 2.27849404\n",
      "Iteration 1093, loss = 2.27762646\n",
      "Iteration 1094, loss = 2.27692898\n",
      "Iteration 1095, loss = 2.27652477\n",
      "Iteration 1096, loss = 2.27672650\n",
      "Iteration 1097, loss = 2.27577510\n",
      "Iteration 1098, loss = 2.27578506\n",
      "Iteration 1099, loss = 2.27409691\n",
      "Iteration 1100, loss = 2.27502570\n",
      "Iteration 1101, loss = 2.27303834\n",
      "Iteration 1102, loss = 2.27339613\n",
      "Iteration 1103, loss = 2.27266967\n",
      "Iteration 1104, loss = 2.27165245\n",
      "Iteration 1105, loss = 2.27128622\n",
      "Iteration 1106, loss = 2.27153518\n",
      "Iteration 1107, loss = 2.27011072\n",
      "Iteration 1108, loss = 2.27002999\n",
      "Iteration 1109, loss = 2.26992120\n",
      "Iteration 1110, loss = 2.26990536\n",
      "Iteration 1111, loss = 2.26946835\n",
      "Iteration 1112, loss = 2.26872984\n",
      "Iteration 1113, loss = 2.26694800\n",
      "Iteration 1114, loss = 2.26763556\n",
      "Iteration 1115, loss = 2.26774146\n",
      "Iteration 1116, loss = 2.26789414\n",
      "Iteration 1117, loss = 2.26645116\n",
      "Iteration 1118, loss = 2.26554921\n",
      "Iteration 1119, loss = 2.26581877\n",
      "Iteration 1120, loss = 2.26503826\n",
      "Iteration 1121, loss = 2.26496264\n",
      "Iteration 1122, loss = 2.26416725\n",
      "Iteration 1123, loss = 2.26275522\n",
      "Iteration 1124, loss = 2.26196662\n",
      "Iteration 1125, loss = 2.26175379\n",
      "Iteration 1126, loss = 2.26125003\n",
      "Iteration 1127, loss = 2.26061015\n",
      "Iteration 1128, loss = 2.26142435\n",
      "Iteration 1129, loss = 2.26239378\n",
      "Iteration 1130, loss = 2.26066327\n",
      "Iteration 1131, loss = 2.25924182\n",
      "Iteration 1132, loss = 2.25920596\n",
      "Iteration 1133, loss = 2.25973929\n",
      "Iteration 1134, loss = 2.25859773\n",
      "Iteration 1135, loss = 2.25837337\n",
      "Iteration 1136, loss = 2.25745985\n",
      "Iteration 1137, loss = 2.25777159\n",
      "Iteration 1138, loss = 2.25757643\n",
      "Iteration 1139, loss = 2.25717697\n",
      "Iteration 1140, loss = 2.25601612\n",
      "Iteration 1141, loss = 2.25576435\n",
      "Iteration 1142, loss = 2.25563313\n",
      "Iteration 1143, loss = 2.25504626\n",
      "Iteration 1144, loss = 2.25353544\n",
      "Iteration 1145, loss = 2.25368309\n",
      "Iteration 1146, loss = 2.25394374\n",
      "Iteration 1147, loss = 2.25484242\n",
      "Iteration 1148, loss = 2.25183979\n",
      "Iteration 1149, loss = 2.25212832\n",
      "Iteration 1150, loss = 2.25259982\n",
      "Iteration 1151, loss = 2.25458088\n",
      "Iteration 1152, loss = 2.25186791\n",
      "Iteration 1153, loss = 2.25416772\n",
      "Iteration 1154, loss = 2.25049214\n",
      "Iteration 1155, loss = 2.25147356\n",
      "Iteration 1156, loss = 2.24907170\n",
      "Iteration 1157, loss = 2.24843668\n",
      "Iteration 1158, loss = 2.24690906\n",
      "Iteration 1159, loss = 2.24916002\n",
      "Iteration 1160, loss = 2.24590443\n",
      "Iteration 1161, loss = 2.24712019\n",
      "Iteration 1162, loss = 2.24620962\n",
      "Iteration 1163, loss = 2.24688608\n",
      "Iteration 1164, loss = 2.24595029\n",
      "Iteration 1165, loss = 2.24459715\n",
      "Iteration 1166, loss = 2.24420544\n",
      "Iteration 1167, loss = 2.24431949\n",
      "Iteration 1168, loss = 2.24461474\n",
      "Iteration 1169, loss = 2.24370020\n",
      "Iteration 1170, loss = 2.24175850\n",
      "Iteration 1171, loss = 2.24222623\n",
      "Iteration 1172, loss = 2.24243626\n",
      "Iteration 1173, loss = 2.24064711\n",
      "Iteration 1174, loss = 2.24065078\n",
      "Iteration 1175, loss = 2.24032056\n",
      "Iteration 1176, loss = 2.23913465\n",
      "Iteration 1177, loss = 2.23896057\n",
      "Iteration 1178, loss = 2.23853834\n",
      "Iteration 1179, loss = 2.23904376\n",
      "Iteration 1180, loss = 2.23851837\n",
      "Iteration 1181, loss = 2.23774517\n",
      "Iteration 1182, loss = 2.23769208\n",
      "Iteration 1183, loss = 2.23727482\n",
      "Iteration 1184, loss = 2.23862855\n",
      "Iteration 1185, loss = 2.23681511\n",
      "Iteration 1186, loss = 2.23454121\n",
      "Iteration 1187, loss = 2.23514784\n",
      "Iteration 1188, loss = 2.23453012\n",
      "Iteration 1189, loss = 2.23415558\n",
      "Iteration 1190, loss = 2.23258928\n",
      "Iteration 1191, loss = 2.23306463\n",
      "Iteration 1192, loss = 2.23243166\n",
      "Iteration 1193, loss = 2.23215625\n",
      "Iteration 1194, loss = 2.23280729\n",
      "Iteration 1195, loss = 2.23215435\n",
      "Iteration 1196, loss = 2.23168284\n",
      "Iteration 1197, loss = 2.23118678\n",
      "Iteration 1198, loss = 2.23037002\n",
      "Iteration 1199, loss = 2.22853555\n",
      "Iteration 1200, loss = 2.23017899\n",
      "Iteration 1201, loss = 2.22952498\n",
      "Iteration 1202, loss = 2.22834453\n",
      "Iteration 1203, loss = 2.22901968\n",
      "Iteration 1204, loss = 2.22813011\n",
      "Iteration 1205, loss = 2.22887458\n",
      "Iteration 1206, loss = 2.22740940\n",
      "Iteration 1207, loss = 2.22920526\n",
      "Iteration 1208, loss = 2.22775578\n",
      "Iteration 1209, loss = 2.22475562\n",
      "Iteration 1210, loss = 2.22511147\n",
      "Iteration 1211, loss = 2.22469540\n",
      "Iteration 1212, loss = 2.22444880\n",
      "Iteration 1213, loss = 2.22457568\n",
      "Iteration 1214, loss = 2.22343264\n",
      "Iteration 1215, loss = 2.22287583\n",
      "Iteration 1216, loss = 2.22211302\n",
      "Iteration 1217, loss = 2.22242484\n",
      "Iteration 1218, loss = 2.22225219\n",
      "Iteration 1219, loss = 2.22450094\n",
      "Iteration 1220, loss = 2.22041171\n",
      "Iteration 1221, loss = 2.22088974\n",
      "Iteration 1222, loss = 2.22201013\n",
      "Iteration 1223, loss = 2.21857737\n",
      "Iteration 1224, loss = 2.22040228\n",
      "Iteration 1225, loss = 2.22105634\n",
      "Iteration 1226, loss = 2.21939348\n",
      "Iteration 1227, loss = 2.21753274\n",
      "Iteration 1228, loss = 2.21912121\n",
      "Iteration 1229, loss = 2.21812839\n",
      "Iteration 1230, loss = 2.21615690\n",
      "Iteration 1231, loss = 2.21512307\n",
      "Iteration 1232, loss = 2.21554477\n",
      "Iteration 1233, loss = 2.21474191\n",
      "Iteration 1234, loss = 2.21450371\n",
      "Iteration 1235, loss = 2.21302441\n",
      "Iteration 1236, loss = 2.21374024\n",
      "Iteration 1237, loss = 2.21330435\n",
      "Iteration 1238, loss = 2.21254141\n",
      "Iteration 1239, loss = 2.21408660\n",
      "Iteration 1240, loss = 2.21434237\n",
      "Iteration 1241, loss = 2.21217469\n",
      "Iteration 1242, loss = 2.21228162\n",
      "Iteration 1243, loss = 2.21154182\n",
      "Iteration 1244, loss = 2.21104659\n",
      "Iteration 1245, loss = 2.21053293\n",
      "Iteration 1246, loss = 2.21080706\n",
      "Iteration 1247, loss = 2.21025633\n",
      "Iteration 1248, loss = 2.21084032\n",
      "Iteration 1249, loss = 2.20883833\n",
      "Iteration 1250, loss = 2.20879573\n",
      "Iteration 1251, loss = 2.20865698\n",
      "Iteration 1252, loss = 2.20823982\n",
      "Iteration 1253, loss = 2.20835997\n",
      "Iteration 1254, loss = 2.20628497\n",
      "Iteration 1255, loss = 2.20599976\n",
      "Iteration 1256, loss = 2.20594897\n",
      "Iteration 1257, loss = 2.20623206\n",
      "Iteration 1258, loss = 2.20424313\n",
      "Iteration 1259, loss = 2.20389073\n",
      "Iteration 1260, loss = 2.20377739\n",
      "Iteration 1261, loss = 2.20389234\n",
      "Iteration 1262, loss = 2.20326208\n",
      "Iteration 1263, loss = 2.20249306\n",
      "Iteration 1264, loss = 2.20358571\n",
      "Iteration 1265, loss = 2.20283050\n",
      "Iteration 1266, loss = 2.20254511\n",
      "Iteration 1267, loss = 2.20269393\n",
      "Iteration 1268, loss = 2.20389452\n",
      "Iteration 1269, loss = 2.20086719\n",
      "Iteration 1270, loss = 2.20157266\n",
      "Iteration 1271, loss = 2.20046937\n",
      "Iteration 1272, loss = 2.20132148\n",
      "Iteration 1273, loss = 2.20001586\n",
      "Iteration 1274, loss = 2.19817835\n",
      "Iteration 1275, loss = 2.20092518\n",
      "Iteration 1276, loss = 2.19751939\n",
      "Iteration 1277, loss = 2.19724942\n",
      "Iteration 1278, loss = 2.19636848\n",
      "Iteration 1279, loss = 2.19768145\n",
      "Iteration 1280, loss = 2.19555187\n",
      "Iteration 1281, loss = 2.19621496\n",
      "Iteration 1282, loss = 2.19576297\n",
      "Iteration 1283, loss = 2.19400264\n",
      "Iteration 1284, loss = 2.19618911\n",
      "Iteration 1285, loss = 2.19496278\n",
      "Iteration 1286, loss = 2.19375699\n",
      "Iteration 1287, loss = 2.19417344\n",
      "Iteration 1288, loss = 2.19319646\n",
      "Iteration 1289, loss = 2.19169621\n",
      "Iteration 1290, loss = 2.19164095\n",
      "Iteration 1291, loss = 2.19360229\n",
      "Iteration 1292, loss = 2.19302312\n",
      "Iteration 1293, loss = 2.19030330\n",
      "Iteration 1294, loss = 2.19000574\n",
      "Iteration 1295, loss = 2.18943519\n",
      "Iteration 1296, loss = 2.19092861\n",
      "Iteration 1297, loss = 2.19021896\n",
      "Iteration 1298, loss = 2.18995780\n",
      "Iteration 1299, loss = 2.18890260\n",
      "Iteration 1300, loss = 2.18801558\n",
      "Iteration 1301, loss = 2.18699275\n",
      "Iteration 1302, loss = 2.18709058\n",
      "Iteration 1303, loss = 2.18680772\n",
      "Iteration 1304, loss = 2.18653540\n",
      "Iteration 1305, loss = 2.18575821\n",
      "Iteration 1306, loss = 2.18663085\n",
      "Iteration 1307, loss = 2.18359778\n",
      "Iteration 1308, loss = 2.18607539\n",
      "Iteration 1309, loss = 2.18495751\n",
      "Iteration 1310, loss = 2.18512710\n",
      "Iteration 1311, loss = 2.18491597\n",
      "Iteration 1312, loss = 2.18385664\n",
      "Iteration 1313, loss = 2.18360299\n",
      "Iteration 1314, loss = 2.18341148\n",
      "Iteration 1315, loss = 2.18117053\n",
      "Iteration 1316, loss = 2.18118265\n",
      "Iteration 1317, loss = 2.18125349\n",
      "Iteration 1318, loss = 2.18118681\n",
      "Iteration 1319, loss = 2.18098721\n",
      "Iteration 1320, loss = 2.17981153\n",
      "Iteration 1321, loss = 2.17900492\n",
      "Iteration 1322, loss = 2.17829582\n",
      "Iteration 1323, loss = 2.17765293\n",
      "Iteration 1324, loss = 2.17809263\n",
      "Iteration 1325, loss = 2.17659970\n",
      "Iteration 1326, loss = 2.17717569\n",
      "Iteration 1327, loss = 2.17738994\n",
      "Iteration 1328, loss = 2.17681162\n",
      "Iteration 1329, loss = 2.17706046\n",
      "Iteration 1330, loss = 2.17762165\n",
      "Iteration 1331, loss = 2.17623865\n",
      "Iteration 1332, loss = 2.17591819\n",
      "Iteration 1333, loss = 2.17357485\n",
      "Iteration 1334, loss = 2.17368199\n",
      "Iteration 1335, loss = 2.17298863\n",
      "Iteration 1336, loss = 2.17343956\n",
      "Iteration 1337, loss = 2.17469355\n",
      "Iteration 1338, loss = 2.17245387\n",
      "Iteration 1339, loss = 2.17720566\n",
      "Iteration 1340, loss = 2.17325891\n",
      "Iteration 1341, loss = 2.17595758\n",
      "Iteration 1342, loss = 2.17320165\n",
      "Iteration 1343, loss = 2.17286147\n",
      "Iteration 1344, loss = 2.17335943\n",
      "Iteration 1345, loss = 2.17053628\n",
      "Iteration 1346, loss = 2.16949108\n",
      "Iteration 1347, loss = 2.16899266\n",
      "Iteration 1348, loss = 2.17064403\n",
      "Iteration 1349, loss = 2.16823486\n",
      "Iteration 1350, loss = 2.16742814\n",
      "Iteration 1351, loss = 2.16741194\n",
      "Iteration 1352, loss = 2.16762386\n",
      "Iteration 1353, loss = 2.16738498\n",
      "Iteration 1354, loss = 2.16754029\n",
      "Iteration 1355, loss = 2.16677811\n",
      "Iteration 1356, loss = 2.16528368\n",
      "Iteration 1357, loss = 2.16501738\n",
      "Iteration 1358, loss = 2.16496629\n",
      "Iteration 1359, loss = 2.16510701\n",
      "Iteration 1360, loss = 2.16755930\n",
      "Iteration 1361, loss = 2.16744140\n",
      "Iteration 1362, loss = 2.16371952\n",
      "Iteration 1363, loss = 2.16360108\n",
      "Iteration 1364, loss = 2.16446386\n",
      "Iteration 1365, loss = 2.16213630\n",
      "Iteration 1366, loss = 2.16269901\n",
      "Iteration 1367, loss = 2.16137974\n",
      "Iteration 1368, loss = 2.16269237\n",
      "Iteration 1369, loss = 2.16132157\n",
      "Iteration 1370, loss = 2.16082837\n",
      "Iteration 1371, loss = 2.16214516\n",
      "Iteration 1372, loss = 2.16092316\n",
      "Iteration 1373, loss = 2.16072476\n",
      "Iteration 1374, loss = 2.16060322\n",
      "Iteration 1375, loss = 2.15772295\n",
      "Iteration 1376, loss = 2.16100757\n",
      "Iteration 1377, loss = 2.15754066\n",
      "Iteration 1378, loss = 2.15787687\n",
      "Iteration 1379, loss = 2.15728521\n",
      "Iteration 1380, loss = 2.15603624\n",
      "Iteration 1381, loss = 2.15609208\n",
      "Iteration 1382, loss = 2.15657783\n",
      "Iteration 1383, loss = 2.15483057\n",
      "Iteration 1384, loss = 2.15764794\n",
      "Iteration 1385, loss = 2.15533467\n",
      "Iteration 1386, loss = 2.15366805\n",
      "Iteration 1387, loss = 2.15366610\n",
      "Iteration 1388, loss = 2.15327307\n",
      "Iteration 1389, loss = 2.15440567\n",
      "Iteration 1390, loss = 2.15378640\n",
      "Iteration 1391, loss = 2.15238611\n",
      "Iteration 1392, loss = 2.15243848\n",
      "Iteration 1393, loss = 2.15206112\n",
      "Iteration 1394, loss = 2.15212364\n",
      "Iteration 1395, loss = 2.15288026\n",
      "Iteration 1396, loss = 2.15375158\n",
      "Iteration 1397, loss = 2.15147870\n",
      "Iteration 1398, loss = 2.15007579\n",
      "Iteration 1399, loss = 2.15030620\n",
      "Iteration 1400, loss = 2.15120833\n",
      "Iteration 1401, loss = 2.14877008\n",
      "Iteration 1402, loss = 2.14957734\n",
      "Iteration 1403, loss = 2.14887207\n",
      "Iteration 1404, loss = 2.14804048\n",
      "Iteration 1405, loss = 2.14743815\n",
      "Iteration 1406, loss = 2.14658036\n",
      "Iteration 1407, loss = 2.14855479\n",
      "Iteration 1408, loss = 2.14632062\n",
      "Iteration 1409, loss = 2.14725916\n",
      "Iteration 1410, loss = 2.14493924\n",
      "Iteration 1411, loss = 2.14513625\n",
      "Iteration 1412, loss = 2.14495054\n",
      "Iteration 1413, loss = 2.14416643\n",
      "Iteration 1414, loss = 2.14458142\n",
      "Iteration 1415, loss = 2.14435011\n",
      "Iteration 1416, loss = 2.14508648\n",
      "Iteration 1417, loss = 2.14436464\n",
      "Iteration 1418, loss = 2.14309064\n",
      "Iteration 1419, loss = 2.14293748\n",
      "Iteration 1420, loss = 2.14127710\n",
      "Iteration 1421, loss = 2.14072035\n",
      "Iteration 1422, loss = 2.14104938\n",
      "Iteration 1423, loss = 2.13990606\n",
      "Iteration 1424, loss = 2.13933505\n",
      "Iteration 1425, loss = 2.13991404\n",
      "Iteration 1426, loss = 2.13913962\n",
      "Iteration 1427, loss = 2.14120726\n",
      "Iteration 1428, loss = 2.13944319\n",
      "Iteration 1429, loss = 2.13976229\n",
      "Iteration 1430, loss = 2.13873468\n",
      "Iteration 1431, loss = 2.13778027\n",
      "Iteration 1432, loss = 2.13789288\n",
      "Iteration 1433, loss = 2.13689144\n",
      "Iteration 1434, loss = 2.13576715\n",
      "Iteration 1435, loss = 2.13772525\n",
      "Iteration 1436, loss = 2.13619687\n",
      "Iteration 1437, loss = 2.13810792\n",
      "Iteration 1438, loss = 2.13525754\n",
      "Iteration 1439, loss = 2.13793408\n",
      "Iteration 1440, loss = 2.13432155\n",
      "Iteration 1441, loss = 2.13533003\n",
      "Iteration 1442, loss = 2.13530469\n",
      "Iteration 1443, loss = 2.13818472\n",
      "Iteration 1444, loss = 2.13515755\n",
      "Iteration 1445, loss = 2.13540708\n",
      "Iteration 1446, loss = 2.13268609\n",
      "Iteration 1447, loss = 2.13136229\n",
      "Iteration 1448, loss = 2.13373386\n",
      "Iteration 1449, loss = 2.13286797\n",
      "Iteration 1450, loss = 2.13312566\n",
      "Iteration 1451, loss = 2.13507811\n",
      "Iteration 1452, loss = 2.13272807\n",
      "Iteration 1453, loss = 2.13176576\n",
      "Iteration 1454, loss = 2.13020017\n",
      "Iteration 1455, loss = 2.13064050\n",
      "Iteration 1456, loss = 2.13067125\n",
      "Iteration 1457, loss = 2.12861017\n",
      "Iteration 1458, loss = 2.12892983\n",
      "Iteration 1459, loss = 2.12994574\n",
      "Iteration 1460, loss = 2.12715811\n",
      "Iteration 1461, loss = 2.12656473\n",
      "Iteration 1462, loss = 2.12936467\n",
      "Iteration 1463, loss = 2.12696834\n",
      "Iteration 1464, loss = 2.12795002\n",
      "Iteration 1465, loss = 2.12620675\n",
      "Iteration 1466, loss = 2.12527936\n",
      "Iteration 1467, loss = 2.12901567\n",
      "Iteration 1468, loss = 2.12638116\n",
      "Iteration 1469, loss = 2.12608349\n",
      "Iteration 1470, loss = 2.12417102\n",
      "Iteration 1471, loss = 2.12268913\n",
      "Iteration 1472, loss = 2.12413161\n",
      "Iteration 1473, loss = 2.12323186\n",
      "Iteration 1474, loss = 2.12218589\n",
      "Iteration 1475, loss = 2.12341906\n",
      "Iteration 1476, loss = 2.12233839\n",
      "Iteration 1477, loss = 2.12187275\n",
      "Iteration 1478, loss = 2.12393955\n",
      "Iteration 1479, loss = 2.12153871\n",
      "Iteration 1480, loss = 2.11973327\n",
      "Iteration 1481, loss = 2.12018746\n",
      "Iteration 1482, loss = 2.11962584\n",
      "Iteration 1483, loss = 2.11979404\n",
      "Iteration 1484, loss = 2.11912698\n",
      "Iteration 1485, loss = 2.11904526\n",
      "Iteration 1486, loss = 2.11922673\n",
      "Iteration 1487, loss = 2.11808312\n",
      "Iteration 1488, loss = 2.11863593\n",
      "Iteration 1489, loss = 2.11905749\n",
      "Iteration 1490, loss = 2.11767880\n",
      "Iteration 1491, loss = 2.11946099\n",
      "Iteration 1492, loss = 2.11828099\n",
      "Iteration 1493, loss = 2.11600722\n",
      "Iteration 1494, loss = 2.11591161\n",
      "Iteration 1495, loss = 2.11462180\n",
      "Iteration 1496, loss = 2.11517705\n",
      "Iteration 1497, loss = 2.11465145\n",
      "Iteration 1498, loss = 2.11470479\n",
      "Iteration 1499, loss = 2.11439889\n",
      "Iteration 1500, loss = 2.11772922\n",
      "Iteration 1501, loss = 2.11481153\n",
      "Iteration 1502, loss = 2.11386592\n",
      "Iteration 1503, loss = 2.11344902\n",
      "Iteration 1504, loss = 2.11346730\n",
      "Iteration 1505, loss = 2.11301092\n",
      "Iteration 1506, loss = 2.11367062\n",
      "Iteration 1507, loss = 2.11229954\n",
      "Iteration 1508, loss = 2.11078384\n",
      "Iteration 1509, loss = 2.11050477\n",
      "Iteration 1510, loss = 2.10966247\n",
      "Iteration 1511, loss = 2.11051561\n",
      "Iteration 1512, loss = 2.11078270\n",
      "Iteration 1513, loss = 2.10995884\n",
      "Iteration 1514, loss = 2.10871944\n",
      "Iteration 1515, loss = 2.10946236\n",
      "Iteration 1516, loss = 2.11019890\n",
      "Iteration 1517, loss = 2.10714775\n",
      "Iteration 1518, loss = 2.10761759\n",
      "Iteration 1519, loss = 2.10655968\n",
      "Iteration 1520, loss = 2.10697392\n",
      "Iteration 1521, loss = 2.10862476\n",
      "Iteration 1522, loss = 2.10638478\n",
      "Iteration 1523, loss = 2.10713585\n",
      "Iteration 1524, loss = 2.10679360\n",
      "Iteration 1525, loss = 2.10419077\n",
      "Iteration 1526, loss = 2.10404760\n",
      "Iteration 1527, loss = 2.10339835\n",
      "Iteration 1528, loss = 2.10433086\n",
      "Iteration 1529, loss = 2.10460008\n",
      "Iteration 1530, loss = 2.10347083\n",
      "Iteration 1531, loss = 2.10439765\n",
      "Iteration 1532, loss = 2.10501435\n",
      "Iteration 1533, loss = 2.10364957\n",
      "Iteration 1534, loss = 2.10304679\n",
      "Iteration 1535, loss = 2.10359740\n",
      "Iteration 1536, loss = 2.10188276\n",
      "Iteration 1537, loss = 2.10120867\n",
      "Iteration 1538, loss = 2.10025749\n",
      "Iteration 1539, loss = 2.10025156\n",
      "Iteration 1540, loss = 2.10011894\n",
      "Iteration 1541, loss = 2.10005695\n",
      "Iteration 1542, loss = 2.09986442\n",
      "Iteration 1543, loss = 2.09907258\n",
      "Iteration 1544, loss = 2.09796850\n",
      "Iteration 1545, loss = 2.09979615\n",
      "Iteration 1546, loss = 2.09886200\n",
      "Iteration 1547, loss = 2.09875161\n",
      "Iteration 1548, loss = 2.09817861\n",
      "Iteration 1549, loss = 2.09936236\n",
      "Iteration 1550, loss = 2.09670457\n",
      "Iteration 1551, loss = 2.09609692\n",
      "Iteration 1552, loss = 2.09617089\n",
      "Iteration 1553, loss = 2.09662133\n",
      "Iteration 1554, loss = 2.09560952\n",
      "Iteration 1555, loss = 2.09544846\n",
      "Iteration 1556, loss = 2.09478293\n",
      "Iteration 1557, loss = 2.09501423\n",
      "Iteration 1558, loss = 2.09370614\n",
      "Iteration 1559, loss = 2.09482910\n",
      "Iteration 1560, loss = 2.09336840\n",
      "Iteration 1561, loss = 2.09584087\n",
      "Iteration 1562, loss = 2.09512751\n",
      "Iteration 1563, loss = 2.09554145\n",
      "Iteration 1564, loss = 2.09284270\n",
      "Iteration 1565, loss = 2.09257812\n",
      "Iteration 1566, loss = 2.09323991\n",
      "Iteration 1567, loss = 2.09117593\n",
      "Iteration 1568, loss = 2.09040882\n",
      "Iteration 1569, loss = 2.09075219\n",
      "Iteration 1570, loss = 2.08914790\n",
      "Iteration 1571, loss = 2.08811374\n",
      "Iteration 1572, loss = 2.08886196\n",
      "Iteration 1573, loss = 2.08898747\n",
      "Iteration 1574, loss = 2.08747107\n",
      "Iteration 1575, loss = 2.08869240\n",
      "Iteration 1576, loss = 2.08934073\n",
      "Iteration 1577, loss = 2.08970221\n",
      "Iteration 1578, loss = 2.08841552\n",
      "Iteration 1579, loss = 2.08720084\n",
      "Iteration 1580, loss = 2.08568027\n",
      "Iteration 1581, loss = 2.08619301\n",
      "Iteration 1582, loss = 2.08654420\n",
      "Iteration 1583, loss = 2.08620662\n",
      "Iteration 1584, loss = 2.08706154\n",
      "Iteration 1585, loss = 2.08650992\n",
      "Iteration 1586, loss = 2.08666116\n",
      "Iteration 1587, loss = 2.08397777\n",
      "Iteration 1588, loss = 2.08481197\n",
      "Iteration 1589, loss = 2.08409554\n",
      "Iteration 1590, loss = 2.08469250\n",
      "Iteration 1591, loss = 2.08476939\n",
      "Iteration 1592, loss = 2.08407712\n",
      "Iteration 1593, loss = 2.08445828\n",
      "Iteration 1594, loss = 2.08459495\n",
      "Iteration 1595, loss = 2.08373723\n",
      "Iteration 1596, loss = 2.08309357\n",
      "Iteration 1597, loss = 2.08149866\n",
      "Iteration 1598, loss = 2.08305071\n",
      "Iteration 1599, loss = 2.08167004\n",
      "Iteration 1600, loss = 2.08080089\n",
      "Iteration 1601, loss = 2.08073867\n",
      "Iteration 1602, loss = 2.08303253\n",
      "Iteration 1603, loss = 2.07907061\n",
      "Iteration 1604, loss = 2.08033771\n",
      "Iteration 1605, loss = 2.08094993\n",
      "Iteration 1606, loss = 2.07966122\n",
      "Iteration 1607, loss = 2.07852557\n",
      "Iteration 1608, loss = 2.07895336\n",
      "Iteration 1609, loss = 2.07773479\n",
      "Iteration 1610, loss = 2.07671252\n",
      "Iteration 1611, loss = 2.07800844\n",
      "Iteration 1612, loss = 2.07686558\n",
      "Iteration 1613, loss = 2.07722050\n",
      "Iteration 1614, loss = 2.07872317\n",
      "Iteration 1615, loss = 2.07530318\n",
      "Iteration 1616, loss = 2.07546664\n",
      "Iteration 1617, loss = 2.07415989\n",
      "Iteration 1618, loss = 2.07493065\n",
      "Iteration 1619, loss = 2.07409950\n",
      "Iteration 1620, loss = 2.07257957\n",
      "Iteration 1621, loss = 2.07324384\n",
      "Iteration 1622, loss = 2.07355116\n",
      "Iteration 1623, loss = 2.07331138\n",
      "Iteration 1624, loss = 2.07250192\n",
      "Iteration 1625, loss = 2.07410025\n",
      "Iteration 1626, loss = 2.07496156\n",
      "Iteration 1627, loss = 2.07253508\n",
      "Iteration 1628, loss = 2.07162062\n",
      "Iteration 1629, loss = 2.07050994\n",
      "Iteration 1630, loss = 2.07141092\n",
      "Iteration 1631, loss = 2.07116894\n",
      "Iteration 1632, loss = 2.06974400\n",
      "Iteration 1633, loss = 2.06911934\n",
      "Iteration 1634, loss = 2.06980746\n",
      "Iteration 1635, loss = 2.06961328\n",
      "Iteration 1636, loss = 2.06981671\n",
      "Iteration 1637, loss = 2.06847366\n",
      "Iteration 1638, loss = 2.06836276\n",
      "Iteration 1639, loss = 2.06828504\n",
      "Iteration 1640, loss = 2.06757109\n",
      "Iteration 1641, loss = 2.06947318\n",
      "Iteration 1642, loss = 2.06782991\n",
      "Iteration 1643, loss = 2.06790886\n",
      "Iteration 1644, loss = 2.06797675\n",
      "Iteration 1645, loss = 2.06714084\n",
      "Iteration 1646, loss = 2.06698178\n",
      "Iteration 1647, loss = 2.06593833\n",
      "Iteration 1648, loss = 2.06618572\n",
      "Iteration 1649, loss = 2.06540364\n",
      "Iteration 1650, loss = 2.06521898\n",
      "Iteration 1651, loss = 2.06411397\n",
      "Iteration 1652, loss = 2.06466271\n",
      "Iteration 1653, loss = 2.06366450\n",
      "Iteration 1654, loss = 2.06313122\n",
      "Iteration 1655, loss = 2.06549301\n",
      "Iteration 1656, loss = 2.06380189\n",
      "Iteration 1657, loss = 2.06269668\n",
      "Iteration 1658, loss = 2.06228215\n",
      "Iteration 1659, loss = 2.06114642\n",
      "Iteration 1660, loss = 2.06175995\n",
      "Iteration 1661, loss = 2.06323246\n",
      "Iteration 1662, loss = 2.06315307\n",
      "Iteration 1663, loss = 2.06263178\n",
      "Iteration 1664, loss = 2.06095707\n",
      "Iteration 1665, loss = 2.05992549\n",
      "Iteration 1666, loss = 2.05984873\n",
      "Iteration 1667, loss = 2.05935455\n",
      "Iteration 1668, loss = 2.06066012\n",
      "Iteration 1669, loss = 2.06032494\n",
      "Iteration 1670, loss = 2.05997730\n",
      "Iteration 1671, loss = 2.05899102\n",
      "Iteration 1672, loss = 2.05819762\n",
      "Iteration 1673, loss = 2.05785735\n",
      "Iteration 1674, loss = 2.05788263\n",
      "Iteration 1675, loss = 2.05707328\n",
      "Iteration 1676, loss = 2.05759677\n",
      "Iteration 1677, loss = 2.05578981\n",
      "Iteration 1678, loss = 2.05783466\n",
      "Iteration 1679, loss = 2.05598807\n",
      "Iteration 1680, loss = 2.05488852\n",
      "Iteration 1681, loss = 2.05468354\n",
      "Iteration 1682, loss = 2.05620134\n",
      "Iteration 1683, loss = 2.05569632\n",
      "Iteration 1684, loss = 2.05397210\n",
      "Iteration 1685, loss = 2.05458529\n",
      "Iteration 1686, loss = 2.05289049\n",
      "Iteration 1687, loss = 2.05370657\n",
      "Iteration 1688, loss = 2.05375808\n",
      "Iteration 1689, loss = 2.05469463\n",
      "Iteration 1690, loss = 2.05332892\n",
      "Iteration 1691, loss = 2.05352408\n",
      "Iteration 1692, loss = 2.05247756\n",
      "Iteration 1693, loss = 2.05263768\n",
      "Iteration 1694, loss = 2.05139445\n",
      "Iteration 1695, loss = 2.05331865\n",
      "Iteration 1696, loss = 2.05108712\n",
      "Iteration 1697, loss = 2.05309938\n",
      "Iteration 1698, loss = 2.05006873\n",
      "Iteration 1699, loss = 2.04901433\n",
      "Iteration 1700, loss = 2.04965790\n",
      "Iteration 1701, loss = 2.04986897\n",
      "Iteration 1702, loss = 2.05012998\n",
      "Iteration 1703, loss = 2.04924339\n",
      "Iteration 1704, loss = 2.04812274\n",
      "Iteration 1705, loss = 2.04783017\n",
      "Iteration 1706, loss = 2.04712988\n",
      "Iteration 1707, loss = 2.04696878\n",
      "Iteration 1708, loss = 2.04783297\n",
      "Iteration 1709, loss = 2.04530399\n",
      "Iteration 1710, loss = 2.04580379\n",
      "Iteration 1711, loss = 2.04738445\n",
      "Iteration 1712, loss = 2.04680971\n",
      "Iteration 1713, loss = 2.04713676\n",
      "Iteration 1714, loss = 2.04580596\n",
      "Iteration 1715, loss = 2.04430065\n",
      "Iteration 1716, loss = 2.04714213\n",
      "Iteration 1717, loss = 2.04659418\n",
      "Iteration 1718, loss = 2.04487912\n",
      "Iteration 1719, loss = 2.04607036\n",
      "Iteration 1720, loss = 2.04418679\n",
      "Iteration 1721, loss = 2.04422347\n",
      "Iteration 1722, loss = 2.04294143\n",
      "Iteration 1723, loss = 2.04370767\n",
      "Iteration 1724, loss = 2.04298652\n",
      "Iteration 1725, loss = 2.04124100\n",
      "Iteration 1726, loss = 2.04330977\n",
      "Iteration 1727, loss = 2.04201746\n",
      "Iteration 1728, loss = 2.04083457\n",
      "Iteration 1729, loss = 2.04192089\n",
      "Iteration 1730, loss = 2.04302393\n",
      "Iteration 1731, loss = 2.04031565\n",
      "Iteration 1732, loss = 2.04078646\n",
      "Iteration 1733, loss = 2.04021500\n",
      "Iteration 1734, loss = 2.03819444\n",
      "Iteration 1735, loss = 2.03955692\n",
      "Iteration 1736, loss = 2.04117293\n",
      "Iteration 1737, loss = 2.04009472\n",
      "Iteration 1738, loss = 2.04027405\n",
      "Iteration 1739, loss = 2.03691401\n",
      "Iteration 1740, loss = 2.03791634\n",
      "Iteration 1741, loss = 2.03795498\n",
      "Iteration 1742, loss = 2.03784739\n",
      "Iteration 1743, loss = 2.03741748\n",
      "Iteration 1744, loss = 2.03540860\n",
      "Iteration 1745, loss = 2.03753599\n",
      "Iteration 1746, loss = 2.03705047\n",
      "Iteration 1747, loss = 2.03650983\n",
      "Iteration 1748, loss = 2.03524026\n",
      "Iteration 1749, loss = 2.03762314\n",
      "Iteration 1750, loss = 2.03529340\n",
      "Iteration 1751, loss = 2.03839838\n",
      "Iteration 1752, loss = 2.03447267\n",
      "Iteration 1753, loss = 2.03432827\n",
      "Iteration 1754, loss = 2.03437843\n",
      "Iteration 1755, loss = 2.03488864\n",
      "Iteration 1756, loss = 2.03274825\n",
      "Iteration 1757, loss = 2.03181009\n",
      "Iteration 1758, loss = 2.03107958\n",
      "Iteration 1759, loss = 2.03303688\n",
      "Iteration 1760, loss = 2.03304228\n",
      "Iteration 1761, loss = 2.03245547\n",
      "Iteration 1762, loss = 2.03179926\n",
      "Iteration 1763, loss = 2.03153080\n",
      "Iteration 1764, loss = 2.03084504\n",
      "Iteration 1765, loss = 2.03200372\n",
      "Iteration 1766, loss = 2.03402414\n",
      "Iteration 1767, loss = 2.03271885\n",
      "Iteration 1768, loss = 2.03114619\n",
      "Iteration 1769, loss = 2.02950790\n",
      "Iteration 1770, loss = 2.02858619\n",
      "Iteration 1771, loss = 2.02908815\n",
      "Iteration 1772, loss = 2.02692876\n",
      "Iteration 1773, loss = 2.02914409\n",
      "Iteration 1774, loss = 2.02831193\n",
      "Iteration 1775, loss = 2.02763442\n",
      "Iteration 1776, loss = 2.02720466\n",
      "Iteration 1777, loss = 2.02854019\n",
      "Iteration 1778, loss = 2.02644008\n",
      "Iteration 1779, loss = 2.02634966\n",
      "Iteration 1780, loss = 2.02591129\n",
      "Iteration 1781, loss = 2.02476370\n",
      "Iteration 1782, loss = 2.02526183\n",
      "Iteration 1783, loss = 2.02445221\n",
      "Iteration 1784, loss = 2.02431384\n",
      "Iteration 1785, loss = 2.02450407\n",
      "Iteration 1786, loss = 2.02354591\n",
      "Iteration 1787, loss = 2.02398123\n",
      "Iteration 1788, loss = 2.02350677\n",
      "Iteration 1789, loss = 2.02676412\n",
      "Iteration 1790, loss = 2.02433543\n",
      "Iteration 1791, loss = 2.02400731\n",
      "Iteration 1792, loss = 2.02368055\n",
      "Iteration 1793, loss = 2.02172358\n",
      "Iteration 1794, loss = 2.02193661\n",
      "Iteration 1795, loss = 2.02215166\n",
      "Iteration 1796, loss = 2.02179336\n",
      "Iteration 1797, loss = 2.02000065\n",
      "Iteration 1798, loss = 2.02082975\n",
      "Iteration 1799, loss = 2.02064990\n",
      "Iteration 1800, loss = 2.02017088\n",
      "Iteration 1801, loss = 2.02256159\n",
      "Iteration 1802, loss = 2.02243785\n",
      "Iteration 1803, loss = 2.01979887\n",
      "Iteration 1804, loss = 2.01813521\n",
      "Iteration 1805, loss = 2.01895789\n",
      "Iteration 1806, loss = 2.01953178\n",
      "Iteration 1807, loss = 2.02143865\n",
      "Iteration 1808, loss = 2.01921591\n",
      "Iteration 1809, loss = 2.01833784\n",
      "Iteration 1810, loss = 2.01890111\n",
      "Iteration 1811, loss = 2.01904879\n",
      "Iteration 1812, loss = 2.01789931\n",
      "Iteration 1813, loss = 2.01700132\n",
      "Iteration 1814, loss = 2.01609680\n",
      "Iteration 1815, loss = 2.01560575\n",
      "Iteration 1816, loss = 2.01917937\n",
      "Iteration 1817, loss = 2.01931226\n",
      "Iteration 1818, loss = 2.01861016\n",
      "Iteration 1819, loss = 2.01560467\n",
      "Iteration 1820, loss = 2.01602564\n",
      "Iteration 1821, loss = 2.01447134\n",
      "Iteration 1822, loss = 2.01468878\n",
      "Iteration 1823, loss = 2.01461333\n",
      "Iteration 1824, loss = 2.01282722\n",
      "Iteration 1825, loss = 2.01637546\n",
      "Iteration 1826, loss = 2.01413194\n",
      "Iteration 1827, loss = 2.01494336\n",
      "Iteration 1828, loss = 2.01391374\n",
      "Iteration 1829, loss = 2.01378925\n",
      "Iteration 1830, loss = 2.01086365\n",
      "Iteration 1831, loss = 2.01236937\n",
      "Iteration 1832, loss = 2.01246381\n",
      "Iteration 1833, loss = 2.01237894\n",
      "Iteration 1834, loss = 2.01204098\n",
      "Iteration 1835, loss = 2.01285067\n",
      "Iteration 1836, loss = 2.01202411\n",
      "Iteration 1837, loss = 2.00996490\n",
      "Iteration 1838, loss = 2.01040598\n",
      "Iteration 1839, loss = 2.01112150\n",
      "Iteration 1840, loss = 2.01111082\n",
      "Iteration 1841, loss = 2.01105899\n",
      "Iteration 1842, loss = 2.00998717\n",
      "Iteration 1843, loss = 2.01084279\n",
      "Iteration 1844, loss = 2.00972133\n",
      "Iteration 1845, loss = 2.00695700\n",
      "Iteration 1846, loss = 2.00861031\n",
      "Iteration 1847, loss = 2.00832959\n",
      "Iteration 1848, loss = 2.00621356\n",
      "Iteration 1849, loss = 2.00661957\n",
      "Iteration 1850, loss = 2.00819721\n",
      "Iteration 1851, loss = 2.00638814\n",
      "Iteration 1852, loss = 2.00752266\n",
      "Iteration 1853, loss = 2.00719104\n",
      "Iteration 1854, loss = 2.00615311\n",
      "Iteration 1855, loss = 2.00642653\n",
      "Iteration 1856, loss = 2.00757770\n",
      "Iteration 1857, loss = 2.00579080\n",
      "Iteration 1858, loss = 2.00609304\n",
      "Iteration 1859, loss = 2.00734616\n",
      "Iteration 1860, loss = 2.00457182\n",
      "Iteration 1861, loss = 2.00446710\n",
      "Iteration 1862, loss = 2.00343191\n",
      "Iteration 1863, loss = 2.00238348\n",
      "Iteration 1864, loss = 2.00162456\n",
      "Iteration 1865, loss = 2.00265966\n",
      "Iteration 1866, loss = 2.00203226\n",
      "Iteration 1867, loss = 2.00207419\n",
      "Iteration 1868, loss = 2.00077600\n",
      "Iteration 1869, loss = 1.99964509\n",
      "Iteration 1870, loss = 2.00067235\n",
      "Iteration 1871, loss = 2.00025266\n",
      "Iteration 1872, loss = 2.00015016\n",
      "Iteration 1873, loss = 1.99904271\n",
      "Iteration 1874, loss = 1.99849848\n",
      "Iteration 1875, loss = 1.99957184\n",
      "Iteration 1876, loss = 2.00011196\n",
      "Iteration 1877, loss = 1.99897197\n",
      "Iteration 1878, loss = 1.99918547\n",
      "Iteration 1879, loss = 1.99813451\n",
      "Iteration 1880, loss = 1.99854355\n",
      "Iteration 1881, loss = 1.99770112\n",
      "Iteration 1882, loss = 1.99884602\n",
      "Iteration 1883, loss = 1.99811546\n",
      "Iteration 1884, loss = 1.99702241\n",
      "Iteration 1885, loss = 1.99674544\n",
      "Iteration 1886, loss = 1.99731003\n",
      "Iteration 1887, loss = 1.99590324\n",
      "Iteration 1888, loss = 1.99660560\n",
      "Iteration 1889, loss = 1.99534063\n",
      "Iteration 1890, loss = 1.99753819\n",
      "Iteration 1891, loss = 1.99605150\n",
      "Iteration 1892, loss = 1.99665177\n",
      "Iteration 1893, loss = 1.99607067\n",
      "Iteration 1894, loss = 1.99609425\n",
      "Iteration 1895, loss = 1.99598379\n",
      "Iteration 1896, loss = 1.99541207\n",
      "Iteration 1897, loss = 1.99521946\n",
      "Iteration 1898, loss = 1.99281345\n",
      "Iteration 1899, loss = 1.99454086\n",
      "Iteration 1900, loss = 1.99281986\n",
      "Iteration 1901, loss = 1.99407510\n",
      "Iteration 1902, loss = 1.99310640\n",
      "Iteration 1903, loss = 1.99182925\n",
      "Iteration 1904, loss = 1.99043319\n",
      "Iteration 1905, loss = 1.99104827\n",
      "Iteration 1906, loss = 1.99073153\n",
      "Iteration 1907, loss = 1.99047539\n",
      "Iteration 1908, loss = 1.99051032\n",
      "Iteration 1909, loss = 1.98998250\n",
      "Iteration 1910, loss = 1.99257589\n",
      "Iteration 1911, loss = 1.99172110\n",
      "Iteration 1912, loss = 1.99190830\n",
      "Iteration 1913, loss = 1.98990037\n",
      "Iteration 1914, loss = 1.99351392\n",
      "Iteration 1915, loss = 1.99164368\n",
      "Iteration 1916, loss = 1.99003242\n",
      "Iteration 1917, loss = 1.99120299\n",
      "Iteration 1918, loss = 1.99127452\n",
      "Iteration 1919, loss = 1.98933497\n",
      "Iteration 1920, loss = 1.98960347\n",
      "Iteration 1921, loss = 1.98696404\n",
      "Iteration 1922, loss = 1.98751122\n",
      "Iteration 1923, loss = 1.98606238\n",
      "Iteration 1924, loss = 1.98516704\n",
      "Iteration 1925, loss = 1.98471690\n",
      "Iteration 1926, loss = 1.98940589\n",
      "Iteration 1927, loss = 1.98787920\n",
      "Iteration 1928, loss = 1.98576827\n",
      "Iteration 1929, loss = 1.98736161\n",
      "Iteration 1930, loss = 1.98983680\n",
      "Iteration 1931, loss = 1.98520782\n",
      "Iteration 1932, loss = 1.98710588\n",
      "Iteration 1933, loss = 1.98606259\n",
      "Iteration 1934, loss = 1.98473572\n",
      "Iteration 1935, loss = 1.98619692\n",
      "Iteration 1936, loss = 1.98467483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15497377\n",
      "Iteration 2, loss = 3.70602703\n",
      "Iteration 3, loss = 3.44265964\n",
      "Iteration 4, loss = 3.30408814\n",
      "Iteration 5, loss = 3.25394789\n",
      "Iteration 6, loss = 3.25565810\n",
      "Iteration 7, loss = 3.25079709\n",
      "Iteration 8, loss = 3.24228004\n",
      "Iteration 9, loss = 3.24237650\n",
      "Iteration 10, loss = 3.24620706\n",
      "Iteration 11, loss = 3.24272441\n",
      "Iteration 12, loss = 3.24187442\n",
      "Iteration 13, loss = 3.24116125\n",
      "Iteration 14, loss = 3.24086035\n",
      "Iteration 15, loss = 3.24344455\n",
      "Iteration 16, loss = 3.24414764\n",
      "Iteration 17, loss = 3.24309569\n",
      "Iteration 18, loss = 3.24090896\n",
      "Iteration 19, loss = 3.24181635\n",
      "Iteration 20, loss = 3.24189624\n",
      "Iteration 21, loss = 3.24264424\n",
      "Iteration 22, loss = 3.24266971\n",
      "Iteration 23, loss = 3.24014870\n",
      "Iteration 24, loss = 3.24012185\n",
      "Iteration 25, loss = 3.24076748\n",
      "Iteration 26, loss = 3.23990926\n",
      "Iteration 27, loss = 3.24253674\n",
      "Iteration 28, loss = 3.24415311\n",
      "Iteration 29, loss = 3.24192821\n",
      "Iteration 30, loss = 3.24027425\n",
      "Iteration 31, loss = 3.23869496\n",
      "Iteration 32, loss = 3.24194734\n",
      "Iteration 33, loss = 3.24480746\n",
      "Iteration 34, loss = 3.24123124\n",
      "Iteration 35, loss = 3.24043471\n",
      "Iteration 36, loss = 3.24055998\n",
      "Iteration 37, loss = 3.24222448\n",
      "Iteration 38, loss = 3.24176398\n",
      "Iteration 39, loss = 3.23905996\n",
      "Iteration 40, loss = 3.23964962\n",
      "Iteration 41, loss = 3.24023859\n",
      "Iteration 42, loss = 3.24359647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15713453\n",
      "Iteration 2, loss = 3.70418845\n",
      "Iteration 3, loss = 3.44714639\n",
      "Iteration 4, loss = 3.30742061\n",
      "Iteration 5, loss = 3.25531446\n",
      "Iteration 6, loss = 3.25750207\n",
      "Iteration 7, loss = 3.25399245\n",
      "Iteration 8, loss = 3.24724678\n",
      "Iteration 9, loss = 3.24545429\n",
      "Iteration 10, loss = 3.24414197\n",
      "Iteration 11, loss = 3.24667773\n",
      "Iteration 12, loss = 3.24752922\n",
      "Iteration 13, loss = 3.24785990\n",
      "Iteration 14, loss = 3.24698392\n",
      "Iteration 15, loss = 3.24583270\n",
      "Iteration 16, loss = 3.24591936\n",
      "Iteration 17, loss = 3.24549760\n",
      "Iteration 18, loss = 3.24508401\n",
      "Iteration 19, loss = 3.24568180\n",
      "Iteration 20, loss = 3.24329276\n",
      "Iteration 21, loss = 3.24550780\n",
      "Iteration 22, loss = 3.24741978\n",
      "Iteration 23, loss = 3.24745373\n",
      "Iteration 24, loss = 3.24563152\n",
      "Iteration 25, loss = 3.24749665\n",
      "Iteration 26, loss = 3.24480593\n",
      "Iteration 27, loss = 3.24503798\n",
      "Iteration 28, loss = 3.24732062\n",
      "Iteration 29, loss = 3.24516727\n",
      "Iteration 30, loss = 3.24632628\n",
      "Iteration 31, loss = 3.24503001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15109438\n",
      "Iteration 2, loss = 3.70098174\n",
      "Iteration 3, loss = 3.44308664\n",
      "Iteration 4, loss = 3.29805746\n",
      "Iteration 5, loss = 3.24466650\n",
      "Iteration 6, loss = 3.25092874\n",
      "Iteration 7, loss = 3.24654257\n",
      "Iteration 8, loss = 3.24140145\n",
      "Iteration 9, loss = 3.24116146\n",
      "Iteration 10, loss = 3.23714695\n",
      "Iteration 11, loss = 3.23788237\n",
      "Iteration 12, loss = 3.23709043\n",
      "Iteration 13, loss = 3.23796513\n",
      "Iteration 14, loss = 3.23834859\n",
      "Iteration 15, loss = 3.23636945\n",
      "Iteration 16, loss = 3.23755140\n",
      "Iteration 17, loss = 3.23782159\n",
      "Iteration 18, loss = 3.23734065\n",
      "Iteration 19, loss = 3.23948883\n",
      "Iteration 20, loss = 3.23885372\n",
      "Iteration 21, loss = 3.24012933\n",
      "Iteration 22, loss = 3.23827466\n",
      "Iteration 23, loss = 3.23892996\n",
      "Iteration 24, loss = 3.23726629\n",
      "Iteration 25, loss = 3.23859606\n",
      "Iteration 26, loss = 3.23738357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15518340\n",
      "Iteration 2, loss = 3.69997663\n",
      "Iteration 3, loss = 3.44520892\n",
      "Iteration 4, loss = 3.29474620\n",
      "Iteration 5, loss = 3.24188196\n",
      "Iteration 6, loss = 3.25016983\n",
      "Iteration 7, loss = 3.24957323\n",
      "Iteration 8, loss = 3.24023314\n",
      "Iteration 9, loss = 3.24034987\n",
      "Iteration 10, loss = 3.23780647\n",
      "Iteration 11, loss = 3.23621989\n",
      "Iteration 12, loss = 3.23762939\n",
      "Iteration 13, loss = 3.23834724\n",
      "Iteration 14, loss = 3.23688582\n",
      "Iteration 15, loss = 3.23634204\n",
      "Iteration 16, loss = 3.23565016\n",
      "Iteration 17, loss = 3.23642109\n",
      "Iteration 18, loss = 3.23666334\n",
      "Iteration 19, loss = 3.23674428\n",
      "Iteration 20, loss = 3.23715813\n",
      "Iteration 21, loss = 3.23820014\n",
      "Iteration 22, loss = 3.23609324\n",
      "Iteration 23, loss = 3.23667485\n",
      "Iteration 24, loss = 3.23591876\n",
      "Iteration 25, loss = 3.23809243\n",
      "Iteration 26, loss = 3.23587530\n",
      "Iteration 27, loss = 3.23811923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15548790\n",
      "Iteration 2, loss = 3.71479297\n",
      "Iteration 3, loss = 3.44463383\n",
      "Iteration 4, loss = 3.29856195\n",
      "Iteration 5, loss = 3.24516500\n",
      "Iteration 6, loss = 3.25651771\n",
      "Iteration 7, loss = 3.25944582\n",
      "Iteration 8, loss = 3.24650920\n",
      "Iteration 9, loss = 3.24301751\n",
      "Iteration 10, loss = 3.24374701\n",
      "Iteration 11, loss = 3.24395998\n",
      "Iteration 12, loss = 3.24169881\n",
      "Iteration 13, loss = 3.24243102\n",
      "Iteration 14, loss = 3.24236047\n",
      "Iteration 15, loss = 3.24243787\n",
      "Iteration 16, loss = 3.24287878\n",
      "Iteration 17, loss = 3.24330866\n",
      "Iteration 18, loss = 3.24296071\n",
      "Iteration 19, loss = 3.24381535\n",
      "Iteration 20, loss = 3.24288945\n",
      "Iteration 21, loss = 3.24212395\n",
      "Iteration 22, loss = 3.24152266\n",
      "Iteration 23, loss = 3.24156374\n",
      "Iteration 24, loss = 3.24166518\n",
      "Iteration 25, loss = 3.24232169\n",
      "Iteration 26, loss = 3.24279840\n",
      "Iteration 27, loss = 3.24187261\n",
      "Iteration 28, loss = 3.24110699\n",
      "Iteration 29, loss = 3.24195392\n",
      "Iteration 30, loss = 3.24103500\n",
      "Iteration 31, loss = 3.24176317\n",
      "Iteration 32, loss = 3.24268550\n",
      "Iteration 33, loss = 3.24148129\n",
      "Iteration 34, loss = 3.24214020\n",
      "Iteration 35, loss = 3.24349928\n",
      "Iteration 36, loss = 3.24307950\n",
      "Iteration 37, loss = 3.24232427\n",
      "Iteration 38, loss = 3.24151005\n",
      "Iteration 39, loss = 3.24245244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23860757\n",
      "Iteration 2, loss = 4.14004841\n",
      "Iteration 3, loss = 4.04608211\n",
      "Iteration 4, loss = 3.95083703\n",
      "Iteration 5, loss = 3.85085384\n",
      "Iteration 6, loss = 3.74779474\n",
      "Iteration 7, loss = 3.64428003\n",
      "Iteration 8, loss = 3.54196288\n",
      "Iteration 9, loss = 3.44551545\n",
      "Iteration 10, loss = 3.36301308\n",
      "Iteration 11, loss = 3.29755833\n",
      "Iteration 12, loss = 3.25627024\n",
      "Iteration 13, loss = 3.23753657\n",
      "Iteration 14, loss = 3.23351359\n",
      "Iteration 15, loss = 3.23238346\n",
      "Iteration 16, loss = 3.23056352\n",
      "Iteration 17, loss = 3.22753165\n",
      "Iteration 18, loss = 3.22678268\n",
      "Iteration 19, loss = 3.22560313\n",
      "Iteration 20, loss = 3.22463085\n",
      "Iteration 21, loss = 3.22438914\n",
      "Iteration 22, loss = 3.22359946\n",
      "Iteration 23, loss = 3.22270125\n",
      "Iteration 24, loss = 3.22187148\n",
      "Iteration 25, loss = 3.22172311\n",
      "Iteration 26, loss = 3.22097707\n",
      "Iteration 27, loss = 3.21982105\n",
      "Iteration 28, loss = 3.21915868\n",
      "Iteration 29, loss = 3.21905896\n",
      "Iteration 30, loss = 3.21857854\n",
      "Iteration 31, loss = 3.21733838\n",
      "Iteration 32, loss = 3.21662774\n",
      "Iteration 33, loss = 3.21559911\n",
      "Iteration 34, loss = 3.21556640\n",
      "Iteration 35, loss = 3.21442552\n",
      "Iteration 36, loss = 3.21371855\n",
      "Iteration 37, loss = 3.21301816\n",
      "Iteration 38, loss = 3.21211185\n",
      "Iteration 39, loss = 3.21159275\n",
      "Iteration 40, loss = 3.21097374\n",
      "Iteration 41, loss = 3.21031188\n",
      "Iteration 42, loss = 3.20993045\n",
      "Iteration 43, loss = 3.20931589\n",
      "Iteration 44, loss = 3.20826291\n",
      "Iteration 45, loss = 3.20802608\n",
      "Iteration 46, loss = 3.20703608\n",
      "Iteration 47, loss = 3.20602188\n",
      "Iteration 48, loss = 3.20498697\n",
      "Iteration 49, loss = 3.20504122\n",
      "Iteration 50, loss = 3.20390338\n",
      "Iteration 51, loss = 3.20338930\n",
      "Iteration 52, loss = 3.20200628\n",
      "Iteration 53, loss = 3.20263923\n",
      "Iteration 54, loss = 3.20184910\n",
      "Iteration 55, loss = 3.20100302\n",
      "Iteration 56, loss = 3.19996108\n",
      "Iteration 57, loss = 3.20031920\n",
      "Iteration 58, loss = 3.19960971\n",
      "Iteration 59, loss = 3.19886298\n",
      "Iteration 60, loss = 3.19793011\n",
      "Iteration 61, loss = 3.19708159\n",
      "Iteration 62, loss = 3.19645547\n",
      "Iteration 63, loss = 3.19572727\n",
      "Iteration 64, loss = 3.19522931\n",
      "Iteration 65, loss = 3.19440484\n",
      "Iteration 66, loss = 3.19387527\n",
      "Iteration 67, loss = 3.19373354\n",
      "Iteration 68, loss = 3.19261757\n",
      "Iteration 69, loss = 3.19244183\n",
      "Iteration 70, loss = 3.19121195\n",
      "Iteration 71, loss = 3.19051893\n",
      "Iteration 72, loss = 3.18968587\n",
      "Iteration 73, loss = 3.18915679\n",
      "Iteration 74, loss = 3.18817529\n",
      "Iteration 75, loss = 3.18860363\n",
      "Iteration 76, loss = 3.18786113\n",
      "Iteration 77, loss = 3.18659005\n",
      "Iteration 78, loss = 3.18686969\n",
      "Iteration 79, loss = 3.18584772\n",
      "Iteration 80, loss = 3.18540045\n",
      "Iteration 81, loss = 3.18480362\n",
      "Iteration 82, loss = 3.18391082\n",
      "Iteration 83, loss = 3.18355174\n",
      "Iteration 84, loss = 3.18311824\n",
      "Iteration 85, loss = 3.18246033\n",
      "Iteration 86, loss = 3.18176484\n",
      "Iteration 87, loss = 3.18122912\n",
      "Iteration 88, loss = 3.18029425\n",
      "Iteration 89, loss = 3.17933933\n",
      "Iteration 90, loss = 3.17904212\n",
      "Iteration 91, loss = 3.17776298\n",
      "Iteration 92, loss = 3.17802644\n",
      "Iteration 93, loss = 3.17715018\n",
      "Iteration 94, loss = 3.17596200\n",
      "Iteration 95, loss = 3.17589951\n",
      "Iteration 96, loss = 3.17490633\n",
      "Iteration 97, loss = 3.17433598\n",
      "Iteration 98, loss = 3.17438200\n",
      "Iteration 99, loss = 3.17310915\n",
      "Iteration 100, loss = 3.17188824\n",
      "Iteration 101, loss = 3.17169489\n",
      "Iteration 102, loss = 3.17128541\n",
      "Iteration 103, loss = 3.17014905\n",
      "Iteration 104, loss = 3.17124432\n",
      "Iteration 105, loss = 3.17003014\n",
      "Iteration 106, loss = 3.16959148\n",
      "Iteration 107, loss = 3.16918217\n",
      "Iteration 108, loss = 3.16837486\n",
      "Iteration 109, loss = 3.16701285\n",
      "Iteration 110, loss = 3.16726591\n",
      "Iteration 111, loss = 3.16720924\n",
      "Iteration 112, loss = 3.16617799\n",
      "Iteration 113, loss = 3.16480886\n",
      "Iteration 114, loss = 3.16471050\n",
      "Iteration 115, loss = 3.16386670\n",
      "Iteration 116, loss = 3.16341216\n",
      "Iteration 117, loss = 3.16288587\n",
      "Iteration 118, loss = 3.16170508\n",
      "Iteration 119, loss = 3.16121271\n",
      "Iteration 120, loss = 3.16187510\n",
      "Iteration 121, loss = 3.16083691\n",
      "Iteration 122, loss = 3.16020460\n",
      "Iteration 123, loss = 3.15915065\n",
      "Iteration 124, loss = 3.15818092\n",
      "Iteration 125, loss = 3.15752672\n",
      "Iteration 126, loss = 3.15788326\n",
      "Iteration 127, loss = 3.15744620\n",
      "Iteration 128, loss = 3.15722446\n",
      "Iteration 129, loss = 3.15732296\n",
      "Iteration 130, loss = 3.15634636\n",
      "Iteration 131, loss = 3.15661778\n",
      "Iteration 132, loss = 3.15610319\n",
      "Iteration 133, loss = 3.15454392\n",
      "Iteration 134, loss = 3.15302966\n",
      "Iteration 135, loss = 3.15286106\n",
      "Iteration 136, loss = 3.15241764\n",
      "Iteration 137, loss = 3.15236796\n",
      "Iteration 138, loss = 3.15156354\n",
      "Iteration 139, loss = 3.15055073\n",
      "Iteration 140, loss = 3.15066739\n",
      "Iteration 141, loss = 3.15165331\n",
      "Iteration 142, loss = 3.15094248\n",
      "Iteration 143, loss = 3.14957479\n",
      "Iteration 144, loss = 3.15075247\n",
      "Iteration 145, loss = 3.14945676\n",
      "Iteration 146, loss = 3.14770364\n",
      "Iteration 147, loss = 3.14719573\n",
      "Iteration 148, loss = 3.14651087\n",
      "Iteration 149, loss = 3.14633218\n",
      "Iteration 150, loss = 3.14586997\n",
      "Iteration 151, loss = 3.14557795\n",
      "Iteration 152, loss = 3.14640195\n",
      "Iteration 153, loss = 3.14517709\n",
      "Iteration 154, loss = 3.14386127\n",
      "Iteration 155, loss = 3.14378652\n",
      "Iteration 156, loss = 3.14406506\n",
      "Iteration 157, loss = 3.14230841\n",
      "Iteration 158, loss = 3.14250574\n",
      "Iteration 159, loss = 3.14185416\n",
      "Iteration 160, loss = 3.14142933\n",
      "Iteration 161, loss = 3.14095144\n",
      "Iteration 162, loss = 3.14037828\n",
      "Iteration 163, loss = 3.14149655\n",
      "Iteration 164, loss = 3.14045391\n",
      "Iteration 165, loss = 3.14030358\n",
      "Iteration 166, loss = 3.14144810\n",
      "Iteration 167, loss = 3.14055740\n",
      "Iteration 168, loss = 3.13814509\n",
      "Iteration 169, loss = 3.13786107\n",
      "Iteration 170, loss = 3.13822958\n",
      "Iteration 171, loss = 3.13840286\n",
      "Iteration 172, loss = 3.13814024\n",
      "Iteration 173, loss = 3.13700859\n",
      "Iteration 174, loss = 3.13676033\n",
      "Iteration 175, loss = 3.13643820\n",
      "Iteration 176, loss = 3.13508092\n",
      "Iteration 177, loss = 3.13525019\n",
      "Iteration 178, loss = 3.13499043\n",
      "Iteration 179, loss = 3.13593567\n",
      "Iteration 180, loss = 3.13319282\n",
      "Iteration 181, loss = 3.13352979\n",
      "Iteration 182, loss = 3.13277191\n",
      "Iteration 183, loss = 3.13230368\n",
      "Iteration 184, loss = 3.13227407\n",
      "Iteration 185, loss = 3.13300336\n",
      "Iteration 186, loss = 3.13146268\n",
      "Iteration 187, loss = 3.13133898\n",
      "Iteration 188, loss = 3.13054457\n",
      "Iteration 189, loss = 3.13023369\n",
      "Iteration 190, loss = 3.12990965\n",
      "Iteration 191, loss = 3.12986463\n",
      "Iteration 192, loss = 3.12882145\n",
      "Iteration 193, loss = 3.13067289\n",
      "Iteration 194, loss = 3.12947248\n",
      "Iteration 195, loss = 3.12779497\n",
      "Iteration 196, loss = 3.12674468\n",
      "Iteration 197, loss = 3.12632708\n",
      "Iteration 198, loss = 3.12605656\n",
      "Iteration 199, loss = 3.12543464\n",
      "Iteration 200, loss = 3.12583454\n",
      "Iteration 201, loss = 3.12562820\n",
      "Iteration 202, loss = 3.12589813\n",
      "Iteration 203, loss = 3.12490184\n",
      "Iteration 204, loss = 3.12365673\n",
      "Iteration 205, loss = 3.12468481\n",
      "Iteration 206, loss = 3.12465239\n",
      "Iteration 207, loss = 3.12379363\n",
      "Iteration 208, loss = 3.12269911\n",
      "Iteration 209, loss = 3.12188406\n",
      "Iteration 210, loss = 3.12334054\n",
      "Iteration 211, loss = 3.12289937\n",
      "Iteration 212, loss = 3.12185998\n",
      "Iteration 213, loss = 3.12154338\n",
      "Iteration 214, loss = 3.12201655\n",
      "Iteration 215, loss = 3.12152979\n",
      "Iteration 216, loss = 3.12042408\n",
      "Iteration 217, loss = 3.11939854\n",
      "Iteration 218, loss = 3.11944712\n",
      "Iteration 219, loss = 3.12022750\n",
      "Iteration 220, loss = 3.12046622\n",
      "Iteration 221, loss = 3.11884791\n",
      "Iteration 222, loss = 3.11825239\n",
      "Iteration 223, loss = 3.11817577\n",
      "Iteration 224, loss = 3.11967759\n",
      "Iteration 225, loss = 3.11803436\n",
      "Iteration 226, loss = 3.11658675\n",
      "Iteration 227, loss = 3.11847531\n",
      "Iteration 228, loss = 3.11681296\n",
      "Iteration 229, loss = 3.11537042\n",
      "Iteration 230, loss = 3.11584627\n",
      "Iteration 231, loss = 3.11538882\n",
      "Iteration 232, loss = 3.11522556\n",
      "Iteration 233, loss = 3.11465059\n",
      "Iteration 234, loss = 3.11427323\n",
      "Iteration 235, loss = 3.11484930\n",
      "Iteration 236, loss = 3.11391527\n",
      "Iteration 237, loss = 3.11306230\n",
      "Iteration 238, loss = 3.11171445\n",
      "Iteration 239, loss = 3.11248471\n",
      "Iteration 240, loss = 3.11167517\n",
      "Iteration 241, loss = 3.11203762\n",
      "Iteration 242, loss = 3.11159651\n",
      "Iteration 243, loss = 3.11112383\n",
      "Iteration 244, loss = 3.11141447\n",
      "Iteration 245, loss = 3.11084229\n",
      "Iteration 246, loss = 3.11089215\n",
      "Iteration 247, loss = 3.10995641\n",
      "Iteration 248, loss = 3.10895205\n",
      "Iteration 249, loss = 3.10955990\n",
      "Iteration 250, loss = 3.11048025\n",
      "Iteration 251, loss = 3.10924734\n",
      "Iteration 252, loss = 3.10828489\n",
      "Iteration 253, loss = 3.10852011\n",
      "Iteration 254, loss = 3.10783351\n",
      "Iteration 255, loss = 3.10763091\n",
      "Iteration 256, loss = 3.10700315\n",
      "Iteration 257, loss = 3.10650068\n",
      "Iteration 258, loss = 3.10693065\n",
      "Iteration 259, loss = 3.10622105\n",
      "Iteration 260, loss = 3.10627195\n",
      "Iteration 261, loss = 3.10703613\n",
      "Iteration 262, loss = 3.10651714\n",
      "Iteration 263, loss = 3.10550610\n",
      "Iteration 264, loss = 3.10534484\n",
      "Iteration 265, loss = 3.10485861\n",
      "Iteration 266, loss = 3.10474140\n",
      "Iteration 267, loss = 3.10436567\n",
      "Iteration 268, loss = 3.10400800\n",
      "Iteration 269, loss = 3.10348572\n",
      "Iteration 270, loss = 3.10348806\n",
      "Iteration 271, loss = 3.10427775\n",
      "Iteration 272, loss = 3.10456801\n",
      "Iteration 273, loss = 3.10343085\n",
      "Iteration 274, loss = 3.10258031\n",
      "Iteration 275, loss = 3.10262640\n",
      "Iteration 276, loss = 3.10195735\n",
      "Iteration 277, loss = 3.10159740\n",
      "Iteration 278, loss = 3.10129651\n",
      "Iteration 279, loss = 3.10083280\n",
      "Iteration 280, loss = 3.10076456\n",
      "Iteration 281, loss = 3.09990184\n",
      "Iteration 282, loss = 3.10028167\n",
      "Iteration 283, loss = 3.09914655\n",
      "Iteration 284, loss = 3.09891697\n",
      "Iteration 285, loss = 3.09977905\n",
      "Iteration 286, loss = 3.09907816\n",
      "Iteration 287, loss = 3.09851135\n",
      "Iteration 288, loss = 3.09838950\n",
      "Iteration 289, loss = 3.09812241\n",
      "Iteration 290, loss = 3.09791377\n",
      "Iteration 291, loss = 3.09752739\n",
      "Iteration 292, loss = 3.09723404\n",
      "Iteration 293, loss = 3.09654851\n",
      "Iteration 294, loss = 3.09626611\n",
      "Iteration 295, loss = 3.09606596\n",
      "Iteration 296, loss = 3.09513467\n",
      "Iteration 297, loss = 3.09466281\n",
      "Iteration 298, loss = 3.09505591\n",
      "Iteration 299, loss = 3.09431422\n",
      "Iteration 300, loss = 3.09418653\n",
      "Iteration 301, loss = 3.09415151\n",
      "Iteration 302, loss = 3.09448169\n",
      "Iteration 303, loss = 3.09442919\n",
      "Iteration 304, loss = 3.09318365\n",
      "Iteration 305, loss = 3.09291734\n",
      "Iteration 306, loss = 3.09284463\n",
      "Iteration 307, loss = 3.09271974\n",
      "Iteration 308, loss = 3.09288399\n",
      "Iteration 309, loss = 3.09181363\n",
      "Iteration 310, loss = 3.09200794\n",
      "Iteration 311, loss = 3.09226930\n",
      "Iteration 312, loss = 3.09241126\n",
      "Iteration 313, loss = 3.09150402\n",
      "Iteration 314, loss = 3.09069208\n",
      "Iteration 315, loss = 3.09014171\n",
      "Iteration 316, loss = 3.09011875\n",
      "Iteration 317, loss = 3.08975880\n",
      "Iteration 318, loss = 3.08989365\n",
      "Iteration 319, loss = 3.09022616\n",
      "Iteration 320, loss = 3.08989904\n",
      "Iteration 321, loss = 3.08864770\n",
      "Iteration 322, loss = 3.08838833\n",
      "Iteration 323, loss = 3.08916545\n",
      "Iteration 324, loss = 3.08875264\n",
      "Iteration 325, loss = 3.08780512\n",
      "Iteration 326, loss = 3.08743505\n",
      "Iteration 327, loss = 3.08684660\n",
      "Iteration 328, loss = 3.08719234\n",
      "Iteration 329, loss = 3.08779971\n",
      "Iteration 330, loss = 3.08710201\n",
      "Iteration 331, loss = 3.08608709\n",
      "Iteration 332, loss = 3.08547630\n",
      "Iteration 333, loss = 3.08734285\n",
      "Iteration 334, loss = 3.08709719\n",
      "Iteration 335, loss = 3.08465686\n",
      "Iteration 336, loss = 3.08451515\n",
      "Iteration 337, loss = 3.08405159\n",
      "Iteration 338, loss = 3.08380247\n",
      "Iteration 339, loss = 3.08390810\n",
      "Iteration 340, loss = 3.08367437\n",
      "Iteration 341, loss = 3.08285137\n",
      "Iteration 342, loss = 3.08284728\n",
      "Iteration 343, loss = 3.08251858\n",
      "Iteration 344, loss = 3.08210640\n",
      "Iteration 345, loss = 3.08247100\n",
      "Iteration 346, loss = 3.08254599\n",
      "Iteration 347, loss = 3.08122389\n",
      "Iteration 348, loss = 3.08180999\n",
      "Iteration 349, loss = 3.08339395\n",
      "Iteration 350, loss = 3.08329135\n",
      "Iteration 351, loss = 3.08178485\n",
      "Iteration 352, loss = 3.08310159\n",
      "Iteration 353, loss = 3.08215499\n",
      "Iteration 354, loss = 3.08067647\n",
      "Iteration 355, loss = 3.08076016\n",
      "Iteration 356, loss = 3.08023190\n",
      "Iteration 357, loss = 3.08027760\n",
      "Iteration 358, loss = 3.08017103\n",
      "Iteration 359, loss = 3.07952729\n",
      "Iteration 360, loss = 3.08060315\n",
      "Iteration 361, loss = 3.07941954\n",
      "Iteration 362, loss = 3.07890033\n",
      "Iteration 363, loss = 3.08030151\n",
      "Iteration 364, loss = 3.07937047\n",
      "Iteration 365, loss = 3.07848090\n",
      "Iteration 366, loss = 3.07873748\n",
      "Iteration 367, loss = 3.07767810\n",
      "Iteration 368, loss = 3.07720667\n",
      "Iteration 369, loss = 3.07729294\n",
      "Iteration 370, loss = 3.07760682\n",
      "Iteration 371, loss = 3.07680182\n",
      "Iteration 372, loss = 3.07775793\n",
      "Iteration 373, loss = 3.07811296\n",
      "Iteration 374, loss = 3.07590079\n",
      "Iteration 375, loss = 3.07694625\n",
      "Iteration 376, loss = 3.07735393\n",
      "Iteration 377, loss = 3.07591129\n",
      "Iteration 378, loss = 3.07465229\n",
      "Iteration 379, loss = 3.07496761\n",
      "Iteration 380, loss = 3.07476564\n",
      "Iteration 381, loss = 3.07582403\n",
      "Iteration 382, loss = 3.07637186\n",
      "Iteration 383, loss = 3.07460050\n",
      "Iteration 384, loss = 3.07477294\n",
      "Iteration 385, loss = 3.07368553\n",
      "Iteration 386, loss = 3.07374987\n",
      "Iteration 387, loss = 3.07449295\n",
      "Iteration 388, loss = 3.07439408\n",
      "Iteration 389, loss = 3.07488401\n",
      "Iteration 390, loss = 3.07490737\n",
      "Iteration 391, loss = 3.07337590\n",
      "Iteration 392, loss = 3.07253933\n",
      "Iteration 393, loss = 3.07223040\n",
      "Iteration 394, loss = 3.07212910\n",
      "Iteration 395, loss = 3.07136824\n",
      "Iteration 396, loss = 3.07108413\n",
      "Iteration 397, loss = 3.07143277\n",
      "Iteration 398, loss = 3.07123650\n",
      "Iteration 399, loss = 3.07075592\n",
      "Iteration 400, loss = 3.07030179\n",
      "Iteration 401, loss = 3.07036064\n",
      "Iteration 402, loss = 3.07025791\n",
      "Iteration 403, loss = 3.07007087\n",
      "Iteration 404, loss = 3.07005492\n",
      "Iteration 405, loss = 3.06970238\n",
      "Iteration 406, loss = 3.07073295\n",
      "Iteration 407, loss = 3.06956361\n",
      "Iteration 408, loss = 3.06879426\n",
      "Iteration 409, loss = 3.06792963\n",
      "Iteration 410, loss = 3.06904411\n",
      "Iteration 411, loss = 3.06770779\n",
      "Iteration 412, loss = 3.06756938\n",
      "Iteration 413, loss = 3.06671326\n",
      "Iteration 414, loss = 3.06767787\n",
      "Iteration 415, loss = 3.06807628\n",
      "Iteration 416, loss = 3.06799107\n",
      "Iteration 417, loss = 3.06771629\n",
      "Iteration 418, loss = 3.06671008\n",
      "Iteration 419, loss = 3.06652966\n",
      "Iteration 420, loss = 3.06692329\n",
      "Iteration 421, loss = 3.06643781\n",
      "Iteration 422, loss = 3.06678823\n",
      "Iteration 423, loss = 3.06714088\n",
      "Iteration 424, loss = 3.06662341\n",
      "Iteration 425, loss = 3.06517584\n",
      "Iteration 426, loss = 3.06535790\n",
      "Iteration 427, loss = 3.06443957\n",
      "Iteration 428, loss = 3.06484616\n",
      "Iteration 429, loss = 3.06484000\n",
      "Iteration 430, loss = 3.06580285\n",
      "Iteration 431, loss = 3.06496611\n",
      "Iteration 432, loss = 3.06407798\n",
      "Iteration 433, loss = 3.06418406\n",
      "Iteration 434, loss = 3.06401915\n",
      "Iteration 435, loss = 3.06421442\n",
      "Iteration 436, loss = 3.06414132\n",
      "Iteration 437, loss = 3.06362454\n",
      "Iteration 438, loss = 3.06366263\n",
      "Iteration 439, loss = 3.06305680\n",
      "Iteration 440, loss = 3.06244940\n",
      "Iteration 441, loss = 3.06252343\n",
      "Iteration 442, loss = 3.06291895\n",
      "Iteration 443, loss = 3.06183708\n",
      "Iteration 444, loss = 3.06178349\n",
      "Iteration 445, loss = 3.06192810\n",
      "Iteration 446, loss = 3.06107056\n",
      "Iteration 447, loss = 3.06177356\n",
      "Iteration 448, loss = 3.06102527\n",
      "Iteration 449, loss = 3.06087947\n",
      "Iteration 450, loss = 3.06019336\n",
      "Iteration 451, loss = 3.06030596\n",
      "Iteration 452, loss = 3.06094645\n",
      "Iteration 453, loss = 3.06183138\n",
      "Iteration 454, loss = 3.06061887\n",
      "Iteration 455, loss = 3.06128494\n",
      "Iteration 456, loss = 3.05965255\n",
      "Iteration 457, loss = 3.05921458\n",
      "Iteration 458, loss = 3.05957060\n",
      "Iteration 459, loss = 3.05985078\n",
      "Iteration 460, loss = 3.05971057\n",
      "Iteration 461, loss = 3.05899132\n",
      "Iteration 462, loss = 3.05891579\n",
      "Iteration 463, loss = 3.05770628\n",
      "Iteration 464, loss = 3.05810032\n",
      "Iteration 465, loss = 3.05836672\n",
      "Iteration 466, loss = 3.05797176\n",
      "Iteration 467, loss = 3.05806642\n",
      "Iteration 468, loss = 3.05773766\n",
      "Iteration 469, loss = 3.05752127\n",
      "Iteration 470, loss = 3.05775468\n",
      "Iteration 471, loss = 3.05715350\n",
      "Iteration 472, loss = 3.05709240\n",
      "Iteration 473, loss = 3.05682971\n",
      "Iteration 474, loss = 3.05678146\n",
      "Iteration 475, loss = 3.05567410\n",
      "Iteration 476, loss = 3.05615498\n",
      "Iteration 477, loss = 3.05639685\n",
      "Iteration 478, loss = 3.05466402\n",
      "Iteration 479, loss = 3.05594514\n",
      "Iteration 480, loss = 3.05624196\n",
      "Iteration 481, loss = 3.05577778\n",
      "Iteration 482, loss = 3.05603422\n",
      "Iteration 483, loss = 3.05632747\n",
      "Iteration 484, loss = 3.05484278\n",
      "Iteration 485, loss = 3.05537629\n",
      "Iteration 486, loss = 3.05391601\n",
      "Iteration 487, loss = 3.05506906\n",
      "Iteration 488, loss = 3.05439047\n",
      "Iteration 489, loss = 3.05379245\n",
      "Iteration 490, loss = 3.05336680\n",
      "Iteration 491, loss = 3.05291764\n",
      "Iteration 492, loss = 3.05309879\n",
      "Iteration 493, loss = 3.05313685\n",
      "Iteration 494, loss = 3.05348805\n",
      "Iteration 495, loss = 3.05230789\n",
      "Iteration 496, loss = 3.05353540\n",
      "Iteration 497, loss = 3.05318595\n",
      "Iteration 498, loss = 3.05238689\n",
      "Iteration 499, loss = 3.05230623\n",
      "Iteration 500, loss = 3.05159687\n",
      "Iteration 501, loss = 3.05181408\n",
      "Iteration 502, loss = 3.05210002\n",
      "Iteration 503, loss = 3.05337106\n",
      "Iteration 504, loss = 3.05211956\n",
      "Iteration 505, loss = 3.05066056\n",
      "Iteration 506, loss = 3.05055223\n",
      "Iteration 507, loss = 3.05033253\n",
      "Iteration 508, loss = 3.05061640\n",
      "Iteration 509, loss = 3.05078075\n",
      "Iteration 510, loss = 3.04984390\n",
      "Iteration 511, loss = 3.04956183\n",
      "Iteration 512, loss = 3.04954470\n",
      "Iteration 513, loss = 3.04930795\n",
      "Iteration 514, loss = 3.04982366\n",
      "Iteration 515, loss = 3.04920931\n",
      "Iteration 516, loss = 3.04930165\n",
      "Iteration 517, loss = 3.04823147\n",
      "Iteration 518, loss = 3.04773768\n",
      "Iteration 519, loss = 3.04878378\n",
      "Iteration 520, loss = 3.04853026\n",
      "Iteration 521, loss = 3.04798155\n",
      "Iteration 522, loss = 3.04830889\n",
      "Iteration 523, loss = 3.04815188\n",
      "Iteration 524, loss = 3.04940280\n",
      "Iteration 525, loss = 3.04751731\n",
      "Iteration 526, loss = 3.04912326\n",
      "Iteration 527, loss = 3.04781431\n",
      "Iteration 528, loss = 3.04699197\n",
      "Iteration 529, loss = 3.04813755\n",
      "Iteration 530, loss = 3.04812394\n",
      "Iteration 531, loss = 3.04646509\n",
      "Iteration 532, loss = 3.04535678\n",
      "Iteration 533, loss = 3.04638680\n",
      "Iteration 534, loss = 3.04651877\n",
      "Iteration 535, loss = 3.04586525\n",
      "Iteration 536, loss = 3.04595445\n",
      "Iteration 537, loss = 3.04586144\n",
      "Iteration 538, loss = 3.04489435\n",
      "Iteration 539, loss = 3.04506319\n",
      "Iteration 540, loss = 3.04560103\n",
      "Iteration 541, loss = 3.04519990\n",
      "Iteration 542, loss = 3.04605363\n",
      "Iteration 543, loss = 3.04518080\n",
      "Iteration 544, loss = 3.04400962\n",
      "Iteration 545, loss = 3.04479846\n",
      "Iteration 546, loss = 3.04534003\n",
      "Iteration 547, loss = 3.04333235\n",
      "Iteration 548, loss = 3.04367206\n",
      "Iteration 549, loss = 3.04333013\n",
      "Iteration 550, loss = 3.04312480\n",
      "Iteration 551, loss = 3.04315045\n",
      "Iteration 552, loss = 3.04282711\n",
      "Iteration 553, loss = 3.04194087\n",
      "Iteration 554, loss = 3.04229108\n",
      "Iteration 555, loss = 3.04242857\n",
      "Iteration 556, loss = 3.04375102\n",
      "Iteration 557, loss = 3.04335268\n",
      "Iteration 558, loss = 3.04225235\n",
      "Iteration 559, loss = 3.04234428\n",
      "Iteration 560, loss = 3.04173144\n",
      "Iteration 561, loss = 3.04092841\n",
      "Iteration 562, loss = 3.04160787\n",
      "Iteration 563, loss = 3.04146344\n",
      "Iteration 564, loss = 3.04212924\n",
      "Iteration 565, loss = 3.04051693\n",
      "Iteration 566, loss = 3.04049991\n",
      "Iteration 567, loss = 3.04049466\n",
      "Iteration 568, loss = 3.03986044\n",
      "Iteration 569, loss = 3.04014967\n",
      "Iteration 570, loss = 3.03975969\n",
      "Iteration 571, loss = 3.03943633\n",
      "Iteration 572, loss = 3.03957917\n",
      "Iteration 573, loss = 3.04006175\n",
      "Iteration 574, loss = 3.03944641\n",
      "Iteration 575, loss = 3.03947201\n",
      "Iteration 576, loss = 3.04027699\n",
      "Iteration 577, loss = 3.04004003\n",
      "Iteration 578, loss = 3.03871295\n",
      "Iteration 579, loss = 3.03910436\n",
      "Iteration 580, loss = 3.03886941\n",
      "Iteration 581, loss = 3.03865005\n",
      "Iteration 582, loss = 3.03875341\n",
      "Iteration 583, loss = 3.03811178\n",
      "Iteration 584, loss = 3.03724038\n",
      "Iteration 585, loss = 3.03858480\n",
      "Iteration 586, loss = 3.03759965\n",
      "Iteration 587, loss = 3.03775517\n",
      "Iteration 588, loss = 3.03712833\n",
      "Iteration 589, loss = 3.03696940\n",
      "Iteration 590, loss = 3.03639139\n",
      "Iteration 591, loss = 3.03600999\n",
      "Iteration 592, loss = 3.03644344\n",
      "Iteration 593, loss = 3.03642421\n",
      "Iteration 594, loss = 3.03682374\n",
      "Iteration 595, loss = 3.03934307\n",
      "Iteration 596, loss = 3.03816035\n",
      "Iteration 597, loss = 3.03632617\n",
      "Iteration 598, loss = 3.03570023\n",
      "Iteration 599, loss = 3.03456542\n",
      "Iteration 600, loss = 3.03539572\n",
      "Iteration 601, loss = 3.03533126\n",
      "Iteration 602, loss = 3.03381969\n",
      "Iteration 603, loss = 3.03457408\n",
      "Iteration 604, loss = 3.03521398\n",
      "Iteration 605, loss = 3.03456331\n",
      "Iteration 606, loss = 3.03466353\n",
      "Iteration 607, loss = 3.03462662\n",
      "Iteration 608, loss = 3.03388909\n",
      "Iteration 609, loss = 3.03450106\n",
      "Iteration 610, loss = 3.03392214\n",
      "Iteration 611, loss = 3.03394544\n",
      "Iteration 612, loss = 3.03365823\n",
      "Iteration 613, loss = 3.03295107\n",
      "Iteration 614, loss = 3.03313867\n",
      "Iteration 615, loss = 3.03276374\n",
      "Iteration 616, loss = 3.03335843\n",
      "Iteration 617, loss = 3.03265921\n",
      "Iteration 618, loss = 3.03253201\n",
      "Iteration 619, loss = 3.03486608\n",
      "Iteration 620, loss = 3.03298706\n",
      "Iteration 621, loss = 3.03099606\n",
      "Iteration 622, loss = 3.03245145\n",
      "Iteration 623, loss = 3.03240477\n",
      "Iteration 624, loss = 3.03256140\n",
      "Iteration 625, loss = 3.03318036\n",
      "Iteration 626, loss = 3.03310144\n",
      "Iteration 627, loss = 3.03174466\n",
      "Iteration 628, loss = 3.03137844\n",
      "Iteration 629, loss = 3.03144163\n",
      "Iteration 630, loss = 3.03133952\n",
      "Iteration 631, loss = 3.03023176\n",
      "Iteration 632, loss = 3.03158158\n",
      "Iteration 633, loss = 3.03061506\n",
      "Iteration 634, loss = 3.03073119\n",
      "Iteration 635, loss = 3.03052786\n",
      "Iteration 636, loss = 3.02970654\n",
      "Iteration 637, loss = 3.03239292\n",
      "Iteration 638, loss = 3.03198971\n",
      "Iteration 639, loss = 3.03044543\n",
      "Iteration 640, loss = 3.03066740\n",
      "Iteration 641, loss = 3.02905840\n",
      "Iteration 642, loss = 3.02957391\n",
      "Iteration 643, loss = 3.02960643\n",
      "Iteration 644, loss = 3.02975539\n",
      "Iteration 645, loss = 3.02916189\n",
      "Iteration 646, loss = 3.02919592\n",
      "Iteration 647, loss = 3.02917143\n",
      "Iteration 648, loss = 3.02836306\n",
      "Iteration 649, loss = 3.02798667\n",
      "Iteration 650, loss = 3.02803972\n",
      "Iteration 651, loss = 3.03056102\n",
      "Iteration 652, loss = 3.02906152\n",
      "Iteration 653, loss = 3.02793421\n",
      "Iteration 654, loss = 3.02781947\n",
      "Iteration 655, loss = 3.02762439\n",
      "Iteration 656, loss = 3.02813364\n",
      "Iteration 657, loss = 3.02748090\n",
      "Iteration 658, loss = 3.02706090\n",
      "Iteration 659, loss = 3.02657554\n",
      "Iteration 660, loss = 3.02640514\n",
      "Iteration 661, loss = 3.02706677\n",
      "Iteration 662, loss = 3.02611464\n",
      "Iteration 663, loss = 3.02716119\n",
      "Iteration 664, loss = 3.02629659\n",
      "Iteration 665, loss = 3.02766021\n",
      "Iteration 666, loss = 3.02733780\n",
      "Iteration 667, loss = 3.02654210\n",
      "Iteration 668, loss = 3.02682064\n",
      "Iteration 669, loss = 3.02582732\n",
      "Iteration 670, loss = 3.02524812\n",
      "Iteration 671, loss = 3.02720913\n",
      "Iteration 672, loss = 3.02445081\n",
      "Iteration 673, loss = 3.02521477\n",
      "Iteration 674, loss = 3.02474857\n",
      "Iteration 675, loss = 3.02482015\n",
      "Iteration 676, loss = 3.02433024\n",
      "Iteration 677, loss = 3.02437301\n",
      "Iteration 678, loss = 3.02381259\n",
      "Iteration 679, loss = 3.02518269\n",
      "Iteration 680, loss = 3.02380139\n",
      "Iteration 681, loss = 3.02477497\n",
      "Iteration 682, loss = 3.02423018\n",
      "Iteration 683, loss = 3.02332898\n",
      "Iteration 684, loss = 3.02408824\n",
      "Iteration 685, loss = 3.02454791\n",
      "Iteration 686, loss = 3.02276755\n",
      "Iteration 687, loss = 3.02337951\n",
      "Iteration 688, loss = 3.02283709\n",
      "Iteration 689, loss = 3.02290070\n",
      "Iteration 690, loss = 3.02207888\n",
      "Iteration 691, loss = 3.02243094\n",
      "Iteration 692, loss = 3.02196871\n",
      "Iteration 693, loss = 3.02192621\n",
      "Iteration 694, loss = 3.02232035\n",
      "Iteration 695, loss = 3.02189578\n",
      "Iteration 696, loss = 3.02116662\n",
      "Iteration 697, loss = 3.02096123\n",
      "Iteration 698, loss = 3.02112297\n",
      "Iteration 699, loss = 3.02182165\n",
      "Iteration 700, loss = 3.02209297\n",
      "Iteration 701, loss = 3.02205314\n",
      "Iteration 702, loss = 3.02120261\n",
      "Iteration 703, loss = 3.02127378\n",
      "Iteration 704, loss = 3.02104758\n",
      "Iteration 705, loss = 3.02051742\n",
      "Iteration 706, loss = 3.02242712\n",
      "Iteration 707, loss = 3.02128574\n",
      "Iteration 708, loss = 3.02108050\n",
      "Iteration 709, loss = 3.02165444\n",
      "Iteration 710, loss = 3.02098897\n",
      "Iteration 711, loss = 3.01977866\n",
      "Iteration 712, loss = 3.01992928\n",
      "Iteration 713, loss = 3.01992846\n",
      "Iteration 714, loss = 3.01894473\n",
      "Iteration 715, loss = 3.01841823\n",
      "Iteration 716, loss = 3.01839946\n",
      "Iteration 717, loss = 3.01868222\n",
      "Iteration 718, loss = 3.01928470\n",
      "Iteration 719, loss = 3.01893392\n",
      "Iteration 720, loss = 3.01911216\n",
      "Iteration 721, loss = 3.01799560\n",
      "Iteration 722, loss = 3.01848194\n",
      "Iteration 723, loss = 3.01684870\n",
      "Iteration 724, loss = 3.01912259\n",
      "Iteration 725, loss = 3.01808487\n",
      "Iteration 726, loss = 3.01894322\n",
      "Iteration 727, loss = 3.01770963\n",
      "Iteration 728, loss = 3.01716767\n",
      "Iteration 729, loss = 3.01747110\n",
      "Iteration 730, loss = 3.01758987\n",
      "Iteration 731, loss = 3.01818759\n",
      "Iteration 732, loss = 3.01733865\n",
      "Iteration 733, loss = 3.01739171\n",
      "Iteration 734, loss = 3.01728937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23968740\n",
      "Iteration 2, loss = 4.14141086\n",
      "Iteration 3, loss = 4.04912901\n",
      "Iteration 4, loss = 3.95332405\n",
      "Iteration 5, loss = 3.85475391\n",
      "Iteration 6, loss = 3.74943358\n",
      "Iteration 7, loss = 3.63943562\n",
      "Iteration 8, loss = 3.53431119\n",
      "Iteration 9, loss = 3.43485443\n",
      "Iteration 10, loss = 3.35485686\n",
      "Iteration 11, loss = 3.29070238\n",
      "Iteration 12, loss = 3.25613806\n",
      "Iteration 13, loss = 3.24012298\n",
      "Iteration 14, loss = 3.23647091\n",
      "Iteration 15, loss = 3.23456782\n",
      "Iteration 16, loss = 3.23271825\n",
      "Iteration 17, loss = 3.22994521\n",
      "Iteration 18, loss = 3.22923065\n",
      "Iteration 19, loss = 3.22833733\n",
      "Iteration 20, loss = 3.22667630\n",
      "Iteration 21, loss = 3.22578885\n",
      "Iteration 22, loss = 3.22516300\n",
      "Iteration 23, loss = 3.22440815\n",
      "Iteration 24, loss = 3.22303255\n",
      "Iteration 25, loss = 3.22215734\n",
      "Iteration 26, loss = 3.22125851\n",
      "Iteration 27, loss = 3.22093751\n",
      "Iteration 28, loss = 3.22020404\n",
      "Iteration 29, loss = 3.21865310\n",
      "Iteration 30, loss = 3.21759789\n",
      "Iteration 31, loss = 3.21713980\n",
      "Iteration 32, loss = 3.21680602\n",
      "Iteration 33, loss = 3.21584059\n",
      "Iteration 34, loss = 3.21521343\n",
      "Iteration 35, loss = 3.21445229\n",
      "Iteration 36, loss = 3.21317722\n",
      "Iteration 37, loss = 3.21275274\n",
      "Iteration 38, loss = 3.21158776\n",
      "Iteration 39, loss = 3.21088517\n",
      "Iteration 40, loss = 3.21035420\n",
      "Iteration 41, loss = 3.21007730\n",
      "Iteration 42, loss = 3.20998831\n",
      "Iteration 43, loss = 3.20885600\n",
      "Iteration 44, loss = 3.20718479\n",
      "Iteration 45, loss = 3.20602290\n",
      "Iteration 46, loss = 3.20488310\n",
      "Iteration 47, loss = 3.20365665\n",
      "Iteration 48, loss = 3.20468926\n",
      "Iteration 49, loss = 3.20375554\n",
      "Iteration 50, loss = 3.20222866\n",
      "Iteration 51, loss = 3.20207828\n",
      "Iteration 52, loss = 3.20109661\n",
      "Iteration 53, loss = 3.20029474\n",
      "Iteration 54, loss = 3.19880506\n",
      "Iteration 55, loss = 3.19829294\n",
      "Iteration 56, loss = 3.19689390\n",
      "Iteration 57, loss = 3.19689147\n",
      "Iteration 58, loss = 3.19584785\n",
      "Iteration 59, loss = 3.19530338\n",
      "Iteration 60, loss = 3.19520914\n",
      "Iteration 61, loss = 3.19482423\n",
      "Iteration 62, loss = 3.19447656\n",
      "Iteration 63, loss = 3.19352997\n",
      "Iteration 64, loss = 3.19253153\n",
      "Iteration 65, loss = 3.19100900\n",
      "Iteration 66, loss = 3.19081630\n",
      "Iteration 67, loss = 3.19067410\n",
      "Iteration 68, loss = 3.18956967\n",
      "Iteration 69, loss = 3.18898183\n",
      "Iteration 70, loss = 3.18913682\n",
      "Iteration 71, loss = 3.18771826\n",
      "Iteration 72, loss = 3.18640378\n",
      "Iteration 73, loss = 3.18623610\n",
      "Iteration 74, loss = 3.18560277\n",
      "Iteration 75, loss = 3.18516952\n",
      "Iteration 76, loss = 3.18544440\n",
      "Iteration 77, loss = 3.18453390\n",
      "Iteration 78, loss = 3.18315494\n",
      "Iteration 79, loss = 3.18338560\n",
      "Iteration 80, loss = 3.18288882\n",
      "Iteration 81, loss = 3.18154169\n",
      "Iteration 82, loss = 3.18102221\n",
      "Iteration 83, loss = 3.18080766\n",
      "Iteration 84, loss = 3.18013939\n",
      "Iteration 85, loss = 3.17941314\n",
      "Iteration 86, loss = 3.17859723\n",
      "Iteration 87, loss = 3.17891037\n",
      "Iteration 88, loss = 3.17900511\n",
      "Iteration 89, loss = 3.17742271\n",
      "Iteration 90, loss = 3.17665122\n",
      "Iteration 91, loss = 3.17599721\n",
      "Iteration 92, loss = 3.17467912\n",
      "Iteration 93, loss = 3.17415271\n",
      "Iteration 94, loss = 3.17335588\n",
      "Iteration 95, loss = 3.17338140\n",
      "Iteration 96, loss = 3.17255658\n",
      "Iteration 97, loss = 3.17177763\n",
      "Iteration 98, loss = 3.17129283\n",
      "Iteration 99, loss = 3.17129966\n",
      "Iteration 100, loss = 3.17023732\n",
      "Iteration 101, loss = 3.16986435\n",
      "Iteration 102, loss = 3.16931593\n",
      "Iteration 103, loss = 3.16851306\n",
      "Iteration 104, loss = 3.16837491\n",
      "Iteration 105, loss = 3.16897576\n",
      "Iteration 106, loss = 3.16819216\n",
      "Iteration 107, loss = 3.16693615\n",
      "Iteration 108, loss = 3.16583715\n",
      "Iteration 109, loss = 3.16533434\n",
      "Iteration 110, loss = 3.16460643\n",
      "Iteration 111, loss = 3.16495983\n",
      "Iteration 112, loss = 3.16320190\n",
      "Iteration 113, loss = 3.16323055\n",
      "Iteration 114, loss = 3.16391899\n",
      "Iteration 115, loss = 3.16437319\n",
      "Iteration 116, loss = 3.16263795\n",
      "Iteration 117, loss = 3.16230462\n",
      "Iteration 118, loss = 3.16193401\n",
      "Iteration 119, loss = 3.16132021\n",
      "Iteration 120, loss = 3.16261126\n",
      "Iteration 121, loss = 3.16108276\n",
      "Iteration 122, loss = 3.15971052\n",
      "Iteration 123, loss = 3.15814146\n",
      "Iteration 124, loss = 3.15801613\n",
      "Iteration 125, loss = 3.15696135\n",
      "Iteration 126, loss = 3.15779281\n",
      "Iteration 127, loss = 3.15776427\n",
      "Iteration 128, loss = 3.15699378\n",
      "Iteration 129, loss = 3.15609935\n",
      "Iteration 130, loss = 3.15616000\n",
      "Iteration 131, loss = 3.15480506\n",
      "Iteration 132, loss = 3.15456480\n",
      "Iteration 133, loss = 3.15357632\n",
      "Iteration 134, loss = 3.15263285\n",
      "Iteration 135, loss = 3.15383296\n",
      "Iteration 136, loss = 3.15269514\n",
      "Iteration 137, loss = 3.15199769\n",
      "Iteration 138, loss = 3.15133666\n",
      "Iteration 139, loss = 3.15158612\n",
      "Iteration 140, loss = 3.15104159\n",
      "Iteration 141, loss = 3.15066399\n",
      "Iteration 142, loss = 3.14937623\n",
      "Iteration 143, loss = 3.14924257\n",
      "Iteration 144, loss = 3.14881790\n",
      "Iteration 145, loss = 3.14810778\n",
      "Iteration 146, loss = 3.14809803\n",
      "Iteration 147, loss = 3.14688831\n",
      "Iteration 148, loss = 3.14609723\n",
      "Iteration 149, loss = 3.14554641\n",
      "Iteration 150, loss = 3.14632127\n",
      "Iteration 151, loss = 3.14521618\n",
      "Iteration 152, loss = 3.14568567\n",
      "Iteration 153, loss = 3.14419423\n",
      "Iteration 154, loss = 3.14502438\n",
      "Iteration 155, loss = 3.14447322\n",
      "Iteration 156, loss = 3.14279247\n",
      "Iteration 157, loss = 3.14260168\n",
      "Iteration 158, loss = 3.14214851\n",
      "Iteration 159, loss = 3.14173445\n",
      "Iteration 160, loss = 3.14133774\n",
      "Iteration 161, loss = 3.14084096\n",
      "Iteration 162, loss = 3.14040355\n",
      "Iteration 163, loss = 3.14081581\n",
      "Iteration 164, loss = 3.14138622\n",
      "Iteration 165, loss = 3.13952793\n",
      "Iteration 166, loss = 3.13862311\n",
      "Iteration 167, loss = 3.13922444\n",
      "Iteration 168, loss = 3.13863976\n",
      "Iteration 169, loss = 3.13769992\n",
      "Iteration 170, loss = 3.13837894\n",
      "Iteration 171, loss = 3.13709567\n",
      "Iteration 172, loss = 3.13947880\n",
      "Iteration 173, loss = 3.13689343\n",
      "Iteration 174, loss = 3.13607837\n",
      "Iteration 175, loss = 3.13769417\n",
      "Iteration 176, loss = 3.13707049\n",
      "Iteration 177, loss = 3.13477870\n",
      "Iteration 178, loss = 3.13398350\n",
      "Iteration 179, loss = 3.13474510\n",
      "Iteration 180, loss = 3.13531837\n",
      "Iteration 181, loss = 3.13366341\n",
      "Iteration 182, loss = 3.13392084\n",
      "Iteration 183, loss = 3.13351912\n",
      "Iteration 184, loss = 3.13294763\n",
      "Iteration 185, loss = 3.13325428\n",
      "Iteration 186, loss = 3.13261799\n",
      "Iteration 187, loss = 3.13184963\n",
      "Iteration 188, loss = 3.13129824\n",
      "Iteration 189, loss = 3.13093212\n",
      "Iteration 190, loss = 3.13059335\n",
      "Iteration 191, loss = 3.13171399\n",
      "Iteration 192, loss = 3.12942254\n",
      "Iteration 193, loss = 3.12867330\n",
      "Iteration 194, loss = 3.12786080\n",
      "Iteration 195, loss = 3.12803185\n",
      "Iteration 196, loss = 3.12733818\n",
      "Iteration 197, loss = 3.12688589\n",
      "Iteration 198, loss = 3.12669516\n",
      "Iteration 199, loss = 3.12613349\n",
      "Iteration 200, loss = 3.12548105\n",
      "Iteration 201, loss = 3.12592878\n",
      "Iteration 202, loss = 3.12553282\n",
      "Iteration 203, loss = 3.12575315\n",
      "Iteration 204, loss = 3.12576220\n",
      "Iteration 205, loss = 3.12505694\n",
      "Iteration 206, loss = 3.12436376\n",
      "Iteration 207, loss = 3.12454063\n",
      "Iteration 208, loss = 3.12336553\n",
      "Iteration 209, loss = 3.12319157\n",
      "Iteration 210, loss = 3.12354882\n",
      "Iteration 211, loss = 3.12297406\n",
      "Iteration 212, loss = 3.12148334\n",
      "Iteration 213, loss = 3.12170321\n",
      "Iteration 214, loss = 3.12200084\n",
      "Iteration 215, loss = 3.12192661\n",
      "Iteration 216, loss = 3.12029690\n",
      "Iteration 217, loss = 3.12147263\n",
      "Iteration 218, loss = 3.12121939\n",
      "Iteration 219, loss = 3.12074137\n",
      "Iteration 220, loss = 3.11931382\n",
      "Iteration 221, loss = 3.11876313\n",
      "Iteration 222, loss = 3.11894244\n",
      "Iteration 223, loss = 3.11823770\n",
      "Iteration 224, loss = 3.11834631\n",
      "Iteration 225, loss = 3.11810657\n",
      "Iteration 226, loss = 3.11726028\n",
      "Iteration 227, loss = 3.11728180\n",
      "Iteration 228, loss = 3.11668101\n",
      "Iteration 229, loss = 3.11679809\n",
      "Iteration 230, loss = 3.11543952\n",
      "Iteration 231, loss = 3.11523093\n",
      "Iteration 232, loss = 3.11543328\n",
      "Iteration 233, loss = 3.11468495\n",
      "Iteration 234, loss = 3.11475768\n",
      "Iteration 235, loss = 3.11483797\n",
      "Iteration 236, loss = 3.11408445\n",
      "Iteration 237, loss = 3.11396715\n",
      "Iteration 238, loss = 3.11409580\n",
      "Iteration 239, loss = 3.11365841\n",
      "Iteration 240, loss = 3.11245739\n",
      "Iteration 241, loss = 3.11325709\n",
      "Iteration 242, loss = 3.11303928\n",
      "Iteration 243, loss = 3.11368486\n",
      "Iteration 244, loss = 3.11177285\n",
      "Iteration 245, loss = 3.11129970\n",
      "Iteration 246, loss = 3.11116472\n",
      "Iteration 247, loss = 3.11013501\n",
      "Iteration 248, loss = 3.10946961\n",
      "Iteration 249, loss = 3.10946086\n",
      "Iteration 250, loss = 3.10972878\n",
      "Iteration 251, loss = 3.11052119\n",
      "Iteration 252, loss = 3.11010525\n",
      "Iteration 253, loss = 3.10891458\n",
      "Iteration 254, loss = 3.10899845\n",
      "Iteration 255, loss = 3.10844977\n",
      "Iteration 256, loss = 3.10796772\n",
      "Iteration 257, loss = 3.10726693\n",
      "Iteration 258, loss = 3.10697019\n",
      "Iteration 259, loss = 3.10710779\n",
      "Iteration 260, loss = 3.10767549\n",
      "Iteration 261, loss = 3.10676019\n",
      "Iteration 262, loss = 3.10541801\n",
      "Iteration 263, loss = 3.10511701\n",
      "Iteration 264, loss = 3.10544274\n",
      "Iteration 265, loss = 3.10501698\n",
      "Iteration 266, loss = 3.10466506\n",
      "Iteration 267, loss = 3.10421835\n",
      "Iteration 268, loss = 3.10530955\n",
      "Iteration 269, loss = 3.10461737\n",
      "Iteration 270, loss = 3.10364577\n",
      "Iteration 271, loss = 3.10341848\n",
      "Iteration 272, loss = 3.10332352\n",
      "Iteration 273, loss = 3.10268842\n",
      "Iteration 274, loss = 3.10206205\n",
      "Iteration 275, loss = 3.10182733\n",
      "Iteration 276, loss = 3.10171625\n",
      "Iteration 277, loss = 3.10139221\n",
      "Iteration 278, loss = 3.10067077\n",
      "Iteration 279, loss = 3.10056329\n",
      "Iteration 280, loss = 3.10123108\n",
      "Iteration 281, loss = 3.10044240\n",
      "Iteration 282, loss = 3.09944849\n",
      "Iteration 283, loss = 3.09907612\n",
      "Iteration 284, loss = 3.09916561\n",
      "Iteration 285, loss = 3.09878557\n",
      "Iteration 286, loss = 3.09893337\n",
      "Iteration 287, loss = 3.09935472\n",
      "Iteration 288, loss = 3.09858099\n",
      "Iteration 289, loss = 3.09814648\n",
      "Iteration 290, loss = 3.09730066\n",
      "Iteration 291, loss = 3.09709914\n",
      "Iteration 292, loss = 3.09696819\n",
      "Iteration 293, loss = 3.09613669\n",
      "Iteration 294, loss = 3.09654771\n",
      "Iteration 295, loss = 3.09637557\n",
      "Iteration 296, loss = 3.09598865\n",
      "Iteration 297, loss = 3.09511929\n",
      "Iteration 298, loss = 3.09489705\n",
      "Iteration 299, loss = 3.09361720\n",
      "Iteration 300, loss = 3.09420382\n",
      "Iteration 301, loss = 3.09437938\n",
      "Iteration 302, loss = 3.09426582\n",
      "Iteration 303, loss = 3.09406396\n",
      "Iteration 304, loss = 3.09388451\n",
      "Iteration 305, loss = 3.09304236\n",
      "Iteration 306, loss = 3.09317889\n",
      "Iteration 307, loss = 3.09299583\n",
      "Iteration 308, loss = 3.09306103\n",
      "Iteration 309, loss = 3.09214009\n",
      "Iteration 310, loss = 3.09142122\n",
      "Iteration 311, loss = 3.09091423\n",
      "Iteration 312, loss = 3.09100842\n",
      "Iteration 313, loss = 3.09190273\n",
      "Iteration 314, loss = 3.09020617\n",
      "Iteration 315, loss = 3.08991482\n",
      "Iteration 316, loss = 3.09056448\n",
      "Iteration 317, loss = 3.09042479\n",
      "Iteration 318, loss = 3.08909751\n",
      "Iteration 319, loss = 3.08860131\n",
      "Iteration 320, loss = 3.08834414\n",
      "Iteration 321, loss = 3.08728928\n",
      "Iteration 322, loss = 3.08719738\n",
      "Iteration 323, loss = 3.08728636\n",
      "Iteration 324, loss = 3.08720376\n",
      "Iteration 325, loss = 3.08756267\n",
      "Iteration 326, loss = 3.08644128\n",
      "Iteration 327, loss = 3.08498744\n",
      "Iteration 328, loss = 3.08580603\n",
      "Iteration 329, loss = 3.08549268\n",
      "Iteration 330, loss = 3.08657567\n",
      "Iteration 331, loss = 3.08627852\n",
      "Iteration 332, loss = 3.08490009\n",
      "Iteration 333, loss = 3.08608008\n",
      "Iteration 334, loss = 3.08520999\n",
      "Iteration 335, loss = 3.08336366\n",
      "Iteration 336, loss = 3.08391457\n",
      "Iteration 337, loss = 3.08331643\n",
      "Iteration 338, loss = 3.08328470\n",
      "Iteration 339, loss = 3.08291832\n",
      "Iteration 340, loss = 3.08286679\n",
      "Iteration 341, loss = 3.08317820\n",
      "Iteration 342, loss = 3.08229626\n",
      "Iteration 343, loss = 3.08242147\n",
      "Iteration 344, loss = 3.08219921\n",
      "Iteration 345, loss = 3.08189913\n",
      "Iteration 346, loss = 3.08130199\n",
      "Iteration 347, loss = 3.08129079\n",
      "Iteration 348, loss = 3.08019051\n",
      "Iteration 349, loss = 3.07954840\n",
      "Iteration 350, loss = 3.07925826\n",
      "Iteration 351, loss = 3.07885675\n",
      "Iteration 352, loss = 3.07967895\n",
      "Iteration 353, loss = 3.07917791\n",
      "Iteration 354, loss = 3.07825531\n",
      "Iteration 355, loss = 3.07969799\n",
      "Iteration 356, loss = 3.07939708\n",
      "Iteration 357, loss = 3.07796474\n",
      "Iteration 358, loss = 3.07868070\n",
      "Iteration 359, loss = 3.07697563\n",
      "Iteration 360, loss = 3.07728254\n",
      "Iteration 361, loss = 3.07765870\n",
      "Iteration 362, loss = 3.07652876\n",
      "Iteration 363, loss = 3.07633090\n",
      "Iteration 364, loss = 3.07744530\n",
      "Iteration 365, loss = 3.07833766\n",
      "Iteration 366, loss = 3.07713836\n",
      "Iteration 367, loss = 3.07556869\n",
      "Iteration 368, loss = 3.07504802\n",
      "Iteration 369, loss = 3.07512524\n",
      "Iteration 370, loss = 3.07511946\n",
      "Iteration 371, loss = 3.07414738\n",
      "Iteration 372, loss = 3.07457430\n",
      "Iteration 373, loss = 3.07402871\n",
      "Iteration 374, loss = 3.07400809\n",
      "Iteration 375, loss = 3.07389620\n",
      "Iteration 376, loss = 3.07332914\n",
      "Iteration 377, loss = 3.07299966\n",
      "Iteration 378, loss = 3.07199972\n",
      "Iteration 379, loss = 3.07121764\n",
      "Iteration 380, loss = 3.07181437\n",
      "Iteration 381, loss = 3.07241039\n",
      "Iteration 382, loss = 3.07206410\n",
      "Iteration 383, loss = 3.07272081\n",
      "Iteration 384, loss = 3.07318245\n",
      "Iteration 385, loss = 3.07152845\n",
      "Iteration 386, loss = 3.07164704\n",
      "Iteration 387, loss = 3.07077285\n",
      "Iteration 388, loss = 3.07188988\n",
      "Iteration 389, loss = 3.07089376\n",
      "Iteration 390, loss = 3.07153208\n",
      "Iteration 391, loss = 3.06953199\n",
      "Iteration 392, loss = 3.06913147\n",
      "Iteration 393, loss = 3.07162443\n",
      "Iteration 394, loss = 3.07101320\n",
      "Iteration 395, loss = 3.06924259\n",
      "Iteration 396, loss = 3.06897824\n",
      "Iteration 397, loss = 3.06741081\n",
      "Iteration 398, loss = 3.06915517\n",
      "Iteration 399, loss = 3.06832389\n",
      "Iteration 400, loss = 3.06850201\n",
      "Iteration 401, loss = 3.06622119\n",
      "Iteration 402, loss = 3.06916506\n",
      "Iteration 403, loss = 3.06856881\n",
      "Iteration 404, loss = 3.06767673\n",
      "Iteration 405, loss = 3.06860606\n",
      "Iteration 406, loss = 3.06695343\n",
      "Iteration 407, loss = 3.06584672\n",
      "Iteration 408, loss = 3.06655033\n",
      "Iteration 409, loss = 3.06546138\n",
      "Iteration 410, loss = 3.06582520\n",
      "Iteration 411, loss = 3.06561280\n",
      "Iteration 412, loss = 3.06485349\n",
      "Iteration 413, loss = 3.06478736\n",
      "Iteration 414, loss = 3.06409530\n",
      "Iteration 415, loss = 3.06418912\n",
      "Iteration 416, loss = 3.06330589\n",
      "Iteration 417, loss = 3.06364118\n",
      "Iteration 418, loss = 3.06399726\n",
      "Iteration 419, loss = 3.06358029\n",
      "Iteration 420, loss = 3.06263898\n",
      "Iteration 421, loss = 3.06241588\n",
      "Iteration 422, loss = 3.06197682\n",
      "Iteration 423, loss = 3.06192030\n",
      "Iteration 424, loss = 3.06235966\n",
      "Iteration 425, loss = 3.06162714\n",
      "Iteration 426, loss = 3.06165282\n",
      "Iteration 427, loss = 3.06072868\n",
      "Iteration 428, loss = 3.06196689\n",
      "Iteration 429, loss = 3.06193940\n",
      "Iteration 430, loss = 3.06125458\n",
      "Iteration 431, loss = 3.06210912\n",
      "Iteration 432, loss = 3.06165263\n",
      "Iteration 433, loss = 3.06087516\n",
      "Iteration 434, loss = 3.05975957\n",
      "Iteration 435, loss = 3.05864412\n",
      "Iteration 436, loss = 3.06003388\n",
      "Iteration 437, loss = 3.05951299\n",
      "Iteration 438, loss = 3.05947847\n",
      "Iteration 439, loss = 3.05851353\n",
      "Iteration 440, loss = 3.05801816\n",
      "Iteration 441, loss = 3.05856045\n",
      "Iteration 442, loss = 3.05882741\n",
      "Iteration 443, loss = 3.05759285\n",
      "Iteration 444, loss = 3.05674276\n",
      "Iteration 445, loss = 3.05940807\n",
      "Iteration 446, loss = 3.05742492\n",
      "Iteration 447, loss = 3.05666089\n",
      "Iteration 448, loss = 3.05648939\n",
      "Iteration 449, loss = 3.05654722\n",
      "Iteration 450, loss = 3.05631941\n",
      "Iteration 451, loss = 3.05642749\n",
      "Iteration 452, loss = 3.05610116\n",
      "Iteration 453, loss = 3.05533253\n",
      "Iteration 454, loss = 3.05464880\n",
      "Iteration 455, loss = 3.05451763\n",
      "Iteration 456, loss = 3.05439773\n",
      "Iteration 457, loss = 3.05477947\n",
      "Iteration 458, loss = 3.05517620\n",
      "Iteration 459, loss = 3.05427753\n",
      "Iteration 460, loss = 3.05407373\n",
      "Iteration 461, loss = 3.05395871\n",
      "Iteration 462, loss = 3.05309933\n",
      "Iteration 463, loss = 3.05275084\n",
      "Iteration 464, loss = 3.05317038\n",
      "Iteration 465, loss = 3.05290198\n",
      "Iteration 466, loss = 3.05237218\n",
      "Iteration 467, loss = 3.05225753\n",
      "Iteration 468, loss = 3.05216581\n",
      "Iteration 469, loss = 3.05270002\n",
      "Iteration 470, loss = 3.05287295\n",
      "Iteration 471, loss = 3.05252538\n",
      "Iteration 472, loss = 3.05161471\n",
      "Iteration 473, loss = 3.05099562\n",
      "Iteration 474, loss = 3.05091275\n",
      "Iteration 475, loss = 3.05125401\n",
      "Iteration 476, loss = 3.05128914\n",
      "Iteration 477, loss = 3.05033754\n",
      "Iteration 478, loss = 3.05004244\n",
      "Iteration 479, loss = 3.04954755\n",
      "Iteration 480, loss = 3.05030283\n",
      "Iteration 481, loss = 3.04889298\n",
      "Iteration 482, loss = 3.05134441\n",
      "Iteration 483, loss = 3.05007388\n",
      "Iteration 484, loss = 3.05173860\n",
      "Iteration 485, loss = 3.04977375\n",
      "Iteration 486, loss = 3.04860939\n",
      "Iteration 487, loss = 3.04856038\n",
      "Iteration 488, loss = 3.04720034\n",
      "Iteration 489, loss = 3.04736936\n",
      "Iteration 490, loss = 3.04771382\n",
      "Iteration 491, loss = 3.04817387\n",
      "Iteration 492, loss = 3.04817919\n",
      "Iteration 493, loss = 3.04753650\n",
      "Iteration 494, loss = 3.04632952\n",
      "Iteration 495, loss = 3.04621690\n",
      "Iteration 496, loss = 3.04654723\n",
      "Iteration 497, loss = 3.04634061\n",
      "Iteration 498, loss = 3.04664666\n",
      "Iteration 499, loss = 3.04769926\n",
      "Iteration 500, loss = 3.04643619\n",
      "Iteration 501, loss = 3.04665050\n",
      "Iteration 502, loss = 3.04571898\n",
      "Iteration 503, loss = 3.04551818\n",
      "Iteration 504, loss = 3.04552300\n",
      "Iteration 505, loss = 3.04502785\n",
      "Iteration 506, loss = 3.04375617\n",
      "Iteration 507, loss = 3.04327656\n",
      "Iteration 508, loss = 3.04372895\n",
      "Iteration 509, loss = 3.04408597\n",
      "Iteration 510, loss = 3.04394066\n",
      "Iteration 511, loss = 3.04249609\n",
      "Iteration 512, loss = 3.04248656\n",
      "Iteration 513, loss = 3.04166254\n",
      "Iteration 514, loss = 3.04170646\n",
      "Iteration 515, loss = 3.04176235\n",
      "Iteration 516, loss = 3.04265938\n",
      "Iteration 517, loss = 3.04217309\n",
      "Iteration 518, loss = 3.04117850\n",
      "Iteration 519, loss = 3.04266917\n",
      "Iteration 520, loss = 3.04330751\n",
      "Iteration 521, loss = 3.04274955\n",
      "Iteration 522, loss = 3.04190087\n",
      "Iteration 523, loss = 3.04088039\n",
      "Iteration 524, loss = 3.04043638\n",
      "Iteration 525, loss = 3.03982897\n",
      "Iteration 526, loss = 3.03955433\n",
      "Iteration 527, loss = 3.04127665\n",
      "Iteration 528, loss = 3.03982180\n",
      "Iteration 529, loss = 3.04055551\n",
      "Iteration 530, loss = 3.04044211\n",
      "Iteration 531, loss = 3.03886274\n",
      "Iteration 532, loss = 3.03845901\n",
      "Iteration 533, loss = 3.03820766\n",
      "Iteration 534, loss = 3.03850536\n",
      "Iteration 535, loss = 3.03878715\n",
      "Iteration 536, loss = 3.03906365\n",
      "Iteration 537, loss = 3.03781384\n",
      "Iteration 538, loss = 3.03961396\n",
      "Iteration 539, loss = 3.03818120\n",
      "Iteration 540, loss = 3.03738994\n",
      "Iteration 541, loss = 3.03795571\n",
      "Iteration 542, loss = 3.03783438\n",
      "Iteration 543, loss = 3.03899907\n",
      "Iteration 544, loss = 3.03826094\n",
      "Iteration 545, loss = 3.03714357\n",
      "Iteration 546, loss = 3.03657546\n",
      "Iteration 547, loss = 3.03629189\n",
      "Iteration 548, loss = 3.03934786\n",
      "Iteration 549, loss = 3.03913240\n",
      "Iteration 550, loss = 3.03771483\n",
      "Iteration 551, loss = 3.03772432\n",
      "Iteration 552, loss = 3.03728890\n",
      "Iteration 553, loss = 3.03532355\n",
      "Iteration 554, loss = 3.03499089\n",
      "Iteration 555, loss = 3.03517330\n",
      "Iteration 556, loss = 3.03427209\n",
      "Iteration 557, loss = 3.03498731\n",
      "Iteration 558, loss = 3.03484928\n",
      "Iteration 559, loss = 3.03434877\n",
      "Iteration 560, loss = 3.03517162\n",
      "Iteration 561, loss = 3.03427519\n",
      "Iteration 562, loss = 3.03486881\n",
      "Iteration 563, loss = 3.03313931\n",
      "Iteration 564, loss = 3.03302298\n",
      "Iteration 565, loss = 3.03333500\n",
      "Iteration 566, loss = 3.03258238\n",
      "Iteration 567, loss = 3.03434559\n",
      "Iteration 568, loss = 3.03328848\n",
      "Iteration 569, loss = 3.03267081\n",
      "Iteration 570, loss = 3.03233548\n",
      "Iteration 571, loss = 3.03265653\n",
      "Iteration 572, loss = 3.03203556\n",
      "Iteration 573, loss = 3.03070341\n",
      "Iteration 574, loss = 3.03091359\n",
      "Iteration 575, loss = 3.03047299\n",
      "Iteration 576, loss = 3.03086348\n",
      "Iteration 577, loss = 3.03058687\n",
      "Iteration 578, loss = 3.03121252\n",
      "Iteration 579, loss = 3.03543316\n",
      "Iteration 580, loss = 3.03241623\n",
      "Iteration 581, loss = 3.02975242\n",
      "Iteration 582, loss = 3.03090203\n",
      "Iteration 583, loss = 3.02989860\n",
      "Iteration 584, loss = 3.02936194\n",
      "Iteration 585, loss = 3.02999698\n",
      "Iteration 586, loss = 3.02941686\n",
      "Iteration 587, loss = 3.02892611\n",
      "Iteration 588, loss = 3.02852518\n",
      "Iteration 589, loss = 3.02876401\n",
      "Iteration 590, loss = 3.02803555\n",
      "Iteration 591, loss = 3.02793313\n",
      "Iteration 592, loss = 3.02816726\n",
      "Iteration 593, loss = 3.02939719\n",
      "Iteration 594, loss = 3.02895025\n",
      "Iteration 595, loss = 3.02881282\n",
      "Iteration 596, loss = 3.02754479\n",
      "Iteration 597, loss = 3.02740554\n",
      "Iteration 598, loss = 3.02709057\n",
      "Iteration 599, loss = 3.02787276\n",
      "Iteration 600, loss = 3.02759814\n",
      "Iteration 601, loss = 3.02762693\n",
      "Iteration 602, loss = 3.02816596\n",
      "Iteration 603, loss = 3.02699973\n",
      "Iteration 604, loss = 3.02685872\n",
      "Iteration 605, loss = 3.02643774\n",
      "Iteration 606, loss = 3.02605025\n",
      "Iteration 607, loss = 3.02557194\n",
      "Iteration 608, loss = 3.02444057\n",
      "Iteration 609, loss = 3.02540155\n",
      "Iteration 610, loss = 3.02486966\n",
      "Iteration 611, loss = 3.02539645\n",
      "Iteration 612, loss = 3.02459599\n",
      "Iteration 613, loss = 3.02404580\n",
      "Iteration 614, loss = 3.02441657\n",
      "Iteration 615, loss = 3.02431521\n",
      "Iteration 616, loss = 3.02413099\n",
      "Iteration 617, loss = 3.02363479\n",
      "Iteration 618, loss = 3.02357914\n",
      "Iteration 619, loss = 3.02451141\n",
      "Iteration 620, loss = 3.02319491\n",
      "Iteration 621, loss = 3.02291146\n",
      "Iteration 622, loss = 3.02278644\n",
      "Iteration 623, loss = 3.02455697\n",
      "Iteration 624, loss = 3.02440450\n",
      "Iteration 625, loss = 3.02286260\n",
      "Iteration 626, loss = 3.02388278\n",
      "Iteration 627, loss = 3.02326361\n",
      "Iteration 628, loss = 3.02300770\n",
      "Iteration 629, loss = 3.02284483\n",
      "Iteration 630, loss = 3.02271754\n",
      "Iteration 631, loss = 3.02291068\n",
      "Iteration 632, loss = 3.02189935\n",
      "Iteration 633, loss = 3.02116291\n",
      "Iteration 634, loss = 3.02164964\n",
      "Iteration 635, loss = 3.02063588\n",
      "Iteration 636, loss = 3.02114280\n",
      "Iteration 637, loss = 3.02218736\n",
      "Iteration 638, loss = 3.02188337\n",
      "Iteration 639, loss = 3.02105087\n",
      "Iteration 640, loss = 3.02073231\n",
      "Iteration 641, loss = 3.02001505\n",
      "Iteration 642, loss = 3.01987665\n",
      "Iteration 643, loss = 3.02000120\n",
      "Iteration 644, loss = 3.01967607\n",
      "Iteration 645, loss = 3.01986216\n",
      "Iteration 646, loss = 3.01899052\n",
      "Iteration 647, loss = 3.01960942\n",
      "Iteration 648, loss = 3.01888673\n",
      "Iteration 649, loss = 3.01886706\n",
      "Iteration 650, loss = 3.01850913\n",
      "Iteration 651, loss = 3.01837188\n",
      "Iteration 652, loss = 3.01831591\n",
      "Iteration 653, loss = 3.01812569\n",
      "Iteration 654, loss = 3.01933248\n",
      "Iteration 655, loss = 3.01878161\n",
      "Iteration 656, loss = 3.01897405\n",
      "Iteration 657, loss = 3.01800425\n",
      "Iteration 658, loss = 3.01777525\n",
      "Iteration 659, loss = 3.01862410\n",
      "Iteration 660, loss = 3.01709387\n",
      "Iteration 661, loss = 3.01599881\n",
      "Iteration 662, loss = 3.01646303\n",
      "Iteration 663, loss = 3.01636029\n",
      "Iteration 664, loss = 3.01678467\n",
      "Iteration 665, loss = 3.01557883\n",
      "Iteration 666, loss = 3.01678716\n",
      "Iteration 667, loss = 3.01690323\n",
      "Iteration 668, loss = 3.01593438\n",
      "Iteration 669, loss = 3.01522736\n",
      "Iteration 670, loss = 3.01608963\n",
      "Iteration 671, loss = 3.01609035\n",
      "Iteration 672, loss = 3.01720823\n",
      "Iteration 673, loss = 3.01510731\n",
      "Iteration 674, loss = 3.01610676\n",
      "Iteration 675, loss = 3.01594406\n",
      "Iteration 676, loss = 3.01688793\n",
      "Iteration 677, loss = 3.01554590\n",
      "Iteration 678, loss = 3.01535326\n",
      "Iteration 679, loss = 3.01591330\n",
      "Iteration 680, loss = 3.01527458\n",
      "Iteration 681, loss = 3.01506167\n",
      "Iteration 682, loss = 3.01408210\n",
      "Iteration 683, loss = 3.01439742\n",
      "Iteration 684, loss = 3.01392909\n",
      "Iteration 685, loss = 3.01425792\n",
      "Iteration 686, loss = 3.01343596\n",
      "Iteration 687, loss = 3.01301680\n",
      "Iteration 688, loss = 3.01269890\n",
      "Iteration 689, loss = 3.01370157\n",
      "Iteration 690, loss = 3.01390857\n",
      "Iteration 691, loss = 3.01220361\n",
      "Iteration 692, loss = 3.01327521\n",
      "Iteration 693, loss = 3.01254623\n",
      "Iteration 694, loss = 3.01194325\n",
      "Iteration 695, loss = 3.01179129\n",
      "Iteration 696, loss = 3.01110301\n",
      "Iteration 697, loss = 3.01138530\n",
      "Iteration 698, loss = 3.01203626\n",
      "Iteration 699, loss = 3.01164080\n",
      "Iteration 700, loss = 3.01157245\n",
      "Iteration 701, loss = 3.01168631\n",
      "Iteration 702, loss = 3.01133438\n",
      "Iteration 703, loss = 3.01034950\n",
      "Iteration 704, loss = 3.00985079\n",
      "Iteration 705, loss = 3.01036376\n",
      "Iteration 706, loss = 3.01121483\n",
      "Iteration 707, loss = 3.01025484\n",
      "Iteration 708, loss = 3.00964805\n",
      "Iteration 709, loss = 3.01077625\n",
      "Iteration 710, loss = 3.01066856\n",
      "Iteration 711, loss = 3.01042675\n",
      "Iteration 712, loss = 3.00926805\n",
      "Iteration 713, loss = 3.01069803\n",
      "Iteration 714, loss = 3.01075959\n",
      "Iteration 715, loss = 3.00988520\n",
      "Iteration 716, loss = 3.00903545\n",
      "Iteration 717, loss = 3.00887575\n",
      "Iteration 718, loss = 3.00837938\n",
      "Iteration 719, loss = 3.00801491\n",
      "Iteration 720, loss = 3.00856396\n",
      "Iteration 721, loss = 3.00853246\n",
      "Iteration 722, loss = 3.00777882\n",
      "Iteration 723, loss = 3.00732271\n",
      "Iteration 724, loss = 3.00707788\n",
      "Iteration 725, loss = 3.00804901\n",
      "Iteration 726, loss = 3.00721507\n",
      "Iteration 727, loss = 3.00734142\n",
      "Iteration 728, loss = 3.00970951\n",
      "Iteration 729, loss = 3.00851069\n",
      "Iteration 730, loss = 3.00814484\n",
      "Iteration 731, loss = 3.00747712\n",
      "Iteration 732, loss = 3.00612824\n",
      "Iteration 733, loss = 3.00605074\n",
      "Iteration 734, loss = 3.00692934\n",
      "Iteration 735, loss = 3.00742288\n",
      "Iteration 736, loss = 3.00606384\n",
      "Iteration 737, loss = 3.00659099\n",
      "Iteration 738, loss = 3.00667769\n",
      "Iteration 739, loss = 3.00548139\n",
      "Iteration 740, loss = 3.00574504\n",
      "Iteration 741, loss = 3.00586759\n",
      "Iteration 742, loss = 3.00592916\n",
      "Iteration 743, loss = 3.00461345\n",
      "Iteration 744, loss = 3.00756836\n",
      "Iteration 745, loss = 3.00511763\n",
      "Iteration 746, loss = 3.00483417\n",
      "Iteration 747, loss = 3.00469873\n",
      "Iteration 748, loss = 3.00468074\n",
      "Iteration 749, loss = 3.00483898\n",
      "Iteration 750, loss = 3.00425997\n",
      "Iteration 751, loss = 3.00409396\n",
      "Iteration 752, loss = 3.00511492\n",
      "Iteration 753, loss = 3.00518428\n",
      "Iteration 754, loss = 3.00438348\n",
      "Iteration 755, loss = 3.00430552\n",
      "Iteration 756, loss = 3.00401668\n",
      "Iteration 757, loss = 3.00375304\n",
      "Iteration 758, loss = 3.00383285\n",
      "Iteration 759, loss = 3.00336721\n",
      "Iteration 760, loss = 3.00350568\n",
      "Iteration 761, loss = 3.00267655\n",
      "Iteration 762, loss = 3.00219892\n",
      "Iteration 763, loss = 3.00222865\n",
      "Iteration 764, loss = 3.00199688\n",
      "Iteration 765, loss = 3.00219691\n",
      "Iteration 766, loss = 3.00304799\n",
      "Iteration 767, loss = 3.00152844\n",
      "Iteration 768, loss = 3.00225091\n",
      "Iteration 769, loss = 3.00171638\n",
      "Iteration 770, loss = 3.00123777\n",
      "Iteration 771, loss = 3.00172312\n",
      "Iteration 772, loss = 3.00212134\n",
      "Iteration 773, loss = 3.00245843\n",
      "Iteration 774, loss = 3.00201205\n",
      "Iteration 775, loss = 3.00102565\n",
      "Iteration 776, loss = 3.00086691\n",
      "Iteration 777, loss = 3.00126295\n",
      "Iteration 778, loss = 3.00100712\n",
      "Iteration 779, loss = 2.99970506\n",
      "Iteration 780, loss = 2.99982966\n",
      "Iteration 781, loss = 2.99930814\n",
      "Iteration 782, loss = 2.99874490\n",
      "Iteration 783, loss = 3.00008972\n",
      "Iteration 784, loss = 3.00084667\n",
      "Iteration 785, loss = 3.00076787\n",
      "Iteration 786, loss = 2.99960276\n",
      "Iteration 787, loss = 2.99915368\n",
      "Iteration 788, loss = 2.99876165\n",
      "Iteration 789, loss = 2.99842253\n",
      "Iteration 790, loss = 2.99833101\n",
      "Iteration 791, loss = 2.99916015\n",
      "Iteration 792, loss = 2.99861326\n",
      "Iteration 793, loss = 3.00005091\n",
      "Iteration 794, loss = 3.00013386\n",
      "Iteration 795, loss = 2.99948137\n",
      "Iteration 796, loss = 2.99883971\n",
      "Iteration 797, loss = 2.99866009\n",
      "Iteration 798, loss = 2.99751778\n",
      "Iteration 799, loss = 2.99777446\n",
      "Iteration 800, loss = 3.00009466\n",
      "Iteration 801, loss = 2.99804587\n",
      "Iteration 802, loss = 2.99744301\n",
      "Iteration 803, loss = 2.99803191\n",
      "Iteration 804, loss = 2.99703204\n",
      "Iteration 805, loss = 2.99643544\n",
      "Iteration 806, loss = 2.99593267\n",
      "Iteration 807, loss = 2.99568167\n",
      "Iteration 808, loss = 2.99611934\n",
      "Iteration 809, loss = 2.99607926\n",
      "Iteration 810, loss = 2.99629552\n",
      "Iteration 811, loss = 2.99689753\n",
      "Iteration 812, loss = 2.99508582\n",
      "Iteration 813, loss = 2.99700465\n",
      "Iteration 814, loss = 2.99668088\n",
      "Iteration 815, loss = 2.99572480\n",
      "Iteration 816, loss = 2.99546835\n",
      "Iteration 817, loss = 2.99556577\n",
      "Iteration 818, loss = 2.99584016\n",
      "Iteration 819, loss = 2.99457720\n",
      "Iteration 820, loss = 2.99505386\n",
      "Iteration 821, loss = 2.99437304\n",
      "Iteration 822, loss = 2.99679690\n",
      "Iteration 823, loss = 2.99577587\n",
      "Iteration 824, loss = 2.99516387\n",
      "Iteration 825, loss = 2.99450457\n",
      "Iteration 826, loss = 2.99498692\n",
      "Iteration 827, loss = 2.99496756\n",
      "Iteration 828, loss = 2.99452532\n",
      "Iteration 829, loss = 2.99358229\n",
      "Iteration 830, loss = 2.99327608\n",
      "Iteration 831, loss = 2.99383339\n",
      "Iteration 832, loss = 2.99372087\n",
      "Iteration 833, loss = 2.99375999\n",
      "Iteration 834, loss = 2.99346517\n",
      "Iteration 835, loss = 2.99300247\n",
      "Iteration 836, loss = 2.99302343\n",
      "Iteration 837, loss = 2.99282229\n",
      "Iteration 838, loss = 2.99284441\n",
      "Iteration 839, loss = 2.99254351\n",
      "Iteration 840, loss = 2.99238872\n",
      "Iteration 841, loss = 2.99234128\n",
      "Iteration 842, loss = 2.99193106\n",
      "Iteration 843, loss = 2.99403475\n",
      "Iteration 844, loss = 2.99366545\n",
      "Iteration 845, loss = 2.99479714\n",
      "Iteration 846, loss = 2.99295350\n",
      "Iteration 847, loss = 2.99303400\n",
      "Iteration 848, loss = 2.99224655\n",
      "Iteration 849, loss = 2.99119840\n",
      "Iteration 850, loss = 2.99264375\n",
      "Iteration 851, loss = 2.99214497\n",
      "Iteration 852, loss = 2.99155841\n",
      "Iteration 853, loss = 2.99237461\n",
      "Iteration 854, loss = 2.99188225\n",
      "Iteration 855, loss = 2.99391290\n",
      "Iteration 856, loss = 2.99174442\n",
      "Iteration 857, loss = 2.99126790\n",
      "Iteration 858, loss = 2.99166100\n",
      "Iteration 859, loss = 2.99082867\n",
      "Iteration 860, loss = 2.99102096\n",
      "Iteration 861, loss = 2.99106748\n",
      "Iteration 862, loss = 2.98995742\n",
      "Iteration 863, loss = 2.98929143\n",
      "Iteration 864, loss = 2.99038890\n",
      "Iteration 865, loss = 2.99020020\n",
      "Iteration 866, loss = 2.99161616\n",
      "Iteration 867, loss = 2.98983502\n",
      "Iteration 868, loss = 2.99033529\n",
      "Iteration 869, loss = 2.98913783\n",
      "Iteration 870, loss = 2.98875087\n",
      "Iteration 871, loss = 2.98935861\n",
      "Iteration 872, loss = 2.98867726\n",
      "Iteration 873, loss = 2.98867703\n",
      "Iteration 874, loss = 2.98782544\n",
      "Iteration 875, loss = 2.98885766\n",
      "Iteration 876, loss = 2.98821952\n",
      "Iteration 877, loss = 2.99025475\n",
      "Iteration 878, loss = 2.98993318\n",
      "Iteration 879, loss = 2.98792551\n",
      "Iteration 880, loss = 2.98761748\n",
      "Iteration 881, loss = 2.98825680\n",
      "Iteration 882, loss = 2.98775662\n",
      "Iteration 883, loss = 2.98942427\n",
      "Iteration 884, loss = 2.98831358\n",
      "Iteration 885, loss = 2.98799843\n",
      "Iteration 886, loss = 2.98762340\n",
      "Iteration 887, loss = 2.98654933\n",
      "Iteration 888, loss = 2.98640896\n",
      "Iteration 889, loss = 2.98780010\n",
      "Iteration 890, loss = 2.98886163\n",
      "Iteration 891, loss = 2.99012929\n",
      "Iteration 892, loss = 2.98716548\n",
      "Iteration 893, loss = 2.98684984\n",
      "Iteration 894, loss = 2.98741529\n",
      "Iteration 895, loss = 2.98717349\n",
      "Iteration 896, loss = 2.98582528\n",
      "Iteration 897, loss = 2.98615779\n",
      "Iteration 898, loss = 2.98618560\n",
      "Iteration 899, loss = 2.98495564\n",
      "Iteration 900, loss = 2.98542968\n",
      "Iteration 901, loss = 2.98511045\n",
      "Iteration 902, loss = 2.98432410\n",
      "Iteration 903, loss = 2.98701111\n",
      "Iteration 904, loss = 2.98603467\n",
      "Iteration 905, loss = 2.98525737\n",
      "Iteration 906, loss = 2.98466295\n",
      "Iteration 907, loss = 2.98630074\n",
      "Iteration 908, loss = 2.98548379\n",
      "Iteration 909, loss = 2.98505861\n",
      "Iteration 910, loss = 2.98512665\n",
      "Iteration 911, loss = 2.98453406\n",
      "Iteration 912, loss = 2.98386700\n",
      "Iteration 913, loss = 2.98500506\n",
      "Iteration 914, loss = 2.98422668\n",
      "Iteration 915, loss = 2.98375858\n",
      "Iteration 916, loss = 2.98510175\n",
      "Iteration 917, loss = 2.98435767\n",
      "Iteration 918, loss = 2.98418513\n",
      "Iteration 919, loss = 2.98523386\n",
      "Iteration 920, loss = 2.98498879\n",
      "Iteration 921, loss = 2.98390745\n",
      "Iteration 922, loss = 2.98457962\n",
      "Iteration 923, loss = 2.98387876\n",
      "Iteration 924, loss = 2.98355128\n",
      "Iteration 925, loss = 2.98340299\n",
      "Iteration 926, loss = 2.98368156\n",
      "Iteration 927, loss = 2.98371945\n",
      "Iteration 928, loss = 2.98288032\n",
      "Iteration 929, loss = 2.98430335\n",
      "Iteration 930, loss = 2.98273365\n",
      "Iteration 931, loss = 2.98221799\n",
      "Iteration 932, loss = 2.98234339\n",
      "Iteration 933, loss = 2.98191375\n",
      "Iteration 934, loss = 2.98242076\n",
      "Iteration 935, loss = 2.98193155\n",
      "Iteration 936, loss = 2.98249505\n",
      "Iteration 937, loss = 2.98173033\n",
      "Iteration 938, loss = 2.98121270\n",
      "Iteration 939, loss = 2.98287412\n",
      "Iteration 940, loss = 2.98154383\n",
      "Iteration 941, loss = 2.98281144\n",
      "Iteration 942, loss = 2.98069521\n",
      "Iteration 943, loss = 2.98206115\n",
      "Iteration 944, loss = 2.98166979\n",
      "Iteration 945, loss = 2.98115523\n",
      "Iteration 946, loss = 2.98132943\n",
      "Iteration 947, loss = 2.98201637\n",
      "Iteration 948, loss = 2.98138327\n",
      "Iteration 949, loss = 2.98274396\n",
      "Iteration 950, loss = 2.98118572\n",
      "Iteration 951, loss = 2.98199294\n",
      "Iteration 952, loss = 2.98119147\n",
      "Iteration 953, loss = 2.97991527\n",
      "Iteration 954, loss = 2.97998021\n",
      "Iteration 955, loss = 2.97986954\n",
      "Iteration 956, loss = 2.98090633\n",
      "Iteration 957, loss = 2.97985102\n",
      "Iteration 958, loss = 2.97938661\n",
      "Iteration 959, loss = 2.97940941\n",
      "Iteration 960, loss = 2.97957959\n",
      "Iteration 961, loss = 2.97998239\n",
      "Iteration 962, loss = 2.97920539\n",
      "Iteration 963, loss = 2.97910881\n",
      "Iteration 964, loss = 2.98056613\n",
      "Iteration 965, loss = 2.97981960\n",
      "Iteration 966, loss = 2.97918692\n",
      "Iteration 967, loss = 2.97973135\n",
      "Iteration 968, loss = 2.97840324\n",
      "Iteration 969, loss = 2.97860496\n",
      "Iteration 970, loss = 2.97865090\n",
      "Iteration 971, loss = 2.97824263\n",
      "Iteration 972, loss = 2.97808563\n",
      "Iteration 973, loss = 2.97768663\n",
      "Iteration 974, loss = 2.97791684\n",
      "Iteration 975, loss = 2.97852489\n",
      "Iteration 976, loss = 2.97850991\n",
      "Iteration 977, loss = 2.97807928\n",
      "Iteration 978, loss = 2.97799994\n",
      "Iteration 979, loss = 2.97827065\n",
      "Iteration 980, loss = 2.97818847\n",
      "Iteration 981, loss = 2.97794747\n",
      "Iteration 982, loss = 2.97798225\n",
      "Iteration 983, loss = 2.97784426\n",
      "Iteration 984, loss = 2.97682407\n",
      "Iteration 985, loss = 2.97967796\n",
      "Iteration 986, loss = 2.97771324\n",
      "Iteration 987, loss = 2.97766535\n",
      "Iteration 988, loss = 2.97632131\n",
      "Iteration 989, loss = 2.97781605\n",
      "Iteration 990, loss = 2.97670215\n",
      "Iteration 991, loss = 2.97721087\n",
      "Iteration 992, loss = 2.97621461\n",
      "Iteration 993, loss = 2.97671289\n",
      "Iteration 994, loss = 2.97570583\n",
      "Iteration 995, loss = 2.97564206\n",
      "Iteration 996, loss = 2.97590200\n",
      "Iteration 997, loss = 2.97630739\n",
      "Iteration 998, loss = 2.97536865\n",
      "Iteration 999, loss = 2.97510325\n",
      "Iteration 1000, loss = 2.97473105\n",
      "Iteration 1001, loss = 2.97507402\n",
      "Iteration 1002, loss = 2.97515246\n",
      "Iteration 1003, loss = 2.97538939\n",
      "Iteration 1004, loss = 2.97570924\n",
      "Iteration 1005, loss = 2.97563139\n",
      "Iteration 1006, loss = 2.97501703\n",
      "Iteration 1007, loss = 2.97517945\n",
      "Iteration 1008, loss = 2.97482494\n",
      "Iteration 1009, loss = 2.97461650\n",
      "Iteration 1010, loss = 2.97425274\n",
      "Iteration 1011, loss = 2.97410485\n",
      "Iteration 1012, loss = 2.97400149\n",
      "Iteration 1013, loss = 2.97429796\n",
      "Iteration 1014, loss = 2.97460438\n",
      "Iteration 1015, loss = 2.97495430\n",
      "Iteration 1016, loss = 2.97385774\n",
      "Iteration 1017, loss = 2.97537681\n",
      "Iteration 1018, loss = 2.97402432\n",
      "Iteration 1019, loss = 2.97516894\n",
      "Iteration 1020, loss = 2.97296883\n",
      "Iteration 1021, loss = 2.97304317\n",
      "Iteration 1022, loss = 2.97214950\n",
      "Iteration 1023, loss = 2.97554088\n",
      "Iteration 1024, loss = 2.97441198\n",
      "Iteration 1025, loss = 2.97482202\n",
      "Iteration 1026, loss = 2.97414639\n",
      "Iteration 1027, loss = 2.97260186\n",
      "Iteration 1028, loss = 2.97186915\n",
      "Iteration 1029, loss = 2.97307655\n",
      "Iteration 1030, loss = 2.97232512\n",
      "Iteration 1031, loss = 2.97301378\n",
      "Iteration 1032, loss = 2.97347872\n",
      "Iteration 1033, loss = 2.97296116\n",
      "Iteration 1034, loss = 2.97315872\n",
      "Iteration 1035, loss = 2.97376767\n",
      "Iteration 1036, loss = 2.97253614\n",
      "Iteration 1037, loss = 2.97135072\n",
      "Iteration 1038, loss = 2.97175149\n",
      "Iteration 1039, loss = 2.97184425\n",
      "Iteration 1040, loss = 2.97115021\n",
      "Iteration 1041, loss = 2.97220598\n",
      "Iteration 1042, loss = 2.97137529\n",
      "Iteration 1043, loss = 2.97129946\n",
      "Iteration 1044, loss = 2.97154525\n",
      "Iteration 1045, loss = 2.97183097\n",
      "Iteration 1046, loss = 2.97083759\n",
      "Iteration 1047, loss = 2.97123369\n",
      "Iteration 1048, loss = 2.97105875\n",
      "Iteration 1049, loss = 2.97146961\n",
      "Iteration 1050, loss = 2.97050542\n",
      "Iteration 1051, loss = 2.97008804\n",
      "Iteration 1052, loss = 2.97004674\n",
      "Iteration 1053, loss = 2.97064274\n",
      "Iteration 1054, loss = 2.97059768\n",
      "Iteration 1055, loss = 2.97067152\n",
      "Iteration 1056, loss = 2.97072172\n",
      "Iteration 1057, loss = 2.96995681\n",
      "Iteration 1058, loss = 2.97069408\n",
      "Iteration 1059, loss = 2.96955231\n",
      "Iteration 1060, loss = 2.96924407\n",
      "Iteration 1061, loss = 2.96933474\n",
      "Iteration 1062, loss = 2.97004312\n",
      "Iteration 1063, loss = 2.97000352\n",
      "Iteration 1064, loss = 2.97180049\n",
      "Iteration 1065, loss = 2.97049475\n",
      "Iteration 1066, loss = 2.97002254\n",
      "Iteration 1067, loss = 2.96959147\n",
      "Iteration 1068, loss = 2.96977119\n",
      "Iteration 1069, loss = 2.96897533\n",
      "Iteration 1070, loss = 2.97025181\n",
      "Iteration 1071, loss = 2.96926153\n",
      "Iteration 1072, loss = 2.96822017\n",
      "Iteration 1073, loss = 2.96887163\n",
      "Iteration 1074, loss = 2.96926523\n",
      "Iteration 1075, loss = 2.96981148\n",
      "Iteration 1076, loss = 2.97051414\n",
      "Iteration 1077, loss = 2.96856674\n",
      "Iteration 1078, loss = 2.96779532\n",
      "Iteration 1079, loss = 2.96846328\n",
      "Iteration 1080, loss = 2.96817023\n",
      "Iteration 1081, loss = 2.96810514\n",
      "Iteration 1082, loss = 2.96846433\n",
      "Iteration 1083, loss = 2.96766978\n",
      "Iteration 1084, loss = 2.96746808\n",
      "Iteration 1085, loss = 2.96646536\n",
      "Iteration 1086, loss = 2.96711448\n",
      "Iteration 1087, loss = 2.96654252\n",
      "Iteration 1088, loss = 2.96806720\n",
      "Iteration 1089, loss = 2.96646493\n",
      "Iteration 1090, loss = 2.96695305\n",
      "Iteration 1091, loss = 2.96753764\n",
      "Iteration 1092, loss = 2.96837036\n",
      "Iteration 1093, loss = 2.97023027\n",
      "Iteration 1094, loss = 2.96812095\n",
      "Iteration 1095, loss = 2.97072410\n",
      "Iteration 1096, loss = 2.96687387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23889490\n",
      "Iteration 2, loss = 4.13955817\n",
      "Iteration 3, loss = 4.04430450\n",
      "Iteration 4, loss = 3.95041156\n",
      "Iteration 5, loss = 3.85148874\n",
      "Iteration 6, loss = 3.74625669\n",
      "Iteration 7, loss = 3.63815804\n",
      "Iteration 8, loss = 3.53051659\n",
      "Iteration 9, loss = 3.43107589\n",
      "Iteration 10, loss = 3.34761963\n",
      "Iteration 11, loss = 3.28865740\n",
      "Iteration 12, loss = 3.25152257\n",
      "Iteration 13, loss = 3.23776148\n",
      "Iteration 14, loss = 3.23287072\n",
      "Iteration 15, loss = 3.23088338\n",
      "Iteration 16, loss = 3.22933083\n",
      "Iteration 17, loss = 3.22842512\n",
      "Iteration 18, loss = 3.22616220\n",
      "Iteration 19, loss = 3.22494283\n",
      "Iteration 20, loss = 3.22409223\n",
      "Iteration 21, loss = 3.22398797\n",
      "Iteration 22, loss = 3.22314321\n",
      "Iteration 23, loss = 3.22259126\n",
      "Iteration 24, loss = 3.22144233\n",
      "Iteration 25, loss = 3.22004426\n",
      "Iteration 26, loss = 3.21951845\n",
      "Iteration 27, loss = 3.21871827\n",
      "Iteration 28, loss = 3.21794222\n",
      "Iteration 29, loss = 3.21646349\n",
      "Iteration 30, loss = 3.21621183\n",
      "Iteration 31, loss = 3.21561563\n",
      "Iteration 32, loss = 3.21495081\n",
      "Iteration 33, loss = 3.21455430\n",
      "Iteration 34, loss = 3.21460110\n",
      "Iteration 35, loss = 3.21392252\n",
      "Iteration 36, loss = 3.21273677\n",
      "Iteration 37, loss = 3.21197754\n",
      "Iteration 38, loss = 3.21090675\n",
      "Iteration 39, loss = 3.20988398\n",
      "Iteration 40, loss = 3.20930159\n",
      "Iteration 41, loss = 3.20886637\n",
      "Iteration 42, loss = 3.20891483\n",
      "Iteration 43, loss = 3.20849382\n",
      "Iteration 44, loss = 3.20769831\n",
      "Iteration 45, loss = 3.20627685\n",
      "Iteration 46, loss = 3.20479589\n",
      "Iteration 47, loss = 3.20398320\n",
      "Iteration 48, loss = 3.20340823\n",
      "Iteration 49, loss = 3.20342701\n",
      "Iteration 50, loss = 3.20287577\n",
      "Iteration 51, loss = 3.20213283\n",
      "Iteration 52, loss = 3.20146117\n",
      "Iteration 53, loss = 3.20143884\n",
      "Iteration 54, loss = 3.20068747\n",
      "Iteration 55, loss = 3.19986406\n",
      "Iteration 56, loss = 3.19893800\n",
      "Iteration 57, loss = 3.19953609\n",
      "Iteration 58, loss = 3.19865748\n",
      "Iteration 59, loss = 3.19794100\n",
      "Iteration 60, loss = 3.19668674\n",
      "Iteration 61, loss = 3.19559352\n",
      "Iteration 62, loss = 3.19534814\n",
      "Iteration 63, loss = 3.19490547\n",
      "Iteration 64, loss = 3.19434535\n",
      "Iteration 65, loss = 3.19263867\n",
      "Iteration 66, loss = 3.19179372\n",
      "Iteration 67, loss = 3.19171498\n",
      "Iteration 68, loss = 3.19054553\n",
      "Iteration 69, loss = 3.18979971\n",
      "Iteration 70, loss = 3.18930163\n",
      "Iteration 71, loss = 3.18928834\n",
      "Iteration 72, loss = 3.18897050\n",
      "Iteration 73, loss = 3.18868220\n",
      "Iteration 74, loss = 3.18783172\n",
      "Iteration 75, loss = 3.18697044\n",
      "Iteration 76, loss = 3.18718804\n",
      "Iteration 77, loss = 3.18638969\n",
      "Iteration 78, loss = 3.18509994\n",
      "Iteration 79, loss = 3.18416296\n",
      "Iteration 80, loss = 3.18468339\n",
      "Iteration 81, loss = 3.18364602\n",
      "Iteration 82, loss = 3.18381844\n",
      "Iteration 83, loss = 3.18307753\n",
      "Iteration 84, loss = 3.18153808\n",
      "Iteration 85, loss = 3.18059419\n",
      "Iteration 86, loss = 3.18074394\n",
      "Iteration 87, loss = 3.17926595\n",
      "Iteration 88, loss = 3.17917867\n",
      "Iteration 89, loss = 3.17899944\n",
      "Iteration 90, loss = 3.17810924\n",
      "Iteration 91, loss = 3.17775196\n",
      "Iteration 92, loss = 3.17741148\n",
      "Iteration 93, loss = 3.17608169\n",
      "Iteration 94, loss = 3.17479484\n",
      "Iteration 95, loss = 3.17488550\n",
      "Iteration 96, loss = 3.17412822\n",
      "Iteration 97, loss = 3.17324491\n",
      "Iteration 98, loss = 3.17291268\n",
      "Iteration 99, loss = 3.17339502\n",
      "Iteration 100, loss = 3.17245935\n",
      "Iteration 101, loss = 3.17207805\n",
      "Iteration 102, loss = 3.17135508\n",
      "Iteration 103, loss = 3.17074572\n",
      "Iteration 104, loss = 3.17005478\n",
      "Iteration 105, loss = 3.17032951\n",
      "Iteration 106, loss = 3.17030766\n",
      "Iteration 107, loss = 3.16925066\n",
      "Iteration 108, loss = 3.16963413\n",
      "Iteration 109, loss = 3.16928911\n",
      "Iteration 110, loss = 3.16860851\n",
      "Iteration 111, loss = 3.16692969\n",
      "Iteration 112, loss = 3.16627377\n",
      "Iteration 113, loss = 3.16653409\n",
      "Iteration 114, loss = 3.16639273\n",
      "Iteration 115, loss = 3.16599763\n",
      "Iteration 116, loss = 3.16577404\n",
      "Iteration 117, loss = 3.16478298\n",
      "Iteration 118, loss = 3.16396862\n",
      "Iteration 119, loss = 3.16325032\n",
      "Iteration 120, loss = 3.16305415\n",
      "Iteration 121, loss = 3.16275512\n",
      "Iteration 122, loss = 3.16162666\n",
      "Iteration 123, loss = 3.16064313\n",
      "Iteration 124, loss = 3.16029502\n",
      "Iteration 125, loss = 3.16000453\n",
      "Iteration 126, loss = 3.15920182\n",
      "Iteration 127, loss = 3.15888734\n",
      "Iteration 128, loss = 3.15919533\n",
      "Iteration 129, loss = 3.15823055\n",
      "Iteration 130, loss = 3.15738110\n",
      "Iteration 131, loss = 3.15665023\n",
      "Iteration 132, loss = 3.15686559\n",
      "Iteration 133, loss = 3.15629693\n",
      "Iteration 134, loss = 3.15559494\n",
      "Iteration 135, loss = 3.15659897\n",
      "Iteration 136, loss = 3.15526366\n",
      "Iteration 137, loss = 3.15425276\n",
      "Iteration 138, loss = 3.15463504\n",
      "Iteration 139, loss = 3.15384292\n",
      "Iteration 140, loss = 3.15319952\n",
      "Iteration 141, loss = 3.15327551\n",
      "Iteration 142, loss = 3.15210417\n",
      "Iteration 143, loss = 3.15158420\n",
      "Iteration 144, loss = 3.15119989\n",
      "Iteration 145, loss = 3.15005239\n",
      "Iteration 146, loss = 3.14990592\n",
      "Iteration 147, loss = 3.14994686\n",
      "Iteration 148, loss = 3.14914788\n",
      "Iteration 149, loss = 3.14947311\n",
      "Iteration 150, loss = 3.14926070\n",
      "Iteration 151, loss = 3.14873136\n",
      "Iteration 152, loss = 3.14784461\n",
      "Iteration 153, loss = 3.14809781\n",
      "Iteration 154, loss = 3.14780626\n",
      "Iteration 155, loss = 3.14758482\n",
      "Iteration 156, loss = 3.14698746\n",
      "Iteration 157, loss = 3.14626639\n",
      "Iteration 158, loss = 3.14557401\n",
      "Iteration 159, loss = 3.14498456\n",
      "Iteration 160, loss = 3.14498853\n",
      "Iteration 161, loss = 3.14420499\n",
      "Iteration 162, loss = 3.14310420\n",
      "Iteration 163, loss = 3.14285837\n",
      "Iteration 164, loss = 3.14289849\n",
      "Iteration 165, loss = 3.14318264\n",
      "Iteration 166, loss = 3.14245358\n",
      "Iteration 167, loss = 3.14307961\n",
      "Iteration 168, loss = 3.14124024\n",
      "Iteration 169, loss = 3.14179455\n",
      "Iteration 170, loss = 3.14043593\n",
      "Iteration 171, loss = 3.13982581\n",
      "Iteration 172, loss = 3.14071473\n",
      "Iteration 173, loss = 3.13929215\n",
      "Iteration 174, loss = 3.13850955\n",
      "Iteration 175, loss = 3.13985656\n",
      "Iteration 176, loss = 3.13836789\n",
      "Iteration 177, loss = 3.13764755\n",
      "Iteration 178, loss = 3.13719327\n",
      "Iteration 179, loss = 3.13692478\n",
      "Iteration 180, loss = 3.13760576\n",
      "Iteration 181, loss = 3.13605187\n",
      "Iteration 182, loss = 3.13700358\n",
      "Iteration 183, loss = 3.13677823\n",
      "Iteration 184, loss = 3.13583403\n",
      "Iteration 185, loss = 3.13516239\n",
      "Iteration 186, loss = 3.13476127\n",
      "Iteration 187, loss = 3.13602949\n",
      "Iteration 188, loss = 3.13472026\n",
      "Iteration 189, loss = 3.13328213\n",
      "Iteration 190, loss = 3.13288732\n",
      "Iteration 191, loss = 3.13300672\n",
      "Iteration 192, loss = 3.13279882\n",
      "Iteration 193, loss = 3.13275645\n",
      "Iteration 194, loss = 3.13334243\n",
      "Iteration 195, loss = 3.13260732\n",
      "Iteration 196, loss = 3.13177991\n",
      "Iteration 197, loss = 3.13161425\n",
      "Iteration 198, loss = 3.13146172\n",
      "Iteration 199, loss = 3.13084428\n",
      "Iteration 200, loss = 3.13112472\n",
      "Iteration 201, loss = 3.13050480\n",
      "Iteration 202, loss = 3.13005687\n",
      "Iteration 203, loss = 3.12967323\n",
      "Iteration 204, loss = 3.12936537\n",
      "Iteration 205, loss = 3.12975420\n",
      "Iteration 206, loss = 3.12993686\n",
      "Iteration 207, loss = 3.12806594\n",
      "Iteration 208, loss = 3.12761265\n",
      "Iteration 209, loss = 3.12782070\n",
      "Iteration 210, loss = 3.12781988\n",
      "Iteration 211, loss = 3.12612106\n",
      "Iteration 212, loss = 3.12595846\n",
      "Iteration 213, loss = 3.12574908\n",
      "Iteration 214, loss = 3.12564129\n",
      "Iteration 215, loss = 3.12534130\n",
      "Iteration 216, loss = 3.12416716\n",
      "Iteration 217, loss = 3.12485143\n",
      "Iteration 218, loss = 3.12380711\n",
      "Iteration 219, loss = 3.12298733\n",
      "Iteration 220, loss = 3.12319180\n",
      "Iteration 221, loss = 3.12375925\n",
      "Iteration 222, loss = 3.12309331\n",
      "Iteration 223, loss = 3.12290532\n",
      "Iteration 224, loss = 3.12461314\n",
      "Iteration 225, loss = 3.12457505\n",
      "Iteration 226, loss = 3.12147737\n",
      "Iteration 227, loss = 3.12240233\n",
      "Iteration 228, loss = 3.12186197\n",
      "Iteration 229, loss = 3.12030487\n",
      "Iteration 230, loss = 3.11996346\n",
      "Iteration 231, loss = 3.12081717\n",
      "Iteration 232, loss = 3.12045116\n",
      "Iteration 233, loss = 3.12012533\n",
      "Iteration 234, loss = 3.11925283\n",
      "Iteration 235, loss = 3.12001395\n",
      "Iteration 236, loss = 3.11897181\n",
      "Iteration 237, loss = 3.12025689\n",
      "Iteration 238, loss = 3.11952229\n",
      "Iteration 239, loss = 3.11792467\n",
      "Iteration 240, loss = 3.11848750\n",
      "Iteration 241, loss = 3.11801606\n",
      "Iteration 242, loss = 3.11787110\n",
      "Iteration 243, loss = 3.11718514\n",
      "Iteration 244, loss = 3.11662240\n",
      "Iteration 245, loss = 3.11617545\n",
      "Iteration 246, loss = 3.11628842\n",
      "Iteration 247, loss = 3.11571164\n",
      "Iteration 248, loss = 3.11586001\n",
      "Iteration 249, loss = 3.11645654\n",
      "Iteration 250, loss = 3.11546736\n",
      "Iteration 251, loss = 3.11529241\n",
      "Iteration 252, loss = 3.11444963\n",
      "Iteration 253, loss = 3.11482104\n",
      "Iteration 254, loss = 3.11539898\n",
      "Iteration 255, loss = 3.11433716\n",
      "Iteration 256, loss = 3.11359604\n",
      "Iteration 257, loss = 3.11333802\n",
      "Iteration 258, loss = 3.11402289\n",
      "Iteration 259, loss = 3.11348579\n",
      "Iteration 260, loss = 3.11170150\n",
      "Iteration 261, loss = 3.11266436\n",
      "Iteration 262, loss = 3.11408751\n",
      "Iteration 263, loss = 3.11264148\n",
      "Iteration 264, loss = 3.11143406\n",
      "Iteration 265, loss = 3.11086733\n",
      "Iteration 266, loss = 3.11075656\n",
      "Iteration 267, loss = 3.11005422\n",
      "Iteration 268, loss = 3.11005209\n",
      "Iteration 269, loss = 3.11102926\n",
      "Iteration 270, loss = 3.10937116\n",
      "Iteration 271, loss = 3.10950108\n",
      "Iteration 272, loss = 3.10916712\n",
      "Iteration 273, loss = 3.10882893\n",
      "Iteration 274, loss = 3.11105618\n",
      "Iteration 275, loss = 3.10963882\n",
      "Iteration 276, loss = 3.10814744\n",
      "Iteration 277, loss = 3.10731450\n",
      "Iteration 278, loss = 3.10838249\n",
      "Iteration 279, loss = 3.10841299\n",
      "Iteration 280, loss = 3.10643899\n",
      "Iteration 281, loss = 3.10651698\n",
      "Iteration 282, loss = 3.10539399\n",
      "Iteration 283, loss = 3.10454725\n",
      "Iteration 284, loss = 3.10523893\n",
      "Iteration 285, loss = 3.10477621\n",
      "Iteration 286, loss = 3.10483235\n",
      "Iteration 287, loss = 3.10528122\n",
      "Iteration 288, loss = 3.10529178\n",
      "Iteration 289, loss = 3.10407628\n",
      "Iteration 290, loss = 3.10373554\n",
      "Iteration 291, loss = 3.10375506\n",
      "Iteration 292, loss = 3.10354119\n",
      "Iteration 293, loss = 3.10328848\n",
      "Iteration 294, loss = 3.10347316\n",
      "Iteration 295, loss = 3.10267649\n",
      "Iteration 296, loss = 3.10231717\n",
      "Iteration 297, loss = 3.10244860\n",
      "Iteration 298, loss = 3.10131068\n",
      "Iteration 299, loss = 3.10136500\n",
      "Iteration 300, loss = 3.10122493\n",
      "Iteration 301, loss = 3.10200334\n",
      "Iteration 302, loss = 3.10122572\n",
      "Iteration 303, loss = 3.10322375\n",
      "Iteration 304, loss = 3.10163736\n",
      "Iteration 305, loss = 3.09993469\n",
      "Iteration 306, loss = 3.09962793\n",
      "Iteration 307, loss = 3.09907398\n",
      "Iteration 308, loss = 3.09947451\n",
      "Iteration 309, loss = 3.09935802\n",
      "Iteration 310, loss = 3.09889674\n",
      "Iteration 311, loss = 3.10186455\n",
      "Iteration 312, loss = 3.10089247\n",
      "Iteration 313, loss = 3.10019050\n",
      "Iteration 314, loss = 3.09896647\n",
      "Iteration 315, loss = 3.09837459\n",
      "Iteration 316, loss = 3.09813968\n",
      "Iteration 317, loss = 3.09785976\n",
      "Iteration 318, loss = 3.09793454\n",
      "Iteration 319, loss = 3.09675225\n",
      "Iteration 320, loss = 3.09658481\n",
      "Iteration 321, loss = 3.09632663\n",
      "Iteration 322, loss = 3.09612573\n",
      "Iteration 323, loss = 3.09545318\n",
      "Iteration 324, loss = 3.09507895\n",
      "Iteration 325, loss = 3.09431952\n",
      "Iteration 326, loss = 3.09449476\n",
      "Iteration 327, loss = 3.09556976\n",
      "Iteration 328, loss = 3.09417528\n",
      "Iteration 329, loss = 3.09421900\n",
      "Iteration 330, loss = 3.09367654\n",
      "Iteration 331, loss = 3.09371899\n",
      "Iteration 332, loss = 3.09264737\n",
      "Iteration 333, loss = 3.09250322\n",
      "Iteration 334, loss = 3.09231832\n",
      "Iteration 335, loss = 3.09347471\n",
      "Iteration 336, loss = 3.09317440\n",
      "Iteration 337, loss = 3.09262034\n",
      "Iteration 338, loss = 3.09234499\n",
      "Iteration 339, loss = 3.09149398\n",
      "Iteration 340, loss = 3.09142719\n",
      "Iteration 341, loss = 3.09091367\n",
      "Iteration 342, loss = 3.09174330\n",
      "Iteration 343, loss = 3.09104500\n",
      "Iteration 344, loss = 3.09015794\n",
      "Iteration 345, loss = 3.09014357\n",
      "Iteration 346, loss = 3.09117401\n",
      "Iteration 347, loss = 3.09163708\n",
      "Iteration 348, loss = 3.09074326\n",
      "Iteration 349, loss = 3.09048008\n",
      "Iteration 350, loss = 3.08913193\n",
      "Iteration 351, loss = 3.08947355\n",
      "Iteration 352, loss = 3.08841878\n",
      "Iteration 353, loss = 3.08821899\n",
      "Iteration 354, loss = 3.08824612\n",
      "Iteration 355, loss = 3.08879284\n",
      "Iteration 356, loss = 3.08874407\n",
      "Iteration 357, loss = 3.08746725\n",
      "Iteration 358, loss = 3.08854802\n",
      "Iteration 359, loss = 3.08693161\n",
      "Iteration 360, loss = 3.08733942\n",
      "Iteration 361, loss = 3.08740599\n",
      "Iteration 362, loss = 3.08707507\n",
      "Iteration 363, loss = 3.08587945\n",
      "Iteration 364, loss = 3.08627990\n",
      "Iteration 365, loss = 3.08666707\n",
      "Iteration 366, loss = 3.08599963\n",
      "Iteration 367, loss = 3.08511375\n",
      "Iteration 368, loss = 3.08578595\n",
      "Iteration 369, loss = 3.08521455\n",
      "Iteration 370, loss = 3.08599308\n",
      "Iteration 371, loss = 3.08497852\n",
      "Iteration 372, loss = 3.08511657\n",
      "Iteration 373, loss = 3.08467586\n",
      "Iteration 374, loss = 3.08337708\n",
      "Iteration 375, loss = 3.08345072\n",
      "Iteration 376, loss = 3.08393581\n",
      "Iteration 377, loss = 3.08265242\n",
      "Iteration 378, loss = 3.08458719\n",
      "Iteration 379, loss = 3.08383067\n",
      "Iteration 380, loss = 3.08313579\n",
      "Iteration 381, loss = 3.08281628\n",
      "Iteration 382, loss = 3.08310091\n",
      "Iteration 383, loss = 3.08224337\n",
      "Iteration 384, loss = 3.08268847\n",
      "Iteration 385, loss = 3.08155282\n",
      "Iteration 386, loss = 3.08175725\n",
      "Iteration 387, loss = 3.08115249\n",
      "Iteration 388, loss = 3.08044660\n",
      "Iteration 389, loss = 3.08073081\n",
      "Iteration 390, loss = 3.08175852\n",
      "Iteration 391, loss = 3.08056146\n",
      "Iteration 392, loss = 3.07955218\n",
      "Iteration 393, loss = 3.08023024\n",
      "Iteration 394, loss = 3.07958656\n",
      "Iteration 395, loss = 3.07918978\n",
      "Iteration 396, loss = 3.07874402\n",
      "Iteration 397, loss = 3.07939105\n",
      "Iteration 398, loss = 3.07933021\n",
      "Iteration 399, loss = 3.07899694\n",
      "Iteration 400, loss = 3.08034047\n",
      "Iteration 401, loss = 3.08069834\n",
      "Iteration 402, loss = 3.07867449\n",
      "Iteration 403, loss = 3.07872526\n",
      "Iteration 404, loss = 3.07810624\n",
      "Iteration 405, loss = 3.07852231\n",
      "Iteration 406, loss = 3.07784292\n",
      "Iteration 407, loss = 3.07740897\n",
      "Iteration 408, loss = 3.07720050\n",
      "Iteration 409, loss = 3.07715430\n",
      "Iteration 410, loss = 3.07619478\n",
      "Iteration 411, loss = 3.07605034\n",
      "Iteration 412, loss = 3.07600722\n",
      "Iteration 413, loss = 3.07651053\n",
      "Iteration 414, loss = 3.07615285\n",
      "Iteration 415, loss = 3.07571527\n",
      "Iteration 416, loss = 3.07564995\n",
      "Iteration 417, loss = 3.07570673\n",
      "Iteration 418, loss = 3.07539791\n",
      "Iteration 419, loss = 3.07464118\n",
      "Iteration 420, loss = 3.07494440\n",
      "Iteration 421, loss = 3.07495605\n",
      "Iteration 422, loss = 3.07376293\n",
      "Iteration 423, loss = 3.07336612\n",
      "Iteration 424, loss = 3.07364907\n",
      "Iteration 425, loss = 3.07407960\n",
      "Iteration 426, loss = 3.07324608\n",
      "Iteration 427, loss = 3.07290531\n",
      "Iteration 428, loss = 3.07307707\n",
      "Iteration 429, loss = 3.07229466\n",
      "Iteration 430, loss = 3.07254357\n",
      "Iteration 431, loss = 3.07224144\n",
      "Iteration 432, loss = 3.07240438\n",
      "Iteration 433, loss = 3.07248619\n",
      "Iteration 434, loss = 3.07178149\n",
      "Iteration 435, loss = 3.07054409\n",
      "Iteration 436, loss = 3.07164138\n",
      "Iteration 437, loss = 3.07093797\n",
      "Iteration 438, loss = 3.07067684\n",
      "Iteration 439, loss = 3.07023199\n",
      "Iteration 440, loss = 3.07062763\n",
      "Iteration 441, loss = 3.06987273\n",
      "Iteration 442, loss = 3.07013231\n",
      "Iteration 443, loss = 3.07135971\n",
      "Iteration 444, loss = 3.07047094\n",
      "Iteration 445, loss = 3.06933452\n",
      "Iteration 446, loss = 3.06943505\n",
      "Iteration 447, loss = 3.06901873\n",
      "Iteration 448, loss = 3.06844645\n",
      "Iteration 449, loss = 3.06835052\n",
      "Iteration 450, loss = 3.06829900\n",
      "Iteration 451, loss = 3.06852832\n",
      "Iteration 452, loss = 3.06810705\n",
      "Iteration 453, loss = 3.06781750\n",
      "Iteration 454, loss = 3.06859672\n",
      "Iteration 455, loss = 3.06850263\n",
      "Iteration 456, loss = 3.06781037\n",
      "Iteration 457, loss = 3.06874497\n",
      "Iteration 458, loss = 3.06709577\n",
      "Iteration 459, loss = 3.06635717\n",
      "Iteration 460, loss = 3.06695339\n",
      "Iteration 461, loss = 3.06614228\n",
      "Iteration 462, loss = 3.06590677\n",
      "Iteration 463, loss = 3.06617460\n",
      "Iteration 464, loss = 3.06586059\n",
      "Iteration 465, loss = 3.06551070\n",
      "Iteration 466, loss = 3.06562154\n",
      "Iteration 467, loss = 3.06542007\n",
      "Iteration 468, loss = 3.06494033\n",
      "Iteration 469, loss = 3.06482273\n",
      "Iteration 470, loss = 3.06431361\n",
      "Iteration 471, loss = 3.06510240\n",
      "Iteration 472, loss = 3.06514036\n",
      "Iteration 473, loss = 3.06392584\n",
      "Iteration 474, loss = 3.06383343\n",
      "Iteration 475, loss = 3.06350141\n",
      "Iteration 476, loss = 3.06367130\n",
      "Iteration 477, loss = 3.06336090\n",
      "Iteration 478, loss = 3.06318482\n",
      "Iteration 479, loss = 3.06321511\n",
      "Iteration 480, loss = 3.06283027\n",
      "Iteration 481, loss = 3.06279753\n",
      "Iteration 482, loss = 3.06280874\n",
      "Iteration 483, loss = 3.06251609\n",
      "Iteration 484, loss = 3.06234263\n",
      "Iteration 485, loss = 3.06297912\n",
      "Iteration 486, loss = 3.06321600\n",
      "Iteration 487, loss = 3.06292991\n",
      "Iteration 488, loss = 3.06167170\n",
      "Iteration 489, loss = 3.06134178\n",
      "Iteration 490, loss = 3.06061236\n",
      "Iteration 491, loss = 3.06040007\n",
      "Iteration 492, loss = 3.06102265\n",
      "Iteration 493, loss = 3.06125758\n",
      "Iteration 494, loss = 3.06221405\n",
      "Iteration 495, loss = 3.06048610\n",
      "Iteration 496, loss = 3.06035254\n",
      "Iteration 497, loss = 3.06011078\n",
      "Iteration 498, loss = 3.06068982\n",
      "Iteration 499, loss = 3.05997630\n",
      "Iteration 500, loss = 3.05913601\n",
      "Iteration 501, loss = 3.05923711\n",
      "Iteration 502, loss = 3.05935589\n",
      "Iteration 503, loss = 3.05901614\n",
      "Iteration 504, loss = 3.05892941\n",
      "Iteration 505, loss = 3.05959833\n",
      "Iteration 506, loss = 3.05953474\n",
      "Iteration 507, loss = 3.05914455\n",
      "Iteration 508, loss = 3.05833886\n",
      "Iteration 509, loss = 3.05884284\n",
      "Iteration 510, loss = 3.05905630\n",
      "Iteration 511, loss = 3.05817767\n",
      "Iteration 512, loss = 3.06056823\n",
      "Iteration 513, loss = 3.05816828\n",
      "Iteration 514, loss = 3.05915760\n",
      "Iteration 515, loss = 3.05816850\n",
      "Iteration 516, loss = 3.05747706\n",
      "Iteration 517, loss = 3.05675614\n",
      "Iteration 518, loss = 3.05761675\n",
      "Iteration 519, loss = 3.05694461\n",
      "Iteration 520, loss = 3.05665324\n",
      "Iteration 521, loss = 3.05781605\n",
      "Iteration 522, loss = 3.05680194\n",
      "Iteration 523, loss = 3.05644912\n",
      "Iteration 524, loss = 3.05681643\n",
      "Iteration 525, loss = 3.05562168\n",
      "Iteration 526, loss = 3.05576341\n",
      "Iteration 527, loss = 3.05534265\n",
      "Iteration 528, loss = 3.05557039\n",
      "Iteration 529, loss = 3.05575764\n",
      "Iteration 530, loss = 3.05761798\n",
      "Iteration 531, loss = 3.05518599\n",
      "Iteration 532, loss = 3.05623881\n",
      "Iteration 533, loss = 3.05425860\n",
      "Iteration 534, loss = 3.05365966\n",
      "Iteration 535, loss = 3.05472609\n",
      "Iteration 536, loss = 3.05399412\n",
      "Iteration 537, loss = 3.05355407\n",
      "Iteration 538, loss = 3.05303910\n",
      "Iteration 539, loss = 3.05304379\n",
      "Iteration 540, loss = 3.05307877\n",
      "Iteration 541, loss = 3.05304666\n",
      "Iteration 542, loss = 3.05411362\n",
      "Iteration 543, loss = 3.05471911\n",
      "Iteration 544, loss = 3.05392505\n",
      "Iteration 545, loss = 3.05264818\n",
      "Iteration 546, loss = 3.05270262\n",
      "Iteration 547, loss = 3.05101661\n",
      "Iteration 548, loss = 3.05201290\n",
      "Iteration 549, loss = 3.05200048\n",
      "Iteration 550, loss = 3.05226422\n",
      "Iteration 551, loss = 3.05321821\n",
      "Iteration 552, loss = 3.05252423\n",
      "Iteration 553, loss = 3.05196737\n",
      "Iteration 554, loss = 3.05117824\n",
      "Iteration 555, loss = 3.05070817\n",
      "Iteration 556, loss = 3.05142748\n",
      "Iteration 557, loss = 3.05164242\n",
      "Iteration 558, loss = 3.05098593\n",
      "Iteration 559, loss = 3.05066697\n",
      "Iteration 560, loss = 3.05133792\n",
      "Iteration 561, loss = 3.04981624\n",
      "Iteration 562, loss = 3.05293284\n",
      "Iteration 563, loss = 3.05131935\n",
      "Iteration 564, loss = 3.04959877\n",
      "Iteration 565, loss = 3.04989708\n",
      "Iteration 566, loss = 3.04977573\n",
      "Iteration 567, loss = 3.04917913\n",
      "Iteration 568, loss = 3.04859725\n",
      "Iteration 569, loss = 3.04905754\n",
      "Iteration 570, loss = 3.04807182\n",
      "Iteration 571, loss = 3.04861539\n",
      "Iteration 572, loss = 3.04948327\n",
      "Iteration 573, loss = 3.05020494\n",
      "Iteration 574, loss = 3.04868935\n",
      "Iteration 575, loss = 3.04880402\n",
      "Iteration 576, loss = 3.04835709\n",
      "Iteration 577, loss = 3.04682274\n",
      "Iteration 578, loss = 3.04827154\n",
      "Iteration 579, loss = 3.04769584\n",
      "Iteration 580, loss = 3.04714580\n",
      "Iteration 581, loss = 3.04815542\n",
      "Iteration 582, loss = 3.04710278\n",
      "Iteration 583, loss = 3.04729289\n",
      "Iteration 584, loss = 3.04756141\n",
      "Iteration 585, loss = 3.04660059\n",
      "Iteration 586, loss = 3.04676786\n",
      "Iteration 587, loss = 3.04743917\n",
      "Iteration 588, loss = 3.04542930\n",
      "Iteration 589, loss = 3.04861233\n",
      "Iteration 590, loss = 3.04691985\n",
      "Iteration 591, loss = 3.04597476\n",
      "Iteration 592, loss = 3.04529541\n",
      "Iteration 593, loss = 3.04520288\n",
      "Iteration 594, loss = 3.04479513\n",
      "Iteration 595, loss = 3.04436017\n",
      "Iteration 596, loss = 3.04452429\n",
      "Iteration 597, loss = 3.04480247\n",
      "Iteration 598, loss = 3.04479430\n",
      "Iteration 599, loss = 3.04336277\n",
      "Iteration 600, loss = 3.04539367\n",
      "Iteration 601, loss = 3.04487429\n",
      "Iteration 602, loss = 3.04363502\n",
      "Iteration 603, loss = 3.04404801\n",
      "Iteration 604, loss = 3.04333980\n",
      "Iteration 605, loss = 3.04198488\n",
      "Iteration 606, loss = 3.04421925\n",
      "Iteration 607, loss = 3.04326629\n",
      "Iteration 608, loss = 3.04327318\n",
      "Iteration 609, loss = 3.04336395\n",
      "Iteration 610, loss = 3.04270006\n",
      "Iteration 611, loss = 3.04239190\n",
      "Iteration 612, loss = 3.04192419\n",
      "Iteration 613, loss = 3.04132634\n",
      "Iteration 614, loss = 3.04276718\n",
      "Iteration 615, loss = 3.04289975\n",
      "Iteration 616, loss = 3.04117696\n",
      "Iteration 617, loss = 3.04177308\n",
      "Iteration 618, loss = 3.04070589\n",
      "Iteration 619, loss = 3.04125234\n",
      "Iteration 620, loss = 3.04174294\n",
      "Iteration 621, loss = 3.04156724\n",
      "Iteration 622, loss = 3.04109031\n",
      "Iteration 623, loss = 3.04140292\n",
      "Iteration 624, loss = 3.04185324\n",
      "Iteration 625, loss = 3.04086616\n",
      "Iteration 626, loss = 3.04023648\n",
      "Iteration 627, loss = 3.04063905\n",
      "Iteration 628, loss = 3.04089295\n",
      "Iteration 629, loss = 3.03990008\n",
      "Iteration 630, loss = 3.04001835\n",
      "Iteration 631, loss = 3.03991509\n",
      "Iteration 632, loss = 3.03945598\n",
      "Iteration 633, loss = 3.03939334\n",
      "Iteration 634, loss = 3.03935982\n",
      "Iteration 635, loss = 3.03914436\n",
      "Iteration 636, loss = 3.03934105\n",
      "Iteration 637, loss = 3.03961478\n",
      "Iteration 638, loss = 3.03904426\n",
      "Iteration 639, loss = 3.03811825\n",
      "Iteration 640, loss = 3.03818451\n",
      "Iteration 641, loss = 3.03798676\n",
      "Iteration 642, loss = 3.03765242\n",
      "Iteration 643, loss = 3.03778825\n",
      "Iteration 644, loss = 3.03789332\n",
      "Iteration 645, loss = 3.03824708\n",
      "Iteration 646, loss = 3.03793342\n",
      "Iteration 647, loss = 3.03862747\n",
      "Iteration 648, loss = 3.03736432\n",
      "Iteration 649, loss = 3.03760728\n",
      "Iteration 650, loss = 3.03742902\n",
      "Iteration 651, loss = 3.03850153\n",
      "Iteration 652, loss = 3.03826113\n",
      "Iteration 653, loss = 3.03672380\n",
      "Iteration 654, loss = 3.03558761\n",
      "Iteration 655, loss = 3.03562095\n",
      "Iteration 656, loss = 3.03540653\n",
      "Iteration 657, loss = 3.03525352\n",
      "Iteration 658, loss = 3.03507135\n",
      "Iteration 659, loss = 3.03556257\n",
      "Iteration 660, loss = 3.03557161\n",
      "Iteration 661, loss = 3.03640053\n",
      "Iteration 662, loss = 3.03641717\n",
      "Iteration 663, loss = 3.03616573\n",
      "Iteration 664, loss = 3.03584905\n",
      "Iteration 665, loss = 3.03531558\n",
      "Iteration 666, loss = 3.03692372\n",
      "Iteration 667, loss = 3.03548055\n",
      "Iteration 668, loss = 3.03406797\n",
      "Iteration 669, loss = 3.03509657\n",
      "Iteration 670, loss = 3.03541608\n",
      "Iteration 671, loss = 3.03472300\n",
      "Iteration 672, loss = 3.03342216\n",
      "Iteration 673, loss = 3.03391552\n",
      "Iteration 674, loss = 3.03353096\n",
      "Iteration 675, loss = 3.03343199\n",
      "Iteration 676, loss = 3.03317998\n",
      "Iteration 677, loss = 3.03337754\n",
      "Iteration 678, loss = 3.03391166\n",
      "Iteration 679, loss = 3.03323172\n",
      "Iteration 680, loss = 3.03332743\n",
      "Iteration 681, loss = 3.03268880\n",
      "Iteration 682, loss = 3.03339740\n",
      "Iteration 683, loss = 3.03399129\n",
      "Iteration 684, loss = 3.03348137\n",
      "Iteration 685, loss = 3.03222532\n",
      "Iteration 686, loss = 3.03206787\n",
      "Iteration 687, loss = 3.03262243\n",
      "Iteration 688, loss = 3.03298477\n",
      "Iteration 689, loss = 3.03304656\n",
      "Iteration 690, loss = 3.03303783\n",
      "Iteration 691, loss = 3.03115529\n",
      "Iteration 692, loss = 3.03361777\n",
      "Iteration 693, loss = 3.03218933\n",
      "Iteration 694, loss = 3.03212707\n",
      "Iteration 695, loss = 3.03123376\n",
      "Iteration 696, loss = 3.03112495\n",
      "Iteration 697, loss = 3.03100051\n",
      "Iteration 698, loss = 3.02956358\n",
      "Iteration 699, loss = 3.03046175\n",
      "Iteration 700, loss = 3.03006613\n",
      "Iteration 701, loss = 3.02968605\n",
      "Iteration 702, loss = 3.02974433\n",
      "Iteration 703, loss = 3.02915308\n",
      "Iteration 704, loss = 3.03046871\n",
      "Iteration 705, loss = 3.03116106\n",
      "Iteration 706, loss = 3.02939842\n",
      "Iteration 707, loss = 3.03143344\n",
      "Iteration 708, loss = 3.02950432\n",
      "Iteration 709, loss = 3.03143641\n",
      "Iteration 710, loss = 3.03175202\n",
      "Iteration 711, loss = 3.03024854\n",
      "Iteration 712, loss = 3.02840157\n",
      "Iteration 713, loss = 3.02787321\n",
      "Iteration 714, loss = 3.02849080\n",
      "Iteration 715, loss = 3.02865899\n",
      "Iteration 716, loss = 3.02870839\n",
      "Iteration 717, loss = 3.02893004\n",
      "Iteration 718, loss = 3.02742683\n",
      "Iteration 719, loss = 3.02691290\n",
      "Iteration 720, loss = 3.02856821\n",
      "Iteration 721, loss = 3.02782068\n",
      "Iteration 722, loss = 3.02778959\n",
      "Iteration 723, loss = 3.02742363\n",
      "Iteration 724, loss = 3.02718847\n",
      "Iteration 725, loss = 3.02689863\n",
      "Iteration 726, loss = 3.02673823\n",
      "Iteration 727, loss = 3.02935730\n",
      "Iteration 728, loss = 3.02694936\n",
      "Iteration 729, loss = 3.02752497\n",
      "Iteration 730, loss = 3.02799913\n",
      "Iteration 731, loss = 3.02722024\n",
      "Iteration 732, loss = 3.02692669\n",
      "Iteration 733, loss = 3.02675669\n",
      "Iteration 734, loss = 3.02680767\n",
      "Iteration 735, loss = 3.02653644\n",
      "Iteration 736, loss = 3.02656567\n",
      "Iteration 737, loss = 3.02530594\n",
      "Iteration 738, loss = 3.02551310\n",
      "Iteration 739, loss = 3.02422848\n",
      "Iteration 740, loss = 3.02485556\n",
      "Iteration 741, loss = 3.02499351\n",
      "Iteration 742, loss = 3.02432746\n",
      "Iteration 743, loss = 3.02406144\n",
      "Iteration 744, loss = 3.02518726\n",
      "Iteration 745, loss = 3.02497553\n",
      "Iteration 746, loss = 3.02385564\n",
      "Iteration 747, loss = 3.02459254\n",
      "Iteration 748, loss = 3.02380270\n",
      "Iteration 749, loss = 3.02419672\n",
      "Iteration 750, loss = 3.02486715\n",
      "Iteration 751, loss = 3.02477531\n",
      "Iteration 752, loss = 3.02482051\n",
      "Iteration 753, loss = 3.02370907\n",
      "Iteration 754, loss = 3.02303938\n",
      "Iteration 755, loss = 3.02255415\n",
      "Iteration 756, loss = 3.02266461\n",
      "Iteration 757, loss = 3.02350980\n",
      "Iteration 758, loss = 3.02273474\n",
      "Iteration 759, loss = 3.02420509\n",
      "Iteration 760, loss = 3.02291811\n",
      "Iteration 761, loss = 3.02252688\n",
      "Iteration 762, loss = 3.02278072\n",
      "Iteration 763, loss = 3.02279144\n",
      "Iteration 764, loss = 3.02418319\n",
      "Iteration 765, loss = 3.02218458\n",
      "Iteration 766, loss = 3.02268784\n",
      "Iteration 767, loss = 3.02237903\n",
      "Iteration 768, loss = 3.02257209\n",
      "Iteration 769, loss = 3.02391354\n",
      "Iteration 770, loss = 3.02074387\n",
      "Iteration 771, loss = 3.02335899\n",
      "Iteration 772, loss = 3.02367135\n",
      "Iteration 773, loss = 3.02125359\n",
      "Iteration 774, loss = 3.02469816\n",
      "Iteration 775, loss = 3.02302801\n",
      "Iteration 776, loss = 3.02107595\n",
      "Iteration 777, loss = 3.02068356\n",
      "Iteration 778, loss = 3.02027177\n",
      "Iteration 779, loss = 3.01988274\n",
      "Iteration 780, loss = 3.02005299\n",
      "Iteration 781, loss = 3.02010449\n",
      "Iteration 782, loss = 3.02011774\n",
      "Iteration 783, loss = 3.02047109\n",
      "Iteration 784, loss = 3.01962581\n",
      "Iteration 785, loss = 3.02012923\n",
      "Iteration 786, loss = 3.02058500\n",
      "Iteration 787, loss = 3.02004979\n",
      "Iteration 788, loss = 3.02018517\n",
      "Iteration 789, loss = 3.01956467\n",
      "Iteration 790, loss = 3.02029851\n",
      "Iteration 791, loss = 3.02021780\n",
      "Iteration 792, loss = 3.01869860\n",
      "Iteration 793, loss = 3.01919118\n",
      "Iteration 794, loss = 3.01929825\n",
      "Iteration 795, loss = 3.01981918\n",
      "Iteration 796, loss = 3.02023287\n",
      "Iteration 797, loss = 3.01792310\n",
      "Iteration 798, loss = 3.01933511\n",
      "Iteration 799, loss = 3.01719056\n",
      "Iteration 800, loss = 3.01941101\n",
      "Iteration 801, loss = 3.01819031\n",
      "Iteration 802, loss = 3.01923495\n",
      "Iteration 803, loss = 3.01807695\n",
      "Iteration 804, loss = 3.01811138\n",
      "Iteration 805, loss = 3.01857199\n",
      "Iteration 806, loss = 3.01793449\n",
      "Iteration 807, loss = 3.01951861\n",
      "Iteration 808, loss = 3.01903769\n",
      "Iteration 809, loss = 3.01730401\n",
      "Iteration 810, loss = 3.01666808\n",
      "Iteration 811, loss = 3.01852700\n",
      "Iteration 812, loss = 3.01665597\n",
      "Iteration 813, loss = 3.01778411\n",
      "Iteration 814, loss = 3.01693288\n",
      "Iteration 815, loss = 3.01682254\n",
      "Iteration 816, loss = 3.01627185\n",
      "Iteration 817, loss = 3.01680509\n",
      "Iteration 818, loss = 3.01568226\n",
      "Iteration 819, loss = 3.01614908\n",
      "Iteration 820, loss = 3.01613318\n",
      "Iteration 821, loss = 3.01523915\n",
      "Iteration 822, loss = 3.01515430\n",
      "Iteration 823, loss = 3.01596560\n",
      "Iteration 824, loss = 3.01573073\n",
      "Iteration 825, loss = 3.01452768\n",
      "Iteration 826, loss = 3.01492958\n",
      "Iteration 827, loss = 3.01461159\n",
      "Iteration 828, loss = 3.01520539\n",
      "Iteration 829, loss = 3.01499428\n",
      "Iteration 830, loss = 3.01400912\n",
      "Iteration 831, loss = 3.01487607\n",
      "Iteration 832, loss = 3.01577626\n",
      "Iteration 833, loss = 3.01484196\n",
      "Iteration 834, loss = 3.01445825\n",
      "Iteration 835, loss = 3.01529593\n",
      "Iteration 836, loss = 3.01425438\n",
      "Iteration 837, loss = 3.01446229\n",
      "Iteration 838, loss = 3.01461613\n",
      "Iteration 839, loss = 3.01459870\n",
      "Iteration 840, loss = 3.01450753\n",
      "Iteration 841, loss = 3.01337798\n",
      "Iteration 842, loss = 3.01252233\n",
      "Iteration 843, loss = 3.01526524\n",
      "Iteration 844, loss = 3.01500514\n",
      "Iteration 845, loss = 3.01323761\n",
      "Iteration 846, loss = 3.01388540\n",
      "Iteration 847, loss = 3.01284281\n",
      "Iteration 848, loss = 3.01243986\n",
      "Iteration 849, loss = 3.01359655\n",
      "Iteration 850, loss = 3.01318150\n",
      "Iteration 851, loss = 3.01251594\n",
      "Iteration 852, loss = 3.01223262\n",
      "Iteration 853, loss = 3.01226379\n",
      "Iteration 854, loss = 3.01214546\n",
      "Iteration 855, loss = 3.01134576\n",
      "Iteration 856, loss = 3.01168228\n",
      "Iteration 857, loss = 3.01107188\n",
      "Iteration 858, loss = 3.01141271\n",
      "Iteration 859, loss = 3.01244793\n",
      "Iteration 860, loss = 3.01161742\n",
      "Iteration 861, loss = 3.01141857\n",
      "Iteration 862, loss = 3.01013984\n",
      "Iteration 863, loss = 3.01544940\n",
      "Iteration 864, loss = 3.01374048\n",
      "Iteration 865, loss = 3.01088324\n",
      "Iteration 866, loss = 3.01049417\n",
      "Iteration 867, loss = 3.01068496\n",
      "Iteration 868, loss = 3.01106862\n",
      "Iteration 869, loss = 3.01030367\n",
      "Iteration 870, loss = 3.01017633\n",
      "Iteration 871, loss = 3.01076055\n",
      "Iteration 872, loss = 3.01029103\n",
      "Iteration 873, loss = 3.00966053\n",
      "Iteration 874, loss = 3.00989011\n",
      "Iteration 875, loss = 3.01075947\n",
      "Iteration 876, loss = 3.00995515\n",
      "Iteration 877, loss = 3.00891159\n",
      "Iteration 878, loss = 3.00922266\n",
      "Iteration 879, loss = 3.00914860\n",
      "Iteration 880, loss = 3.00937281\n",
      "Iteration 881, loss = 3.00989230\n",
      "Iteration 882, loss = 3.00866573\n",
      "Iteration 883, loss = 3.00922871\n",
      "Iteration 884, loss = 3.00993431\n",
      "Iteration 885, loss = 3.00841128\n",
      "Iteration 886, loss = 3.00977906\n",
      "Iteration 887, loss = 3.00865840\n",
      "Iteration 888, loss = 3.00836434\n",
      "Iteration 889, loss = 3.00928373\n",
      "Iteration 890, loss = 3.00923258\n",
      "Iteration 891, loss = 3.00993264\n",
      "Iteration 892, loss = 3.00941288\n",
      "Iteration 893, loss = 3.00711859\n",
      "Iteration 894, loss = 3.01068069\n",
      "Iteration 895, loss = 3.00917668\n",
      "Iteration 896, loss = 3.00914992\n",
      "Iteration 897, loss = 3.00831949\n",
      "Iteration 898, loss = 3.00813824\n",
      "Iteration 899, loss = 3.00739739\n",
      "Iteration 900, loss = 3.00640476\n",
      "Iteration 901, loss = 3.00688023\n",
      "Iteration 902, loss = 3.00643645\n",
      "Iteration 903, loss = 3.00734380\n",
      "Iteration 904, loss = 3.00664741\n",
      "Iteration 905, loss = 3.00633399\n",
      "Iteration 906, loss = 3.00758831\n",
      "Iteration 907, loss = 3.00671654\n",
      "Iteration 908, loss = 3.00630004\n",
      "Iteration 909, loss = 3.00661080\n",
      "Iteration 910, loss = 3.00684024\n",
      "Iteration 911, loss = 3.00830831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23653368\n",
      "Iteration 2, loss = 4.13865820\n",
      "Iteration 3, loss = 4.04284325\n",
      "Iteration 4, loss = 3.94598342\n",
      "Iteration 5, loss = 3.84718521\n",
      "Iteration 6, loss = 3.74351921\n",
      "Iteration 7, loss = 3.63971546\n",
      "Iteration 8, loss = 3.53682344\n",
      "Iteration 9, loss = 3.44068351\n",
      "Iteration 10, loss = 3.35556728\n",
      "Iteration 11, loss = 3.29416654\n",
      "Iteration 12, loss = 3.25274274\n",
      "Iteration 13, loss = 3.23495768\n",
      "Iteration 14, loss = 3.23073299\n",
      "Iteration 15, loss = 3.23061265\n",
      "Iteration 16, loss = 3.22872128\n",
      "Iteration 17, loss = 3.22597939\n",
      "Iteration 18, loss = 3.22447023\n",
      "Iteration 19, loss = 3.22341929\n",
      "Iteration 20, loss = 3.22226048\n",
      "Iteration 21, loss = 3.22200307\n",
      "Iteration 22, loss = 3.22147816\n",
      "Iteration 23, loss = 3.22066695\n",
      "Iteration 24, loss = 3.21953142\n",
      "Iteration 25, loss = 3.21892732\n",
      "Iteration 26, loss = 3.21809843\n",
      "Iteration 27, loss = 3.21671635\n",
      "Iteration 28, loss = 3.21603265\n",
      "Iteration 29, loss = 3.21523191\n",
      "Iteration 30, loss = 3.21401485\n",
      "Iteration 31, loss = 3.21375895\n",
      "Iteration 32, loss = 3.21331346\n",
      "Iteration 33, loss = 3.21292564\n",
      "Iteration 34, loss = 3.21251836\n",
      "Iteration 35, loss = 3.21191262\n",
      "Iteration 36, loss = 3.21127340\n",
      "Iteration 37, loss = 3.20995253\n",
      "Iteration 38, loss = 3.20928149\n",
      "Iteration 39, loss = 3.20844183\n",
      "Iteration 40, loss = 3.20790016\n",
      "Iteration 41, loss = 3.20689562\n",
      "Iteration 42, loss = 3.20663351\n",
      "Iteration 43, loss = 3.20537713\n",
      "Iteration 44, loss = 3.20571659\n",
      "Iteration 45, loss = 3.20412845\n",
      "Iteration 46, loss = 3.20404429\n",
      "Iteration 47, loss = 3.20382966\n",
      "Iteration 48, loss = 3.20274585\n",
      "Iteration 49, loss = 3.20307255\n",
      "Iteration 50, loss = 3.20204713\n",
      "Iteration 51, loss = 3.20156826\n",
      "Iteration 52, loss = 3.20039261\n",
      "Iteration 53, loss = 3.19957281\n",
      "Iteration 54, loss = 3.19851632\n",
      "Iteration 55, loss = 3.19696710\n",
      "Iteration 56, loss = 3.19699045\n",
      "Iteration 57, loss = 3.19615594\n",
      "Iteration 58, loss = 3.19522887\n",
      "Iteration 59, loss = 3.19494748\n",
      "Iteration 60, loss = 3.19440060\n",
      "Iteration 61, loss = 3.19341120\n",
      "Iteration 62, loss = 3.19339845\n",
      "Iteration 63, loss = 3.19309782\n",
      "Iteration 64, loss = 3.19287664\n",
      "Iteration 65, loss = 3.19214609\n",
      "Iteration 66, loss = 3.19073493\n",
      "Iteration 67, loss = 3.18954049\n",
      "Iteration 68, loss = 3.18891185\n",
      "Iteration 69, loss = 3.18878794\n",
      "Iteration 70, loss = 3.18803993\n",
      "Iteration 71, loss = 3.18777496\n",
      "Iteration 72, loss = 3.18636218\n",
      "Iteration 73, loss = 3.18608362\n",
      "Iteration 74, loss = 3.18552415\n",
      "Iteration 75, loss = 3.18468173\n",
      "Iteration 76, loss = 3.18383206\n",
      "Iteration 77, loss = 3.18373480\n",
      "Iteration 78, loss = 3.18305877\n",
      "Iteration 79, loss = 3.18225556\n",
      "Iteration 80, loss = 3.18177256\n",
      "Iteration 81, loss = 3.17989646\n",
      "Iteration 82, loss = 3.17969944\n",
      "Iteration 83, loss = 3.17883783\n",
      "Iteration 84, loss = 3.17893317\n",
      "Iteration 85, loss = 3.17842580\n",
      "Iteration 86, loss = 3.17796102\n",
      "Iteration 87, loss = 3.17729619\n",
      "Iteration 88, loss = 3.17744718\n",
      "Iteration 89, loss = 3.17639152\n",
      "Iteration 90, loss = 3.17588744\n",
      "Iteration 91, loss = 3.17500757\n",
      "Iteration 92, loss = 3.17395400\n",
      "Iteration 93, loss = 3.17410856\n",
      "Iteration 94, loss = 3.17350700\n",
      "Iteration 95, loss = 3.17269751\n",
      "Iteration 96, loss = 3.17197444\n",
      "Iteration 97, loss = 3.17175068\n",
      "Iteration 98, loss = 3.17051958\n",
      "Iteration 99, loss = 3.17013776\n",
      "Iteration 100, loss = 3.17002649\n",
      "Iteration 101, loss = 3.16922472\n",
      "Iteration 102, loss = 3.16773131\n",
      "Iteration 103, loss = 3.16776200\n",
      "Iteration 104, loss = 3.16749820\n",
      "Iteration 105, loss = 3.16764394\n",
      "Iteration 106, loss = 3.16629384\n",
      "Iteration 107, loss = 3.16522400\n",
      "Iteration 108, loss = 3.16483088\n",
      "Iteration 109, loss = 3.16472099\n",
      "Iteration 110, loss = 3.16444050\n",
      "Iteration 111, loss = 3.16332532\n",
      "Iteration 112, loss = 3.16260449\n",
      "Iteration 113, loss = 3.16204725\n",
      "Iteration 114, loss = 3.16139478\n",
      "Iteration 115, loss = 3.16112123\n",
      "Iteration 116, loss = 3.16065676\n",
      "Iteration 117, loss = 3.16042683\n",
      "Iteration 118, loss = 3.16020893\n",
      "Iteration 119, loss = 3.15957814\n",
      "Iteration 120, loss = 3.15898278\n",
      "Iteration 121, loss = 3.15829168\n",
      "Iteration 122, loss = 3.15827371\n",
      "Iteration 123, loss = 3.15883752\n",
      "Iteration 124, loss = 3.15789997\n",
      "Iteration 125, loss = 3.15705983\n",
      "Iteration 126, loss = 3.15725945\n",
      "Iteration 127, loss = 3.15664717\n",
      "Iteration 128, loss = 3.15494401\n",
      "Iteration 129, loss = 3.15430439\n",
      "Iteration 130, loss = 3.15385003\n",
      "Iteration 131, loss = 3.15456811\n",
      "Iteration 132, loss = 3.15544741\n",
      "Iteration 133, loss = 3.15469195\n",
      "Iteration 134, loss = 3.15334275\n",
      "Iteration 135, loss = 3.15207806\n",
      "Iteration 136, loss = 3.15193885\n",
      "Iteration 137, loss = 3.15167718\n",
      "Iteration 138, loss = 3.15104459\n",
      "Iteration 139, loss = 3.15082967\n",
      "Iteration 140, loss = 3.15001974\n",
      "Iteration 141, loss = 3.14975468\n",
      "Iteration 142, loss = 3.14886426\n",
      "Iteration 143, loss = 3.14792472\n",
      "Iteration 144, loss = 3.14815910\n",
      "Iteration 145, loss = 3.14801500\n",
      "Iteration 146, loss = 3.14795178\n",
      "Iteration 147, loss = 3.14727717\n",
      "Iteration 148, loss = 3.14639178\n",
      "Iteration 149, loss = 3.14655297\n",
      "Iteration 150, loss = 3.14703128\n",
      "Iteration 151, loss = 3.14511195\n",
      "Iteration 152, loss = 3.14506324\n",
      "Iteration 153, loss = 3.14513238\n",
      "Iteration 154, loss = 3.14452719\n",
      "Iteration 155, loss = 3.14423866\n",
      "Iteration 156, loss = 3.14349162\n",
      "Iteration 157, loss = 3.14467949\n",
      "Iteration 158, loss = 3.14358283\n",
      "Iteration 159, loss = 3.14208963\n",
      "Iteration 160, loss = 3.14144602\n",
      "Iteration 161, loss = 3.14141659\n",
      "Iteration 162, loss = 3.14110436\n",
      "Iteration 163, loss = 3.14067720\n",
      "Iteration 164, loss = 3.13982702\n",
      "Iteration 165, loss = 3.13969231\n",
      "Iteration 166, loss = 3.13990707\n",
      "Iteration 167, loss = 3.13867615\n",
      "Iteration 168, loss = 3.13923120\n",
      "Iteration 169, loss = 3.13840433\n",
      "Iteration 170, loss = 3.13770352\n",
      "Iteration 171, loss = 3.13803384\n",
      "Iteration 172, loss = 3.13788518\n",
      "Iteration 173, loss = 3.13695718\n",
      "Iteration 174, loss = 3.13664167\n",
      "Iteration 175, loss = 3.13658511\n",
      "Iteration 176, loss = 3.13603988\n",
      "Iteration 177, loss = 3.13546056\n",
      "Iteration 178, loss = 3.13538806\n",
      "Iteration 179, loss = 3.13512955\n",
      "Iteration 180, loss = 3.13526223\n",
      "Iteration 181, loss = 3.13371322\n",
      "Iteration 182, loss = 3.13399659\n",
      "Iteration 183, loss = 3.13330736\n",
      "Iteration 184, loss = 3.13316910\n",
      "Iteration 185, loss = 3.13375693\n",
      "Iteration 186, loss = 3.13347370\n",
      "Iteration 187, loss = 3.13326249\n",
      "Iteration 188, loss = 3.13335159\n",
      "Iteration 189, loss = 3.13180144\n",
      "Iteration 190, loss = 3.13175365\n",
      "Iteration 191, loss = 3.13165957\n",
      "Iteration 192, loss = 3.13009482\n",
      "Iteration 193, loss = 3.13032689\n",
      "Iteration 194, loss = 3.12970881\n",
      "Iteration 195, loss = 3.13090113\n",
      "Iteration 196, loss = 3.12931115\n",
      "Iteration 197, loss = 3.12979096\n",
      "Iteration 198, loss = 3.12831693\n",
      "Iteration 199, loss = 3.12817745\n",
      "Iteration 200, loss = 3.12803125\n",
      "Iteration 201, loss = 3.12811294\n",
      "Iteration 202, loss = 3.12771316\n",
      "Iteration 203, loss = 3.12761280\n",
      "Iteration 204, loss = 3.12716754\n",
      "Iteration 205, loss = 3.12633432\n",
      "Iteration 206, loss = 3.12618176\n",
      "Iteration 207, loss = 3.12595040\n",
      "Iteration 208, loss = 3.12519434\n",
      "Iteration 209, loss = 3.12466393\n",
      "Iteration 210, loss = 3.12474218\n",
      "Iteration 211, loss = 3.12509631\n",
      "Iteration 212, loss = 3.12448701\n",
      "Iteration 213, loss = 3.12342223\n",
      "Iteration 214, loss = 3.12302078\n",
      "Iteration 215, loss = 3.12289081\n",
      "Iteration 216, loss = 3.12212284\n",
      "Iteration 217, loss = 3.12231524\n",
      "Iteration 218, loss = 3.12168292\n",
      "Iteration 219, loss = 3.12142092\n",
      "Iteration 220, loss = 3.12292079\n",
      "Iteration 221, loss = 3.12211869\n",
      "Iteration 222, loss = 3.12148946\n",
      "Iteration 223, loss = 3.12091080\n",
      "Iteration 224, loss = 3.12081280\n",
      "Iteration 225, loss = 3.12024996\n",
      "Iteration 226, loss = 3.11983837\n",
      "Iteration 227, loss = 3.11940718\n",
      "Iteration 228, loss = 3.11919915\n",
      "Iteration 229, loss = 3.11922919\n",
      "Iteration 230, loss = 3.11981184\n",
      "Iteration 231, loss = 3.12007695\n",
      "Iteration 232, loss = 3.11849895\n",
      "Iteration 233, loss = 3.11822192\n",
      "Iteration 234, loss = 3.11840916\n",
      "Iteration 235, loss = 3.11686788\n",
      "Iteration 236, loss = 3.11657805\n",
      "Iteration 237, loss = 3.11631056\n",
      "Iteration 238, loss = 3.11630271\n",
      "Iteration 239, loss = 3.11573648\n",
      "Iteration 240, loss = 3.11579019\n",
      "Iteration 241, loss = 3.11517437\n",
      "Iteration 242, loss = 3.11463216\n",
      "Iteration 243, loss = 3.11484055\n",
      "Iteration 244, loss = 3.11418227\n",
      "Iteration 245, loss = 3.11379176\n",
      "Iteration 246, loss = 3.11637938\n",
      "Iteration 247, loss = 3.11509456\n",
      "Iteration 248, loss = 3.11501819\n",
      "Iteration 249, loss = 3.11412709\n",
      "Iteration 250, loss = 3.11368164\n",
      "Iteration 251, loss = 3.11264492\n",
      "Iteration 252, loss = 3.11297440\n",
      "Iteration 253, loss = 3.11172822\n",
      "Iteration 254, loss = 3.11128844\n",
      "Iteration 255, loss = 3.11296365\n",
      "Iteration 256, loss = 3.11261213\n",
      "Iteration 257, loss = 3.11175655\n",
      "Iteration 258, loss = 3.11118903\n",
      "Iteration 259, loss = 3.11170301\n",
      "Iteration 260, loss = 3.11123360\n",
      "Iteration 261, loss = 3.11022731\n",
      "Iteration 262, loss = 3.10940888\n",
      "Iteration 263, loss = 3.11013464\n",
      "Iteration 264, loss = 3.10932597\n",
      "Iteration 265, loss = 3.10943687\n",
      "Iteration 266, loss = 3.10858012\n",
      "Iteration 267, loss = 3.10795067\n",
      "Iteration 268, loss = 3.10800891\n",
      "Iteration 269, loss = 3.10698096\n",
      "Iteration 270, loss = 3.10891287\n",
      "Iteration 271, loss = 3.10897428\n",
      "Iteration 272, loss = 3.10663549\n",
      "Iteration 273, loss = 3.10644620\n",
      "Iteration 274, loss = 3.10594406\n",
      "Iteration 275, loss = 3.10632940\n",
      "Iteration 276, loss = 3.10576940\n",
      "Iteration 277, loss = 3.10573115\n",
      "Iteration 278, loss = 3.10647918\n",
      "Iteration 279, loss = 3.10516812\n",
      "Iteration 280, loss = 3.10460533\n",
      "Iteration 281, loss = 3.10725458\n",
      "Iteration 282, loss = 3.10582464\n",
      "Iteration 283, loss = 3.10427616\n",
      "Iteration 284, loss = 3.10588270\n",
      "Iteration 285, loss = 3.10340868\n",
      "Iteration 286, loss = 3.10331078\n",
      "Iteration 287, loss = 3.10335854\n",
      "Iteration 288, loss = 3.10319958\n",
      "Iteration 289, loss = 3.10282226\n",
      "Iteration 290, loss = 3.10207439\n",
      "Iteration 291, loss = 3.10137244\n",
      "Iteration 292, loss = 3.10144025\n",
      "Iteration 293, loss = 3.10137205\n",
      "Iteration 294, loss = 3.10120166\n",
      "Iteration 295, loss = 3.10125319\n",
      "Iteration 296, loss = 3.10081551\n",
      "Iteration 297, loss = 3.10079579\n",
      "Iteration 298, loss = 3.10079554\n",
      "Iteration 299, loss = 3.10255233\n",
      "Iteration 300, loss = 3.09951035\n",
      "Iteration 301, loss = 3.09957808\n",
      "Iteration 302, loss = 3.09941017\n",
      "Iteration 303, loss = 3.09980522\n",
      "Iteration 304, loss = 3.09964559\n",
      "Iteration 305, loss = 3.09904604\n",
      "Iteration 306, loss = 3.09925738\n",
      "Iteration 307, loss = 3.09829921\n",
      "Iteration 308, loss = 3.09777248\n",
      "Iteration 309, loss = 3.09709341\n",
      "Iteration 310, loss = 3.09775757\n",
      "Iteration 311, loss = 3.09838350\n",
      "Iteration 312, loss = 3.09740108\n",
      "Iteration 313, loss = 3.09620664\n",
      "Iteration 314, loss = 3.09675078\n",
      "Iteration 315, loss = 3.09575712\n",
      "Iteration 316, loss = 3.09540457\n",
      "Iteration 317, loss = 3.09825421\n",
      "Iteration 318, loss = 3.09797454\n",
      "Iteration 319, loss = 3.09515109\n",
      "Iteration 320, loss = 3.09575492\n",
      "Iteration 321, loss = 3.09521796\n",
      "Iteration 322, loss = 3.09434111\n",
      "Iteration 323, loss = 3.09459566\n",
      "Iteration 324, loss = 3.09363457\n",
      "Iteration 325, loss = 3.09333052\n",
      "Iteration 326, loss = 3.09362974\n",
      "Iteration 327, loss = 3.09363729\n",
      "Iteration 328, loss = 3.09470012\n",
      "Iteration 329, loss = 3.09850072\n",
      "Iteration 330, loss = 3.09517895\n",
      "Iteration 331, loss = 3.09571050\n",
      "Iteration 332, loss = 3.09261296\n",
      "Iteration 333, loss = 3.09487592\n",
      "Iteration 334, loss = 3.09298962\n",
      "Iteration 335, loss = 3.09250206\n",
      "Iteration 336, loss = 3.09278945\n",
      "Iteration 337, loss = 3.09060493\n",
      "Iteration 338, loss = 3.09026958\n",
      "Iteration 339, loss = 3.09272105\n",
      "Iteration 340, loss = 3.09172380\n",
      "Iteration 341, loss = 3.09010341\n",
      "Iteration 342, loss = 3.09086806\n",
      "Iteration 343, loss = 3.08970413\n",
      "Iteration 344, loss = 3.08816136\n",
      "Iteration 345, loss = 3.08940233\n",
      "Iteration 346, loss = 3.08836671\n",
      "Iteration 347, loss = 3.08786210\n",
      "Iteration 348, loss = 3.08883564\n",
      "Iteration 349, loss = 3.08784456\n",
      "Iteration 350, loss = 3.08808734\n",
      "Iteration 351, loss = 3.08934504\n",
      "Iteration 352, loss = 3.08686203\n",
      "Iteration 353, loss = 3.08660956\n",
      "Iteration 354, loss = 3.08688333\n",
      "Iteration 355, loss = 3.08725555\n",
      "Iteration 356, loss = 3.08661907\n",
      "Iteration 357, loss = 3.08635912\n",
      "Iteration 358, loss = 3.08666881\n",
      "Iteration 359, loss = 3.08637108\n",
      "Iteration 360, loss = 3.08538237\n",
      "Iteration 361, loss = 3.08464183\n",
      "Iteration 362, loss = 3.08522173\n",
      "Iteration 363, loss = 3.08484499\n",
      "Iteration 364, loss = 3.08451326\n",
      "Iteration 365, loss = 3.08466568\n",
      "Iteration 366, loss = 3.08482453\n",
      "Iteration 367, loss = 3.08572846\n",
      "Iteration 368, loss = 3.08269478\n",
      "Iteration 369, loss = 3.08278682\n",
      "Iteration 370, loss = 3.08348794\n",
      "Iteration 371, loss = 3.08328793\n",
      "Iteration 372, loss = 3.08296407\n",
      "Iteration 373, loss = 3.08248303\n",
      "Iteration 374, loss = 3.08212182\n",
      "Iteration 375, loss = 3.08196062\n",
      "Iteration 376, loss = 3.08131602\n",
      "Iteration 377, loss = 3.08239116\n",
      "Iteration 378, loss = 3.08246077\n",
      "Iteration 379, loss = 3.08141502\n",
      "Iteration 380, loss = 3.08066781\n",
      "Iteration 381, loss = 3.08018761\n",
      "Iteration 382, loss = 3.08075827\n",
      "Iteration 383, loss = 3.08100088\n",
      "Iteration 384, loss = 3.08000012\n",
      "Iteration 385, loss = 3.08081503\n",
      "Iteration 386, loss = 3.07920034\n",
      "Iteration 387, loss = 3.07992621\n",
      "Iteration 388, loss = 3.07960265\n",
      "Iteration 389, loss = 3.07852361\n",
      "Iteration 390, loss = 3.07881560\n",
      "Iteration 391, loss = 3.07776348\n",
      "Iteration 392, loss = 3.07822572\n",
      "Iteration 393, loss = 3.07824377\n",
      "Iteration 394, loss = 3.07738401\n",
      "Iteration 395, loss = 3.07801202\n",
      "Iteration 396, loss = 3.07781041\n",
      "Iteration 397, loss = 3.07641995\n",
      "Iteration 398, loss = 3.07699016\n",
      "Iteration 399, loss = 3.07655166\n",
      "Iteration 400, loss = 3.07695383\n",
      "Iteration 401, loss = 3.07670282\n",
      "Iteration 402, loss = 3.07614341\n",
      "Iteration 403, loss = 3.07646341\n",
      "Iteration 404, loss = 3.07629324\n",
      "Iteration 405, loss = 3.07519134\n",
      "Iteration 406, loss = 3.07502939\n",
      "Iteration 407, loss = 3.07476558\n",
      "Iteration 408, loss = 3.07505844\n",
      "Iteration 409, loss = 3.07387022\n",
      "Iteration 410, loss = 3.07477252\n",
      "Iteration 411, loss = 3.07355389\n",
      "Iteration 412, loss = 3.07356687\n",
      "Iteration 413, loss = 3.07389476\n",
      "Iteration 414, loss = 3.07531867\n",
      "Iteration 415, loss = 3.07399675\n",
      "Iteration 416, loss = 3.07330572\n",
      "Iteration 417, loss = 3.07282647\n",
      "Iteration 418, loss = 3.07265130\n",
      "Iteration 419, loss = 3.07232015\n",
      "Iteration 420, loss = 3.07289003\n",
      "Iteration 421, loss = 3.07271847\n",
      "Iteration 422, loss = 3.07203887\n",
      "Iteration 423, loss = 3.07069300\n",
      "Iteration 424, loss = 3.07057406\n",
      "Iteration 425, loss = 3.07094297\n",
      "Iteration 426, loss = 3.07027216\n",
      "Iteration 427, loss = 3.07100197\n",
      "Iteration 428, loss = 3.07087198\n",
      "Iteration 429, loss = 3.07088229\n",
      "Iteration 430, loss = 3.07067832\n",
      "Iteration 431, loss = 3.07025704\n",
      "Iteration 432, loss = 3.07018658\n",
      "Iteration 433, loss = 3.06911995\n",
      "Iteration 434, loss = 3.06914590\n",
      "Iteration 435, loss = 3.06883378\n",
      "Iteration 436, loss = 3.06876401\n",
      "Iteration 437, loss = 3.06954490\n",
      "Iteration 438, loss = 3.06827403\n",
      "Iteration 439, loss = 3.06805510\n",
      "Iteration 440, loss = 3.06803841\n",
      "Iteration 441, loss = 3.06773973\n",
      "Iteration 442, loss = 3.06663173\n",
      "Iteration 443, loss = 3.06687436\n",
      "Iteration 444, loss = 3.06762770\n",
      "Iteration 445, loss = 3.06679764\n",
      "Iteration 446, loss = 3.06612347\n",
      "Iteration 447, loss = 3.06661692\n",
      "Iteration 448, loss = 3.06589658\n",
      "Iteration 449, loss = 3.06560250\n",
      "Iteration 450, loss = 3.06673615\n",
      "Iteration 451, loss = 3.06518715\n",
      "Iteration 452, loss = 3.06504184\n",
      "Iteration 453, loss = 3.06478184\n",
      "Iteration 454, loss = 3.06525391\n",
      "Iteration 455, loss = 3.06502891\n",
      "Iteration 456, loss = 3.06447025\n",
      "Iteration 457, loss = 3.06520625\n",
      "Iteration 458, loss = 3.06438498\n",
      "Iteration 459, loss = 3.06426041\n",
      "Iteration 460, loss = 3.06444377\n",
      "Iteration 461, loss = 3.06464273\n",
      "Iteration 462, loss = 3.06399563\n",
      "Iteration 463, loss = 3.06470550\n",
      "Iteration 464, loss = 3.06388748\n",
      "Iteration 465, loss = 3.06315839\n",
      "Iteration 466, loss = 3.06278591\n",
      "Iteration 467, loss = 3.06308857\n",
      "Iteration 468, loss = 3.06259813\n",
      "Iteration 469, loss = 3.06257786\n",
      "Iteration 470, loss = 3.06182967\n",
      "Iteration 471, loss = 3.06138881\n",
      "Iteration 472, loss = 3.06193045\n",
      "Iteration 473, loss = 3.06189614\n",
      "Iteration 474, loss = 3.06112875\n",
      "Iteration 475, loss = 3.06177760\n",
      "Iteration 476, loss = 3.06069660\n",
      "Iteration 477, loss = 3.06015563\n",
      "Iteration 478, loss = 3.05964480\n",
      "Iteration 479, loss = 3.05996985\n",
      "Iteration 480, loss = 3.05952544\n",
      "Iteration 481, loss = 3.05940056\n",
      "Iteration 482, loss = 3.05941466\n",
      "Iteration 483, loss = 3.05964321\n",
      "Iteration 484, loss = 3.05836688\n",
      "Iteration 485, loss = 3.05829980\n",
      "Iteration 486, loss = 3.05831379\n",
      "Iteration 487, loss = 3.05929854\n",
      "Iteration 488, loss = 3.05835222\n",
      "Iteration 489, loss = 3.05716267\n",
      "Iteration 490, loss = 3.05733276\n",
      "Iteration 491, loss = 3.05715381\n",
      "Iteration 492, loss = 3.05754148\n",
      "Iteration 493, loss = 3.05712456\n",
      "Iteration 494, loss = 3.05738263\n",
      "Iteration 495, loss = 3.05716056\n",
      "Iteration 496, loss = 3.05817703\n",
      "Iteration 497, loss = 3.05778370\n",
      "Iteration 498, loss = 3.05821352\n",
      "Iteration 499, loss = 3.05731962\n",
      "Iteration 500, loss = 3.05594239\n",
      "Iteration 501, loss = 3.05546494\n",
      "Iteration 502, loss = 3.05500268\n",
      "Iteration 503, loss = 3.05484265\n",
      "Iteration 504, loss = 3.05450442\n",
      "Iteration 505, loss = 3.05607626\n",
      "Iteration 506, loss = 3.05516095\n",
      "Iteration 507, loss = 3.05475881\n",
      "Iteration 508, loss = 3.05425658\n",
      "Iteration 509, loss = 3.05428254\n",
      "Iteration 510, loss = 3.05435595\n",
      "Iteration 511, loss = 3.05524720\n",
      "Iteration 512, loss = 3.05425855\n",
      "Iteration 513, loss = 3.05279482\n",
      "Iteration 514, loss = 3.05294373\n",
      "Iteration 515, loss = 3.05430906\n",
      "Iteration 516, loss = 3.05303956\n",
      "Iteration 517, loss = 3.05312569\n",
      "Iteration 518, loss = 3.05183025\n",
      "Iteration 519, loss = 3.05199167\n",
      "Iteration 520, loss = 3.05244194\n",
      "Iteration 521, loss = 3.05239780\n",
      "Iteration 522, loss = 3.05189512\n",
      "Iteration 523, loss = 3.05117022\n",
      "Iteration 524, loss = 3.05111397\n",
      "Iteration 525, loss = 3.05018960\n",
      "Iteration 526, loss = 3.05140134\n",
      "Iteration 527, loss = 3.05251347\n",
      "Iteration 528, loss = 3.05087801\n",
      "Iteration 529, loss = 3.05126857\n",
      "Iteration 530, loss = 3.05234644\n",
      "Iteration 531, loss = 3.04989072\n",
      "Iteration 532, loss = 3.04942854\n",
      "Iteration 533, loss = 3.05008938\n",
      "Iteration 534, loss = 3.05026957\n",
      "Iteration 535, loss = 3.04979768\n",
      "Iteration 536, loss = 3.04907897\n",
      "Iteration 537, loss = 3.04882987\n",
      "Iteration 538, loss = 3.04904622\n",
      "Iteration 539, loss = 3.04990641\n",
      "Iteration 540, loss = 3.04862344\n",
      "Iteration 541, loss = 3.04747242\n",
      "Iteration 542, loss = 3.04725167\n",
      "Iteration 543, loss = 3.04799512\n",
      "Iteration 544, loss = 3.04732127\n",
      "Iteration 545, loss = 3.04790444\n",
      "Iteration 546, loss = 3.04880195\n",
      "Iteration 547, loss = 3.04618098\n",
      "Iteration 548, loss = 3.04642094\n",
      "Iteration 549, loss = 3.04720695\n",
      "Iteration 550, loss = 3.04674566\n",
      "Iteration 551, loss = 3.04754394\n",
      "Iteration 552, loss = 3.04729954\n",
      "Iteration 553, loss = 3.04648361\n",
      "Iteration 554, loss = 3.04721820\n",
      "Iteration 555, loss = 3.04580448\n",
      "Iteration 556, loss = 3.04540395\n",
      "Iteration 557, loss = 3.04403867\n",
      "Iteration 558, loss = 3.04465865\n",
      "Iteration 559, loss = 3.04443517\n",
      "Iteration 560, loss = 3.04484593\n",
      "Iteration 561, loss = 3.04391259\n",
      "Iteration 562, loss = 3.04501533\n",
      "Iteration 563, loss = 3.04489473\n",
      "Iteration 564, loss = 3.04476229\n",
      "Iteration 565, loss = 3.04373768\n",
      "Iteration 566, loss = 3.04337754\n",
      "Iteration 567, loss = 3.04321615\n",
      "Iteration 568, loss = 3.04380848\n",
      "Iteration 569, loss = 3.04296813\n",
      "Iteration 570, loss = 3.04237247\n",
      "Iteration 571, loss = 3.04697254\n",
      "Iteration 572, loss = 3.04530759\n",
      "Iteration 573, loss = 3.04209046\n",
      "Iteration 574, loss = 3.04290166\n",
      "Iteration 575, loss = 3.04268041\n",
      "Iteration 576, loss = 3.04235560\n",
      "Iteration 577, loss = 3.04244713\n",
      "Iteration 578, loss = 3.04342193\n",
      "Iteration 579, loss = 3.04106454\n",
      "Iteration 580, loss = 3.04125167\n",
      "Iteration 581, loss = 3.04138035\n",
      "Iteration 582, loss = 3.04071133\n",
      "Iteration 583, loss = 3.04071679\n",
      "Iteration 584, loss = 3.03999840\n",
      "Iteration 585, loss = 3.04127312\n",
      "Iteration 586, loss = 3.04077631\n",
      "Iteration 587, loss = 3.04090685\n",
      "Iteration 588, loss = 3.04040088\n",
      "Iteration 589, loss = 3.03988012\n",
      "Iteration 590, loss = 3.03954214\n",
      "Iteration 591, loss = 3.03940218\n",
      "Iteration 592, loss = 3.03888701\n",
      "Iteration 593, loss = 3.03958662\n",
      "Iteration 594, loss = 3.03814915\n",
      "Iteration 595, loss = 3.03855281\n",
      "Iteration 596, loss = 3.03840020\n",
      "Iteration 597, loss = 3.03927811\n",
      "Iteration 598, loss = 3.03923372\n",
      "Iteration 599, loss = 3.03858941\n",
      "Iteration 600, loss = 3.04005877\n",
      "Iteration 601, loss = 3.03870817\n",
      "Iteration 602, loss = 3.03740075\n",
      "Iteration 603, loss = 3.03783173\n",
      "Iteration 604, loss = 3.03772014\n",
      "Iteration 605, loss = 3.03691325\n",
      "Iteration 606, loss = 3.03763194\n",
      "Iteration 607, loss = 3.03618329\n",
      "Iteration 608, loss = 3.03649599\n",
      "Iteration 609, loss = 3.03513708\n",
      "Iteration 610, loss = 3.03645925\n",
      "Iteration 611, loss = 3.03792780\n",
      "Iteration 612, loss = 3.03739327\n",
      "Iteration 613, loss = 3.03527534\n",
      "Iteration 614, loss = 3.03614131\n",
      "Iteration 615, loss = 3.03558273\n",
      "Iteration 616, loss = 3.03663583\n",
      "Iteration 617, loss = 3.03557044\n",
      "Iteration 618, loss = 3.03486861\n",
      "Iteration 619, loss = 3.03502543\n",
      "Iteration 620, loss = 3.03418782\n",
      "Iteration 621, loss = 3.03393823\n",
      "Iteration 622, loss = 3.03467720\n",
      "Iteration 623, loss = 3.03450169\n",
      "Iteration 624, loss = 3.03397034\n",
      "Iteration 625, loss = 3.03459805\n",
      "Iteration 626, loss = 3.03362964\n",
      "Iteration 627, loss = 3.03391406\n",
      "Iteration 628, loss = 3.03433903\n",
      "Iteration 629, loss = 3.03352634\n",
      "Iteration 630, loss = 3.03369039\n",
      "Iteration 631, loss = 3.03291085\n",
      "Iteration 632, loss = 3.03243828\n",
      "Iteration 633, loss = 3.03269369\n",
      "Iteration 634, loss = 3.03216215\n",
      "Iteration 635, loss = 3.03164851\n",
      "Iteration 636, loss = 3.03237482\n",
      "Iteration 637, loss = 3.03190534\n",
      "Iteration 638, loss = 3.03189798\n",
      "Iteration 639, loss = 3.03118606\n",
      "Iteration 640, loss = 3.03126344\n",
      "Iteration 641, loss = 3.03039652\n",
      "Iteration 642, loss = 3.03002998\n",
      "Iteration 643, loss = 3.03063138\n",
      "Iteration 644, loss = 3.02978584\n",
      "Iteration 645, loss = 3.02997480\n",
      "Iteration 646, loss = 3.03003143\n",
      "Iteration 647, loss = 3.03138090\n",
      "Iteration 648, loss = 3.02905491\n",
      "Iteration 649, loss = 3.03006335\n",
      "Iteration 650, loss = 3.03012840\n",
      "Iteration 651, loss = 3.02906051\n",
      "Iteration 652, loss = 3.02856069\n",
      "Iteration 653, loss = 3.02993164\n",
      "Iteration 654, loss = 3.02926640\n",
      "Iteration 655, loss = 3.02953988\n",
      "Iteration 656, loss = 3.02885930\n",
      "Iteration 657, loss = 3.02878319\n",
      "Iteration 658, loss = 3.02783618\n",
      "Iteration 659, loss = 3.02797810\n",
      "Iteration 660, loss = 3.02817638\n",
      "Iteration 661, loss = 3.02722879\n",
      "Iteration 662, loss = 3.02777310\n",
      "Iteration 663, loss = 3.02854103\n",
      "Iteration 664, loss = 3.02712039\n",
      "Iteration 665, loss = 3.02753874\n",
      "Iteration 666, loss = 3.02775715\n",
      "Iteration 667, loss = 3.02698709\n",
      "Iteration 668, loss = 3.02600200\n",
      "Iteration 669, loss = 3.02710379\n",
      "Iteration 670, loss = 3.02621656\n",
      "Iteration 671, loss = 3.02678955\n",
      "Iteration 672, loss = 3.02570352\n",
      "Iteration 673, loss = 3.02549805\n",
      "Iteration 674, loss = 3.02526319\n",
      "Iteration 675, loss = 3.02517653\n",
      "Iteration 676, loss = 3.02547559\n",
      "Iteration 677, loss = 3.02562487\n",
      "Iteration 678, loss = 3.02500505\n",
      "Iteration 679, loss = 3.02479577\n",
      "Iteration 680, loss = 3.02441221\n",
      "Iteration 681, loss = 3.02500733\n",
      "Iteration 682, loss = 3.02568604\n",
      "Iteration 683, loss = 3.02536458\n",
      "Iteration 684, loss = 3.02406072\n",
      "Iteration 685, loss = 3.02391607\n",
      "Iteration 686, loss = 3.02799339\n",
      "Iteration 687, loss = 3.02402937\n",
      "Iteration 688, loss = 3.02341797\n",
      "Iteration 689, loss = 3.02392750\n",
      "Iteration 690, loss = 3.02226630\n",
      "Iteration 691, loss = 3.02264618\n",
      "Iteration 692, loss = 3.02269864\n",
      "Iteration 693, loss = 3.02298523\n",
      "Iteration 694, loss = 3.02312251\n",
      "Iteration 695, loss = 3.02192769\n",
      "Iteration 696, loss = 3.02254498\n",
      "Iteration 697, loss = 3.02235240\n",
      "Iteration 698, loss = 3.02253514\n",
      "Iteration 699, loss = 3.02204411\n",
      "Iteration 700, loss = 3.02161073\n",
      "Iteration 701, loss = 3.02241954\n",
      "Iteration 702, loss = 3.02162207\n",
      "Iteration 703, loss = 3.02155608\n",
      "Iteration 704, loss = 3.02145943\n",
      "Iteration 705, loss = 3.02216456\n",
      "Iteration 706, loss = 3.02239055\n",
      "Iteration 707, loss = 3.02076623\n",
      "Iteration 708, loss = 3.02069759\n",
      "Iteration 709, loss = 3.02073857\n",
      "Iteration 710, loss = 3.02032759\n",
      "Iteration 711, loss = 3.02003342\n",
      "Iteration 712, loss = 3.02139891\n",
      "Iteration 713, loss = 3.01949350\n",
      "Iteration 714, loss = 3.01911890\n",
      "Iteration 715, loss = 3.01953723\n",
      "Iteration 716, loss = 3.01971398\n",
      "Iteration 717, loss = 3.01946750\n",
      "Iteration 718, loss = 3.01911763\n",
      "Iteration 719, loss = 3.01788964\n",
      "Iteration 720, loss = 3.01816388\n",
      "Iteration 721, loss = 3.02083980\n",
      "Iteration 722, loss = 3.01929263\n",
      "Iteration 723, loss = 3.01798119\n",
      "Iteration 724, loss = 3.01934222\n",
      "Iteration 725, loss = 3.01758415\n",
      "Iteration 726, loss = 3.01887606\n",
      "Iteration 727, loss = 3.01842974\n",
      "Iteration 728, loss = 3.01852858\n",
      "Iteration 729, loss = 3.01820380\n",
      "Iteration 730, loss = 3.01612131\n",
      "Iteration 731, loss = 3.01839157\n",
      "Iteration 732, loss = 3.01808680\n",
      "Iteration 733, loss = 3.01803173\n",
      "Iteration 734, loss = 3.01768836\n",
      "Iteration 735, loss = 3.01688192\n",
      "Iteration 736, loss = 3.01672300\n",
      "Iteration 737, loss = 3.01678655\n",
      "Iteration 738, loss = 3.01622612\n",
      "Iteration 739, loss = 3.01612217\n",
      "Iteration 740, loss = 3.01695634\n",
      "Iteration 741, loss = 3.01653964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.23890746\n",
      "Iteration 2, loss = 4.13905171\n",
      "Iteration 3, loss = 4.04347525\n",
      "Iteration 4, loss = 3.94635863\n",
      "Iteration 5, loss = 3.84622494\n",
      "Iteration 6, loss = 3.74283068\n",
      "Iteration 7, loss = 3.63562060\n",
      "Iteration 8, loss = 3.53071508\n",
      "Iteration 9, loss = 3.43443021\n",
      "Iteration 10, loss = 3.35463672\n",
      "Iteration 11, loss = 3.29074984\n",
      "Iteration 12, loss = 3.25439275\n",
      "Iteration 13, loss = 3.24089635\n",
      "Iteration 14, loss = 3.23481577\n",
      "Iteration 15, loss = 3.23367183\n",
      "Iteration 16, loss = 3.23255307\n",
      "Iteration 17, loss = 3.23008063\n",
      "Iteration 18, loss = 3.22923180\n",
      "Iteration 19, loss = 3.22873840\n",
      "Iteration 20, loss = 3.22768902\n",
      "Iteration 21, loss = 3.22728896\n",
      "Iteration 22, loss = 3.22610796\n",
      "Iteration 23, loss = 3.22592475\n",
      "Iteration 24, loss = 3.22516362\n",
      "Iteration 25, loss = 3.22380488\n",
      "Iteration 26, loss = 3.22315344\n",
      "Iteration 27, loss = 3.22286016\n",
      "Iteration 28, loss = 3.22190543\n",
      "Iteration 29, loss = 3.22165669\n",
      "Iteration 30, loss = 3.22056353\n",
      "Iteration 31, loss = 3.21968419\n",
      "Iteration 32, loss = 3.21931705\n",
      "Iteration 33, loss = 3.21847083\n",
      "Iteration 34, loss = 3.21787673\n",
      "Iteration 35, loss = 3.21738864\n",
      "Iteration 36, loss = 3.21668876\n",
      "Iteration 37, loss = 3.21567133\n",
      "Iteration 38, loss = 3.21538268\n",
      "Iteration 39, loss = 3.21477340\n",
      "Iteration 40, loss = 3.21426733\n",
      "Iteration 41, loss = 3.21324703\n",
      "Iteration 42, loss = 3.21258792\n",
      "Iteration 43, loss = 3.21186687\n",
      "Iteration 44, loss = 3.21136346\n",
      "Iteration 45, loss = 3.21087954\n",
      "Iteration 46, loss = 3.21001820\n",
      "Iteration 47, loss = 3.20990163\n",
      "Iteration 48, loss = 3.21024816\n",
      "Iteration 49, loss = 3.20869070\n",
      "Iteration 50, loss = 3.20652436\n",
      "Iteration 51, loss = 3.20705523\n",
      "Iteration 52, loss = 3.20596109\n",
      "Iteration 53, loss = 3.20438972\n",
      "Iteration 54, loss = 3.20442467\n",
      "Iteration 55, loss = 3.20331259\n",
      "Iteration 56, loss = 3.20258539\n",
      "Iteration 57, loss = 3.20225551\n",
      "Iteration 58, loss = 3.20174165\n",
      "Iteration 59, loss = 3.20135287\n",
      "Iteration 60, loss = 3.20049489\n",
      "Iteration 61, loss = 3.19948043\n",
      "Iteration 62, loss = 3.19888968\n",
      "Iteration 63, loss = 3.19839627\n",
      "Iteration 64, loss = 3.19725508\n",
      "Iteration 65, loss = 3.19652989\n",
      "Iteration 66, loss = 3.19547172\n",
      "Iteration 67, loss = 3.19585781\n",
      "Iteration 68, loss = 3.19467372\n",
      "Iteration 69, loss = 3.19412226\n",
      "Iteration 70, loss = 3.19344427\n",
      "Iteration 71, loss = 3.19280956\n",
      "Iteration 72, loss = 3.19393109\n",
      "Iteration 73, loss = 3.19180604\n",
      "Iteration 74, loss = 3.19085298\n",
      "Iteration 75, loss = 3.19067555\n",
      "Iteration 76, loss = 3.18970820\n",
      "Iteration 77, loss = 3.18916070\n",
      "Iteration 78, loss = 3.18816483\n",
      "Iteration 79, loss = 3.18762401\n",
      "Iteration 80, loss = 3.18721520\n",
      "Iteration 81, loss = 3.18608875\n",
      "Iteration 82, loss = 3.18612518\n",
      "Iteration 83, loss = 3.18575678\n",
      "Iteration 84, loss = 3.18557912\n",
      "Iteration 85, loss = 3.18480037\n",
      "Iteration 86, loss = 3.18495885\n",
      "Iteration 87, loss = 3.18339987\n",
      "Iteration 88, loss = 3.18285259\n",
      "Iteration 89, loss = 3.18113267\n",
      "Iteration 90, loss = 3.18075835\n",
      "Iteration 91, loss = 3.18074293\n",
      "Iteration 92, loss = 3.17985238\n",
      "Iteration 93, loss = 3.17870436\n",
      "Iteration 94, loss = 3.17782892\n",
      "Iteration 95, loss = 3.17785426\n",
      "Iteration 96, loss = 3.17686003\n",
      "Iteration 97, loss = 3.17620538\n",
      "Iteration 98, loss = 3.17563785\n",
      "Iteration 99, loss = 3.17490015\n",
      "Iteration 100, loss = 3.17467175\n",
      "Iteration 101, loss = 3.17481059\n",
      "Iteration 102, loss = 3.17439930\n",
      "Iteration 103, loss = 3.17328714\n",
      "Iteration 104, loss = 3.17211157\n",
      "Iteration 105, loss = 3.17176661\n",
      "Iteration 106, loss = 3.17309399\n",
      "Iteration 107, loss = 3.17133376\n",
      "Iteration 108, loss = 3.17063403\n",
      "Iteration 109, loss = 3.17008844\n",
      "Iteration 110, loss = 3.16928665\n",
      "Iteration 111, loss = 3.16875150\n",
      "Iteration 112, loss = 3.16783011\n",
      "Iteration 113, loss = 3.16830778\n",
      "Iteration 114, loss = 3.16781171\n",
      "Iteration 115, loss = 3.16682663\n",
      "Iteration 116, loss = 3.16703443\n",
      "Iteration 117, loss = 3.16688625\n",
      "Iteration 118, loss = 3.16535988\n",
      "Iteration 119, loss = 3.16388331\n",
      "Iteration 120, loss = 3.16391009\n",
      "Iteration 121, loss = 3.16365272\n",
      "Iteration 122, loss = 3.16264484\n",
      "Iteration 123, loss = 3.16221619\n",
      "Iteration 124, loss = 3.16200356\n",
      "Iteration 125, loss = 3.16153970\n",
      "Iteration 126, loss = 3.16079702\n",
      "Iteration 127, loss = 3.16050790\n",
      "Iteration 128, loss = 3.15984592\n",
      "Iteration 129, loss = 3.15966776\n",
      "Iteration 130, loss = 3.15878467\n",
      "Iteration 131, loss = 3.15926467\n",
      "Iteration 132, loss = 3.15861471\n",
      "Iteration 133, loss = 3.15701674\n",
      "Iteration 134, loss = 3.15678880\n",
      "Iteration 135, loss = 3.15716648\n",
      "Iteration 136, loss = 3.15649279\n",
      "Iteration 137, loss = 3.15582881\n",
      "Iteration 138, loss = 3.15615053\n",
      "Iteration 139, loss = 3.15435784\n",
      "Iteration 140, loss = 3.15491325\n",
      "Iteration 141, loss = 3.15382124\n",
      "Iteration 142, loss = 3.15318891\n",
      "Iteration 143, loss = 3.15278207\n",
      "Iteration 144, loss = 3.15255924\n",
      "Iteration 145, loss = 3.15209308\n",
      "Iteration 146, loss = 3.15132655\n",
      "Iteration 147, loss = 3.15050175\n",
      "Iteration 148, loss = 3.15146911\n",
      "Iteration 149, loss = 3.15038218\n",
      "Iteration 150, loss = 3.14970249\n",
      "Iteration 151, loss = 3.14930570\n",
      "Iteration 152, loss = 3.14887928\n",
      "Iteration 153, loss = 3.14874600\n",
      "Iteration 154, loss = 3.14849070\n",
      "Iteration 155, loss = 3.14714311\n",
      "Iteration 156, loss = 3.14664740\n",
      "Iteration 157, loss = 3.14660142\n",
      "Iteration 158, loss = 3.14592843\n",
      "Iteration 159, loss = 3.14537731\n",
      "Iteration 160, loss = 3.14454363\n",
      "Iteration 161, loss = 3.14503280\n",
      "Iteration 162, loss = 3.14385764\n",
      "Iteration 163, loss = 3.14408977\n",
      "Iteration 164, loss = 3.14452786\n",
      "Iteration 165, loss = 3.14261938\n",
      "Iteration 166, loss = 3.14208081\n",
      "Iteration 167, loss = 3.14309087\n",
      "Iteration 168, loss = 3.14201972\n",
      "Iteration 169, loss = 3.14100807\n",
      "Iteration 170, loss = 3.14076905\n",
      "Iteration 171, loss = 3.13988716\n",
      "Iteration 172, loss = 3.14003128\n",
      "Iteration 173, loss = 3.13921008\n",
      "Iteration 174, loss = 3.13889556\n",
      "Iteration 175, loss = 3.13821428\n",
      "Iteration 176, loss = 3.13755007\n",
      "Iteration 177, loss = 3.13678138\n",
      "Iteration 178, loss = 3.13694686\n",
      "Iteration 179, loss = 3.13624328\n",
      "Iteration 180, loss = 3.13629737\n",
      "Iteration 181, loss = 3.13743474\n",
      "Iteration 182, loss = 3.13640459\n",
      "Iteration 183, loss = 3.13526042\n",
      "Iteration 184, loss = 3.13450404\n",
      "Iteration 185, loss = 3.13346795\n",
      "Iteration 186, loss = 3.13437961\n",
      "Iteration 187, loss = 3.13358003\n",
      "Iteration 188, loss = 3.13285777\n",
      "Iteration 189, loss = 3.13280576\n",
      "Iteration 190, loss = 3.13181166\n",
      "Iteration 191, loss = 3.13160070\n",
      "Iteration 192, loss = 3.13223453\n",
      "Iteration 193, loss = 3.13176066\n",
      "Iteration 194, loss = 3.13138572\n",
      "Iteration 195, loss = 3.13105432\n",
      "Iteration 196, loss = 3.13146699\n",
      "Iteration 197, loss = 3.13089159\n",
      "Iteration 198, loss = 3.12930668\n",
      "Iteration 199, loss = 3.12944402\n",
      "Iteration 200, loss = 3.12877163\n",
      "Iteration 201, loss = 3.12780917\n",
      "Iteration 202, loss = 3.12681470\n",
      "Iteration 203, loss = 3.12690575\n",
      "Iteration 204, loss = 3.12661437\n",
      "Iteration 205, loss = 3.12613497\n",
      "Iteration 206, loss = 3.12643032\n",
      "Iteration 207, loss = 3.12513138\n",
      "Iteration 208, loss = 3.12511572\n",
      "Iteration 209, loss = 3.12529175\n",
      "Iteration 210, loss = 3.12388112\n",
      "Iteration 211, loss = 3.12562792\n",
      "Iteration 212, loss = 3.12627206\n",
      "Iteration 213, loss = 3.12423620\n",
      "Iteration 214, loss = 3.12400617\n",
      "Iteration 215, loss = 3.12346057\n",
      "Iteration 216, loss = 3.12322276\n",
      "Iteration 217, loss = 3.12234588\n",
      "Iteration 218, loss = 3.12192092\n",
      "Iteration 219, loss = 3.12190649\n",
      "Iteration 220, loss = 3.12168040\n",
      "Iteration 221, loss = 3.12137294\n",
      "Iteration 222, loss = 3.12027350\n",
      "Iteration 223, loss = 3.11953737\n",
      "Iteration 224, loss = 3.11924760\n",
      "Iteration 225, loss = 3.12032983\n",
      "Iteration 226, loss = 3.11997657\n",
      "Iteration 227, loss = 3.11935168\n",
      "Iteration 228, loss = 3.11907575\n",
      "Iteration 229, loss = 3.11870623\n",
      "Iteration 230, loss = 3.11818635\n",
      "Iteration 231, loss = 3.11785131\n",
      "Iteration 232, loss = 3.11749501\n",
      "Iteration 233, loss = 3.11681929\n",
      "Iteration 234, loss = 3.11748458\n",
      "Iteration 235, loss = 3.11687650\n",
      "Iteration 236, loss = 3.11623212\n",
      "Iteration 237, loss = 3.11561749\n",
      "Iteration 238, loss = 3.11587403\n",
      "Iteration 239, loss = 3.11479401\n",
      "Iteration 240, loss = 3.11437415\n",
      "Iteration 241, loss = 3.11473186\n",
      "Iteration 242, loss = 3.11398533\n",
      "Iteration 243, loss = 3.11619233\n",
      "Iteration 244, loss = 3.11447602\n",
      "Iteration 245, loss = 3.11356212\n",
      "Iteration 246, loss = 3.11427433\n",
      "Iteration 247, loss = 3.11200865\n",
      "Iteration 248, loss = 3.11220436\n",
      "Iteration 249, loss = 3.11135909\n",
      "Iteration 250, loss = 3.11155879\n",
      "Iteration 251, loss = 3.11174108\n",
      "Iteration 252, loss = 3.11101360\n",
      "Iteration 253, loss = 3.10990087\n",
      "Iteration 254, loss = 3.10952943\n",
      "Iteration 255, loss = 3.10962852\n",
      "Iteration 256, loss = 3.10916079\n",
      "Iteration 257, loss = 3.10810601\n",
      "Iteration 258, loss = 3.10797677\n",
      "Iteration 259, loss = 3.10840177\n",
      "Iteration 260, loss = 3.10791833\n",
      "Iteration 261, loss = 3.10717198\n",
      "Iteration 262, loss = 3.10784075\n",
      "Iteration 263, loss = 3.10771095\n",
      "Iteration 264, loss = 3.10659663\n",
      "Iteration 265, loss = 3.10690357\n",
      "Iteration 266, loss = 3.10640779\n",
      "Iteration 267, loss = 3.10516055\n",
      "Iteration 268, loss = 3.10476090\n",
      "Iteration 269, loss = 3.10515987\n",
      "Iteration 270, loss = 3.10505960\n",
      "Iteration 271, loss = 3.10385032\n",
      "Iteration 272, loss = 3.10389423\n",
      "Iteration 273, loss = 3.10350384\n",
      "Iteration 274, loss = 3.10309852\n",
      "Iteration 275, loss = 3.10205100\n",
      "Iteration 276, loss = 3.10308667\n",
      "Iteration 277, loss = 3.10179307\n",
      "Iteration 278, loss = 3.10146392\n",
      "Iteration 279, loss = 3.10539191\n",
      "Iteration 280, loss = 3.10296373\n",
      "Iteration 281, loss = 3.10117239\n",
      "Iteration 282, loss = 3.10131317\n",
      "Iteration 283, loss = 3.10080597\n",
      "Iteration 284, loss = 3.09982984\n",
      "Iteration 285, loss = 3.09984812\n",
      "Iteration 286, loss = 3.10022581\n",
      "Iteration 287, loss = 3.09970660\n",
      "Iteration 288, loss = 3.10026739\n",
      "Iteration 289, loss = 3.09972903\n",
      "Iteration 290, loss = 3.09844776\n",
      "Iteration 291, loss = 3.09770497\n",
      "Iteration 292, loss = 3.09727189\n",
      "Iteration 293, loss = 3.09714520\n",
      "Iteration 294, loss = 3.09736390\n",
      "Iteration 295, loss = 3.09645144\n",
      "Iteration 296, loss = 3.09593458\n",
      "Iteration 297, loss = 3.09639785\n",
      "Iteration 298, loss = 3.09560369\n",
      "Iteration 299, loss = 3.09580175\n",
      "Iteration 300, loss = 3.09690397\n",
      "Iteration 301, loss = 3.09559306\n",
      "Iteration 302, loss = 3.09437803\n",
      "Iteration 303, loss = 3.09405913\n",
      "Iteration 304, loss = 3.09449989\n",
      "Iteration 305, loss = 3.09585409\n",
      "Iteration 306, loss = 3.09414757\n",
      "Iteration 307, loss = 3.09288542\n",
      "Iteration 308, loss = 3.09251178\n",
      "Iteration 309, loss = 3.09302524\n",
      "Iteration 310, loss = 3.09233372\n",
      "Iteration 311, loss = 3.09234029\n",
      "Iteration 312, loss = 3.09212270\n",
      "Iteration 313, loss = 3.09177631\n",
      "Iteration 314, loss = 3.09098371\n",
      "Iteration 315, loss = 3.09150984\n",
      "Iteration 316, loss = 3.09103579\n",
      "Iteration 317, loss = 3.09085199\n",
      "Iteration 318, loss = 3.09124559\n",
      "Iteration 319, loss = 3.09059190\n",
      "Iteration 320, loss = 3.08845443\n",
      "Iteration 321, loss = 3.08875741\n",
      "Iteration 322, loss = 3.09017731\n",
      "Iteration 323, loss = 3.08897483\n",
      "Iteration 324, loss = 3.08825028\n",
      "Iteration 325, loss = 3.09020968\n",
      "Iteration 326, loss = 3.08813676\n",
      "Iteration 327, loss = 3.08831453\n",
      "Iteration 328, loss = 3.08680259\n",
      "Iteration 329, loss = 3.08658054\n",
      "Iteration 330, loss = 3.08681292\n",
      "Iteration 331, loss = 3.08808376\n",
      "Iteration 332, loss = 3.08741853\n",
      "Iteration 333, loss = 3.08643119\n",
      "Iteration 334, loss = 3.08759780\n",
      "Iteration 335, loss = 3.08501126\n",
      "Iteration 336, loss = 3.08452023\n",
      "Iteration 337, loss = 3.08469688\n",
      "Iteration 338, loss = 3.08429984\n",
      "Iteration 339, loss = 3.08355051\n",
      "Iteration 340, loss = 3.08365871\n",
      "Iteration 341, loss = 3.08340381\n",
      "Iteration 342, loss = 3.08359172\n",
      "Iteration 343, loss = 3.08265210\n",
      "Iteration 344, loss = 3.08342590\n",
      "Iteration 345, loss = 3.08297006\n",
      "Iteration 346, loss = 3.08181304\n",
      "Iteration 347, loss = 3.08235733\n",
      "Iteration 348, loss = 3.08241570\n",
      "Iteration 349, loss = 3.08179562\n",
      "Iteration 350, loss = 3.08110369\n",
      "Iteration 351, loss = 3.08155746\n",
      "Iteration 352, loss = 3.08114092\n",
      "Iteration 353, loss = 3.08100874\n",
      "Iteration 354, loss = 3.08062599\n",
      "Iteration 355, loss = 3.08009202\n",
      "Iteration 356, loss = 3.07875551\n",
      "Iteration 357, loss = 3.07940779\n",
      "Iteration 358, loss = 3.07882902\n",
      "Iteration 359, loss = 3.07864858\n",
      "Iteration 360, loss = 3.07849173\n",
      "Iteration 361, loss = 3.07894102\n",
      "Iteration 362, loss = 3.07853197\n",
      "Iteration 363, loss = 3.07810649\n",
      "Iteration 364, loss = 3.07741545\n",
      "Iteration 365, loss = 3.07756342\n",
      "Iteration 366, loss = 3.07734216\n",
      "Iteration 367, loss = 3.07746599\n",
      "Iteration 368, loss = 3.07609535\n",
      "Iteration 369, loss = 3.07677610\n",
      "Iteration 370, loss = 3.07547344\n",
      "Iteration 371, loss = 3.07638514\n",
      "Iteration 372, loss = 3.07601570\n",
      "Iteration 373, loss = 3.07746774\n",
      "Iteration 374, loss = 3.07612819\n",
      "Iteration 375, loss = 3.07513908\n",
      "Iteration 376, loss = 3.07450392\n",
      "Iteration 377, loss = 3.07416962\n",
      "Iteration 378, loss = 3.07369839\n",
      "Iteration 379, loss = 3.07344014\n",
      "Iteration 380, loss = 3.07315227\n",
      "Iteration 381, loss = 3.07355606\n",
      "Iteration 382, loss = 3.07318662\n",
      "Iteration 383, loss = 3.07200362\n",
      "Iteration 384, loss = 3.07141864\n",
      "Iteration 385, loss = 3.07183362\n",
      "Iteration 386, loss = 3.07254016\n",
      "Iteration 387, loss = 3.07147976\n",
      "Iteration 388, loss = 3.07142530\n",
      "Iteration 389, loss = 3.07144321\n",
      "Iteration 390, loss = 3.07109822\n",
      "Iteration 391, loss = 3.07089049\n",
      "Iteration 392, loss = 3.07104282\n",
      "Iteration 393, loss = 3.07019886\n",
      "Iteration 394, loss = 3.07075900\n",
      "Iteration 395, loss = 3.06947874\n",
      "Iteration 396, loss = 3.07284232\n",
      "Iteration 397, loss = 3.07106617\n",
      "Iteration 398, loss = 3.07028587\n",
      "Iteration 399, loss = 3.06956726\n",
      "Iteration 400, loss = 3.06850785\n",
      "Iteration 401, loss = 3.06784986\n",
      "Iteration 402, loss = 3.06979585\n",
      "Iteration 403, loss = 3.06763476\n",
      "Iteration 404, loss = 3.06742197\n",
      "Iteration 405, loss = 3.06781222\n",
      "Iteration 406, loss = 3.06781377\n",
      "Iteration 407, loss = 3.06725335\n",
      "Iteration 408, loss = 3.06891027\n",
      "Iteration 409, loss = 3.06688970\n",
      "Iteration 410, loss = 3.06695180\n",
      "Iteration 411, loss = 3.06716608\n",
      "Iteration 412, loss = 3.06614589\n",
      "Iteration 413, loss = 3.06539978\n",
      "Iteration 414, loss = 3.06554312\n",
      "Iteration 415, loss = 3.06560227\n",
      "Iteration 416, loss = 3.06590364\n",
      "Iteration 417, loss = 3.06449337\n",
      "Iteration 418, loss = 3.06421690\n",
      "Iteration 419, loss = 3.06456860\n",
      "Iteration 420, loss = 3.06384758\n",
      "Iteration 421, loss = 3.06349804\n",
      "Iteration 422, loss = 3.06421538\n",
      "Iteration 423, loss = 3.06349690\n",
      "Iteration 424, loss = 3.06312535\n",
      "Iteration 425, loss = 3.06295886\n",
      "Iteration 426, loss = 3.06312186\n",
      "Iteration 427, loss = 3.06246349\n",
      "Iteration 428, loss = 3.06182996\n",
      "Iteration 429, loss = 3.06241713\n",
      "Iteration 430, loss = 3.06325086\n",
      "Iteration 431, loss = 3.06215755\n",
      "Iteration 432, loss = 3.06053764\n",
      "Iteration 433, loss = 3.06122708\n",
      "Iteration 434, loss = 3.06112604\n",
      "Iteration 435, loss = 3.06127401\n",
      "Iteration 436, loss = 3.06129253\n",
      "Iteration 437, loss = 3.05976070\n",
      "Iteration 438, loss = 3.06004673\n",
      "Iteration 439, loss = 3.06017983\n",
      "Iteration 440, loss = 3.05908950\n",
      "Iteration 441, loss = 3.05991415\n",
      "Iteration 442, loss = 3.05919435\n",
      "Iteration 443, loss = 3.05857798\n",
      "Iteration 444, loss = 3.06049201\n",
      "Iteration 445, loss = 3.06115503\n",
      "Iteration 446, loss = 3.05770730\n",
      "Iteration 447, loss = 3.05933222\n",
      "Iteration 448, loss = 3.05898997\n",
      "Iteration 449, loss = 3.05856641\n",
      "Iteration 450, loss = 3.05717524\n",
      "Iteration 451, loss = 3.05789568\n",
      "Iteration 452, loss = 3.05731367\n",
      "Iteration 453, loss = 3.05774353\n",
      "Iteration 454, loss = 3.05720003\n",
      "Iteration 455, loss = 3.05624179\n",
      "Iteration 456, loss = 3.05719751\n",
      "Iteration 457, loss = 3.05591400\n",
      "Iteration 458, loss = 3.05599589\n",
      "Iteration 459, loss = 3.05540946\n",
      "Iteration 460, loss = 3.05447557\n",
      "Iteration 461, loss = 3.05471707\n",
      "Iteration 462, loss = 3.05573352\n",
      "Iteration 463, loss = 3.05362475\n",
      "Iteration 464, loss = 3.05307675\n",
      "Iteration 465, loss = 3.05262072\n",
      "Iteration 466, loss = 3.05318741\n",
      "Iteration 467, loss = 3.05308923\n",
      "Iteration 468, loss = 3.05289079\n",
      "Iteration 469, loss = 3.05274005\n",
      "Iteration 470, loss = 3.05285360\n",
      "Iteration 471, loss = 3.05151315\n",
      "Iteration 472, loss = 3.05129939\n",
      "Iteration 473, loss = 3.05158102\n",
      "Iteration 474, loss = 3.05136777\n",
      "Iteration 475, loss = 3.05013179\n",
      "Iteration 476, loss = 3.05199365\n",
      "Iteration 477, loss = 3.05118693\n",
      "Iteration 478, loss = 3.05052475\n",
      "Iteration 479, loss = 3.05014432\n",
      "Iteration 480, loss = 3.05013452\n",
      "Iteration 481, loss = 3.05085839\n",
      "Iteration 482, loss = 3.04979003\n",
      "Iteration 483, loss = 3.05057147\n",
      "Iteration 484, loss = 3.04930671\n",
      "Iteration 485, loss = 3.04958963\n",
      "Iteration 486, loss = 3.04923545\n",
      "Iteration 487, loss = 3.04896760\n",
      "Iteration 488, loss = 3.04899038\n",
      "Iteration 489, loss = 3.04781639\n",
      "Iteration 490, loss = 3.04832476\n",
      "Iteration 491, loss = 3.04762984\n",
      "Iteration 492, loss = 3.04784447\n",
      "Iteration 493, loss = 3.04704862\n",
      "Iteration 494, loss = 3.04671726\n",
      "Iteration 495, loss = 3.04756058\n",
      "Iteration 496, loss = 3.04722192\n",
      "Iteration 497, loss = 3.04649513\n",
      "Iteration 498, loss = 3.04586232\n",
      "Iteration 499, loss = 3.04626059\n",
      "Iteration 500, loss = 3.04629389\n",
      "Iteration 501, loss = 3.04549557\n",
      "Iteration 502, loss = 3.04547776\n",
      "Iteration 503, loss = 3.04482129\n",
      "Iteration 504, loss = 3.04477441\n",
      "Iteration 505, loss = 3.04530018\n",
      "Iteration 506, loss = 3.04510661\n",
      "Iteration 507, loss = 3.04470067\n",
      "Iteration 508, loss = 3.04473748\n",
      "Iteration 509, loss = 3.04425970\n",
      "Iteration 510, loss = 3.04594620\n",
      "Iteration 511, loss = 3.04452990\n",
      "Iteration 512, loss = 3.04404297\n",
      "Iteration 513, loss = 3.04343886\n",
      "Iteration 514, loss = 3.04407190\n",
      "Iteration 515, loss = 3.04437296\n",
      "Iteration 516, loss = 3.04234756\n",
      "Iteration 517, loss = 3.04325681\n",
      "Iteration 518, loss = 3.04288034\n",
      "Iteration 519, loss = 3.04272724\n",
      "Iteration 520, loss = 3.04227681\n",
      "Iteration 521, loss = 3.04288344\n",
      "Iteration 522, loss = 3.04111427\n",
      "Iteration 523, loss = 3.04126054\n",
      "Iteration 524, loss = 3.04114374\n",
      "Iteration 525, loss = 3.04071715\n",
      "Iteration 526, loss = 3.04188988\n",
      "Iteration 527, loss = 3.04121718\n",
      "Iteration 528, loss = 3.04008579\n",
      "Iteration 529, loss = 3.03980147\n",
      "Iteration 530, loss = 3.04012435\n",
      "Iteration 531, loss = 3.04132146\n",
      "Iteration 532, loss = 3.04112185\n",
      "Iteration 533, loss = 3.04178552\n",
      "Iteration 534, loss = 3.04074083\n",
      "Iteration 535, loss = 3.03984752\n",
      "Iteration 536, loss = 3.03981871\n",
      "Iteration 537, loss = 3.03910079\n",
      "Iteration 538, loss = 3.04034991\n",
      "Iteration 539, loss = 3.04027833\n",
      "Iteration 540, loss = 3.03952755\n",
      "Iteration 541, loss = 3.03858969\n",
      "Iteration 542, loss = 3.03831884\n",
      "Iteration 543, loss = 3.03873661\n",
      "Iteration 544, loss = 3.03738161\n",
      "Iteration 545, loss = 3.03677185\n",
      "Iteration 546, loss = 3.03744097\n",
      "Iteration 547, loss = 3.03726731\n",
      "Iteration 548, loss = 3.03711207\n",
      "Iteration 549, loss = 3.03730193\n",
      "Iteration 550, loss = 3.03690700\n",
      "Iteration 551, loss = 3.03640791\n",
      "Iteration 552, loss = 3.03593229\n",
      "Iteration 553, loss = 3.03537946\n",
      "Iteration 554, loss = 3.03549855\n",
      "Iteration 555, loss = 3.03515744\n",
      "Iteration 556, loss = 3.03564706\n",
      "Iteration 557, loss = 3.03604791\n",
      "Iteration 558, loss = 3.03496762\n",
      "Iteration 559, loss = 3.03612770\n",
      "Iteration 560, loss = 3.03590357\n",
      "Iteration 561, loss = 3.03369022\n",
      "Iteration 562, loss = 3.03383596\n",
      "Iteration 563, loss = 3.03399357\n",
      "Iteration 564, loss = 3.03385983\n",
      "Iteration 565, loss = 3.03373737\n",
      "Iteration 566, loss = 3.03350646\n",
      "Iteration 567, loss = 3.03384276\n",
      "Iteration 568, loss = 3.03336605\n",
      "Iteration 569, loss = 3.03323287\n",
      "Iteration 570, loss = 3.03287565\n",
      "Iteration 571, loss = 3.03257814\n",
      "Iteration 572, loss = 3.03248346\n",
      "Iteration 573, loss = 3.03404177\n",
      "Iteration 574, loss = 3.03360981\n",
      "Iteration 575, loss = 3.03511505\n",
      "Iteration 576, loss = 3.03252560\n",
      "Iteration 577, loss = 3.03161029\n",
      "Iteration 578, loss = 3.03329409\n",
      "Iteration 579, loss = 3.03189806\n",
      "Iteration 580, loss = 3.03218770\n",
      "Iteration 581, loss = 3.03206532\n",
      "Iteration 582, loss = 3.03101120\n",
      "Iteration 583, loss = 3.03071869\n",
      "Iteration 584, loss = 3.03120201\n",
      "Iteration 585, loss = 3.03075681\n",
      "Iteration 586, loss = 3.02993967\n",
      "Iteration 587, loss = 3.03000948\n",
      "Iteration 588, loss = 3.02882233\n",
      "Iteration 589, loss = 3.02974052\n",
      "Iteration 590, loss = 3.03228346\n",
      "Iteration 591, loss = 3.02872702\n",
      "Iteration 592, loss = 3.02990520\n",
      "Iteration 593, loss = 3.02886110\n",
      "Iteration 594, loss = 3.03300863\n",
      "Iteration 595, loss = 3.03046402\n",
      "Iteration 596, loss = 3.02846271\n",
      "Iteration 597, loss = 3.02827462\n",
      "Iteration 598, loss = 3.02819367\n",
      "Iteration 599, loss = 3.02765186\n",
      "Iteration 600, loss = 3.02748999\n",
      "Iteration 601, loss = 3.02746735\n",
      "Iteration 602, loss = 3.02773437\n",
      "Iteration 603, loss = 3.02724832\n",
      "Iteration 604, loss = 3.02773631\n",
      "Iteration 605, loss = 3.02746101\n",
      "Iteration 606, loss = 3.02604569\n",
      "Iteration 607, loss = 3.02678344\n",
      "Iteration 608, loss = 3.02647386\n",
      "Iteration 609, loss = 3.02595044\n",
      "Iteration 610, loss = 3.02651294\n",
      "Iteration 611, loss = 3.02678318\n",
      "Iteration 612, loss = 3.02753700\n",
      "Iteration 613, loss = 3.02714503\n",
      "Iteration 614, loss = 3.02610558\n",
      "Iteration 615, loss = 3.02568484\n",
      "Iteration 616, loss = 3.02739957\n",
      "Iteration 617, loss = 3.02553855\n",
      "Iteration 618, loss = 3.02648339\n",
      "Iteration 619, loss = 3.02572143\n",
      "Iteration 620, loss = 3.02499627\n",
      "Iteration 621, loss = 3.02444205\n",
      "Iteration 622, loss = 3.02495890\n",
      "Iteration 623, loss = 3.02370601\n",
      "Iteration 624, loss = 3.02424344\n",
      "Iteration 625, loss = 3.02453909\n",
      "Iteration 626, loss = 3.02422865\n",
      "Iteration 627, loss = 3.02389779\n",
      "Iteration 628, loss = 3.02331638\n",
      "Iteration 629, loss = 3.02296007\n",
      "Iteration 630, loss = 3.02287891\n",
      "Iteration 631, loss = 3.02230626\n",
      "Iteration 632, loss = 3.02209768\n",
      "Iteration 633, loss = 3.02182356\n",
      "Iteration 634, loss = 3.02155557\n",
      "Iteration 635, loss = 3.02214181\n",
      "Iteration 636, loss = 3.02119202\n",
      "Iteration 637, loss = 3.02190864\n",
      "Iteration 638, loss = 3.02096767\n",
      "Iteration 639, loss = 3.02180787\n",
      "Iteration 640, loss = 3.02274140\n",
      "Iteration 641, loss = 3.02156974\n",
      "Iteration 642, loss = 3.02021363\n",
      "Iteration 643, loss = 3.01996792\n",
      "Iteration 644, loss = 3.02074185\n",
      "Iteration 645, loss = 3.02068682\n",
      "Iteration 646, loss = 3.02030403\n",
      "Iteration 647, loss = 3.02083531\n",
      "Iteration 648, loss = 3.02004419\n",
      "Iteration 649, loss = 3.02009591\n",
      "Iteration 650, loss = 3.02022788\n",
      "Iteration 651, loss = 3.01984564\n",
      "Iteration 652, loss = 3.01942688\n",
      "Iteration 653, loss = 3.01938980\n",
      "Iteration 654, loss = 3.01977636\n",
      "Iteration 655, loss = 3.01940353\n",
      "Iteration 656, loss = 3.01969027\n",
      "Iteration 657, loss = 3.02041285\n",
      "Iteration 658, loss = 3.01897112\n",
      "Iteration 659, loss = 3.01965087\n",
      "Iteration 660, loss = 3.01904642\n",
      "Iteration 661, loss = 3.01941377\n",
      "Iteration 662, loss = 3.01792869\n",
      "Iteration 663, loss = 3.01779714\n",
      "Iteration 664, loss = 3.01794869\n",
      "Iteration 665, loss = 3.01714962\n",
      "Iteration 666, loss = 3.01715141\n",
      "Iteration 667, loss = 3.02034906\n",
      "Iteration 668, loss = 3.01815075\n",
      "Iteration 669, loss = 3.01638678\n",
      "Iteration 670, loss = 3.01618906\n",
      "Iteration 671, loss = 3.01616495\n",
      "Iteration 672, loss = 3.01624931\n",
      "Iteration 673, loss = 3.01550436\n",
      "Iteration 674, loss = 3.01571048\n",
      "Iteration 675, loss = 3.01529131\n",
      "Iteration 676, loss = 3.01559118\n",
      "Iteration 677, loss = 3.01608860\n",
      "Iteration 678, loss = 3.01591445\n",
      "Iteration 679, loss = 3.01588175\n",
      "Iteration 680, loss = 3.01656877\n",
      "Iteration 681, loss = 3.01545957\n",
      "Iteration 682, loss = 3.01520552\n",
      "Iteration 683, loss = 3.01512760\n",
      "Iteration 684, loss = 3.01412891\n",
      "Iteration 685, loss = 3.01397004\n",
      "Iteration 686, loss = 3.01478597\n",
      "Iteration 687, loss = 3.01651112\n",
      "Iteration 688, loss = 3.01531632\n",
      "Iteration 689, loss = 3.01375999\n",
      "Iteration 690, loss = 3.01489348\n",
      "Iteration 691, loss = 3.01371910\n",
      "Iteration 692, loss = 3.01475439\n",
      "Iteration 693, loss = 3.01325180\n",
      "Iteration 694, loss = 3.01488688\n",
      "Iteration 695, loss = 3.01334479\n",
      "Iteration 696, loss = 3.01255572\n",
      "Iteration 697, loss = 3.01299551\n",
      "Iteration 698, loss = 3.01237045\n",
      "Iteration 699, loss = 3.01204611\n",
      "Iteration 700, loss = 3.01267738\n",
      "Iteration 701, loss = 3.01299650\n",
      "Iteration 702, loss = 3.01259368\n",
      "Iteration 703, loss = 3.01216271\n",
      "Iteration 704, loss = 3.01144480\n",
      "Iteration 705, loss = 3.01063683\n",
      "Iteration 706, loss = 3.01130420\n",
      "Iteration 707, loss = 3.01062991\n",
      "Iteration 708, loss = 3.01183095\n",
      "Iteration 709, loss = 3.01116228\n",
      "Iteration 710, loss = 3.01091666\n",
      "Iteration 711, loss = 3.01125068\n",
      "Iteration 712, loss = 3.01034932\n",
      "Iteration 713, loss = 3.01063015\n",
      "Iteration 714, loss = 3.00999928\n",
      "Iteration 715, loss = 3.00967833\n",
      "Iteration 716, loss = 3.00999446\n",
      "Iteration 717, loss = 3.00908616\n",
      "Iteration 718, loss = 3.00855992\n",
      "Iteration 719, loss = 3.00826705\n",
      "Iteration 720, loss = 3.01027701\n",
      "Iteration 721, loss = 3.00951548\n",
      "Iteration 722, loss = 3.00924888\n",
      "Iteration 723, loss = 3.00970589\n",
      "Iteration 724, loss = 3.00891739\n",
      "Iteration 725, loss = 3.00792739\n",
      "Iteration 726, loss = 3.00829484\n",
      "Iteration 727, loss = 3.00789673\n",
      "Iteration 728, loss = 3.00995816\n",
      "Iteration 729, loss = 3.00867892\n",
      "Iteration 730, loss = 3.00854684\n",
      "Iteration 731, loss = 3.00820432\n",
      "Iteration 732, loss = 3.00783901\n",
      "Iteration 733, loss = 3.00687968\n",
      "Iteration 734, loss = 3.00648755\n",
      "Iteration 735, loss = 3.00863352\n",
      "Iteration 736, loss = 3.00900399\n",
      "Iteration 737, loss = 3.00795521\n",
      "Iteration 738, loss = 3.00779972\n",
      "Iteration 739, loss = 3.00622262\n",
      "Iteration 740, loss = 3.00643265\n",
      "Iteration 741, loss = 3.00619980\n",
      "Iteration 742, loss = 3.00596302\n",
      "Iteration 743, loss = 3.00525655\n",
      "Iteration 744, loss = 3.00690687\n",
      "Iteration 745, loss = 3.00697584\n",
      "Iteration 746, loss = 3.00551979\n",
      "Iteration 747, loss = 3.00700854\n",
      "Iteration 748, loss = 3.00714614\n",
      "Iteration 749, loss = 3.00649200\n",
      "Iteration 750, loss = 3.00576472\n",
      "Iteration 751, loss = 3.00629343\n",
      "Iteration 752, loss = 3.00532129\n",
      "Iteration 753, loss = 3.00527641\n",
      "Iteration 754, loss = 3.00635151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.16045497\n",
      "Iteration 2, loss = 4.11992027\n",
      "Iteration 3, loss = 4.08049880\n",
      "Iteration 4, loss = 4.04096482\n",
      "Iteration 5, loss = 3.99855394\n",
      "Iteration 6, loss = 3.95405792\n",
      "Iteration 7, loss = 3.90773025\n",
      "Iteration 8, loss = 3.85766903\n",
      "Iteration 9, loss = 3.80619535\n",
      "Iteration 10, loss = 3.75292506\n",
      "Iteration 11, loss = 3.69623664\n",
      "Iteration 12, loss = 3.64094978\n",
      "Iteration 13, loss = 3.58430701\n",
      "Iteration 14, loss = 3.52962342\n",
      "Iteration 15, loss = 3.47485894\n",
      "Iteration 16, loss = 3.42468319\n",
      "Iteration 17, loss = 3.37895239\n",
      "Iteration 18, loss = 3.34059959\n",
      "Iteration 19, loss = 3.30742106\n",
      "Iteration 20, loss = 3.28298733\n",
      "Iteration 21, loss = 3.26426851\n",
      "Iteration 22, loss = 3.25162764\n",
      "Iteration 23, loss = 3.24278434\n",
      "Iteration 24, loss = 3.23615456\n",
      "Iteration 25, loss = 3.23228620\n",
      "Iteration 26, loss = 3.22968349\n",
      "Iteration 27, loss = 3.22788710\n",
      "Iteration 28, loss = 3.22678931\n",
      "Iteration 29, loss = 3.22575092\n",
      "Iteration 30, loss = 3.22477630\n",
      "Iteration 31, loss = 3.22389990\n",
      "Iteration 32, loss = 3.22285954\n",
      "Iteration 33, loss = 3.22180794\n",
      "Iteration 34, loss = 3.22093846\n",
      "Iteration 35, loss = 3.22039438\n",
      "Iteration 36, loss = 3.21948632\n",
      "Iteration 37, loss = 3.21856936\n",
      "Iteration 38, loss = 3.21797946\n",
      "Iteration 39, loss = 3.21729461\n",
      "Iteration 40, loss = 3.21657321\n",
      "Iteration 41, loss = 3.21611660\n",
      "Iteration 42, loss = 3.21530643\n",
      "Iteration 43, loss = 3.21483868\n",
      "Iteration 44, loss = 3.21403907\n",
      "Iteration 45, loss = 3.21317899\n",
      "Iteration 46, loss = 3.21247863\n",
      "Iteration 47, loss = 3.21181780\n",
      "Iteration 48, loss = 3.21149435\n",
      "Iteration 49, loss = 3.21058232\n",
      "Iteration 50, loss = 3.21023578\n",
      "Iteration 51, loss = 3.20964954\n",
      "Iteration 52, loss = 3.20900654\n",
      "Iteration 53, loss = 3.20835975\n",
      "Iteration 54, loss = 3.20776017\n",
      "Iteration 55, loss = 3.20699630\n",
      "Iteration 56, loss = 3.20651275\n",
      "Iteration 57, loss = 3.20604646\n",
      "Iteration 58, loss = 3.20505893\n",
      "Iteration 59, loss = 3.20439617\n",
      "Iteration 60, loss = 3.20425758\n",
      "Iteration 61, loss = 3.20333799\n",
      "Iteration 62, loss = 3.20277897\n",
      "Iteration 63, loss = 3.20223526\n",
      "Iteration 64, loss = 3.20157973\n",
      "Iteration 65, loss = 3.20088385\n",
      "Iteration 66, loss = 3.20025628\n",
      "Iteration 67, loss = 3.19990343\n",
      "Iteration 68, loss = 3.19932744\n",
      "Iteration 69, loss = 3.19879342\n",
      "Iteration 70, loss = 3.19789093\n",
      "Iteration 71, loss = 3.19736807\n",
      "Iteration 72, loss = 3.19683298\n",
      "Iteration 73, loss = 3.19581608\n",
      "Iteration 74, loss = 3.19565791\n",
      "Iteration 75, loss = 3.19511396\n",
      "Iteration 76, loss = 3.19437682\n",
      "Iteration 77, loss = 3.19379165\n",
      "Iteration 78, loss = 3.19295614\n",
      "Iteration 79, loss = 3.19219558\n",
      "Iteration 80, loss = 3.19189274\n",
      "Iteration 81, loss = 3.19086208\n",
      "Iteration 82, loss = 3.19031784\n",
      "Iteration 83, loss = 3.18995825\n",
      "Iteration 84, loss = 3.18940463\n",
      "Iteration 85, loss = 3.18883985\n",
      "Iteration 86, loss = 3.18834649\n",
      "Iteration 87, loss = 3.18765352\n",
      "Iteration 88, loss = 3.18708320\n",
      "Iteration 89, loss = 3.18663989\n",
      "Iteration 90, loss = 3.18550544\n",
      "Iteration 91, loss = 3.18489868\n",
      "Iteration 92, loss = 3.18425219\n",
      "Iteration 93, loss = 3.18377261\n",
      "Iteration 94, loss = 3.18284824\n",
      "Iteration 95, loss = 3.18224815\n",
      "Iteration 96, loss = 3.18151176\n",
      "Iteration 97, loss = 3.18076963\n",
      "Iteration 98, loss = 3.18053540\n",
      "Iteration 99, loss = 3.17984383\n",
      "Iteration 100, loss = 3.17931547\n",
      "Iteration 101, loss = 3.17884088\n",
      "Iteration 102, loss = 3.17818588\n",
      "Iteration 103, loss = 3.17727518\n",
      "Iteration 104, loss = 3.17693046\n",
      "Iteration 105, loss = 3.17628816\n",
      "Iteration 106, loss = 3.17546838\n",
      "Iteration 107, loss = 3.17503404\n",
      "Iteration 108, loss = 3.17410977\n",
      "Iteration 109, loss = 3.17339327\n",
      "Iteration 110, loss = 3.17266638\n",
      "Iteration 111, loss = 3.17208349\n",
      "Iteration 112, loss = 3.17126491\n",
      "Iteration 113, loss = 3.17045602\n",
      "Iteration 114, loss = 3.16991740\n",
      "Iteration 115, loss = 3.16934695\n",
      "Iteration 116, loss = 3.16872201\n",
      "Iteration 117, loss = 3.16762977\n",
      "Iteration 118, loss = 3.16710013\n",
      "Iteration 119, loss = 3.16662195\n",
      "Iteration 120, loss = 3.16604219\n",
      "Iteration 121, loss = 3.16535928\n",
      "Iteration 122, loss = 3.16491867\n",
      "Iteration 123, loss = 3.16456783\n",
      "Iteration 124, loss = 3.16403627\n",
      "Iteration 125, loss = 3.16323659\n",
      "Iteration 126, loss = 3.16249960\n",
      "Iteration 127, loss = 3.16197698\n",
      "Iteration 128, loss = 3.16086410\n",
      "Iteration 129, loss = 3.16040184\n",
      "Iteration 130, loss = 3.16020212\n",
      "Iteration 131, loss = 3.15981477\n",
      "Iteration 132, loss = 3.15877665\n",
      "Iteration 133, loss = 3.15826238\n",
      "Iteration 134, loss = 3.15702999\n",
      "Iteration 135, loss = 3.15628400\n",
      "Iteration 136, loss = 3.15585560\n",
      "Iteration 137, loss = 3.15523203\n",
      "Iteration 138, loss = 3.15437059\n",
      "Iteration 139, loss = 3.15433929\n",
      "Iteration 140, loss = 3.15390026\n",
      "Iteration 141, loss = 3.15266663\n",
      "Iteration 142, loss = 3.15212100\n",
      "Iteration 143, loss = 3.15197026\n",
      "Iteration 144, loss = 3.15089758\n",
      "Iteration 145, loss = 3.15046289\n",
      "Iteration 146, loss = 3.15033830\n",
      "Iteration 147, loss = 3.14936559\n",
      "Iteration 148, loss = 3.14849383\n",
      "Iteration 149, loss = 3.14818849\n",
      "Iteration 150, loss = 3.14740958\n",
      "Iteration 151, loss = 3.14615145\n",
      "Iteration 152, loss = 3.14559811\n",
      "Iteration 153, loss = 3.14541112\n",
      "Iteration 154, loss = 3.14457392\n",
      "Iteration 155, loss = 3.14423958\n",
      "Iteration 156, loss = 3.14338851\n",
      "Iteration 157, loss = 3.14289005\n",
      "Iteration 158, loss = 3.14206150\n",
      "Iteration 159, loss = 3.14147815\n",
      "Iteration 160, loss = 3.14058560\n",
      "Iteration 161, loss = 3.14003788\n",
      "Iteration 162, loss = 3.13939205\n",
      "Iteration 163, loss = 3.13890845\n",
      "Iteration 164, loss = 3.13842364\n",
      "Iteration 165, loss = 3.13788898\n",
      "Iteration 166, loss = 3.13728759\n",
      "Iteration 167, loss = 3.13652920\n",
      "Iteration 168, loss = 3.13585447\n",
      "Iteration 169, loss = 3.13528029\n",
      "Iteration 170, loss = 3.13454556\n",
      "Iteration 171, loss = 3.13433036\n",
      "Iteration 172, loss = 3.13345711\n",
      "Iteration 173, loss = 3.13320990\n",
      "Iteration 174, loss = 3.13242775\n",
      "Iteration 175, loss = 3.13155987\n",
      "Iteration 176, loss = 3.13112253\n",
      "Iteration 177, loss = 3.13063704\n",
      "Iteration 178, loss = 3.12965958\n",
      "Iteration 179, loss = 3.12923439\n",
      "Iteration 180, loss = 3.12838955\n",
      "Iteration 181, loss = 3.12807153\n",
      "Iteration 182, loss = 3.12759485\n",
      "Iteration 183, loss = 3.12670744\n",
      "Iteration 184, loss = 3.12667888\n",
      "Iteration 185, loss = 3.12607640\n",
      "Iteration 186, loss = 3.12526937\n",
      "Iteration 187, loss = 3.12461454\n",
      "Iteration 188, loss = 3.12384920\n",
      "Iteration 189, loss = 3.12324590\n",
      "Iteration 190, loss = 3.12277845\n",
      "Iteration 191, loss = 3.12200686\n",
      "Iteration 192, loss = 3.12180041\n",
      "Iteration 193, loss = 3.12096642\n",
      "Iteration 194, loss = 3.12030912\n",
      "Iteration 195, loss = 3.12022942\n",
      "Iteration 196, loss = 3.11950328\n",
      "Iteration 197, loss = 3.11853174\n",
      "Iteration 198, loss = 3.11800068\n",
      "Iteration 199, loss = 3.11767298\n",
      "Iteration 200, loss = 3.11703792\n",
      "Iteration 201, loss = 3.11615044\n",
      "Iteration 202, loss = 3.11537168\n",
      "Iteration 203, loss = 3.11502909\n",
      "Iteration 204, loss = 3.11457333\n",
      "Iteration 205, loss = 3.11397929\n",
      "Iteration 206, loss = 3.11342056\n",
      "Iteration 207, loss = 3.11253047\n",
      "Iteration 208, loss = 3.11217343\n",
      "Iteration 209, loss = 3.11201124\n",
      "Iteration 210, loss = 3.11111242\n",
      "Iteration 211, loss = 3.11040193\n",
      "Iteration 212, loss = 3.10957969\n",
      "Iteration 213, loss = 3.10907851\n",
      "Iteration 214, loss = 3.10843963\n",
      "Iteration 215, loss = 3.10791211\n",
      "Iteration 216, loss = 3.10732655\n",
      "Iteration 217, loss = 3.10621203\n",
      "Iteration 218, loss = 3.10586457\n",
      "Iteration 219, loss = 3.10542166\n",
      "Iteration 220, loss = 3.10481786\n",
      "Iteration 221, loss = 3.10431452\n",
      "Iteration 222, loss = 3.10394677\n",
      "Iteration 223, loss = 3.10301268\n",
      "Iteration 224, loss = 3.10315965\n",
      "Iteration 225, loss = 3.10248688\n",
      "Iteration 226, loss = 3.10148746\n",
      "Iteration 227, loss = 3.10052713\n",
      "Iteration 228, loss = 3.10106442\n",
      "Iteration 229, loss = 3.10041008\n",
      "Iteration 230, loss = 3.09943961\n",
      "Iteration 231, loss = 3.09918125\n",
      "Iteration 232, loss = 3.09828045\n",
      "Iteration 233, loss = 3.09697070\n",
      "Iteration 234, loss = 3.09655864\n",
      "Iteration 235, loss = 3.09603842\n",
      "Iteration 236, loss = 3.09546801\n",
      "Iteration 237, loss = 3.09492277\n",
      "Iteration 238, loss = 3.09427176\n",
      "Iteration 239, loss = 3.09348043\n",
      "Iteration 240, loss = 3.09309569\n",
      "Iteration 241, loss = 3.09275020\n",
      "Iteration 242, loss = 3.09195108\n",
      "Iteration 243, loss = 3.09154302\n",
      "Iteration 244, loss = 3.09132153\n",
      "Iteration 245, loss = 3.09056058\n",
      "Iteration 246, loss = 3.08997762\n",
      "Iteration 247, loss = 3.08912610\n",
      "Iteration 248, loss = 3.08810659\n",
      "Iteration 249, loss = 3.08767858\n",
      "Iteration 250, loss = 3.08735259\n",
      "Iteration 251, loss = 3.08684392\n",
      "Iteration 252, loss = 3.08601533\n",
      "Iteration 253, loss = 3.08568044\n",
      "Iteration 254, loss = 3.08527063\n",
      "Iteration 255, loss = 3.08460331\n",
      "Iteration 256, loss = 3.08415728\n",
      "Iteration 257, loss = 3.08387429\n",
      "Iteration 258, loss = 3.08310791\n",
      "Iteration 259, loss = 3.08233329\n",
      "Iteration 260, loss = 3.08117392\n",
      "Iteration 261, loss = 3.08085953\n",
      "Iteration 262, loss = 3.08084008\n",
      "Iteration 263, loss = 3.07984987\n",
      "Iteration 264, loss = 3.07908143\n",
      "Iteration 265, loss = 3.07847490\n",
      "Iteration 266, loss = 3.07782511\n",
      "Iteration 267, loss = 3.07824057\n",
      "Iteration 268, loss = 3.07754024\n",
      "Iteration 269, loss = 3.07660280\n",
      "Iteration 270, loss = 3.07655377\n",
      "Iteration 271, loss = 3.07545849\n",
      "Iteration 272, loss = 3.07506540\n",
      "Iteration 273, loss = 3.07477973\n",
      "Iteration 274, loss = 3.07473134\n",
      "Iteration 275, loss = 3.07442762\n",
      "Iteration 276, loss = 3.07292242\n",
      "Iteration 277, loss = 3.07277405\n",
      "Iteration 278, loss = 3.07239573\n",
      "Iteration 279, loss = 3.07181832\n",
      "Iteration 280, loss = 3.07118254\n",
      "Iteration 281, loss = 3.07017782\n",
      "Iteration 282, loss = 3.06977263\n",
      "Iteration 283, loss = 3.06923658\n",
      "Iteration 284, loss = 3.06862006\n",
      "Iteration 285, loss = 3.06811437\n",
      "Iteration 286, loss = 3.06799002\n",
      "Iteration 287, loss = 3.06766639\n",
      "Iteration 288, loss = 3.06663186\n",
      "Iteration 289, loss = 3.06581673\n",
      "Iteration 290, loss = 3.06525771\n",
      "Iteration 291, loss = 3.06475774\n",
      "Iteration 292, loss = 3.06422287\n",
      "Iteration 293, loss = 3.06409935\n",
      "Iteration 294, loss = 3.06346838\n",
      "Iteration 295, loss = 3.06262627\n",
      "Iteration 296, loss = 3.06209147\n",
      "Iteration 297, loss = 3.06155500\n",
      "Iteration 298, loss = 3.06110168\n",
      "Iteration 299, loss = 3.06031759\n",
      "Iteration 300, loss = 3.06002254\n",
      "Iteration 301, loss = 3.05977021\n",
      "Iteration 302, loss = 3.05884169\n",
      "Iteration 303, loss = 3.05870881\n",
      "Iteration 304, loss = 3.05884499\n",
      "Iteration 305, loss = 3.05781106\n",
      "Iteration 306, loss = 3.05662528\n",
      "Iteration 307, loss = 3.05671645\n",
      "Iteration 308, loss = 3.05608797\n",
      "Iteration 309, loss = 3.05510153\n",
      "Iteration 310, loss = 3.05442000\n",
      "Iteration 311, loss = 3.05443086\n",
      "Iteration 312, loss = 3.05380945\n",
      "Iteration 313, loss = 3.05400038\n",
      "Iteration 314, loss = 3.05301964\n",
      "Iteration 315, loss = 3.05248614\n",
      "Iteration 316, loss = 3.05253691\n",
      "Iteration 317, loss = 3.05230245\n",
      "Iteration 318, loss = 3.05066164\n",
      "Iteration 319, loss = 3.05006751\n",
      "Iteration 320, loss = 3.04972676\n",
      "Iteration 321, loss = 3.04962720\n",
      "Iteration 322, loss = 3.04947399\n",
      "Iteration 323, loss = 3.04788904\n",
      "Iteration 324, loss = 3.04759706\n",
      "Iteration 325, loss = 3.04739328\n",
      "Iteration 326, loss = 3.04665405\n",
      "Iteration 327, loss = 3.04613545\n",
      "Iteration 328, loss = 3.04526572\n",
      "Iteration 329, loss = 3.04472806\n",
      "Iteration 330, loss = 3.04429605\n",
      "Iteration 331, loss = 3.04340965\n",
      "Iteration 332, loss = 3.04285540\n",
      "Iteration 333, loss = 3.04234377\n",
      "Iteration 334, loss = 3.04202333\n",
      "Iteration 335, loss = 3.04160717\n",
      "Iteration 336, loss = 3.04145933\n",
      "Iteration 337, loss = 3.04129313\n",
      "Iteration 338, loss = 3.04016480\n",
      "Iteration 339, loss = 3.03998717\n",
      "Iteration 340, loss = 3.03941875\n",
      "Iteration 341, loss = 3.03907729\n",
      "Iteration 342, loss = 3.03853621\n",
      "Iteration 343, loss = 3.03828833\n",
      "Iteration 344, loss = 3.03792820\n",
      "Iteration 345, loss = 3.03712582\n",
      "Iteration 346, loss = 3.03658051\n",
      "Iteration 347, loss = 3.03576371\n",
      "Iteration 348, loss = 3.03518355\n",
      "Iteration 349, loss = 3.03471107\n",
      "Iteration 350, loss = 3.03409059\n",
      "Iteration 351, loss = 3.03376229\n",
      "Iteration 352, loss = 3.03358134\n",
      "Iteration 353, loss = 3.03295323\n",
      "Iteration 354, loss = 3.03272632\n",
      "Iteration 355, loss = 3.03202190\n",
      "Iteration 356, loss = 3.03131625\n",
      "Iteration 357, loss = 3.03212651\n",
      "Iteration 358, loss = 3.03108828\n",
      "Iteration 359, loss = 3.02935019\n",
      "Iteration 360, loss = 3.02909701\n",
      "Iteration 361, loss = 3.02865496\n",
      "Iteration 362, loss = 3.02826720\n",
      "Iteration 363, loss = 3.02841791\n",
      "Iteration 364, loss = 3.02786834\n",
      "Iteration 365, loss = 3.02674040\n",
      "Iteration 366, loss = 3.02632442\n",
      "Iteration 367, loss = 3.02556906\n",
      "Iteration 368, loss = 3.02519908\n",
      "Iteration 369, loss = 3.02460071\n",
      "Iteration 370, loss = 3.02434677\n",
      "Iteration 371, loss = 3.02389377\n",
      "Iteration 372, loss = 3.02340002\n",
      "Iteration 373, loss = 3.02334811\n",
      "Iteration 374, loss = 3.02333064\n",
      "Iteration 375, loss = 3.02177738\n",
      "Iteration 376, loss = 3.02157200\n",
      "Iteration 377, loss = 3.02117849\n",
      "Iteration 378, loss = 3.02024226\n",
      "Iteration 379, loss = 3.02023438\n",
      "Iteration 380, loss = 3.02034009\n",
      "Iteration 381, loss = 3.01946447\n",
      "Iteration 382, loss = 3.01850837\n",
      "Iteration 383, loss = 3.01800532\n",
      "Iteration 384, loss = 3.01748923\n",
      "Iteration 385, loss = 3.01703263\n",
      "Iteration 386, loss = 3.01676885\n",
      "Iteration 387, loss = 3.01676806\n",
      "Iteration 388, loss = 3.01580782\n",
      "Iteration 389, loss = 3.01521599\n",
      "Iteration 390, loss = 3.01480382\n",
      "Iteration 391, loss = 3.01405914\n",
      "Iteration 392, loss = 3.01360896\n",
      "Iteration 393, loss = 3.01314429\n",
      "Iteration 394, loss = 3.01290534\n",
      "Iteration 395, loss = 3.01244523\n",
      "Iteration 396, loss = 3.01173593\n",
      "Iteration 397, loss = 3.01192797\n",
      "Iteration 398, loss = 3.01114275\n",
      "Iteration 399, loss = 3.01033475\n",
      "Iteration 400, loss = 3.00991426\n",
      "Iteration 401, loss = 3.00956198\n",
      "Iteration 402, loss = 3.00943155\n",
      "Iteration 403, loss = 3.00844340\n",
      "Iteration 404, loss = 3.00838074\n",
      "Iteration 405, loss = 3.00790374\n",
      "Iteration 406, loss = 3.00754844\n",
      "Iteration 407, loss = 3.00679185\n",
      "Iteration 408, loss = 3.00643747\n",
      "Iteration 409, loss = 3.00596496\n",
      "Iteration 410, loss = 3.00540012\n",
      "Iteration 411, loss = 3.00482451\n",
      "Iteration 412, loss = 3.00446855\n",
      "Iteration 413, loss = 3.00409095\n",
      "Iteration 414, loss = 3.00341664\n",
      "Iteration 415, loss = 3.00332695\n",
      "Iteration 416, loss = 3.00294564\n",
      "Iteration 417, loss = 3.00229239\n",
      "Iteration 418, loss = 3.00150273\n",
      "Iteration 419, loss = 3.00081592\n",
      "Iteration 420, loss = 3.00033194\n",
      "Iteration 421, loss = 3.00002264\n",
      "Iteration 422, loss = 2.99910516\n",
      "Iteration 423, loss = 2.99860590\n",
      "Iteration 424, loss = 2.99822178\n",
      "Iteration 425, loss = 2.99791013\n",
      "Iteration 426, loss = 2.99739207\n",
      "Iteration 427, loss = 2.99698514\n",
      "Iteration 428, loss = 2.99633760\n",
      "Iteration 429, loss = 2.99592164\n",
      "Iteration 430, loss = 2.99517541\n",
      "Iteration 431, loss = 2.99459376\n",
      "Iteration 432, loss = 2.99424090\n",
      "Iteration 433, loss = 2.99414988\n",
      "Iteration 434, loss = 2.99383910\n",
      "Iteration 435, loss = 2.99295979\n",
      "Iteration 436, loss = 2.99291818\n",
      "Iteration 437, loss = 2.99218335\n",
      "Iteration 438, loss = 2.99170476\n",
      "Iteration 439, loss = 2.99103149\n",
      "Iteration 440, loss = 2.99054874\n",
      "Iteration 441, loss = 2.98981509\n",
      "Iteration 442, loss = 2.98971419\n",
      "Iteration 443, loss = 2.98931445\n",
      "Iteration 444, loss = 2.98874842\n",
      "Iteration 445, loss = 2.98938101\n",
      "Iteration 446, loss = 2.98788419\n",
      "Iteration 447, loss = 2.98767217\n",
      "Iteration 448, loss = 2.98716554\n",
      "Iteration 449, loss = 2.98663852\n",
      "Iteration 450, loss = 2.98587273\n",
      "Iteration 451, loss = 2.98554001\n",
      "Iteration 452, loss = 2.98512860\n",
      "Iteration 453, loss = 2.98465130\n",
      "Iteration 454, loss = 2.98396497\n",
      "Iteration 455, loss = 2.98401876\n",
      "Iteration 456, loss = 2.98309698\n",
      "Iteration 457, loss = 2.98272806\n",
      "Iteration 458, loss = 2.98222717\n",
      "Iteration 459, loss = 2.98170245\n",
      "Iteration 460, loss = 2.98137326\n",
      "Iteration 461, loss = 2.98113712\n",
      "Iteration 462, loss = 2.98050713\n",
      "Iteration 463, loss = 2.98023726\n",
      "Iteration 464, loss = 2.97977336\n",
      "Iteration 465, loss = 2.97926248\n",
      "Iteration 466, loss = 2.97853074\n",
      "Iteration 467, loss = 2.97782230\n",
      "Iteration 468, loss = 2.97726189\n",
      "Iteration 469, loss = 2.97719038\n",
      "Iteration 470, loss = 2.97700208\n",
      "Iteration 471, loss = 2.97669262\n",
      "Iteration 472, loss = 2.97600450\n",
      "Iteration 473, loss = 2.97538703\n",
      "Iteration 474, loss = 2.97498873\n",
      "Iteration 475, loss = 2.97438673\n",
      "Iteration 476, loss = 2.97385494\n",
      "Iteration 477, loss = 2.97320774\n",
      "Iteration 478, loss = 2.97329572\n",
      "Iteration 479, loss = 2.97260910\n",
      "Iteration 480, loss = 2.97239290\n",
      "Iteration 481, loss = 2.97212365\n",
      "Iteration 482, loss = 2.97094055\n",
      "Iteration 483, loss = 2.97062574\n",
      "Iteration 484, loss = 2.96979137\n",
      "Iteration 485, loss = 2.96972209\n",
      "Iteration 486, loss = 2.96951835\n",
      "Iteration 487, loss = 2.96893733\n",
      "Iteration 488, loss = 2.96938471\n",
      "Iteration 489, loss = 2.96960975\n",
      "Iteration 490, loss = 2.96834275\n",
      "Iteration 491, loss = 2.96755922\n",
      "Iteration 492, loss = 2.96699254\n",
      "Iteration 493, loss = 2.96627754\n",
      "Iteration 494, loss = 2.96577454\n",
      "Iteration 495, loss = 2.96552595\n",
      "Iteration 496, loss = 2.96528019\n",
      "Iteration 497, loss = 2.96496013\n",
      "Iteration 498, loss = 2.96406978\n",
      "Iteration 499, loss = 2.96362754\n",
      "Iteration 500, loss = 2.96314605\n",
      "Iteration 501, loss = 2.96284260\n",
      "Iteration 502, loss = 2.96253028\n",
      "Iteration 503, loss = 2.96187123\n",
      "Iteration 504, loss = 2.96123342\n",
      "Iteration 505, loss = 2.96097868\n",
      "Iteration 506, loss = 2.96036109\n",
      "Iteration 507, loss = 2.96005393\n",
      "Iteration 508, loss = 2.95956851\n",
      "Iteration 509, loss = 2.95917633\n",
      "Iteration 510, loss = 2.95858128\n",
      "Iteration 511, loss = 2.95864747\n",
      "Iteration 512, loss = 2.95817318\n",
      "Iteration 513, loss = 2.95769437\n",
      "Iteration 514, loss = 2.95738151\n",
      "Iteration 515, loss = 2.95686078\n",
      "Iteration 516, loss = 2.95656797\n",
      "Iteration 517, loss = 2.95619005\n",
      "Iteration 518, loss = 2.95533132\n",
      "Iteration 519, loss = 2.95488878\n",
      "Iteration 520, loss = 2.95503812\n",
      "Iteration 521, loss = 2.95490254\n",
      "Iteration 522, loss = 2.95421102\n",
      "Iteration 523, loss = 2.95273960\n",
      "Iteration 524, loss = 2.95238409\n",
      "Iteration 525, loss = 2.95187809\n",
      "Iteration 526, loss = 2.95155589\n",
      "Iteration 527, loss = 2.95108917\n",
      "Iteration 528, loss = 2.95052491\n",
      "Iteration 529, loss = 2.95023090\n",
      "Iteration 530, loss = 2.94986894\n",
      "Iteration 531, loss = 2.94958600\n",
      "Iteration 532, loss = 2.94876573\n",
      "Iteration 533, loss = 2.94796568\n",
      "Iteration 534, loss = 2.94836362\n",
      "Iteration 535, loss = 2.94823537\n",
      "Iteration 536, loss = 2.94720710\n",
      "Iteration 537, loss = 2.94696657\n",
      "Iteration 538, loss = 2.94653882\n",
      "Iteration 539, loss = 2.94607969\n",
      "Iteration 540, loss = 2.94546729\n",
      "Iteration 541, loss = 2.94500510\n",
      "Iteration 542, loss = 2.94438663\n",
      "Iteration 543, loss = 2.94371800\n",
      "Iteration 544, loss = 2.94351004\n",
      "Iteration 545, loss = 2.94370443\n",
      "Iteration 546, loss = 2.94293468\n",
      "Iteration 547, loss = 2.94236654\n",
      "Iteration 548, loss = 2.94169802\n",
      "Iteration 549, loss = 2.94140997\n",
      "Iteration 550, loss = 2.94109440\n",
      "Iteration 551, loss = 2.94064313\n",
      "Iteration 552, loss = 2.94030804\n",
      "Iteration 553, loss = 2.93977167\n",
      "Iteration 554, loss = 2.93950629\n",
      "Iteration 555, loss = 2.93885749\n",
      "Iteration 556, loss = 2.93826676\n",
      "Iteration 557, loss = 2.93776232\n",
      "Iteration 558, loss = 2.93783912\n",
      "Iteration 559, loss = 2.93700196\n",
      "Iteration 560, loss = 2.93651593\n",
      "Iteration 561, loss = 2.93601136\n",
      "Iteration 562, loss = 2.93561902\n",
      "Iteration 563, loss = 2.93526390\n",
      "Iteration 564, loss = 2.93487070\n",
      "Iteration 565, loss = 2.93472335\n",
      "Iteration 566, loss = 2.93405758\n",
      "Iteration 567, loss = 2.93394764\n",
      "Iteration 568, loss = 2.93319009\n",
      "Iteration 569, loss = 2.93263159\n",
      "Iteration 570, loss = 2.93262250\n",
      "Iteration 571, loss = 2.93246518\n",
      "Iteration 572, loss = 2.93187654\n",
      "Iteration 573, loss = 2.93135656\n",
      "Iteration 574, loss = 2.93120616\n",
      "Iteration 575, loss = 2.93037601\n",
      "Iteration 576, loss = 2.93005731\n",
      "Iteration 577, loss = 2.92981696\n",
      "Iteration 578, loss = 2.92919573\n",
      "Iteration 579, loss = 2.92861185\n",
      "Iteration 580, loss = 2.92837019\n",
      "Iteration 581, loss = 2.92821498\n",
      "Iteration 582, loss = 2.92807560\n",
      "Iteration 583, loss = 2.92729976\n",
      "Iteration 584, loss = 2.92710022\n",
      "Iteration 585, loss = 2.92648910\n",
      "Iteration 586, loss = 2.92627333\n",
      "Iteration 587, loss = 2.92530753\n",
      "Iteration 588, loss = 2.92521468\n",
      "Iteration 589, loss = 2.92449387\n",
      "Iteration 590, loss = 2.92423382\n",
      "Iteration 591, loss = 2.92388643\n",
      "Iteration 592, loss = 2.92315426\n",
      "Iteration 593, loss = 2.92249027\n",
      "Iteration 594, loss = 2.92197137\n",
      "Iteration 595, loss = 2.92172218\n",
      "Iteration 596, loss = 2.92144824\n",
      "Iteration 597, loss = 2.92130923\n",
      "Iteration 598, loss = 2.92064390\n",
      "Iteration 599, loss = 2.92031390\n",
      "Iteration 600, loss = 2.91970070\n",
      "Iteration 601, loss = 2.91929305\n",
      "Iteration 602, loss = 2.91931842\n",
      "Iteration 603, loss = 2.91903281\n",
      "Iteration 604, loss = 2.91832474\n",
      "Iteration 605, loss = 2.91804894\n",
      "Iteration 606, loss = 2.91725095\n",
      "Iteration 607, loss = 2.91708043\n",
      "Iteration 608, loss = 2.91732572\n",
      "Iteration 609, loss = 2.91617960\n",
      "Iteration 610, loss = 2.91552252\n",
      "Iteration 611, loss = 2.91585415\n",
      "Iteration 612, loss = 2.91526358\n",
      "Iteration 613, loss = 2.91469247\n",
      "Iteration 614, loss = 2.91405521\n",
      "Iteration 615, loss = 2.91373764\n",
      "Iteration 616, loss = 2.91314236\n",
      "Iteration 617, loss = 2.91287889\n",
      "Iteration 618, loss = 2.91240180\n",
      "Iteration 619, loss = 2.91252900\n",
      "Iteration 620, loss = 2.91167594\n",
      "Iteration 621, loss = 2.91168523\n",
      "Iteration 622, loss = 2.91122952\n",
      "Iteration 623, loss = 2.91079279\n",
      "Iteration 624, loss = 2.91033419\n",
      "Iteration 625, loss = 2.90989774\n",
      "Iteration 626, loss = 2.90929237\n",
      "Iteration 627, loss = 2.90934871\n",
      "Iteration 628, loss = 2.90876551\n",
      "Iteration 629, loss = 2.90823694\n",
      "Iteration 630, loss = 2.90770120\n",
      "Iteration 631, loss = 2.90724355\n",
      "Iteration 632, loss = 2.90656062\n",
      "Iteration 633, loss = 2.90640465\n",
      "Iteration 634, loss = 2.90579566\n",
      "Iteration 635, loss = 2.90565959\n",
      "Iteration 636, loss = 2.90517183\n",
      "Iteration 637, loss = 2.90472141\n",
      "Iteration 638, loss = 2.90486272\n",
      "Iteration 639, loss = 2.90410140\n",
      "Iteration 640, loss = 2.90338424\n",
      "Iteration 641, loss = 2.90298731\n",
      "Iteration 642, loss = 2.90269104\n",
      "Iteration 643, loss = 2.90230851\n",
      "Iteration 644, loss = 2.90169138\n",
      "Iteration 645, loss = 2.90153776\n",
      "Iteration 646, loss = 2.90110780\n",
      "Iteration 647, loss = 2.90077926\n",
      "Iteration 648, loss = 2.90006842\n",
      "Iteration 649, loss = 2.89973145\n",
      "Iteration 650, loss = 2.89936636\n",
      "Iteration 651, loss = 2.89921175\n",
      "Iteration 652, loss = 2.89858148\n",
      "Iteration 653, loss = 2.89829539\n",
      "Iteration 654, loss = 2.89801653\n",
      "Iteration 655, loss = 2.89715797\n",
      "Iteration 656, loss = 2.89691726\n",
      "Iteration 657, loss = 2.89689838\n",
      "Iteration 658, loss = 2.89627314\n",
      "Iteration 659, loss = 2.89551803\n",
      "Iteration 660, loss = 2.89557533\n",
      "Iteration 661, loss = 2.89500313\n",
      "Iteration 662, loss = 2.89470432\n",
      "Iteration 663, loss = 2.89418184\n",
      "Iteration 664, loss = 2.89394297\n",
      "Iteration 665, loss = 2.89349202\n",
      "Iteration 666, loss = 2.89315047\n",
      "Iteration 667, loss = 2.89268227\n",
      "Iteration 668, loss = 2.89206683\n",
      "Iteration 669, loss = 2.89179687\n",
      "Iteration 670, loss = 2.89122220\n",
      "Iteration 671, loss = 2.89100488\n",
      "Iteration 672, loss = 2.89028516\n",
      "Iteration 673, loss = 2.89035088\n",
      "Iteration 674, loss = 2.88990262\n",
      "Iteration 675, loss = 2.88943301\n",
      "Iteration 676, loss = 2.88904015\n",
      "Iteration 677, loss = 2.88841008\n",
      "Iteration 678, loss = 2.88792652\n",
      "Iteration 679, loss = 2.88747443\n",
      "Iteration 680, loss = 2.88737792\n",
      "Iteration 681, loss = 2.88687609\n",
      "Iteration 682, loss = 2.88641321\n",
      "Iteration 683, loss = 2.88631934\n",
      "Iteration 684, loss = 2.88607774\n",
      "Iteration 685, loss = 2.88592396\n",
      "Iteration 686, loss = 2.88548610\n",
      "Iteration 687, loss = 2.88512636\n",
      "Iteration 688, loss = 2.88484705\n",
      "Iteration 689, loss = 2.88444679\n",
      "Iteration 690, loss = 2.88348304\n",
      "Iteration 691, loss = 2.88318664\n",
      "Iteration 692, loss = 2.88323830\n",
      "Iteration 693, loss = 2.88296524\n",
      "Iteration 694, loss = 2.88196379\n",
      "Iteration 695, loss = 2.88177991\n",
      "Iteration 696, loss = 2.88133052\n",
      "Iteration 697, loss = 2.88080728\n",
      "Iteration 698, loss = 2.88032884\n",
      "Iteration 699, loss = 2.88024521\n",
      "Iteration 700, loss = 2.88012318\n",
      "Iteration 701, loss = 2.87972283\n",
      "Iteration 702, loss = 2.87875059\n",
      "Iteration 703, loss = 2.87857758\n",
      "Iteration 704, loss = 2.87845702\n",
      "Iteration 705, loss = 2.87786406\n",
      "Iteration 706, loss = 2.87743998\n",
      "Iteration 707, loss = 2.87747332\n",
      "Iteration 708, loss = 2.87720599\n",
      "Iteration 709, loss = 2.87704466\n",
      "Iteration 710, loss = 2.87633470\n",
      "Iteration 711, loss = 2.87620773\n",
      "Iteration 712, loss = 2.87539598\n",
      "Iteration 713, loss = 2.87469946\n",
      "Iteration 714, loss = 2.87483294\n",
      "Iteration 715, loss = 2.87475962\n",
      "Iteration 716, loss = 2.87352583\n",
      "Iteration 717, loss = 2.87343539\n",
      "Iteration 718, loss = 2.87315861\n",
      "Iteration 719, loss = 2.87296661\n",
      "Iteration 720, loss = 2.87235390\n",
      "Iteration 721, loss = 2.87191005\n",
      "Iteration 722, loss = 2.87210042\n",
      "Iteration 723, loss = 2.87173740\n",
      "Iteration 724, loss = 2.87045828\n",
      "Iteration 725, loss = 2.87053882\n",
      "Iteration 726, loss = 2.86981046\n",
      "Iteration 727, loss = 2.86945621\n",
      "Iteration 728, loss = 2.86912360\n",
      "Iteration 729, loss = 2.86831209\n",
      "Iteration 730, loss = 2.86828577\n",
      "Iteration 731, loss = 2.86778302\n",
      "Iteration 732, loss = 2.86763237\n",
      "Iteration 733, loss = 2.86770888\n",
      "Iteration 734, loss = 2.86730715\n",
      "Iteration 735, loss = 2.86672048\n",
      "Iteration 736, loss = 2.86619166\n",
      "Iteration 737, loss = 2.86653417\n",
      "Iteration 738, loss = 2.86604894\n",
      "Iteration 739, loss = 2.86592712\n",
      "Iteration 740, loss = 2.86566918\n",
      "Iteration 741, loss = 2.86482029\n",
      "Iteration 742, loss = 2.86414742\n",
      "Iteration 743, loss = 2.86393267\n",
      "Iteration 744, loss = 2.86329154\n",
      "Iteration 745, loss = 2.86337727\n",
      "Iteration 746, loss = 2.86267191\n",
      "Iteration 747, loss = 2.86223857\n",
      "Iteration 748, loss = 2.86186710\n",
      "Iteration 749, loss = 2.86127821\n",
      "Iteration 750, loss = 2.86139732\n",
      "Iteration 751, loss = 2.86052411\n",
      "Iteration 752, loss = 2.86028733\n",
      "Iteration 753, loss = 2.86023451\n",
      "Iteration 754, loss = 2.85987098\n",
      "Iteration 755, loss = 2.85921192\n",
      "Iteration 756, loss = 2.85941258\n",
      "Iteration 757, loss = 2.85968680\n",
      "Iteration 758, loss = 2.85871973\n",
      "Iteration 759, loss = 2.85825618\n",
      "Iteration 760, loss = 2.85769451\n",
      "Iteration 761, loss = 2.85757589\n",
      "Iteration 762, loss = 2.85722062\n",
      "Iteration 763, loss = 2.85644514\n",
      "Iteration 764, loss = 2.85602117\n",
      "Iteration 765, loss = 2.85556801\n",
      "Iteration 766, loss = 2.85570025\n",
      "Iteration 767, loss = 2.85541280\n",
      "Iteration 768, loss = 2.85498556\n",
      "Iteration 769, loss = 2.85473162\n",
      "Iteration 770, loss = 2.85418535\n",
      "Iteration 771, loss = 2.85352765\n",
      "Iteration 772, loss = 2.85344811\n",
      "Iteration 773, loss = 2.85401844\n",
      "Iteration 774, loss = 2.85302234\n",
      "Iteration 775, loss = 2.85243116\n",
      "Iteration 776, loss = 2.85217253\n",
      "Iteration 777, loss = 2.85161946\n",
      "Iteration 778, loss = 2.85108702\n",
      "Iteration 779, loss = 2.85134799\n",
      "Iteration 780, loss = 2.85056649\n",
      "Iteration 781, loss = 2.85015763\n",
      "Iteration 782, loss = 2.84976477\n",
      "Iteration 783, loss = 2.84976383\n",
      "Iteration 784, loss = 2.84938894\n",
      "Iteration 785, loss = 2.84886405\n",
      "Iteration 786, loss = 2.84876270\n",
      "Iteration 787, loss = 2.84839317\n",
      "Iteration 788, loss = 2.84869541\n",
      "Iteration 789, loss = 2.84811725\n",
      "Iteration 790, loss = 2.84768101\n",
      "Iteration 791, loss = 2.84774333\n",
      "Iteration 792, loss = 2.84635438\n",
      "Iteration 793, loss = 2.84581373\n",
      "Iteration 794, loss = 2.84547597\n",
      "Iteration 795, loss = 2.84525505\n",
      "Iteration 796, loss = 2.84513076\n",
      "Iteration 797, loss = 2.84488786\n",
      "Iteration 798, loss = 2.84437348\n",
      "Iteration 799, loss = 2.84425495\n",
      "Iteration 800, loss = 2.84338906\n",
      "Iteration 801, loss = 2.84294200\n",
      "Iteration 802, loss = 2.84263482\n",
      "Iteration 803, loss = 2.84232538\n",
      "Iteration 804, loss = 2.84248936\n",
      "Iteration 805, loss = 2.84176994\n",
      "Iteration 806, loss = 2.84198492\n",
      "Iteration 807, loss = 2.84158172\n",
      "Iteration 808, loss = 2.84103267\n",
      "Iteration 809, loss = 2.84045082\n",
      "Iteration 810, loss = 2.84008395\n",
      "Iteration 811, loss = 2.84025071\n",
      "Iteration 812, loss = 2.83997687\n",
      "Iteration 813, loss = 2.83920763\n",
      "Iteration 814, loss = 2.83857131\n",
      "Iteration 815, loss = 2.83837973\n",
      "Iteration 816, loss = 2.83811183\n",
      "Iteration 817, loss = 2.83749917\n",
      "Iteration 818, loss = 2.83729915\n",
      "Iteration 819, loss = 2.83701772\n",
      "Iteration 820, loss = 2.83714740\n",
      "Iteration 821, loss = 2.83638349\n",
      "Iteration 822, loss = 2.83605038\n",
      "Iteration 823, loss = 2.83609261\n",
      "Iteration 824, loss = 2.83595229\n",
      "Iteration 825, loss = 2.83608865\n",
      "Iteration 826, loss = 2.83471082\n",
      "Iteration 827, loss = 2.83453681\n",
      "Iteration 828, loss = 2.83400292\n",
      "Iteration 829, loss = 2.83423324\n",
      "Iteration 830, loss = 2.83433004\n",
      "Iteration 831, loss = 2.83349660\n",
      "Iteration 832, loss = 2.83257082\n",
      "Iteration 833, loss = 2.83204627\n",
      "Iteration 834, loss = 2.83191725\n",
      "Iteration 835, loss = 2.83147434\n",
      "Iteration 836, loss = 2.83107504\n",
      "Iteration 837, loss = 2.83171172\n",
      "Iteration 838, loss = 2.83032537\n",
      "Iteration 839, loss = 2.83071628\n",
      "Iteration 840, loss = 2.82995620\n",
      "Iteration 841, loss = 2.82980083\n",
      "Iteration 842, loss = 2.82973924\n",
      "Iteration 843, loss = 2.82882019\n",
      "Iteration 844, loss = 2.82819940\n",
      "Iteration 845, loss = 2.82934445\n",
      "Iteration 846, loss = 2.82882045\n",
      "Iteration 847, loss = 2.82759103\n",
      "Iteration 848, loss = 2.82689443\n",
      "Iteration 849, loss = 2.82674107\n",
      "Iteration 850, loss = 2.82610792\n",
      "Iteration 851, loss = 2.82622253\n",
      "Iteration 852, loss = 2.82594137\n",
      "Iteration 853, loss = 2.82554802\n",
      "Iteration 854, loss = 2.82498405\n",
      "Iteration 855, loss = 2.82495915\n",
      "Iteration 856, loss = 2.82500175\n",
      "Iteration 857, loss = 2.82449323\n",
      "Iteration 858, loss = 2.82411776\n",
      "Iteration 859, loss = 2.82366306\n",
      "Iteration 860, loss = 2.82373006\n",
      "Iteration 861, loss = 2.82296374\n",
      "Iteration 862, loss = 2.82285095\n",
      "Iteration 863, loss = 2.82293825\n",
      "Iteration 864, loss = 2.82207772\n",
      "Iteration 865, loss = 2.82238992\n",
      "Iteration 866, loss = 2.82157180\n",
      "Iteration 867, loss = 2.82058815\n",
      "Iteration 868, loss = 2.82081681\n",
      "Iteration 869, loss = 2.82038619\n",
      "Iteration 870, loss = 2.81982670\n",
      "Iteration 871, loss = 2.81918401\n",
      "Iteration 872, loss = 2.81949998\n",
      "Iteration 873, loss = 2.81867889\n",
      "Iteration 874, loss = 2.81825050\n",
      "Iteration 875, loss = 2.81810095\n",
      "Iteration 876, loss = 2.81795702\n",
      "Iteration 877, loss = 2.81763470\n",
      "Iteration 878, loss = 2.81736659\n",
      "Iteration 879, loss = 2.81721805\n",
      "Iteration 880, loss = 2.81699963\n",
      "Iteration 881, loss = 2.81615268\n",
      "Iteration 882, loss = 2.81571164\n",
      "Iteration 883, loss = 2.81551466\n",
      "Iteration 884, loss = 2.81528454\n",
      "Iteration 885, loss = 2.81505937\n",
      "Iteration 886, loss = 2.81434209\n",
      "Iteration 887, loss = 2.81430503\n",
      "Iteration 888, loss = 2.81433199\n",
      "Iteration 889, loss = 2.81411686\n",
      "Iteration 890, loss = 2.81363015\n",
      "Iteration 891, loss = 2.81313758\n",
      "Iteration 892, loss = 2.81325563\n",
      "Iteration 893, loss = 2.81287437\n",
      "Iteration 894, loss = 2.81216129\n",
      "Iteration 895, loss = 2.81184456\n",
      "Iteration 896, loss = 2.81198313\n",
      "Iteration 897, loss = 2.81192473\n",
      "Iteration 898, loss = 2.81143835\n",
      "Iteration 899, loss = 2.81081426\n",
      "Iteration 900, loss = 2.81003578\n",
      "Iteration 901, loss = 2.81063147\n",
      "Iteration 902, loss = 2.81029779\n",
      "Iteration 903, loss = 2.80965710\n",
      "Iteration 904, loss = 2.80867165\n",
      "Iteration 905, loss = 2.80817231\n",
      "Iteration 906, loss = 2.80826258\n",
      "Iteration 907, loss = 2.80807859\n",
      "Iteration 908, loss = 2.80764271\n",
      "Iteration 909, loss = 2.80749880\n",
      "Iteration 910, loss = 2.80781955\n",
      "Iteration 911, loss = 2.80662810\n",
      "Iteration 912, loss = 2.80649605\n",
      "Iteration 913, loss = 2.80667824\n",
      "Iteration 914, loss = 2.80597735\n",
      "Iteration 915, loss = 2.80589021\n",
      "Iteration 916, loss = 2.80548637\n",
      "Iteration 917, loss = 2.80554189\n",
      "Iteration 918, loss = 2.80504755\n",
      "Iteration 919, loss = 2.80468721\n",
      "Iteration 920, loss = 2.80383636\n",
      "Iteration 921, loss = 2.80424523\n",
      "Iteration 922, loss = 2.80385695\n",
      "Iteration 923, loss = 2.80324286\n",
      "Iteration 924, loss = 2.80277567\n",
      "Iteration 925, loss = 2.80246064\n",
      "Iteration 926, loss = 2.80212839\n",
      "Iteration 927, loss = 2.80169470\n",
      "Iteration 928, loss = 2.80153335\n",
      "Iteration 929, loss = 2.80144148\n",
      "Iteration 930, loss = 2.80089769\n",
      "Iteration 931, loss = 2.80097037\n",
      "Iteration 932, loss = 2.80056202\n",
      "Iteration 933, loss = 2.80058585\n",
      "Iteration 934, loss = 2.80022477\n",
      "Iteration 935, loss = 2.79965417\n",
      "Iteration 936, loss = 2.79958510\n",
      "Iteration 937, loss = 2.79908246\n",
      "Iteration 938, loss = 2.79872836\n",
      "Iteration 939, loss = 2.79892426\n",
      "Iteration 940, loss = 2.79856503\n",
      "Iteration 941, loss = 2.79795647\n",
      "Iteration 942, loss = 2.79784919\n",
      "Iteration 943, loss = 2.79711539\n",
      "Iteration 944, loss = 2.79687643\n",
      "Iteration 945, loss = 2.79647052\n",
      "Iteration 946, loss = 2.79582551\n",
      "Iteration 947, loss = 2.79573299\n",
      "Iteration 948, loss = 2.79572839\n",
      "Iteration 949, loss = 2.79504358\n",
      "Iteration 950, loss = 2.79505488\n",
      "Iteration 951, loss = 2.79415980\n",
      "Iteration 952, loss = 2.79416085\n",
      "Iteration 953, loss = 2.79434128\n",
      "Iteration 954, loss = 2.79377563\n",
      "Iteration 955, loss = 2.79342039\n",
      "Iteration 956, loss = 2.79335664\n",
      "Iteration 957, loss = 2.79314406\n",
      "Iteration 958, loss = 2.79288510\n",
      "Iteration 959, loss = 2.79236653\n",
      "Iteration 960, loss = 2.79236217\n",
      "Iteration 961, loss = 2.79170152\n",
      "Iteration 962, loss = 2.79106009\n",
      "Iteration 963, loss = 2.79096149\n",
      "Iteration 964, loss = 2.79082547\n",
      "Iteration 965, loss = 2.79080644\n",
      "Iteration 966, loss = 2.79011906\n",
      "Iteration 967, loss = 2.78969905\n",
      "Iteration 968, loss = 2.78880606\n",
      "Iteration 969, loss = 2.78883380\n",
      "Iteration 970, loss = 2.78850140\n",
      "Iteration 971, loss = 2.78858208\n",
      "Iteration 972, loss = 2.78807306\n",
      "Iteration 973, loss = 2.78790539\n",
      "Iteration 974, loss = 2.78773829\n",
      "Iteration 975, loss = 2.78716382\n",
      "Iteration 976, loss = 2.78708323\n",
      "Iteration 977, loss = 2.78698736\n",
      "Iteration 978, loss = 2.78637815\n",
      "Iteration 979, loss = 2.78579714\n",
      "Iteration 980, loss = 2.78545509\n",
      "Iteration 981, loss = 2.78565771\n",
      "Iteration 982, loss = 2.78512043\n",
      "Iteration 983, loss = 2.78484444\n",
      "Iteration 984, loss = 2.78461235\n",
      "Iteration 985, loss = 2.78432305\n",
      "Iteration 986, loss = 2.78377928\n",
      "Iteration 987, loss = 2.78322192\n",
      "Iteration 988, loss = 2.78316267\n",
      "Iteration 989, loss = 2.78222714\n",
      "Iteration 990, loss = 2.78235467\n",
      "Iteration 991, loss = 2.78230111\n",
      "Iteration 992, loss = 2.78214335\n",
      "Iteration 993, loss = 2.78201873\n",
      "Iteration 994, loss = 2.78186906\n",
      "Iteration 995, loss = 2.78164771\n",
      "Iteration 996, loss = 2.78106551\n",
      "Iteration 997, loss = 2.78130841\n",
      "Iteration 998, loss = 2.78076260\n",
      "Iteration 999, loss = 2.78016756\n",
      "Iteration 1000, loss = 2.78050613\n",
      "Iteration 1001, loss = 2.77975333\n",
      "Iteration 1002, loss = 2.77940574\n",
      "Iteration 1003, loss = 2.77893253\n",
      "Iteration 1004, loss = 2.77860096\n",
      "Iteration 1005, loss = 2.77839575\n",
      "Iteration 1006, loss = 2.77816252\n",
      "Iteration 1007, loss = 2.77820251\n",
      "Iteration 1008, loss = 2.77768307\n",
      "Iteration 1009, loss = 2.77698933\n",
      "Iteration 1010, loss = 2.77717462\n",
      "Iteration 1011, loss = 2.77681565\n",
      "Iteration 1012, loss = 2.77625681\n",
      "Iteration 1013, loss = 2.77584538\n",
      "Iteration 1014, loss = 2.77611777\n",
      "Iteration 1015, loss = 2.77633995\n",
      "Iteration 1016, loss = 2.77508936\n",
      "Iteration 1017, loss = 2.77447798\n",
      "Iteration 1018, loss = 2.77459818\n",
      "Iteration 1019, loss = 2.77408530\n",
      "Iteration 1020, loss = 2.77383454\n",
      "Iteration 1021, loss = 2.77354285\n",
      "Iteration 1022, loss = 2.77313596\n",
      "Iteration 1023, loss = 2.77324263\n",
      "Iteration 1024, loss = 2.77397545\n",
      "Iteration 1025, loss = 2.77364339\n",
      "Iteration 1026, loss = 2.77208634\n",
      "Iteration 1027, loss = 2.77198806\n",
      "Iteration 1028, loss = 2.77130142\n",
      "Iteration 1029, loss = 2.77169031\n",
      "Iteration 1030, loss = 2.77094399\n",
      "Iteration 1031, loss = 2.77104158\n",
      "Iteration 1032, loss = 2.77084287\n",
      "Iteration 1033, loss = 2.77042337\n",
      "Iteration 1034, loss = 2.77013437\n",
      "Iteration 1035, loss = 2.76948604\n",
      "Iteration 1036, loss = 2.77015819\n",
      "Iteration 1037, loss = 2.76962941\n",
      "Iteration 1038, loss = 2.76890786\n",
      "Iteration 1039, loss = 2.76930630\n",
      "Iteration 1040, loss = 2.76923327\n",
      "Iteration 1041, loss = 2.76797290\n",
      "Iteration 1042, loss = 2.76808979\n",
      "Iteration 1043, loss = 2.76846450\n",
      "Iteration 1044, loss = 2.76808106\n",
      "Iteration 1045, loss = 2.76738276\n",
      "Iteration 1046, loss = 2.76621281\n",
      "Iteration 1047, loss = 2.76629449\n",
      "Iteration 1048, loss = 2.76673262\n",
      "Iteration 1049, loss = 2.76627790\n",
      "Iteration 1050, loss = 2.76551868\n",
      "Iteration 1051, loss = 2.76516681\n",
      "Iteration 1052, loss = 2.76551912\n",
      "Iteration 1053, loss = 2.76461087\n",
      "Iteration 1054, loss = 2.76512575\n",
      "Iteration 1055, loss = 2.76462109\n",
      "Iteration 1056, loss = 2.76398989\n",
      "Iteration 1057, loss = 2.76316240\n",
      "Iteration 1058, loss = 2.76314692\n",
      "Iteration 1059, loss = 2.76244146\n",
      "Iteration 1060, loss = 2.76261540\n",
      "Iteration 1061, loss = 2.76228303\n",
      "Iteration 1062, loss = 2.76188626\n",
      "Iteration 1063, loss = 2.76183294\n",
      "Iteration 1064, loss = 2.76107588\n",
      "Iteration 1065, loss = 2.76122904\n",
      "Iteration 1066, loss = 2.76118223\n",
      "Iteration 1067, loss = 2.76080999\n",
      "Iteration 1068, loss = 2.76050249\n",
      "Iteration 1069, loss = 2.76033917\n",
      "Iteration 1070, loss = 2.76001712\n",
      "Iteration 1071, loss = 2.75989332\n",
      "Iteration 1072, loss = 2.76005748\n",
      "Iteration 1073, loss = 2.75965757\n",
      "Iteration 1074, loss = 2.75872725\n",
      "Iteration 1075, loss = 2.75934765\n",
      "Iteration 1076, loss = 2.75948144\n",
      "Iteration 1077, loss = 2.75862730\n",
      "Iteration 1078, loss = 2.75827910\n",
      "Iteration 1079, loss = 2.75790008\n",
      "Iteration 1080, loss = 2.75771035\n",
      "Iteration 1081, loss = 2.75669931\n",
      "Iteration 1082, loss = 2.75682831\n",
      "Iteration 1083, loss = 2.75628089\n",
      "Iteration 1084, loss = 2.75678844\n",
      "Iteration 1085, loss = 2.75640591\n",
      "Iteration 1086, loss = 2.75630753\n",
      "Iteration 1087, loss = 2.75573020\n",
      "Iteration 1088, loss = 2.75542621\n",
      "Iteration 1089, loss = 2.75493854\n",
      "Iteration 1090, loss = 2.75484163\n",
      "Iteration 1091, loss = 2.75412601\n",
      "Iteration 1092, loss = 2.75392962\n",
      "Iteration 1093, loss = 2.75357031\n",
      "Iteration 1094, loss = 2.75351379\n",
      "Iteration 1095, loss = 2.75283340\n",
      "Iteration 1096, loss = 2.75277984\n",
      "Iteration 1097, loss = 2.75318135\n",
      "Iteration 1098, loss = 2.75282424\n",
      "Iteration 1099, loss = 2.75193787\n",
      "Iteration 1100, loss = 2.75154362\n",
      "Iteration 1101, loss = 2.75155914\n",
      "Iteration 1102, loss = 2.75117370\n",
      "Iteration 1103, loss = 2.75099259\n",
      "Iteration 1104, loss = 2.75083131\n",
      "Iteration 1105, loss = 2.75089699\n",
      "Iteration 1106, loss = 2.75093948\n",
      "Iteration 1107, loss = 2.75019861\n",
      "Iteration 1108, loss = 2.74941909\n",
      "Iteration 1109, loss = 2.74962264\n",
      "Iteration 1110, loss = 2.74909272\n",
      "Iteration 1111, loss = 2.74942895\n",
      "Iteration 1112, loss = 2.74863624\n",
      "Iteration 1113, loss = 2.74838055\n",
      "Iteration 1114, loss = 2.74788565\n",
      "Iteration 1115, loss = 2.74752103\n",
      "Iteration 1116, loss = 2.74745602\n",
      "Iteration 1117, loss = 2.74743599\n",
      "Iteration 1118, loss = 2.74724072\n",
      "Iteration 1119, loss = 2.74734818\n",
      "Iteration 1120, loss = 2.74659689\n",
      "Iteration 1121, loss = 2.74637239\n",
      "Iteration 1122, loss = 2.74581702\n",
      "Iteration 1123, loss = 2.74570038\n",
      "Iteration 1124, loss = 2.74546040\n",
      "Iteration 1125, loss = 2.74477270\n",
      "Iteration 1126, loss = 2.74477224\n",
      "Iteration 1127, loss = 2.74429856\n",
      "Iteration 1128, loss = 2.74371469\n",
      "Iteration 1129, loss = 2.74415083\n",
      "Iteration 1130, loss = 2.74360529\n",
      "Iteration 1131, loss = 2.74311857\n",
      "Iteration 1132, loss = 2.74300289\n",
      "Iteration 1133, loss = 2.74299517\n",
      "Iteration 1134, loss = 2.74309141\n",
      "Iteration 1135, loss = 2.74287881\n",
      "Iteration 1136, loss = 2.74226354\n",
      "Iteration 1137, loss = 2.74223965\n",
      "Iteration 1138, loss = 2.74190466\n",
      "Iteration 1139, loss = 2.74117667\n",
      "Iteration 1140, loss = 2.74103067\n",
      "Iteration 1141, loss = 2.74122466\n",
      "Iteration 1142, loss = 2.74115963\n",
      "Iteration 1143, loss = 2.74069099\n",
      "Iteration 1144, loss = 2.74001571\n",
      "Iteration 1145, loss = 2.74000626\n",
      "Iteration 1146, loss = 2.73938217\n",
      "Iteration 1147, loss = 2.73897097\n",
      "Iteration 1148, loss = 2.73926572\n",
      "Iteration 1149, loss = 2.73904481\n",
      "Iteration 1150, loss = 2.73854642\n",
      "Iteration 1151, loss = 2.73888708\n",
      "Iteration 1152, loss = 2.73899732\n",
      "Iteration 1153, loss = 2.73807387\n",
      "Iteration 1154, loss = 2.73813440\n",
      "Iteration 1155, loss = 2.73763805\n",
      "Iteration 1156, loss = 2.73738721\n",
      "Iteration 1157, loss = 2.73703231\n",
      "Iteration 1158, loss = 2.73676961\n",
      "Iteration 1159, loss = 2.73666494\n",
      "Iteration 1160, loss = 2.73574440\n",
      "Iteration 1161, loss = 2.73566152\n",
      "Iteration 1162, loss = 2.73541449\n",
      "Iteration 1163, loss = 2.73541850\n",
      "Iteration 1164, loss = 2.73495713\n",
      "Iteration 1165, loss = 2.73465213\n",
      "Iteration 1166, loss = 2.73488596\n",
      "Iteration 1167, loss = 2.73460774\n",
      "Iteration 1168, loss = 2.73401852\n",
      "Iteration 1169, loss = 2.73375157\n",
      "Iteration 1170, loss = 2.73367343\n",
      "Iteration 1171, loss = 2.73356267\n",
      "Iteration 1172, loss = 2.73283770\n",
      "Iteration 1173, loss = 2.73368798\n",
      "Iteration 1174, loss = 2.73297199\n",
      "Iteration 1175, loss = 2.73260325\n",
      "Iteration 1176, loss = 2.73233677\n",
      "Iteration 1177, loss = 2.73204020\n",
      "Iteration 1178, loss = 2.73159448\n",
      "Iteration 1179, loss = 2.73151443\n",
      "Iteration 1180, loss = 2.73117273\n",
      "Iteration 1181, loss = 2.73104919\n",
      "Iteration 1182, loss = 2.73047424\n",
      "Iteration 1183, loss = 2.72976510\n",
      "Iteration 1184, loss = 2.72988361\n",
      "Iteration 1185, loss = 2.72967808\n",
      "Iteration 1186, loss = 2.72962565\n",
      "Iteration 1187, loss = 2.72936820\n",
      "Iteration 1188, loss = 2.72820168\n",
      "Iteration 1189, loss = 2.72853992\n",
      "Iteration 1190, loss = 2.72867256\n",
      "Iteration 1191, loss = 2.72823438\n",
      "Iteration 1192, loss = 2.72766308\n",
      "Iteration 1193, loss = 2.72789110\n",
      "Iteration 1194, loss = 2.72741265\n",
      "Iteration 1195, loss = 2.72727113\n",
      "Iteration 1196, loss = 2.72698360\n",
      "Iteration 1197, loss = 2.72715526\n",
      "Iteration 1198, loss = 2.72675632\n",
      "Iteration 1199, loss = 2.72660340\n",
      "Iteration 1200, loss = 2.72542239\n",
      "Iteration 1201, loss = 2.72549512\n",
      "Iteration 1202, loss = 2.72560984\n",
      "Iteration 1203, loss = 2.72511224\n",
      "Iteration 1204, loss = 2.72501221\n",
      "Iteration 1205, loss = 2.72457294\n",
      "Iteration 1206, loss = 2.72399187\n",
      "Iteration 1207, loss = 2.72412353\n",
      "Iteration 1208, loss = 2.72413878\n",
      "Iteration 1209, loss = 2.72377628\n",
      "Iteration 1210, loss = 2.72343506\n",
      "Iteration 1211, loss = 2.72296814\n",
      "Iteration 1212, loss = 2.72298976\n",
      "Iteration 1213, loss = 2.72283535\n",
      "Iteration 1214, loss = 2.72258714\n",
      "Iteration 1215, loss = 2.72216974\n",
      "Iteration 1216, loss = 2.72208526\n",
      "Iteration 1217, loss = 2.72182351\n",
      "Iteration 1218, loss = 2.72173406\n",
      "Iteration 1219, loss = 2.72178028\n",
      "Iteration 1220, loss = 2.72066422\n",
      "Iteration 1221, loss = 2.72068931\n",
      "Iteration 1222, loss = 2.72047161\n",
      "Iteration 1223, loss = 2.71996488\n",
      "Iteration 1224, loss = 2.71969022\n",
      "Iteration 1225, loss = 2.71952081\n",
      "Iteration 1226, loss = 2.71914454\n",
      "Iteration 1227, loss = 2.71901207\n",
      "Iteration 1228, loss = 2.71896314\n",
      "Iteration 1229, loss = 2.71885653\n",
      "Iteration 1230, loss = 2.71857068\n",
      "Iteration 1231, loss = 2.71821623\n",
      "Iteration 1232, loss = 2.71787321\n",
      "Iteration 1233, loss = 2.71768372\n",
      "Iteration 1234, loss = 2.71752697\n",
      "Iteration 1235, loss = 2.71730053\n",
      "Iteration 1236, loss = 2.71715826\n",
      "Iteration 1237, loss = 2.71729401\n",
      "Iteration 1238, loss = 2.71658550\n",
      "Iteration 1239, loss = 2.71635281\n",
      "Iteration 1240, loss = 2.71569871\n",
      "Iteration 1241, loss = 2.71579968\n",
      "Iteration 1242, loss = 2.71636257\n",
      "Iteration 1243, loss = 2.71528087\n",
      "Iteration 1244, loss = 2.71467666\n",
      "Iteration 1245, loss = 2.71460608\n",
      "Iteration 1246, loss = 2.71478283\n",
      "Iteration 1247, loss = 2.71421294\n",
      "Iteration 1248, loss = 2.71406038\n",
      "Iteration 1249, loss = 2.71401527\n",
      "Iteration 1250, loss = 2.71356067\n",
      "Iteration 1251, loss = 2.71338271\n",
      "Iteration 1252, loss = 2.71337911\n",
      "Iteration 1253, loss = 2.71326961\n",
      "Iteration 1254, loss = 2.71330303\n",
      "Iteration 1255, loss = 2.71299904\n",
      "Iteration 1256, loss = 2.71265168\n",
      "Iteration 1257, loss = 2.71168587\n",
      "Iteration 1258, loss = 2.71233471\n",
      "Iteration 1259, loss = 2.71215997\n",
      "Iteration 1260, loss = 2.71143191\n",
      "Iteration 1261, loss = 2.71123676\n",
      "Iteration 1262, loss = 2.71092113\n",
      "Iteration 1263, loss = 2.71020041\n",
      "Iteration 1264, loss = 2.71022453\n",
      "Iteration 1265, loss = 2.70995360\n",
      "Iteration 1266, loss = 2.70992703\n",
      "Iteration 1267, loss = 2.70980074\n",
      "Iteration 1268, loss = 2.70933280\n",
      "Iteration 1269, loss = 2.70928379\n",
      "Iteration 1, loss = 4.16017660\n",
      "Iteration 2, loss = 4.11940492\n",
      "Iteration 3, loss = 4.07943361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 4.03944902\n",
      "Iteration 5, loss = 3.99798894\n",
      "Iteration 6, loss = 3.95426399\n",
      "Iteration 7, loss = 3.90806736\n",
      "Iteration 8, loss = 3.85912084\n",
      "Iteration 9, loss = 3.80677085\n",
      "Iteration 10, loss = 3.75286114\n",
      "Iteration 11, loss = 3.69595225\n",
      "Iteration 12, loss = 3.64148373\n",
      "Iteration 13, loss = 3.58442595\n",
      "Iteration 14, loss = 3.52887801\n",
      "Iteration 15, loss = 3.47540255\n",
      "Iteration 16, loss = 3.42602285\n",
      "Iteration 17, loss = 3.38247096\n",
      "Iteration 18, loss = 3.34535143\n",
      "Iteration 19, loss = 3.31476494\n",
      "Iteration 20, loss = 3.29050046\n",
      "Iteration 21, loss = 3.27122842\n",
      "Iteration 22, loss = 3.25667041\n",
      "Iteration 23, loss = 3.24768533\n",
      "Iteration 24, loss = 3.24144527\n",
      "Iteration 25, loss = 3.23661225\n",
      "Iteration 26, loss = 3.23386326\n",
      "Iteration 27, loss = 3.23189836\n",
      "Iteration 28, loss = 3.23045010\n",
      "Iteration 29, loss = 3.22912774\n",
      "Iteration 30, loss = 3.22827606\n",
      "Iteration 31, loss = 3.22741572\n",
      "Iteration 32, loss = 3.22670196\n",
      "Iteration 33, loss = 3.22594383\n",
      "Iteration 34, loss = 3.22519588\n",
      "Iteration 35, loss = 3.22404490\n",
      "Iteration 36, loss = 3.22353460\n",
      "Iteration 37, loss = 3.22259323\n",
      "Iteration 38, loss = 3.22180982\n",
      "Iteration 39, loss = 3.22105301\n",
      "Iteration 40, loss = 3.22021324\n",
      "Iteration 41, loss = 3.21948745\n",
      "Iteration 42, loss = 3.21895090\n",
      "Iteration 43, loss = 3.21861908\n",
      "Iteration 44, loss = 3.21784738\n",
      "Iteration 45, loss = 3.21689101\n",
      "Iteration 46, loss = 3.21604620\n",
      "Iteration 47, loss = 3.21544036\n",
      "Iteration 48, loss = 3.21487866\n",
      "Iteration 49, loss = 3.21426603\n",
      "Iteration 50, loss = 3.21354106\n",
      "Iteration 51, loss = 3.21316490\n",
      "Iteration 52, loss = 3.21274639\n",
      "Iteration 53, loss = 3.21216653\n",
      "Iteration 54, loss = 3.21153352\n",
      "Iteration 55, loss = 3.21111032\n",
      "Iteration 56, loss = 3.21014566\n",
      "Iteration 57, loss = 3.20953431\n",
      "Iteration 58, loss = 3.20860920\n",
      "Iteration 59, loss = 3.20841521\n",
      "Iteration 60, loss = 3.20768269\n",
      "Iteration 61, loss = 3.20746131\n",
      "Iteration 62, loss = 3.20671182\n",
      "Iteration 63, loss = 3.20605220\n",
      "Iteration 64, loss = 3.20484889\n",
      "Iteration 65, loss = 3.20434268\n",
      "Iteration 66, loss = 3.20380878\n",
      "Iteration 67, loss = 3.20352639\n",
      "Iteration 68, loss = 3.20279550\n",
      "Iteration 69, loss = 3.20205953\n",
      "Iteration 70, loss = 3.20131377\n",
      "Iteration 71, loss = 3.20077145\n",
      "Iteration 72, loss = 3.19981811\n",
      "Iteration 73, loss = 3.19933409\n",
      "Iteration 74, loss = 3.19870285\n",
      "Iteration 75, loss = 3.19802056\n",
      "Iteration 76, loss = 3.19727437\n",
      "Iteration 77, loss = 3.19638753\n",
      "Iteration 78, loss = 3.19572252\n",
      "Iteration 79, loss = 3.19537742\n",
      "Iteration 80, loss = 3.19430095\n",
      "Iteration 81, loss = 3.19369048\n",
      "Iteration 82, loss = 3.19275123\n",
      "Iteration 83, loss = 3.19225351\n",
      "Iteration 84, loss = 3.19178304\n",
      "Iteration 85, loss = 3.19092788\n",
      "Iteration 86, loss = 3.19007828\n",
      "Iteration 87, loss = 3.18959254\n",
      "Iteration 88, loss = 3.18867677\n",
      "Iteration 89, loss = 3.18785279\n",
      "Iteration 90, loss = 3.18706131\n",
      "Iteration 91, loss = 3.18620411\n",
      "Iteration 92, loss = 3.18572798\n",
      "Iteration 93, loss = 3.18510435\n",
      "Iteration 94, loss = 3.18440869\n",
      "Iteration 95, loss = 3.18364037\n",
      "Iteration 96, loss = 3.18298249\n",
      "Iteration 97, loss = 3.18233897\n",
      "Iteration 98, loss = 3.18198622\n",
      "Iteration 99, loss = 3.18108567\n",
      "Iteration 100, loss = 3.18063431\n",
      "Iteration 101, loss = 3.17997846\n",
      "Iteration 102, loss = 3.17910608\n",
      "Iteration 103, loss = 3.17873034\n",
      "Iteration 104, loss = 3.17771628\n",
      "Iteration 105, loss = 3.17708327\n",
      "Iteration 106, loss = 3.17626926\n",
      "Iteration 107, loss = 3.17564673\n",
      "Iteration 108, loss = 3.17490671\n",
      "Iteration 109, loss = 3.17402548\n",
      "Iteration 110, loss = 3.17335726\n",
      "Iteration 111, loss = 3.17270457\n",
      "Iteration 112, loss = 3.17228085\n",
      "Iteration 113, loss = 3.17133849\n",
      "Iteration 114, loss = 3.17056317\n",
      "Iteration 115, loss = 3.16970622\n",
      "Iteration 116, loss = 3.16923285\n",
      "Iteration 117, loss = 3.16836580\n",
      "Iteration 118, loss = 3.16787743\n",
      "Iteration 119, loss = 3.16713512\n",
      "Iteration 120, loss = 3.16642174\n",
      "Iteration 121, loss = 3.16576355\n",
      "Iteration 122, loss = 3.16539998\n",
      "Iteration 123, loss = 3.16509351\n",
      "Iteration 124, loss = 3.16455930\n",
      "Iteration 125, loss = 3.16366251\n",
      "Iteration 126, loss = 3.16266653\n",
      "Iteration 127, loss = 3.16199954\n",
      "Iteration 128, loss = 3.16175533\n",
      "Iteration 129, loss = 3.16056682\n",
      "Iteration 130, loss = 3.15981005\n",
      "Iteration 131, loss = 3.15878815\n",
      "Iteration 132, loss = 3.15832380\n",
      "Iteration 133, loss = 3.15773024\n",
      "Iteration 134, loss = 3.15699999\n",
      "Iteration 135, loss = 3.15630667\n",
      "Iteration 136, loss = 3.15535191\n",
      "Iteration 137, loss = 3.15466110\n",
      "Iteration 138, loss = 3.15415866\n",
      "Iteration 139, loss = 3.15359505\n",
      "Iteration 140, loss = 3.15301535\n",
      "Iteration 141, loss = 3.15239657\n",
      "Iteration 142, loss = 3.15189178\n",
      "Iteration 143, loss = 3.15110801\n",
      "Iteration 144, loss = 3.14994458\n",
      "Iteration 145, loss = 3.14994492\n",
      "Iteration 146, loss = 3.14989536\n",
      "Iteration 147, loss = 3.14930006\n",
      "Iteration 148, loss = 3.14822326\n",
      "Iteration 149, loss = 3.14725683\n",
      "Iteration 150, loss = 3.14695836\n",
      "Iteration 151, loss = 3.14636890\n",
      "Iteration 152, loss = 3.14556501\n",
      "Iteration 153, loss = 3.14607151\n",
      "Iteration 154, loss = 3.14495385\n",
      "Iteration 155, loss = 3.14391228\n",
      "Iteration 156, loss = 3.14321665\n",
      "Iteration 157, loss = 3.14250470\n",
      "Iteration 158, loss = 3.14180055\n",
      "Iteration 159, loss = 3.14107561\n",
      "Iteration 160, loss = 3.14040779\n",
      "Iteration 161, loss = 3.13981805\n",
      "Iteration 162, loss = 3.13934579\n",
      "Iteration 163, loss = 3.13864816\n",
      "Iteration 164, loss = 3.13820661\n",
      "Iteration 165, loss = 3.13698446\n",
      "Iteration 166, loss = 3.13637943\n",
      "Iteration 167, loss = 3.13619355\n",
      "Iteration 168, loss = 3.13527072\n",
      "Iteration 169, loss = 3.13477467\n",
      "Iteration 170, loss = 3.13374003\n",
      "Iteration 171, loss = 3.13309924\n",
      "Iteration 172, loss = 3.13258070\n",
      "Iteration 173, loss = 3.13270708\n",
      "Iteration 174, loss = 3.13213670\n",
      "Iteration 175, loss = 3.13123072\n",
      "Iteration 176, loss = 3.13038955\n",
      "Iteration 177, loss = 3.12942979\n",
      "Iteration 178, loss = 3.12920049\n",
      "Iteration 179, loss = 3.12875055\n",
      "Iteration 180, loss = 3.12845618\n",
      "Iteration 181, loss = 3.12766501\n",
      "Iteration 182, loss = 3.12658399\n",
      "Iteration 183, loss = 3.12574435\n",
      "Iteration 184, loss = 3.12523340\n",
      "Iteration 185, loss = 3.12475396\n",
      "Iteration 186, loss = 3.12412554\n",
      "Iteration 187, loss = 3.12350035\n",
      "Iteration 188, loss = 3.12297652\n",
      "Iteration 189, loss = 3.12221892\n",
      "Iteration 190, loss = 3.12143457\n",
      "Iteration 191, loss = 3.12082934\n",
      "Iteration 192, loss = 3.12036423\n",
      "Iteration 193, loss = 3.11984365\n",
      "Iteration 194, loss = 3.11935020\n",
      "Iteration 195, loss = 3.11936646\n",
      "Iteration 196, loss = 3.11874639\n",
      "Iteration 197, loss = 3.11790032\n",
      "Iteration 198, loss = 3.11704725\n",
      "Iteration 199, loss = 3.11653343\n",
      "Iteration 200, loss = 3.11534415\n",
      "Iteration 201, loss = 3.11479710\n",
      "Iteration 202, loss = 3.11408608\n",
      "Iteration 203, loss = 3.11407984\n",
      "Iteration 204, loss = 3.11353454\n",
      "Iteration 205, loss = 3.11283545\n",
      "Iteration 206, loss = 3.11224974\n",
      "Iteration 207, loss = 3.11154979\n",
      "Iteration 208, loss = 3.11035925\n",
      "Iteration 209, loss = 3.11056080\n",
      "Iteration 210, loss = 3.11057950\n",
      "Iteration 211, loss = 3.10964982\n",
      "Iteration 212, loss = 3.10912425\n",
      "Iteration 213, loss = 3.10885032\n",
      "Iteration 214, loss = 3.10779559\n",
      "Iteration 215, loss = 3.10719130\n",
      "Iteration 216, loss = 3.10670531\n",
      "Iteration 217, loss = 3.10589326\n",
      "Iteration 218, loss = 3.10499018\n",
      "Iteration 219, loss = 3.10430965\n",
      "Iteration 220, loss = 3.10392747\n",
      "Iteration 221, loss = 3.10314273\n",
      "Iteration 222, loss = 3.10271009\n",
      "Iteration 223, loss = 3.10191601\n",
      "Iteration 224, loss = 3.10189798\n",
      "Iteration 225, loss = 3.10157684\n",
      "Iteration 226, loss = 3.10096908\n",
      "Iteration 227, loss = 3.10001921\n",
      "Iteration 228, loss = 3.09955315\n",
      "Iteration 229, loss = 3.09892584\n",
      "Iteration 230, loss = 3.09876482\n",
      "Iteration 231, loss = 3.09812445\n",
      "Iteration 232, loss = 3.09754334\n",
      "Iteration 233, loss = 3.09650148\n",
      "Iteration 234, loss = 3.09578117\n",
      "Iteration 235, loss = 3.09544858\n",
      "Iteration 236, loss = 3.09459451\n",
      "Iteration 237, loss = 3.09416803\n",
      "Iteration 238, loss = 3.09358012\n",
      "Iteration 239, loss = 3.09304849\n",
      "Iteration 240, loss = 3.09249011\n",
      "Iteration 241, loss = 3.09193001\n",
      "Iteration 242, loss = 3.09142729\n",
      "Iteration 243, loss = 3.09076772\n",
      "Iteration 244, loss = 3.09012827\n",
      "Iteration 245, loss = 3.08951967\n",
      "Iteration 246, loss = 3.08926754\n",
      "Iteration 247, loss = 3.08904197\n",
      "Iteration 248, loss = 3.08810680\n",
      "Iteration 249, loss = 3.08784976\n",
      "Iteration 250, loss = 3.08730973\n",
      "Iteration 251, loss = 3.08702967\n",
      "Iteration 252, loss = 3.08631150\n",
      "Iteration 253, loss = 3.08556688\n",
      "Iteration 254, loss = 3.08500737\n",
      "Iteration 255, loss = 3.08466272\n",
      "Iteration 256, loss = 3.08370771\n",
      "Iteration 257, loss = 3.08315478\n",
      "Iteration 258, loss = 3.08269531\n",
      "Iteration 259, loss = 3.08213302\n",
      "Iteration 260, loss = 3.08162040\n",
      "Iteration 261, loss = 3.08120771\n",
      "Iteration 262, loss = 3.07989044\n",
      "Iteration 263, loss = 3.07988903\n",
      "Iteration 264, loss = 3.07885912\n",
      "Iteration 265, loss = 3.07887271\n",
      "Iteration 266, loss = 3.07802299\n",
      "Iteration 267, loss = 3.07741174\n",
      "Iteration 268, loss = 3.07681274\n",
      "Iteration 269, loss = 3.07678645\n",
      "Iteration 270, loss = 3.07625355\n",
      "Iteration 271, loss = 3.07543532\n",
      "Iteration 272, loss = 3.07487415\n",
      "Iteration 273, loss = 3.07400161\n",
      "Iteration 274, loss = 3.07381781\n",
      "Iteration 275, loss = 3.07332334\n",
      "Iteration 276, loss = 3.07317334\n",
      "Iteration 277, loss = 3.07227848\n",
      "Iteration 278, loss = 3.07174279\n",
      "Iteration 279, loss = 3.07122113\n",
      "Iteration 280, loss = 3.07041126\n",
      "Iteration 281, loss = 3.07060442\n",
      "Iteration 282, loss = 3.07014160\n",
      "Iteration 283, loss = 3.06943744\n",
      "Iteration 284, loss = 3.06891636\n",
      "Iteration 285, loss = 3.06854247\n",
      "Iteration 286, loss = 3.06820278\n",
      "Iteration 287, loss = 3.06761991\n",
      "Iteration 288, loss = 3.06655609\n",
      "Iteration 289, loss = 3.06594694\n",
      "Iteration 290, loss = 3.06562599\n",
      "Iteration 291, loss = 3.06517353\n",
      "Iteration 292, loss = 3.06431568\n",
      "Iteration 293, loss = 3.06381230\n",
      "Iteration 294, loss = 3.06372590\n",
      "Iteration 295, loss = 3.06315643\n",
      "Iteration 296, loss = 3.06204643\n",
      "Iteration 297, loss = 3.06207075\n",
      "Iteration 298, loss = 3.06145321\n",
      "Iteration 299, loss = 3.06109453\n",
      "Iteration 300, loss = 3.06027633\n",
      "Iteration 301, loss = 3.05956712\n",
      "Iteration 302, loss = 3.05864467\n",
      "Iteration 303, loss = 3.05853225\n",
      "Iteration 304, loss = 3.05804114\n",
      "Iteration 305, loss = 3.05738111\n",
      "Iteration 306, loss = 3.05686872\n",
      "Iteration 307, loss = 3.05655386\n",
      "Iteration 308, loss = 3.05636349\n",
      "Iteration 309, loss = 3.05589667\n",
      "Iteration 310, loss = 3.05503248\n",
      "Iteration 311, loss = 3.05473293\n",
      "Iteration 312, loss = 3.05476014\n",
      "Iteration 313, loss = 3.05332573\n",
      "Iteration 314, loss = 3.05311134\n",
      "Iteration 315, loss = 3.05277140\n",
      "Iteration 316, loss = 3.05228748\n",
      "Iteration 317, loss = 3.05141029\n",
      "Iteration 318, loss = 3.05073677\n",
      "Iteration 319, loss = 3.05004999\n",
      "Iteration 320, loss = 3.04958271\n",
      "Iteration 321, loss = 3.04926484\n",
      "Iteration 322, loss = 3.04873214\n",
      "Iteration 323, loss = 3.04793527\n",
      "Iteration 324, loss = 3.04771973\n",
      "Iteration 325, loss = 3.04774609\n",
      "Iteration 326, loss = 3.04726294\n",
      "Iteration 327, loss = 3.04702737\n",
      "Iteration 328, loss = 3.04645750\n",
      "Iteration 329, loss = 3.04618555\n",
      "Iteration 330, loss = 3.04502561\n",
      "Iteration 331, loss = 3.04430701\n",
      "Iteration 332, loss = 3.04373408\n",
      "Iteration 333, loss = 3.04308172\n",
      "Iteration 334, loss = 3.04264494\n",
      "Iteration 335, loss = 3.04193280\n",
      "Iteration 336, loss = 3.04214796\n",
      "Iteration 337, loss = 3.04122458\n",
      "Iteration 338, loss = 3.04058732\n",
      "Iteration 339, loss = 3.04055997\n",
      "Iteration 340, loss = 3.03981194\n",
      "Iteration 341, loss = 3.03894774\n",
      "Iteration 342, loss = 3.03874221\n",
      "Iteration 343, loss = 3.03802205\n",
      "Iteration 344, loss = 3.03774942\n",
      "Iteration 345, loss = 3.03769382\n",
      "Iteration 346, loss = 3.03748726\n",
      "Iteration 347, loss = 3.03670450\n",
      "Iteration 348, loss = 3.03692561\n",
      "Iteration 349, loss = 3.03614885\n",
      "Iteration 350, loss = 3.03538583\n",
      "Iteration 351, loss = 3.03469769\n",
      "Iteration 352, loss = 3.03384095\n",
      "Iteration 353, loss = 3.03334691\n",
      "Iteration 354, loss = 3.03323613\n",
      "Iteration 355, loss = 3.03235253\n",
      "Iteration 356, loss = 3.03251375\n",
      "Iteration 357, loss = 3.03141222\n",
      "Iteration 358, loss = 3.03082837\n",
      "Iteration 359, loss = 3.03010266\n",
      "Iteration 360, loss = 3.03006033\n",
      "Iteration 361, loss = 3.02966810\n",
      "Iteration 362, loss = 3.03016847\n",
      "Iteration 363, loss = 3.02861366\n",
      "Iteration 364, loss = 3.02822198\n",
      "Iteration 365, loss = 3.02742392\n",
      "Iteration 366, loss = 3.02713163\n",
      "Iteration 367, loss = 3.02663991\n",
      "Iteration 368, loss = 3.02649142\n",
      "Iteration 369, loss = 3.02605778\n",
      "Iteration 370, loss = 3.02545143\n",
      "Iteration 371, loss = 3.02498688\n",
      "Iteration 372, loss = 3.02460890\n",
      "Iteration 373, loss = 3.02411893\n",
      "Iteration 374, loss = 3.02390974\n",
      "Iteration 375, loss = 3.02278122\n",
      "Iteration 376, loss = 3.02216088\n",
      "Iteration 377, loss = 3.02220733\n",
      "Iteration 378, loss = 3.02164279\n",
      "Iteration 379, loss = 3.02134093\n",
      "Iteration 380, loss = 3.02148652\n",
      "Iteration 381, loss = 3.02042181\n",
      "Iteration 382, loss = 3.01991632\n",
      "Iteration 383, loss = 3.01944389\n",
      "Iteration 384, loss = 3.01948901\n",
      "Iteration 385, loss = 3.01865605\n",
      "Iteration 386, loss = 3.01777436\n",
      "Iteration 387, loss = 3.01675034\n",
      "Iteration 388, loss = 3.01625934\n",
      "Iteration 389, loss = 3.01572702\n",
      "Iteration 390, loss = 3.01537183\n",
      "Iteration 391, loss = 3.01503362\n",
      "Iteration 392, loss = 3.01507854\n",
      "Iteration 393, loss = 3.01444511\n",
      "Iteration 394, loss = 3.01449414\n",
      "Iteration 395, loss = 3.01335117\n",
      "Iteration 396, loss = 3.01301832\n",
      "Iteration 397, loss = 3.01287427\n",
      "Iteration 398, loss = 3.01161906\n",
      "Iteration 399, loss = 3.01117065\n",
      "Iteration 400, loss = 3.01132719\n",
      "Iteration 401, loss = 3.01129466\n",
      "Iteration 402, loss = 3.01065732\n",
      "Iteration 403, loss = 3.00915246\n",
      "Iteration 404, loss = 3.00890023\n",
      "Iteration 405, loss = 3.00837218\n",
      "Iteration 406, loss = 3.00783291\n",
      "Iteration 407, loss = 3.00757552\n",
      "Iteration 408, loss = 3.00724812\n",
      "Iteration 409, loss = 3.00658213\n",
      "Iteration 410, loss = 3.00629849\n",
      "Iteration 411, loss = 3.00572090\n",
      "Iteration 412, loss = 3.00541343\n",
      "Iteration 413, loss = 3.00479176\n",
      "Iteration 414, loss = 3.00452294\n",
      "Iteration 415, loss = 3.00419348\n",
      "Iteration 416, loss = 3.00345391\n",
      "Iteration 417, loss = 3.00285638\n",
      "Iteration 418, loss = 3.00237013\n",
      "Iteration 419, loss = 3.00241908\n",
      "Iteration 420, loss = 3.00141236\n",
      "Iteration 421, loss = 3.00097259\n",
      "Iteration 422, loss = 3.00045871\n",
      "Iteration 423, loss = 3.00020747\n",
      "Iteration 424, loss = 2.99973446\n",
      "Iteration 425, loss = 2.99936419\n",
      "Iteration 426, loss = 2.99916546\n",
      "Iteration 427, loss = 2.99875357\n",
      "Iteration 428, loss = 2.99840828\n",
      "Iteration 429, loss = 2.99746638\n",
      "Iteration 430, loss = 2.99760136\n",
      "Iteration 431, loss = 2.99640730\n",
      "Iteration 432, loss = 2.99629504\n",
      "Iteration 433, loss = 2.99722079\n",
      "Iteration 434, loss = 2.99630673\n",
      "Iteration 435, loss = 2.99420713\n",
      "Iteration 436, loss = 2.99413954\n",
      "Iteration 437, loss = 2.99399421\n",
      "Iteration 438, loss = 2.99333404\n",
      "Iteration 439, loss = 2.99412722\n",
      "Iteration 440, loss = 2.99331558\n",
      "Iteration 441, loss = 2.99191003\n",
      "Iteration 442, loss = 2.99115734\n",
      "Iteration 443, loss = 2.99067528\n",
      "Iteration 444, loss = 2.99034517\n",
      "Iteration 445, loss = 2.98973867\n",
      "Iteration 446, loss = 2.98886354\n",
      "Iteration 447, loss = 2.98957310\n",
      "Iteration 448, loss = 2.98873547\n",
      "Iteration 449, loss = 2.98790901\n",
      "Iteration 450, loss = 2.98790211\n",
      "Iteration 451, loss = 2.98735460\n",
      "Iteration 452, loss = 2.98679227\n",
      "Iteration 453, loss = 2.98732578\n",
      "Iteration 454, loss = 2.98732177\n",
      "Iteration 455, loss = 2.98567239\n",
      "Iteration 456, loss = 2.98647497\n",
      "Iteration 457, loss = 2.98616103\n",
      "Iteration 458, loss = 2.98514207\n",
      "Iteration 459, loss = 2.98396367\n",
      "Iteration 460, loss = 2.98291602\n",
      "Iteration 461, loss = 2.98266622\n",
      "Iteration 462, loss = 2.98223251\n",
      "Iteration 463, loss = 2.98169843\n",
      "Iteration 464, loss = 2.98095762\n",
      "Iteration 465, loss = 2.98063542\n",
      "Iteration 466, loss = 2.98040823\n",
      "Iteration 467, loss = 2.97996607\n",
      "Iteration 468, loss = 2.97979043\n",
      "Iteration 469, loss = 2.97925365\n",
      "Iteration 470, loss = 2.97862356\n",
      "Iteration 471, loss = 2.97864145\n",
      "Iteration 472, loss = 2.97800693\n",
      "Iteration 473, loss = 2.97738242\n",
      "Iteration 474, loss = 2.97666698\n",
      "Iteration 475, loss = 2.97645612\n",
      "Iteration 476, loss = 2.97600973\n",
      "Iteration 477, loss = 2.97559270\n",
      "Iteration 478, loss = 2.97483309\n",
      "Iteration 479, loss = 2.97497320\n",
      "Iteration 480, loss = 2.97431419\n",
      "Iteration 481, loss = 2.97371760\n",
      "Iteration 482, loss = 2.97357459\n",
      "Iteration 483, loss = 2.97239538\n",
      "Iteration 484, loss = 2.97271779\n",
      "Iteration 485, loss = 2.97176995\n",
      "Iteration 486, loss = 2.97114672\n",
      "Iteration 487, loss = 2.97055689\n",
      "Iteration 488, loss = 2.97057858\n",
      "Iteration 489, loss = 2.96989787\n",
      "Iteration 490, loss = 2.97017090\n",
      "Iteration 491, loss = 2.96933724\n",
      "Iteration 492, loss = 2.96858195\n",
      "Iteration 493, loss = 2.96804140\n",
      "Iteration 494, loss = 2.96750329\n",
      "Iteration 495, loss = 2.96787032\n",
      "Iteration 496, loss = 2.96770668\n",
      "Iteration 497, loss = 2.96719827\n",
      "Iteration 498, loss = 2.96637596\n",
      "Iteration 499, loss = 2.96517632\n",
      "Iteration 500, loss = 2.96549833\n",
      "Iteration 501, loss = 2.96489362\n",
      "Iteration 502, loss = 2.96402746\n",
      "Iteration 503, loss = 2.96396358\n",
      "Iteration 504, loss = 2.96422059\n",
      "Iteration 505, loss = 2.96331602\n",
      "Iteration 506, loss = 2.96309023\n",
      "Iteration 507, loss = 2.96255770\n",
      "Iteration 508, loss = 2.96207744\n",
      "Iteration 509, loss = 2.96145603\n",
      "Iteration 510, loss = 2.96063700\n",
      "Iteration 511, loss = 2.96095308\n",
      "Iteration 512, loss = 2.96090890\n",
      "Iteration 513, loss = 2.96013809\n",
      "Iteration 514, loss = 2.95897335\n",
      "Iteration 515, loss = 2.95830008\n",
      "Iteration 516, loss = 2.95797099\n",
      "Iteration 517, loss = 2.95751925\n",
      "Iteration 518, loss = 2.95728979\n",
      "Iteration 519, loss = 2.95719827\n",
      "Iteration 520, loss = 2.95640118\n",
      "Iteration 521, loss = 2.95591002\n",
      "Iteration 522, loss = 2.95511005\n",
      "Iteration 523, loss = 2.95459696\n",
      "Iteration 524, loss = 2.95552886\n",
      "Iteration 525, loss = 2.95543773\n",
      "Iteration 526, loss = 2.95433584\n",
      "Iteration 527, loss = 2.95349553\n",
      "Iteration 528, loss = 2.95334995\n",
      "Iteration 529, loss = 2.95261667\n",
      "Iteration 530, loss = 2.95223179\n",
      "Iteration 531, loss = 2.95135595\n",
      "Iteration 532, loss = 2.95086540\n",
      "Iteration 533, loss = 2.95064168\n",
      "Iteration 534, loss = 2.95028739\n",
      "Iteration 535, loss = 2.94956827\n",
      "Iteration 536, loss = 2.94923732\n",
      "Iteration 537, loss = 2.94909465\n",
      "Iteration 538, loss = 2.94865938\n",
      "Iteration 539, loss = 2.94798730\n",
      "Iteration 540, loss = 2.94709813\n",
      "Iteration 541, loss = 2.94787463\n",
      "Iteration 542, loss = 2.94761830\n",
      "Iteration 543, loss = 2.94678437\n",
      "Iteration 544, loss = 2.94626770\n",
      "Iteration 545, loss = 2.94608747\n",
      "Iteration 546, loss = 2.94586090\n",
      "Iteration 547, loss = 2.94580024\n",
      "Iteration 548, loss = 2.94457991\n",
      "Iteration 549, loss = 2.94436148\n",
      "Iteration 550, loss = 2.94371923\n",
      "Iteration 551, loss = 2.94307750\n",
      "Iteration 552, loss = 2.94289711\n",
      "Iteration 553, loss = 2.94242512\n",
      "Iteration 554, loss = 2.94177517\n",
      "Iteration 555, loss = 2.94113331\n",
      "Iteration 556, loss = 2.94088079\n",
      "Iteration 557, loss = 2.94024904\n",
      "Iteration 558, loss = 2.93977010\n",
      "Iteration 559, loss = 2.93998861\n",
      "Iteration 560, loss = 2.93930142\n",
      "Iteration 561, loss = 2.93877018\n",
      "Iteration 562, loss = 2.93842759\n",
      "Iteration 563, loss = 2.93837824\n",
      "Iteration 564, loss = 2.93745073\n",
      "Iteration 565, loss = 2.93677647\n",
      "Iteration 566, loss = 2.93632473\n",
      "Iteration 567, loss = 2.93578300\n",
      "Iteration 568, loss = 2.93542802\n",
      "Iteration 569, loss = 2.93535342\n",
      "Iteration 570, loss = 2.93474216\n",
      "Iteration 571, loss = 2.93428839\n",
      "Iteration 572, loss = 2.93394394\n",
      "Iteration 573, loss = 2.93372117\n",
      "Iteration 574, loss = 2.93309984\n",
      "Iteration 575, loss = 2.93344770\n",
      "Iteration 576, loss = 2.93254348\n",
      "Iteration 577, loss = 2.93216160\n",
      "Iteration 578, loss = 2.93168095\n",
      "Iteration 579, loss = 2.93165685\n",
      "Iteration 580, loss = 2.93140402\n",
      "Iteration 581, loss = 2.93060449\n",
      "Iteration 582, loss = 2.93011758\n",
      "Iteration 583, loss = 2.92993818\n",
      "Iteration 584, loss = 2.92908807\n",
      "Iteration 585, loss = 2.92858013\n",
      "Iteration 586, loss = 2.92839912\n",
      "Iteration 587, loss = 2.92800792\n",
      "Iteration 588, loss = 2.92722602\n",
      "Iteration 589, loss = 2.92689458\n",
      "Iteration 590, loss = 2.92664477\n",
      "Iteration 591, loss = 2.92735812\n",
      "Iteration 592, loss = 2.92611095\n",
      "Iteration 593, loss = 2.92555208\n",
      "Iteration 594, loss = 2.92552137\n",
      "Iteration 595, loss = 2.92524956\n",
      "Iteration 596, loss = 2.92474572\n",
      "Iteration 597, loss = 2.92448721\n",
      "Iteration 598, loss = 2.92411520\n",
      "Iteration 599, loss = 2.92279147\n",
      "Iteration 600, loss = 2.92288755\n",
      "Iteration 601, loss = 2.92263554\n",
      "Iteration 602, loss = 2.92275345\n",
      "Iteration 603, loss = 2.92174618\n",
      "Iteration 604, loss = 2.92150564\n",
      "Iteration 605, loss = 2.92109649\n",
      "Iteration 606, loss = 2.92043441\n",
      "Iteration 607, loss = 2.92017769\n",
      "Iteration 608, loss = 2.92006832\n",
      "Iteration 609, loss = 2.91983469\n",
      "Iteration 610, loss = 2.91904435\n",
      "Iteration 611, loss = 2.91845460\n",
      "Iteration 612, loss = 2.91771156\n",
      "Iteration 613, loss = 2.91729497\n",
      "Iteration 614, loss = 2.91682912\n",
      "Iteration 615, loss = 2.91703343\n",
      "Iteration 616, loss = 2.91632946\n",
      "Iteration 617, loss = 2.91736107\n",
      "Iteration 618, loss = 2.91626265\n",
      "Iteration 619, loss = 2.91540810\n",
      "Iteration 620, loss = 2.91472149\n",
      "Iteration 621, loss = 2.91430571\n",
      "Iteration 622, loss = 2.91420691\n",
      "Iteration 623, loss = 2.91375631\n",
      "Iteration 624, loss = 2.91338083\n",
      "Iteration 625, loss = 2.91263551\n",
      "Iteration 626, loss = 2.91212646\n",
      "Iteration 627, loss = 2.91208062\n",
      "Iteration 628, loss = 2.91141129\n",
      "Iteration 629, loss = 2.91098783\n",
      "Iteration 630, loss = 2.91092286\n",
      "Iteration 631, loss = 2.91071871\n",
      "Iteration 632, loss = 2.90958002\n",
      "Iteration 633, loss = 2.90938432\n",
      "Iteration 634, loss = 2.90909439\n",
      "Iteration 635, loss = 2.90900152\n",
      "Iteration 636, loss = 2.90811647\n",
      "Iteration 637, loss = 2.90796283\n",
      "Iteration 638, loss = 2.90818372\n",
      "Iteration 639, loss = 2.90677062\n",
      "Iteration 640, loss = 2.90656267\n",
      "Iteration 641, loss = 2.90627932\n",
      "Iteration 642, loss = 2.90586340\n",
      "Iteration 643, loss = 2.90634613\n",
      "Iteration 644, loss = 2.90539590\n",
      "Iteration 645, loss = 2.90500046\n",
      "Iteration 646, loss = 2.90433913\n",
      "Iteration 647, loss = 2.90421584\n",
      "Iteration 648, loss = 2.90471568\n",
      "Iteration 649, loss = 2.90385355\n",
      "Iteration 650, loss = 2.90292595\n",
      "Iteration 651, loss = 2.90266166\n",
      "Iteration 652, loss = 2.90203728\n",
      "Iteration 653, loss = 2.90171479\n",
      "Iteration 654, loss = 2.90115861\n",
      "Iteration 655, loss = 2.90058951\n",
      "Iteration 656, loss = 2.90095978\n",
      "Iteration 657, loss = 2.89972230\n",
      "Iteration 658, loss = 2.90004896\n",
      "Iteration 659, loss = 2.89935478\n",
      "Iteration 660, loss = 2.89887529\n",
      "Iteration 661, loss = 2.89881421\n",
      "Iteration 662, loss = 2.89817474\n",
      "Iteration 663, loss = 2.89776517\n",
      "Iteration 664, loss = 2.89729629\n",
      "Iteration 665, loss = 2.89690519\n",
      "Iteration 666, loss = 2.89659240\n",
      "Iteration 667, loss = 2.89680373\n",
      "Iteration 668, loss = 2.89688084\n",
      "Iteration 669, loss = 2.89582587\n",
      "Iteration 670, loss = 2.89491491\n",
      "Iteration 671, loss = 2.89463331\n",
      "Iteration 672, loss = 2.89431374\n",
      "Iteration 673, loss = 2.89418559\n",
      "Iteration 674, loss = 2.89401317\n",
      "Iteration 675, loss = 2.89309870\n",
      "Iteration 676, loss = 2.89302083\n",
      "Iteration 677, loss = 2.89279759\n",
      "Iteration 678, loss = 2.89224001\n",
      "Iteration 679, loss = 2.89185236\n",
      "Iteration 680, loss = 2.89204266\n",
      "Iteration 681, loss = 2.89153962\n",
      "Iteration 682, loss = 2.89052299\n",
      "Iteration 683, loss = 2.89078594\n",
      "Iteration 684, loss = 2.89123794\n",
      "Iteration 685, loss = 2.89005509\n",
      "Iteration 686, loss = 2.88952632\n",
      "Iteration 687, loss = 2.88919671\n",
      "Iteration 688, loss = 2.88804192\n",
      "Iteration 689, loss = 2.88828410\n",
      "Iteration 690, loss = 2.88808840\n",
      "Iteration 691, loss = 2.88748923\n",
      "Iteration 692, loss = 2.88698065\n",
      "Iteration 693, loss = 2.88673226\n",
      "Iteration 694, loss = 2.88586508\n",
      "Iteration 695, loss = 2.88631356\n",
      "Iteration 696, loss = 2.88534016\n",
      "Iteration 697, loss = 2.88479228\n",
      "Iteration 698, loss = 2.88454144\n",
      "Iteration 699, loss = 2.88377383\n",
      "Iteration 700, loss = 2.88359826\n",
      "Iteration 701, loss = 2.88332938\n",
      "Iteration 702, loss = 2.88306696\n",
      "Iteration 703, loss = 2.88293467\n",
      "Iteration 704, loss = 2.88252073\n",
      "Iteration 705, loss = 2.88219656\n",
      "Iteration 706, loss = 2.88139199\n",
      "Iteration 707, loss = 2.88148132\n",
      "Iteration 708, loss = 2.88160027\n",
      "Iteration 709, loss = 2.88141502\n",
      "Iteration 710, loss = 2.88035951\n",
      "Iteration 711, loss = 2.87969683\n",
      "Iteration 712, loss = 2.87949193\n",
      "Iteration 713, loss = 2.87960698\n",
      "Iteration 714, loss = 2.87861755\n",
      "Iteration 715, loss = 2.87851371\n",
      "Iteration 716, loss = 2.87844595\n",
      "Iteration 717, loss = 2.87801684\n",
      "Iteration 718, loss = 2.87756513\n",
      "Iteration 719, loss = 2.87677021\n",
      "Iteration 720, loss = 2.87634151\n",
      "Iteration 721, loss = 2.87602161\n",
      "Iteration 722, loss = 2.87588097\n",
      "Iteration 723, loss = 2.87522986\n",
      "Iteration 724, loss = 2.87517557\n",
      "Iteration 725, loss = 2.87499714\n",
      "Iteration 726, loss = 2.87441932\n",
      "Iteration 727, loss = 2.87362955\n",
      "Iteration 728, loss = 2.87332657\n",
      "Iteration 729, loss = 2.87360039\n",
      "Iteration 730, loss = 2.87268435\n",
      "Iteration 731, loss = 2.87253014\n",
      "Iteration 732, loss = 2.87209753\n",
      "Iteration 733, loss = 2.87188248\n",
      "Iteration 734, loss = 2.87197488\n",
      "Iteration 735, loss = 2.87173014\n",
      "Iteration 736, loss = 2.87191063\n",
      "Iteration 737, loss = 2.87069226\n",
      "Iteration 738, loss = 2.86981757\n",
      "Iteration 739, loss = 2.87029968\n",
      "Iteration 740, loss = 2.86884622\n",
      "Iteration 741, loss = 2.86895718\n",
      "Iteration 742, loss = 2.86930662\n",
      "Iteration 743, loss = 2.86951982\n",
      "Iteration 744, loss = 2.86837303\n",
      "Iteration 745, loss = 2.86706587\n",
      "Iteration 746, loss = 2.86728796\n",
      "Iteration 747, loss = 2.86719800\n",
      "Iteration 748, loss = 2.86686503\n",
      "Iteration 749, loss = 2.86683650\n",
      "Iteration 750, loss = 2.86622348\n",
      "Iteration 751, loss = 2.86515694\n",
      "Iteration 752, loss = 2.86484987\n",
      "Iteration 753, loss = 2.86410900\n",
      "Iteration 754, loss = 2.86394247\n",
      "Iteration 755, loss = 2.86345368\n",
      "Iteration 756, loss = 2.86359467\n",
      "Iteration 757, loss = 2.86363183\n",
      "Iteration 758, loss = 2.86323407\n",
      "Iteration 759, loss = 2.86289169\n",
      "Iteration 760, loss = 2.86176556\n",
      "Iteration 761, loss = 2.86131138\n",
      "Iteration 762, loss = 2.86143076\n",
      "Iteration 763, loss = 2.86110720\n",
      "Iteration 764, loss = 2.86054658\n",
      "Iteration 765, loss = 2.86010534\n",
      "Iteration 766, loss = 2.85931915\n",
      "Iteration 767, loss = 2.85936931\n",
      "Iteration 768, loss = 2.85908022\n",
      "Iteration 769, loss = 2.85880538\n",
      "Iteration 770, loss = 2.85849943\n",
      "Iteration 771, loss = 2.85759858\n",
      "Iteration 772, loss = 2.85746769\n",
      "Iteration 773, loss = 2.85721024\n",
      "Iteration 774, loss = 2.85795512\n",
      "Iteration 775, loss = 2.85781884\n",
      "Iteration 776, loss = 2.85647945\n",
      "Iteration 777, loss = 2.85577469\n",
      "Iteration 778, loss = 2.85589310\n",
      "Iteration 779, loss = 2.85544092\n",
      "Iteration 780, loss = 2.85504136\n",
      "Iteration 781, loss = 2.85438458\n",
      "Iteration 782, loss = 2.85408011\n",
      "Iteration 783, loss = 2.85354368\n",
      "Iteration 784, loss = 2.85310586\n",
      "Iteration 785, loss = 2.85324909\n",
      "Iteration 786, loss = 2.85283369\n",
      "Iteration 787, loss = 2.85218113\n",
      "Iteration 788, loss = 2.85213669\n",
      "Iteration 789, loss = 2.85194909\n",
      "Iteration 790, loss = 2.85081884\n",
      "Iteration 791, loss = 2.85128744\n",
      "Iteration 792, loss = 2.85072243\n",
      "Iteration 793, loss = 2.85101743\n",
      "Iteration 794, loss = 2.85021164\n",
      "Iteration 795, loss = 2.84903266\n",
      "Iteration 796, loss = 2.84909170\n",
      "Iteration 797, loss = 2.84933374\n",
      "Iteration 798, loss = 2.84930468\n",
      "Iteration 799, loss = 2.84896178\n",
      "Iteration 800, loss = 2.84826424\n",
      "Iteration 801, loss = 2.84767440\n",
      "Iteration 802, loss = 2.84796244\n",
      "Iteration 803, loss = 2.84690020\n",
      "Iteration 804, loss = 2.84680311\n",
      "Iteration 805, loss = 2.84671389\n",
      "Iteration 806, loss = 2.84594012\n",
      "Iteration 807, loss = 2.84543822\n",
      "Iteration 808, loss = 2.84513793\n",
      "Iteration 809, loss = 2.84519483\n",
      "Iteration 810, loss = 2.84449311\n",
      "Iteration 811, loss = 2.84378635\n",
      "Iteration 812, loss = 2.84373906\n",
      "Iteration 813, loss = 2.84311991\n",
      "Iteration 814, loss = 2.84288551\n",
      "Iteration 815, loss = 2.84305632\n",
      "Iteration 816, loss = 2.84266694\n",
      "Iteration 817, loss = 2.84241516\n",
      "Iteration 818, loss = 2.84170526\n",
      "Iteration 819, loss = 2.84090385\n",
      "Iteration 820, loss = 2.84105161\n",
      "Iteration 821, loss = 2.84057910\n",
      "Iteration 822, loss = 2.84014033\n",
      "Iteration 823, loss = 2.83986967\n",
      "Iteration 824, loss = 2.83972201\n",
      "Iteration 825, loss = 2.83944663\n",
      "Iteration 826, loss = 2.83913421\n",
      "Iteration 827, loss = 2.83888091\n",
      "Iteration 828, loss = 2.83858643\n",
      "Iteration 829, loss = 2.83805791\n",
      "Iteration 830, loss = 2.83796252\n",
      "Iteration 831, loss = 2.83756333\n",
      "Iteration 832, loss = 2.83699820\n",
      "Iteration 833, loss = 2.83655395\n",
      "Iteration 834, loss = 2.83655121\n",
      "Iteration 835, loss = 2.83608110\n",
      "Iteration 836, loss = 2.83603210\n",
      "Iteration 837, loss = 2.83551107\n",
      "Iteration 838, loss = 2.83485918\n",
      "Iteration 839, loss = 2.83457839\n",
      "Iteration 840, loss = 2.83406935\n",
      "Iteration 841, loss = 2.83367053\n",
      "Iteration 842, loss = 2.83335251\n",
      "Iteration 843, loss = 2.83311540\n",
      "Iteration 844, loss = 2.83361937\n",
      "Iteration 845, loss = 2.83424599\n",
      "Iteration 846, loss = 2.83242811\n",
      "Iteration 847, loss = 2.83250037\n",
      "Iteration 848, loss = 2.83273742\n",
      "Iteration 849, loss = 2.83117371\n",
      "Iteration 850, loss = 2.83069735\n",
      "Iteration 851, loss = 2.83093902\n",
      "Iteration 852, loss = 2.83006985\n",
      "Iteration 853, loss = 2.82937306\n",
      "Iteration 854, loss = 2.82944770\n",
      "Iteration 855, loss = 2.82890610\n",
      "Iteration 856, loss = 2.82882609\n",
      "Iteration 857, loss = 2.82878734\n",
      "Iteration 858, loss = 2.82851292\n",
      "Iteration 859, loss = 2.82825715\n",
      "Iteration 860, loss = 2.82787892\n",
      "Iteration 861, loss = 2.82731181\n",
      "Iteration 862, loss = 2.82661454\n",
      "Iteration 863, loss = 2.82615942\n",
      "Iteration 864, loss = 2.82645534\n",
      "Iteration 865, loss = 2.82614892\n",
      "Iteration 866, loss = 2.82543725\n",
      "Iteration 867, loss = 2.82467143\n",
      "Iteration 868, loss = 2.82437462\n",
      "Iteration 869, loss = 2.82480744\n",
      "Iteration 870, loss = 2.82408204\n",
      "Iteration 871, loss = 2.82404214\n",
      "Iteration 872, loss = 2.82395756\n",
      "Iteration 873, loss = 2.82404862\n",
      "Iteration 874, loss = 2.82283282\n",
      "Iteration 875, loss = 2.82225738\n",
      "Iteration 876, loss = 2.82178555\n",
      "Iteration 877, loss = 2.82152100\n",
      "Iteration 878, loss = 2.82171053\n",
      "Iteration 879, loss = 2.82126232\n",
      "Iteration 880, loss = 2.82130407\n",
      "Iteration 881, loss = 2.82078357\n",
      "Iteration 882, loss = 2.82011820\n",
      "Iteration 883, loss = 2.81959131\n",
      "Iteration 884, loss = 2.81925924\n",
      "Iteration 885, loss = 2.81899200\n",
      "Iteration 886, loss = 2.81902430\n",
      "Iteration 887, loss = 2.81907727\n",
      "Iteration 888, loss = 2.81849764\n",
      "Iteration 889, loss = 2.81829585\n",
      "Iteration 890, loss = 2.81779766\n",
      "Iteration 891, loss = 2.81766036\n",
      "Iteration 892, loss = 2.81710570\n",
      "Iteration 893, loss = 2.81697694\n",
      "Iteration 894, loss = 2.81580296\n",
      "Iteration 895, loss = 2.81602883\n",
      "Iteration 896, loss = 2.81568394\n",
      "Iteration 897, loss = 2.81579032\n",
      "Iteration 898, loss = 2.81511506\n",
      "Iteration 899, loss = 2.81454787\n",
      "Iteration 900, loss = 2.81441041\n",
      "Iteration 901, loss = 2.81409497\n",
      "Iteration 902, loss = 2.81384591\n",
      "Iteration 903, loss = 2.81277258\n",
      "Iteration 904, loss = 2.81351693\n",
      "Iteration 905, loss = 2.81283970\n",
      "Iteration 906, loss = 2.81224155\n",
      "Iteration 907, loss = 2.81150850\n",
      "Iteration 908, loss = 2.81163698\n",
      "Iteration 909, loss = 2.81098290\n",
      "Iteration 910, loss = 2.81086048\n",
      "Iteration 911, loss = 2.81048643\n",
      "Iteration 912, loss = 2.81061271\n",
      "Iteration 913, loss = 2.81039289\n",
      "Iteration 914, loss = 2.80981670\n",
      "Iteration 915, loss = 2.80965159\n",
      "Iteration 916, loss = 2.81038056\n",
      "Iteration 917, loss = 2.80876553\n",
      "Iteration 918, loss = 2.80845710\n",
      "Iteration 919, loss = 2.80928294\n",
      "Iteration 920, loss = 2.80909726\n",
      "Iteration 921, loss = 2.80771365\n",
      "Iteration 922, loss = 2.80702504\n",
      "Iteration 923, loss = 2.80771589\n",
      "Iteration 924, loss = 2.80617829\n",
      "Iteration 925, loss = 2.80612248\n",
      "Iteration 926, loss = 2.80607876\n",
      "Iteration 927, loss = 2.80516266\n",
      "Iteration 928, loss = 2.80584027\n",
      "Iteration 929, loss = 2.80500235\n",
      "Iteration 930, loss = 2.80501461\n",
      "Iteration 931, loss = 2.80501995\n",
      "Iteration 932, loss = 2.80397113\n",
      "Iteration 933, loss = 2.80357514\n",
      "Iteration 934, loss = 2.80328229\n",
      "Iteration 935, loss = 2.80293663\n",
      "Iteration 936, loss = 2.80244736\n",
      "Iteration 937, loss = 2.80243190\n",
      "Iteration 938, loss = 2.80207626\n",
      "Iteration 939, loss = 2.80196693\n",
      "Iteration 940, loss = 2.80151412\n",
      "Iteration 941, loss = 2.80108246\n",
      "Iteration 942, loss = 2.80121312\n",
      "Iteration 943, loss = 2.80042638\n",
      "Iteration 944, loss = 2.80008158\n",
      "Iteration 945, loss = 2.80008800\n",
      "Iteration 946, loss = 2.79996572\n",
      "Iteration 947, loss = 2.79924175\n",
      "Iteration 948, loss = 2.79937910\n",
      "Iteration 949, loss = 2.79873126\n",
      "Iteration 950, loss = 2.79866980\n",
      "Iteration 951, loss = 2.79782415\n",
      "Iteration 952, loss = 2.79746997\n",
      "Iteration 953, loss = 2.79722537\n",
      "Iteration 954, loss = 2.79720746\n",
      "Iteration 955, loss = 2.79670965\n",
      "Iteration 956, loss = 2.79667257\n",
      "Iteration 957, loss = 2.79593116\n",
      "Iteration 958, loss = 2.79599867\n",
      "Iteration 959, loss = 2.79563766\n",
      "Iteration 960, loss = 2.79498860\n",
      "Iteration 961, loss = 2.79593659\n",
      "Iteration 962, loss = 2.79494663\n",
      "Iteration 963, loss = 2.79375923\n",
      "Iteration 964, loss = 2.79409397\n",
      "Iteration 965, loss = 2.79351017\n",
      "Iteration 966, loss = 2.79304516\n",
      "Iteration 967, loss = 2.79301909\n",
      "Iteration 968, loss = 2.79266638\n",
      "Iteration 969, loss = 2.79213359\n",
      "Iteration 970, loss = 2.79277089\n",
      "Iteration 971, loss = 2.79299338\n",
      "Iteration 972, loss = 2.79244388\n",
      "Iteration 973, loss = 2.79125049\n",
      "Iteration 974, loss = 2.79067768\n",
      "Iteration 975, loss = 2.79049944\n",
      "Iteration 976, loss = 2.79098989\n",
      "Iteration 977, loss = 2.79016547\n",
      "Iteration 978, loss = 2.78958043\n",
      "Iteration 979, loss = 2.78956688\n",
      "Iteration 980, loss = 2.78879619\n",
      "Iteration 981, loss = 2.78967348\n",
      "Iteration 982, loss = 2.78865668\n",
      "Iteration 983, loss = 2.78793550\n",
      "Iteration 984, loss = 2.78750343\n",
      "Iteration 985, loss = 2.78757889\n",
      "Iteration 986, loss = 2.78695746\n",
      "Iteration 987, loss = 2.78654467\n",
      "Iteration 988, loss = 2.78720439\n",
      "Iteration 989, loss = 2.78692424\n",
      "Iteration 990, loss = 2.78597646\n",
      "Iteration 991, loss = 2.78521851\n",
      "Iteration 992, loss = 2.78504149\n",
      "Iteration 993, loss = 2.78483327\n",
      "Iteration 994, loss = 2.78492874\n",
      "Iteration 995, loss = 2.78452481\n",
      "Iteration 996, loss = 2.78354915\n",
      "Iteration 997, loss = 2.78335420\n",
      "Iteration 998, loss = 2.78264850\n",
      "Iteration 999, loss = 2.78213352\n",
      "Iteration 1000, loss = 2.78210144\n",
      "Iteration 1001, loss = 2.78192722\n",
      "Iteration 1002, loss = 2.78193958\n",
      "Iteration 1003, loss = 2.78122529\n",
      "Iteration 1004, loss = 2.78081640\n",
      "Iteration 1005, loss = 2.78036092\n",
      "Iteration 1006, loss = 2.78007233\n",
      "Iteration 1007, loss = 2.78039190\n",
      "Iteration 1008, loss = 2.78000924\n",
      "Iteration 1009, loss = 2.78089385\n",
      "Iteration 1010, loss = 2.77989617\n",
      "Iteration 1011, loss = 2.77871658\n",
      "Iteration 1012, loss = 2.77845819\n",
      "Iteration 1013, loss = 2.77782990\n",
      "Iteration 1014, loss = 2.77782743\n",
      "Iteration 1015, loss = 2.77750692\n",
      "Iteration 1016, loss = 2.77715813\n",
      "Iteration 1017, loss = 2.77689468\n",
      "Iteration 1018, loss = 2.77669241\n",
      "Iteration 1019, loss = 2.77675785\n",
      "Iteration 1020, loss = 2.77615856\n",
      "Iteration 1021, loss = 2.77608663\n",
      "Iteration 1022, loss = 2.77538976\n",
      "Iteration 1023, loss = 2.77455736\n",
      "Iteration 1024, loss = 2.77536613\n",
      "Iteration 1025, loss = 2.77529355\n",
      "Iteration 1026, loss = 2.77435164\n",
      "Iteration 1027, loss = 2.77461886\n",
      "Iteration 1028, loss = 2.77360911\n",
      "Iteration 1029, loss = 2.77408384\n",
      "Iteration 1030, loss = 2.77386483\n",
      "Iteration 1031, loss = 2.77314324\n",
      "Iteration 1032, loss = 2.77257274\n",
      "Iteration 1033, loss = 2.77235414\n",
      "Iteration 1034, loss = 2.77236324\n",
      "Iteration 1035, loss = 2.77191458\n",
      "Iteration 1036, loss = 2.77124345\n",
      "Iteration 1037, loss = 2.77061532\n",
      "Iteration 1038, loss = 2.77030074\n",
      "Iteration 1039, loss = 2.77083298\n",
      "Iteration 1040, loss = 2.77013818\n",
      "Iteration 1041, loss = 2.76950920\n",
      "Iteration 1042, loss = 2.76963572\n",
      "Iteration 1043, loss = 2.76917733\n",
      "Iteration 1044, loss = 2.76854414\n",
      "Iteration 1045, loss = 2.76823641\n",
      "Iteration 1046, loss = 2.76828734\n",
      "Iteration 1047, loss = 2.76779747\n",
      "Iteration 1048, loss = 2.76738365\n",
      "Iteration 1049, loss = 2.76793283\n",
      "Iteration 1050, loss = 2.76765758\n",
      "Iteration 1051, loss = 2.76722093\n",
      "Iteration 1052, loss = 2.76701233\n",
      "Iteration 1053, loss = 2.76680136\n",
      "Iteration 1054, loss = 2.76654192\n",
      "Iteration 1055, loss = 2.76664278\n",
      "Iteration 1056, loss = 2.76543734\n",
      "Iteration 1057, loss = 2.76543382\n",
      "Iteration 1058, loss = 2.76515300\n",
      "Iteration 1059, loss = 2.76488351\n",
      "Iteration 1060, loss = 2.76420581\n",
      "Iteration 1061, loss = 2.76375086\n",
      "Iteration 1062, loss = 2.76344443\n",
      "Iteration 1063, loss = 2.76346642\n",
      "Iteration 1064, loss = 2.76272723\n",
      "Iteration 1065, loss = 2.76265245\n",
      "Iteration 1066, loss = 2.76225095\n",
      "Iteration 1067, loss = 2.76204271\n",
      "Iteration 1068, loss = 2.76179206\n",
      "Iteration 1069, loss = 2.76112324\n",
      "Iteration 1070, loss = 2.76116293\n",
      "Iteration 1071, loss = 2.76078795\n",
      "Iteration 1072, loss = 2.76069824\n",
      "Iteration 1073, loss = 2.76042697\n",
      "Iteration 1074, loss = 2.75997862\n",
      "Iteration 1075, loss = 2.76026785\n",
      "Iteration 1076, loss = 2.75890126\n",
      "Iteration 1077, loss = 2.75951839\n",
      "Iteration 1078, loss = 2.75936548\n",
      "Iteration 1079, loss = 2.75811825\n",
      "Iteration 1080, loss = 2.75778667\n",
      "Iteration 1081, loss = 2.75825280\n",
      "Iteration 1082, loss = 2.75754778\n",
      "Iteration 1083, loss = 2.75706398\n",
      "Iteration 1084, loss = 2.75676220\n",
      "Iteration 1085, loss = 2.75728459\n",
      "Iteration 1086, loss = 2.75637247\n",
      "Iteration 1087, loss = 2.75628804\n",
      "Iteration 1088, loss = 2.75595158\n",
      "Iteration 1089, loss = 2.75562592\n",
      "Iteration 1090, loss = 2.75541461\n",
      "Iteration 1091, loss = 2.75510906\n",
      "Iteration 1092, loss = 2.75496629\n",
      "Iteration 1093, loss = 2.75458929\n",
      "Iteration 1094, loss = 2.75494834\n",
      "Iteration 1095, loss = 2.75468648\n",
      "Iteration 1096, loss = 2.75410334\n",
      "Iteration 1097, loss = 2.75300101\n",
      "Iteration 1098, loss = 2.75242860\n",
      "Iteration 1099, loss = 2.75267989\n",
      "Iteration 1100, loss = 2.75243557\n",
      "Iteration 1101, loss = 2.75200867\n",
      "Iteration 1102, loss = 2.75189734\n",
      "Iteration 1103, loss = 2.75147764\n",
      "Iteration 1104, loss = 2.75133602\n",
      "Iteration 1105, loss = 2.75063272\n",
      "Iteration 1106, loss = 2.75093914\n",
      "Iteration 1107, loss = 2.75099754\n",
      "Iteration 1108, loss = 2.75049450\n",
      "Iteration 1109, loss = 2.75041268\n",
      "Iteration 1110, loss = 2.75047094\n",
      "Iteration 1111, loss = 2.74990830\n",
      "Iteration 1112, loss = 2.74901592\n",
      "Iteration 1113, loss = 2.74869588\n",
      "Iteration 1114, loss = 2.74875844\n",
      "Iteration 1115, loss = 2.74824399\n",
      "Iteration 1116, loss = 2.74752971\n",
      "Iteration 1117, loss = 2.74797143\n",
      "Iteration 1118, loss = 2.74723991\n",
      "Iteration 1119, loss = 2.74747967\n",
      "Iteration 1120, loss = 2.74623541\n",
      "Iteration 1121, loss = 2.74723849\n",
      "Iteration 1122, loss = 2.74776561\n",
      "Iteration 1123, loss = 2.74655397\n",
      "Iteration 1124, loss = 2.74599872\n",
      "Iteration 1125, loss = 2.74590102\n",
      "Iteration 1126, loss = 2.74630437\n",
      "Iteration 1127, loss = 2.74654921\n",
      "Iteration 1128, loss = 2.74590357\n",
      "Iteration 1129, loss = 2.74509381\n",
      "Iteration 1130, loss = 2.74502024\n",
      "Iteration 1131, loss = 2.74511847\n",
      "Iteration 1132, loss = 2.74401751\n",
      "Iteration 1133, loss = 2.74331410\n",
      "Iteration 1134, loss = 2.74489275\n",
      "Iteration 1135, loss = 2.74360550\n",
      "Iteration 1136, loss = 2.74242725\n",
      "Iteration 1137, loss = 2.74287154\n",
      "Iteration 1138, loss = 2.74268639\n",
      "Iteration 1139, loss = 2.74160748\n",
      "Iteration 1140, loss = 2.74147091\n",
      "Iteration 1141, loss = 2.74117800\n",
      "Iteration 1142, loss = 2.74075862\n",
      "Iteration 1143, loss = 2.74061179\n",
      "Iteration 1144, loss = 2.74054990\n",
      "Iteration 1145, loss = 2.73986622\n",
      "Iteration 1146, loss = 2.73942110\n",
      "Iteration 1147, loss = 2.73923916\n",
      "Iteration 1148, loss = 2.73856868\n",
      "Iteration 1149, loss = 2.73856638\n",
      "Iteration 1150, loss = 2.73838659\n",
      "Iteration 1151, loss = 2.73851075\n",
      "Iteration 1152, loss = 2.73774594\n",
      "Iteration 1153, loss = 2.73722854\n",
      "Iteration 1154, loss = 2.73705604\n",
      "Iteration 1155, loss = 2.73756372\n",
      "Iteration 1156, loss = 2.73743426\n",
      "Iteration 1157, loss = 2.73674273\n",
      "Iteration 1158, loss = 2.73652712\n",
      "Iteration 1159, loss = 2.73560062\n",
      "Iteration 1160, loss = 2.73585219\n",
      "Iteration 1161, loss = 2.73547349\n",
      "Iteration 1162, loss = 2.73527067\n",
      "Iteration 1163, loss = 2.73469601\n",
      "Iteration 1164, loss = 2.73509012\n",
      "Iteration 1165, loss = 2.73490540\n",
      "Iteration 1166, loss = 2.73407911\n",
      "Iteration 1167, loss = 2.73345842\n",
      "Iteration 1168, loss = 2.73364876\n",
      "Iteration 1169, loss = 2.73346715\n",
      "Iteration 1170, loss = 2.73309918\n",
      "Iteration 1171, loss = 2.73269412\n",
      "Iteration 1172, loss = 2.73256946\n",
      "Iteration 1173, loss = 2.73275921\n",
      "Iteration 1174, loss = 2.73176140\n",
      "Iteration 1175, loss = 2.73144010\n",
      "Iteration 1176, loss = 2.73107424\n",
      "Iteration 1177, loss = 2.73126869\n",
      "Iteration 1178, loss = 2.73085055\n",
      "Iteration 1179, loss = 2.73096776\n",
      "Iteration 1180, loss = 2.73021236\n",
      "Iteration 1181, loss = 2.72944750\n",
      "Iteration 1182, loss = 2.72946926\n",
      "Iteration 1183, loss = 2.72900872\n",
      "Iteration 1184, loss = 2.72904667\n",
      "Iteration 1185, loss = 2.72876739\n",
      "Iteration 1186, loss = 2.72849887\n",
      "Iteration 1187, loss = 2.72836723\n",
      "Iteration 1188, loss = 2.72802171\n",
      "Iteration 1189, loss = 2.72735342\n",
      "Iteration 1190, loss = 2.72733791\n",
      "Iteration 1191, loss = 2.72680916\n",
      "Iteration 1192, loss = 2.72664871\n",
      "Iteration 1193, loss = 2.72617924\n",
      "Iteration 1194, loss = 2.72664233\n",
      "Iteration 1195, loss = 2.72635688\n",
      "Iteration 1196, loss = 2.72617747\n",
      "Iteration 1197, loss = 2.72604601\n",
      "Iteration 1198, loss = 2.72545756\n",
      "Iteration 1199, loss = 2.72575806\n",
      "Iteration 1200, loss = 2.72537989\n",
      "Iteration 1201, loss = 2.72459178\n",
      "Iteration 1202, loss = 2.72473046\n",
      "Iteration 1203, loss = 2.72420844\n",
      "Iteration 1204, loss = 2.72409201\n",
      "Iteration 1205, loss = 2.72359303\n",
      "Iteration 1206, loss = 2.72365153\n",
      "Iteration 1207, loss = 2.72315333\n",
      "Iteration 1208, loss = 2.72252763\n",
      "Iteration 1209, loss = 2.72218576\n",
      "Iteration 1210, loss = 2.72196731\n",
      "Iteration 1211, loss = 2.72210046\n",
      "Iteration 1212, loss = 2.72165726\n",
      "Iteration 1213, loss = 2.72148486\n",
      "Iteration 1214, loss = 2.72107408\n",
      "Iteration 1215, loss = 2.72040994\n",
      "Iteration 1216, loss = 2.72059612\n",
      "Iteration 1217, loss = 2.72009155\n",
      "Iteration 1218, loss = 2.72041147\n",
      "Iteration 1219, loss = 2.71969211\n",
      "Iteration 1220, loss = 2.71934888\n",
      "Iteration 1221, loss = 2.71927456\n",
      "Iteration 1222, loss = 2.71981915\n",
      "Iteration 1223, loss = 2.71903179\n",
      "Iteration 1224, loss = 2.71863795\n",
      "Iteration 1225, loss = 2.71759010\n",
      "Iteration 1226, loss = 2.71759552\n",
      "Iteration 1227, loss = 2.71720672\n",
      "Iteration 1228, loss = 2.71675903\n",
      "Iteration 1229, loss = 2.71648973\n",
      "Iteration 1230, loss = 2.71638568\n",
      "Iteration 1231, loss = 2.71610550\n",
      "Iteration 1232, loss = 2.71616236\n",
      "Iteration 1233, loss = 2.71578559\n",
      "Iteration 1234, loss = 2.71542196\n",
      "Iteration 1235, loss = 2.71515611\n",
      "Iteration 1236, loss = 2.71441513\n",
      "Iteration 1237, loss = 2.71440351\n",
      "Iteration 1238, loss = 2.71424254\n",
      "Iteration 1239, loss = 2.71388929\n",
      "Iteration 1240, loss = 2.71376620\n",
      "Iteration 1241, loss = 2.71318367\n",
      "Iteration 1242, loss = 2.71339972\n",
      "Iteration 1243, loss = 2.71333513\n",
      "Iteration 1244, loss = 2.71304689\n",
      "Iteration 1245, loss = 2.71243591\n",
      "Iteration 1246, loss = 2.71262129\n",
      "Iteration 1247, loss = 2.71264117\n",
      "Iteration 1248, loss = 2.71234444\n",
      "Iteration 1249, loss = 2.71171640\n",
      "Iteration 1250, loss = 2.71106594\n",
      "Iteration 1251, loss = 2.71086129\n",
      "Iteration 1252, loss = 2.71062177\n",
      "Iteration 1253, loss = 2.71059807\n",
      "Iteration 1254, loss = 2.71045236\n",
      "Iteration 1255, loss = 2.71014320\n",
      "Iteration 1256, loss = 2.70994549\n",
      "Iteration 1257, loss = 2.70990527\n",
      "Iteration 1258, loss = 2.70959828\n",
      "Iteration 1259, loss = 2.70906513\n",
      "Iteration 1260, loss = 2.70895331\n",
      "Iteration 1261, loss = 2.70870868\n",
      "Iteration 1262, loss = 2.70874758\n",
      "Iteration 1263, loss = 2.70871240\n",
      "Iteration 1264, loss = 2.70817602\n",
      "Iteration 1265, loss = 2.70779358\n",
      "Iteration 1266, loss = 2.70737163\n",
      "Iteration 1267, loss = 2.70684971\n",
      "Iteration 1268, loss = 2.70640774\n",
      "Iteration 1269, loss = 2.70653201\n",
      "Iteration 1, loss = 4.15967023\n",
      "Iteration 2, loss = 4.11883197\n",
      "Iteration 3, loss = 4.07890079\n",
      "Iteration 4, loss = 4.03879060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 3.99632666\n",
      "Iteration 6, loss = 3.95203282\n",
      "Iteration 7, loss = 3.90509035\n",
      "Iteration 8, loss = 3.85598663\n",
      "Iteration 9, loss = 3.80346162\n",
      "Iteration 10, loss = 3.74922420\n",
      "Iteration 11, loss = 3.69298181\n",
      "Iteration 12, loss = 3.63625728\n",
      "Iteration 13, loss = 3.57948530\n",
      "Iteration 14, loss = 3.52473206\n",
      "Iteration 15, loss = 3.47299961\n",
      "Iteration 16, loss = 3.42513456\n",
      "Iteration 17, loss = 3.38153158\n",
      "Iteration 18, loss = 3.34469024\n",
      "Iteration 19, loss = 3.31154597\n",
      "Iteration 20, loss = 3.28572430\n",
      "Iteration 21, loss = 3.26552831\n",
      "Iteration 22, loss = 3.25169783\n",
      "Iteration 23, loss = 3.24208530\n",
      "Iteration 24, loss = 3.23598324\n",
      "Iteration 25, loss = 3.23172490\n",
      "Iteration 26, loss = 3.22950364\n",
      "Iteration 27, loss = 3.22771064\n",
      "Iteration 28, loss = 3.22700398\n",
      "Iteration 29, loss = 3.22586263\n",
      "Iteration 30, loss = 3.22540181\n",
      "Iteration 31, loss = 3.22435666\n",
      "Iteration 32, loss = 3.22339675\n",
      "Iteration 33, loss = 3.22293261\n",
      "Iteration 34, loss = 3.22209486\n",
      "Iteration 35, loss = 3.22117515\n",
      "Iteration 36, loss = 3.22038850\n",
      "Iteration 37, loss = 3.22002910\n",
      "Iteration 38, loss = 3.21938454\n",
      "Iteration 39, loss = 3.21844375\n",
      "Iteration 40, loss = 3.21771130\n",
      "Iteration 41, loss = 3.21754635\n",
      "Iteration 42, loss = 3.21697109\n",
      "Iteration 43, loss = 3.21651039\n",
      "Iteration 44, loss = 3.21589230\n",
      "Iteration 45, loss = 3.21523617\n",
      "Iteration 46, loss = 3.21443010\n",
      "Iteration 47, loss = 3.21437596\n",
      "Iteration 48, loss = 3.21333367\n",
      "Iteration 49, loss = 3.21293946\n",
      "Iteration 50, loss = 3.21224058\n",
      "Iteration 51, loss = 3.21168604\n",
      "Iteration 52, loss = 3.21102459\n",
      "Iteration 53, loss = 3.21026344\n",
      "Iteration 54, loss = 3.20979408\n",
      "Iteration 55, loss = 3.20939783\n",
      "Iteration 56, loss = 3.20861848\n",
      "Iteration 57, loss = 3.20822133\n",
      "Iteration 58, loss = 3.20764045\n",
      "Iteration 59, loss = 3.20711402\n",
      "Iteration 60, loss = 3.20677843\n",
      "Iteration 61, loss = 3.20644195\n",
      "Iteration 62, loss = 3.20563001\n",
      "Iteration 63, loss = 3.20475650\n",
      "Iteration 64, loss = 3.20409885\n",
      "Iteration 65, loss = 3.20354526\n",
      "Iteration 66, loss = 3.20306062\n",
      "Iteration 67, loss = 3.20233296\n",
      "Iteration 68, loss = 3.20162813\n",
      "Iteration 69, loss = 3.20105743\n",
      "Iteration 70, loss = 3.20063961\n",
      "Iteration 71, loss = 3.19970944\n",
      "Iteration 72, loss = 3.19940319\n",
      "Iteration 73, loss = 3.19849858\n",
      "Iteration 74, loss = 3.19818918\n",
      "Iteration 75, loss = 3.19737735\n",
      "Iteration 76, loss = 3.19679316\n",
      "Iteration 77, loss = 3.19614106\n",
      "Iteration 78, loss = 3.19543971\n",
      "Iteration 79, loss = 3.19472390\n",
      "Iteration 80, loss = 3.19445188\n",
      "Iteration 81, loss = 3.19410881\n",
      "Iteration 82, loss = 3.19358663\n",
      "Iteration 83, loss = 3.19252003\n",
      "Iteration 84, loss = 3.19199606\n",
      "Iteration 85, loss = 3.19123366\n",
      "Iteration 86, loss = 3.19082944\n",
      "Iteration 87, loss = 3.19024139\n",
      "Iteration 88, loss = 3.18933804\n",
      "Iteration 89, loss = 3.18909449\n",
      "Iteration 90, loss = 3.18854009\n",
      "Iteration 91, loss = 3.18764345\n",
      "Iteration 92, loss = 3.18716307\n",
      "Iteration 93, loss = 3.18642713\n",
      "Iteration 94, loss = 3.18595400\n",
      "Iteration 95, loss = 3.18500915\n",
      "Iteration 96, loss = 3.18436800\n",
      "Iteration 97, loss = 3.18383029\n",
      "Iteration 98, loss = 3.18330188\n",
      "Iteration 99, loss = 3.18241450\n",
      "Iteration 100, loss = 3.18213142\n",
      "Iteration 101, loss = 3.18154905\n",
      "Iteration 102, loss = 3.18106022\n",
      "Iteration 103, loss = 3.18028884\n",
      "Iteration 104, loss = 3.17995125\n",
      "Iteration 105, loss = 3.17923438\n",
      "Iteration 106, loss = 3.17859767\n",
      "Iteration 107, loss = 3.17795560\n",
      "Iteration 108, loss = 3.17724422\n",
      "Iteration 109, loss = 3.17668176\n",
      "Iteration 110, loss = 3.17606695\n",
      "Iteration 111, loss = 3.17550795\n",
      "Iteration 112, loss = 3.17486008\n",
      "Iteration 113, loss = 3.17419349\n",
      "Iteration 114, loss = 3.17349366\n",
      "Iteration 115, loss = 3.17287387\n",
      "Iteration 116, loss = 3.17256430\n",
      "Iteration 117, loss = 3.17185049\n",
      "Iteration 118, loss = 3.17120427\n",
      "Iteration 119, loss = 3.17071156\n",
      "Iteration 120, loss = 3.17008998\n",
      "Iteration 121, loss = 3.16952178\n",
      "Iteration 122, loss = 3.16944417\n",
      "Iteration 123, loss = 3.16845158\n",
      "Iteration 124, loss = 3.16767399\n",
      "Iteration 125, loss = 3.16693337\n",
      "Iteration 126, loss = 3.16711125\n",
      "Iteration 127, loss = 3.16673659\n",
      "Iteration 128, loss = 3.16576364\n",
      "Iteration 129, loss = 3.16515590\n",
      "Iteration 130, loss = 3.16442789\n",
      "Iteration 131, loss = 3.16366790\n",
      "Iteration 132, loss = 3.16295199\n",
      "Iteration 133, loss = 3.16229892\n",
      "Iteration 134, loss = 3.16269859\n",
      "Iteration 135, loss = 3.16197250\n",
      "Iteration 136, loss = 3.16109426\n",
      "Iteration 137, loss = 3.16019905\n",
      "Iteration 138, loss = 3.15927608\n",
      "Iteration 139, loss = 3.15907746\n",
      "Iteration 140, loss = 3.15813289\n",
      "Iteration 141, loss = 3.15743345\n",
      "Iteration 142, loss = 3.15730075\n",
      "Iteration 143, loss = 3.15666552\n",
      "Iteration 144, loss = 3.15590828\n",
      "Iteration 145, loss = 3.15512748\n",
      "Iteration 146, loss = 3.15443878\n",
      "Iteration 147, loss = 3.15396031\n",
      "Iteration 148, loss = 3.15357479\n",
      "Iteration 149, loss = 3.15279035\n",
      "Iteration 150, loss = 3.15230710\n",
      "Iteration 151, loss = 3.15181731\n",
      "Iteration 152, loss = 3.15125143\n",
      "Iteration 153, loss = 3.15082547\n",
      "Iteration 154, loss = 3.15053831\n",
      "Iteration 155, loss = 3.14987039\n",
      "Iteration 156, loss = 3.14936912\n",
      "Iteration 157, loss = 3.14836681\n",
      "Iteration 158, loss = 3.14790583\n",
      "Iteration 159, loss = 3.14712060\n",
      "Iteration 160, loss = 3.14646042\n",
      "Iteration 161, loss = 3.14572847\n",
      "Iteration 162, loss = 3.14524103\n",
      "Iteration 163, loss = 3.14503838\n",
      "Iteration 164, loss = 3.14424858\n",
      "Iteration 165, loss = 3.14404019\n",
      "Iteration 166, loss = 3.14317827\n",
      "Iteration 167, loss = 3.14259882\n",
      "Iteration 168, loss = 3.14197406\n",
      "Iteration 169, loss = 3.14192011\n",
      "Iteration 170, loss = 3.14107730\n",
      "Iteration 171, loss = 3.14049203\n",
      "Iteration 172, loss = 3.13993756\n",
      "Iteration 173, loss = 3.13948928\n",
      "Iteration 174, loss = 3.13886601\n",
      "Iteration 175, loss = 3.13812180\n",
      "Iteration 176, loss = 3.13749254\n",
      "Iteration 177, loss = 3.13677621\n",
      "Iteration 178, loss = 3.13641880\n",
      "Iteration 179, loss = 3.13626838\n",
      "Iteration 180, loss = 3.13547007\n",
      "Iteration 181, loss = 3.13504391\n",
      "Iteration 182, loss = 3.13430048\n",
      "Iteration 183, loss = 3.13370695\n",
      "Iteration 184, loss = 3.13353980\n",
      "Iteration 185, loss = 3.13241766\n",
      "Iteration 186, loss = 3.13241565\n",
      "Iteration 187, loss = 3.13232538\n",
      "Iteration 188, loss = 3.13157642\n",
      "Iteration 189, loss = 3.13065560\n",
      "Iteration 190, loss = 3.13009870\n",
      "Iteration 191, loss = 3.12994431\n",
      "Iteration 192, loss = 3.12925015\n",
      "Iteration 193, loss = 3.12864534\n",
      "Iteration 194, loss = 3.12838995\n",
      "Iteration 195, loss = 3.12775476\n",
      "Iteration 196, loss = 3.12706658\n",
      "Iteration 197, loss = 3.12608912\n",
      "Iteration 198, loss = 3.12650848\n",
      "Iteration 199, loss = 3.12593988\n",
      "Iteration 200, loss = 3.12521007\n",
      "Iteration 201, loss = 3.12466339\n",
      "Iteration 202, loss = 3.12382877\n",
      "Iteration 203, loss = 3.12344283\n",
      "Iteration 204, loss = 3.12263858\n",
      "Iteration 205, loss = 3.12275501\n",
      "Iteration 206, loss = 3.12266929\n",
      "Iteration 207, loss = 3.12178149\n",
      "Iteration 208, loss = 3.12085213\n",
      "Iteration 209, loss = 3.12024944\n",
      "Iteration 210, loss = 3.11983405\n",
      "Iteration 211, loss = 3.11922192\n",
      "Iteration 212, loss = 3.11925209\n",
      "Iteration 213, loss = 3.11816509\n",
      "Iteration 214, loss = 3.11772525\n",
      "Iteration 215, loss = 3.11728664\n",
      "Iteration 216, loss = 3.11677572\n",
      "Iteration 217, loss = 3.11616009\n",
      "Iteration 218, loss = 3.11591392\n",
      "Iteration 219, loss = 3.11507603\n",
      "Iteration 220, loss = 3.11451422\n",
      "Iteration 221, loss = 3.11404869\n",
      "Iteration 222, loss = 3.11360970\n",
      "Iteration 223, loss = 3.11298934\n",
      "Iteration 224, loss = 3.11251146\n",
      "Iteration 225, loss = 3.11181198\n",
      "Iteration 226, loss = 3.11127361\n",
      "Iteration 227, loss = 3.11061069\n",
      "Iteration 228, loss = 3.11021261\n",
      "Iteration 229, loss = 3.11023088\n",
      "Iteration 230, loss = 3.10926670\n",
      "Iteration 231, loss = 3.10889373\n",
      "Iteration 232, loss = 3.10846467\n",
      "Iteration 233, loss = 3.10803712\n",
      "Iteration 234, loss = 3.10713426\n",
      "Iteration 235, loss = 3.10645690\n",
      "Iteration 236, loss = 3.10570453\n",
      "Iteration 237, loss = 3.10568164\n",
      "Iteration 238, loss = 3.10494822\n",
      "Iteration 239, loss = 3.10429253\n",
      "Iteration 240, loss = 3.10387871\n",
      "Iteration 241, loss = 3.10290498\n",
      "Iteration 242, loss = 3.10302094\n",
      "Iteration 243, loss = 3.10249033\n",
      "Iteration 244, loss = 3.10198517\n",
      "Iteration 245, loss = 3.10157693\n",
      "Iteration 246, loss = 3.10111509\n",
      "Iteration 247, loss = 3.10046295\n",
      "Iteration 248, loss = 3.09984740\n",
      "Iteration 249, loss = 3.09908751\n",
      "Iteration 250, loss = 3.09886228\n",
      "Iteration 251, loss = 3.09794810\n",
      "Iteration 252, loss = 3.09778573\n",
      "Iteration 253, loss = 3.09693563\n",
      "Iteration 254, loss = 3.09659477\n",
      "Iteration 255, loss = 3.09643466\n",
      "Iteration 256, loss = 3.09668620\n",
      "Iteration 257, loss = 3.09542804\n",
      "Iteration 258, loss = 3.09489865\n",
      "Iteration 259, loss = 3.09402849\n",
      "Iteration 260, loss = 3.09384859\n",
      "Iteration 261, loss = 3.09345764\n",
      "Iteration 262, loss = 3.09258515\n",
      "Iteration 263, loss = 3.09214483\n",
      "Iteration 264, loss = 3.09170763\n",
      "Iteration 265, loss = 3.09115680\n",
      "Iteration 266, loss = 3.09078713\n",
      "Iteration 267, loss = 3.09008879\n",
      "Iteration 268, loss = 3.08949794\n",
      "Iteration 269, loss = 3.08888321\n",
      "Iteration 270, loss = 3.08835612\n",
      "Iteration 271, loss = 3.08852399\n",
      "Iteration 272, loss = 3.08795186\n",
      "Iteration 273, loss = 3.08718935\n",
      "Iteration 274, loss = 3.08715479\n",
      "Iteration 275, loss = 3.08667588\n",
      "Iteration 276, loss = 3.08616690\n",
      "Iteration 277, loss = 3.08612603\n",
      "Iteration 278, loss = 3.08574379\n",
      "Iteration 279, loss = 3.08476157\n",
      "Iteration 280, loss = 3.08409712\n",
      "Iteration 281, loss = 3.08364384\n",
      "Iteration 282, loss = 3.08292398\n",
      "Iteration 283, loss = 3.08325713\n",
      "Iteration 284, loss = 3.08251411\n",
      "Iteration 285, loss = 3.08176851\n",
      "Iteration 286, loss = 3.08127500\n",
      "Iteration 287, loss = 3.08060229\n",
      "Iteration 288, loss = 3.07982675\n",
      "Iteration 289, loss = 3.07948967\n",
      "Iteration 290, loss = 3.07935992\n",
      "Iteration 291, loss = 3.07878223\n",
      "Iteration 292, loss = 3.07846246\n",
      "Iteration 293, loss = 3.07774896\n",
      "Iteration 294, loss = 3.07740696\n",
      "Iteration 295, loss = 3.07687089\n",
      "Iteration 296, loss = 3.07613435\n",
      "Iteration 297, loss = 3.07564179\n",
      "Iteration 298, loss = 3.07651399\n",
      "Iteration 299, loss = 3.07632686\n",
      "Iteration 300, loss = 3.07474633\n",
      "Iteration 301, loss = 3.07367714\n",
      "Iteration 302, loss = 3.07337163\n",
      "Iteration 303, loss = 3.07293222\n",
      "Iteration 304, loss = 3.07257950\n",
      "Iteration 305, loss = 3.07179138\n",
      "Iteration 306, loss = 3.07147582\n",
      "Iteration 307, loss = 3.07113628\n",
      "Iteration 308, loss = 3.07070864\n",
      "Iteration 309, loss = 3.07022500\n",
      "Iteration 310, loss = 3.06933451\n",
      "Iteration 311, loss = 3.06899447\n",
      "Iteration 312, loss = 3.06863245\n",
      "Iteration 313, loss = 3.06778607\n",
      "Iteration 314, loss = 3.06737627\n",
      "Iteration 315, loss = 3.06683630\n",
      "Iteration 316, loss = 3.06658281\n",
      "Iteration 317, loss = 3.06608411\n",
      "Iteration 318, loss = 3.06593508\n",
      "Iteration 319, loss = 3.06539424\n",
      "Iteration 320, loss = 3.06466843\n",
      "Iteration 321, loss = 3.06415661\n",
      "Iteration 322, loss = 3.06370993\n",
      "Iteration 323, loss = 3.06353295\n",
      "Iteration 324, loss = 3.06300968\n",
      "Iteration 325, loss = 3.06252655\n",
      "Iteration 326, loss = 3.06187052\n",
      "Iteration 327, loss = 3.06167856\n",
      "Iteration 328, loss = 3.06156522\n",
      "Iteration 329, loss = 3.06094111\n",
      "Iteration 330, loss = 3.06040849\n",
      "Iteration 331, loss = 3.06041444\n",
      "Iteration 332, loss = 3.05911783\n",
      "Iteration 333, loss = 3.05841653\n",
      "Iteration 334, loss = 3.05830504\n",
      "Iteration 335, loss = 3.05737059\n",
      "Iteration 336, loss = 3.05744146\n",
      "Iteration 337, loss = 3.05693295\n",
      "Iteration 338, loss = 3.05624401\n",
      "Iteration 339, loss = 3.05563594\n",
      "Iteration 340, loss = 3.05529451\n",
      "Iteration 341, loss = 3.05534576\n",
      "Iteration 342, loss = 3.05479356\n",
      "Iteration 343, loss = 3.05448683\n",
      "Iteration 344, loss = 3.05394357\n",
      "Iteration 345, loss = 3.05337561\n",
      "Iteration 346, loss = 3.05283989\n",
      "Iteration 347, loss = 3.05245742\n",
      "Iteration 348, loss = 3.05195955\n",
      "Iteration 349, loss = 3.05119876\n",
      "Iteration 350, loss = 3.05141070\n",
      "Iteration 351, loss = 3.05076738\n",
      "Iteration 352, loss = 3.05011605\n",
      "Iteration 353, loss = 3.04956937\n",
      "Iteration 354, loss = 3.04898924\n",
      "Iteration 355, loss = 3.04804751\n",
      "Iteration 356, loss = 3.04782995\n",
      "Iteration 357, loss = 3.04742159\n",
      "Iteration 358, loss = 3.04733498\n",
      "Iteration 359, loss = 3.04644153\n",
      "Iteration 360, loss = 3.04609483\n",
      "Iteration 361, loss = 3.04589362\n",
      "Iteration 362, loss = 3.04607228\n",
      "Iteration 363, loss = 3.04575040\n",
      "Iteration 364, loss = 3.04467607\n",
      "Iteration 365, loss = 3.04407976\n",
      "Iteration 366, loss = 3.04348098\n",
      "Iteration 367, loss = 3.04423223\n",
      "Iteration 368, loss = 3.04320698\n",
      "Iteration 369, loss = 3.04218388\n",
      "Iteration 370, loss = 3.04196419\n",
      "Iteration 371, loss = 3.04150852\n",
      "Iteration 372, loss = 3.04099914\n",
      "Iteration 373, loss = 3.04053003\n",
      "Iteration 374, loss = 3.04032266\n",
      "Iteration 375, loss = 3.03990323\n",
      "Iteration 376, loss = 3.03906847\n",
      "Iteration 377, loss = 3.03950703\n",
      "Iteration 378, loss = 3.03853442\n",
      "Iteration 379, loss = 3.03784350\n",
      "Iteration 380, loss = 3.03744376\n",
      "Iteration 381, loss = 3.03638222\n",
      "Iteration 382, loss = 3.03622168\n",
      "Iteration 383, loss = 3.03588162\n",
      "Iteration 384, loss = 3.03545490\n",
      "Iteration 385, loss = 3.03485393\n",
      "Iteration 386, loss = 3.03417555\n",
      "Iteration 387, loss = 3.03394889\n",
      "Iteration 388, loss = 3.03362041\n",
      "Iteration 389, loss = 3.03299870\n",
      "Iteration 390, loss = 3.03314394\n",
      "Iteration 391, loss = 3.03296290\n",
      "Iteration 392, loss = 3.03210728\n",
      "Iteration 393, loss = 3.03172767\n",
      "Iteration 394, loss = 3.03145164\n",
      "Iteration 395, loss = 3.03092799\n",
      "Iteration 396, loss = 3.03015594\n",
      "Iteration 397, loss = 3.02988165\n",
      "Iteration 398, loss = 3.02957873\n",
      "Iteration 399, loss = 3.02915324\n",
      "Iteration 400, loss = 3.02855249\n",
      "Iteration 401, loss = 3.02788784\n",
      "Iteration 402, loss = 3.02734338\n",
      "Iteration 403, loss = 3.02695253\n",
      "Iteration 404, loss = 3.02684951\n",
      "Iteration 405, loss = 3.02630502\n",
      "Iteration 406, loss = 3.02575065\n",
      "Iteration 407, loss = 3.02544038\n",
      "Iteration 408, loss = 3.02505041\n",
      "Iteration 409, loss = 3.02446720\n",
      "Iteration 410, loss = 3.02406298\n",
      "Iteration 411, loss = 3.02365460\n",
      "Iteration 412, loss = 3.02345570\n",
      "Iteration 413, loss = 3.02293802\n",
      "Iteration 414, loss = 3.02209524\n",
      "Iteration 415, loss = 3.02204412\n",
      "Iteration 416, loss = 3.02140336\n",
      "Iteration 417, loss = 3.02060741\n",
      "Iteration 418, loss = 3.02016490\n",
      "Iteration 419, loss = 3.02006883\n",
      "Iteration 420, loss = 3.02062825\n",
      "Iteration 421, loss = 3.02033046\n",
      "Iteration 422, loss = 3.01884222\n",
      "Iteration 423, loss = 3.01812994\n",
      "Iteration 424, loss = 3.01768374\n",
      "Iteration 425, loss = 3.01707666\n",
      "Iteration 426, loss = 3.01678400\n",
      "Iteration 427, loss = 3.01628222\n",
      "Iteration 428, loss = 3.01587758\n",
      "Iteration 429, loss = 3.01598377\n",
      "Iteration 430, loss = 3.01510882\n",
      "Iteration 431, loss = 3.01465224\n",
      "Iteration 432, loss = 3.01390215\n",
      "Iteration 433, loss = 3.01336180\n",
      "Iteration 434, loss = 3.01282773\n",
      "Iteration 435, loss = 3.01307412\n",
      "Iteration 436, loss = 3.01233995\n",
      "Iteration 437, loss = 3.01195754\n",
      "Iteration 438, loss = 3.01145800\n",
      "Iteration 439, loss = 3.01189967\n",
      "Iteration 440, loss = 3.01137096\n",
      "Iteration 441, loss = 3.01050767\n",
      "Iteration 442, loss = 3.00985196\n",
      "Iteration 443, loss = 3.00947867\n",
      "Iteration 444, loss = 3.00893569\n",
      "Iteration 445, loss = 3.00818435\n",
      "Iteration 446, loss = 3.00848830\n",
      "Iteration 447, loss = 3.00856323\n",
      "Iteration 448, loss = 3.00718511\n",
      "Iteration 449, loss = 3.00814062\n",
      "Iteration 450, loss = 3.00789551\n",
      "Iteration 451, loss = 3.00625093\n",
      "Iteration 452, loss = 3.00505900\n",
      "Iteration 453, loss = 3.00518382\n",
      "Iteration 454, loss = 3.00558388\n",
      "Iteration 455, loss = 3.00487073\n",
      "Iteration 456, loss = 3.00438945\n",
      "Iteration 457, loss = 3.00374815\n",
      "Iteration 458, loss = 3.00472732\n",
      "Iteration 459, loss = 3.00285664\n",
      "Iteration 460, loss = 3.00247056\n",
      "Iteration 461, loss = 3.00251982\n",
      "Iteration 462, loss = 3.00165850\n",
      "Iteration 463, loss = 3.00102598\n",
      "Iteration 464, loss = 3.00108283\n",
      "Iteration 465, loss = 3.00093143\n",
      "Iteration 466, loss = 2.99999003\n",
      "Iteration 467, loss = 2.99901026\n",
      "Iteration 468, loss = 2.99852317\n",
      "Iteration 469, loss = 2.99818372\n",
      "Iteration 470, loss = 2.99789421\n",
      "Iteration 471, loss = 2.99745991\n",
      "Iteration 472, loss = 2.99675958\n",
      "Iteration 473, loss = 2.99680440\n",
      "Iteration 474, loss = 2.99602867\n",
      "Iteration 475, loss = 2.99575335\n",
      "Iteration 476, loss = 2.99551484\n",
      "Iteration 477, loss = 2.99496698\n",
      "Iteration 478, loss = 2.99487852\n",
      "Iteration 479, loss = 2.99444792\n",
      "Iteration 480, loss = 2.99391555\n",
      "Iteration 481, loss = 2.99392893\n",
      "Iteration 482, loss = 2.99341143\n",
      "Iteration 483, loss = 2.99309038\n",
      "Iteration 484, loss = 2.99175924\n",
      "Iteration 485, loss = 2.99115621\n",
      "Iteration 486, loss = 2.99098891\n",
      "Iteration 487, loss = 2.99112437\n",
      "Iteration 488, loss = 2.99078898\n",
      "Iteration 489, loss = 2.98991571\n",
      "Iteration 490, loss = 2.98929207\n",
      "Iteration 491, loss = 2.98922747\n",
      "Iteration 492, loss = 2.98820659\n",
      "Iteration 493, loss = 2.98766602\n",
      "Iteration 494, loss = 2.98709801\n",
      "Iteration 495, loss = 2.98719224\n",
      "Iteration 496, loss = 2.98654723\n",
      "Iteration 497, loss = 2.98612261\n",
      "Iteration 498, loss = 2.98578909\n",
      "Iteration 499, loss = 2.98560809\n",
      "Iteration 500, loss = 2.98503902\n",
      "Iteration 501, loss = 2.98426179\n",
      "Iteration 502, loss = 2.98419516\n",
      "Iteration 503, loss = 2.98406223\n",
      "Iteration 504, loss = 2.98317340\n",
      "Iteration 505, loss = 2.98274969\n",
      "Iteration 506, loss = 2.98260992\n",
      "Iteration 507, loss = 2.98224449\n",
      "Iteration 508, loss = 2.98191137\n",
      "Iteration 509, loss = 2.98160045\n",
      "Iteration 510, loss = 2.98137383\n",
      "Iteration 511, loss = 2.98041139\n",
      "Iteration 512, loss = 2.98002204\n",
      "Iteration 513, loss = 2.97988742\n",
      "Iteration 514, loss = 2.97955587\n",
      "Iteration 515, loss = 2.97903030\n",
      "Iteration 516, loss = 2.97910221\n",
      "Iteration 517, loss = 2.97836750\n",
      "Iteration 518, loss = 2.97801609\n",
      "Iteration 519, loss = 2.97765288\n",
      "Iteration 520, loss = 2.97744345\n",
      "Iteration 521, loss = 2.97682869\n",
      "Iteration 522, loss = 2.97576227\n",
      "Iteration 523, loss = 2.97530463\n",
      "Iteration 524, loss = 2.97503658\n",
      "Iteration 525, loss = 2.97439810\n",
      "Iteration 526, loss = 2.97456806\n",
      "Iteration 527, loss = 2.97451277\n",
      "Iteration 528, loss = 2.97371861\n",
      "Iteration 529, loss = 2.97277145\n",
      "Iteration 530, loss = 2.97276733\n",
      "Iteration 531, loss = 2.97232679\n",
      "Iteration 532, loss = 2.97185154\n",
      "Iteration 533, loss = 2.97149894\n",
      "Iteration 534, loss = 2.97124127\n",
      "Iteration 535, loss = 2.97057981\n",
      "Iteration 536, loss = 2.97028868\n",
      "Iteration 537, loss = 2.96990338\n",
      "Iteration 538, loss = 2.97002271\n",
      "Iteration 539, loss = 2.96926981\n",
      "Iteration 540, loss = 2.96839800\n",
      "Iteration 541, loss = 2.96806436\n",
      "Iteration 542, loss = 2.96758286\n",
      "Iteration 543, loss = 2.96698710\n",
      "Iteration 544, loss = 2.96716564\n",
      "Iteration 545, loss = 2.96695314\n",
      "Iteration 546, loss = 2.96614077\n",
      "Iteration 547, loss = 2.96589612\n",
      "Iteration 548, loss = 2.96525898\n",
      "Iteration 549, loss = 2.96480686\n",
      "Iteration 550, loss = 2.96491290\n",
      "Iteration 551, loss = 2.96438218\n",
      "Iteration 552, loss = 2.96365524\n",
      "Iteration 553, loss = 2.96313209\n",
      "Iteration 554, loss = 2.96246819\n",
      "Iteration 555, loss = 2.96246894\n",
      "Iteration 556, loss = 2.96212094\n",
      "Iteration 557, loss = 2.96206489\n",
      "Iteration 558, loss = 2.96116999\n",
      "Iteration 559, loss = 2.96131557\n",
      "Iteration 560, loss = 2.96073501\n",
      "Iteration 561, loss = 2.96009325\n",
      "Iteration 562, loss = 2.95981574\n",
      "Iteration 563, loss = 2.96002776\n",
      "Iteration 564, loss = 2.95927825\n",
      "Iteration 565, loss = 2.95863339\n",
      "Iteration 566, loss = 2.95808269\n",
      "Iteration 567, loss = 2.95771020\n",
      "Iteration 568, loss = 2.95764813\n",
      "Iteration 569, loss = 2.95669383\n",
      "Iteration 570, loss = 2.95631528\n",
      "Iteration 571, loss = 2.95649589\n",
      "Iteration 572, loss = 2.95581779\n",
      "Iteration 573, loss = 2.95522292\n",
      "Iteration 574, loss = 2.95472580\n",
      "Iteration 575, loss = 2.95449624\n",
      "Iteration 576, loss = 2.95465383\n",
      "Iteration 577, loss = 2.95391579\n",
      "Iteration 578, loss = 2.95307511\n",
      "Iteration 579, loss = 2.95358524\n",
      "Iteration 580, loss = 2.95304301\n",
      "Iteration 581, loss = 2.95226725\n",
      "Iteration 582, loss = 2.95221714\n",
      "Iteration 583, loss = 2.95178098\n",
      "Iteration 584, loss = 2.95115065\n",
      "Iteration 585, loss = 2.95053795\n",
      "Iteration 586, loss = 2.95033201\n",
      "Iteration 587, loss = 2.94987610\n",
      "Iteration 588, loss = 2.94954550\n",
      "Iteration 589, loss = 2.94967967\n",
      "Iteration 590, loss = 2.94908029\n",
      "Iteration 591, loss = 2.94831523\n",
      "Iteration 592, loss = 2.94780253\n",
      "Iteration 593, loss = 2.94744039\n",
      "Iteration 594, loss = 2.94736940\n",
      "Iteration 595, loss = 2.94670850\n",
      "Iteration 596, loss = 2.94637150\n",
      "Iteration 597, loss = 2.94568273\n",
      "Iteration 598, loss = 2.94685071\n",
      "Iteration 599, loss = 2.94611059\n",
      "Iteration 600, loss = 2.94455988\n",
      "Iteration 601, loss = 2.94435227\n",
      "Iteration 602, loss = 2.94387274\n",
      "Iteration 603, loss = 2.94380872\n",
      "Iteration 604, loss = 2.94352787\n",
      "Iteration 605, loss = 2.94267149\n",
      "Iteration 606, loss = 2.94262854\n",
      "Iteration 607, loss = 2.94242974\n",
      "Iteration 608, loss = 2.94205006\n",
      "Iteration 609, loss = 2.94146828\n",
      "Iteration 610, loss = 2.94098163\n",
      "Iteration 611, loss = 2.94041517\n",
      "Iteration 612, loss = 2.94181044\n",
      "Iteration 613, loss = 2.94115343\n",
      "Iteration 614, loss = 2.93985482\n",
      "Iteration 615, loss = 2.93957457\n",
      "Iteration 616, loss = 2.93873797\n",
      "Iteration 617, loss = 2.93827163\n",
      "Iteration 618, loss = 2.93776796\n",
      "Iteration 619, loss = 2.93744655\n",
      "Iteration 620, loss = 2.93707197\n",
      "Iteration 621, loss = 2.93732452\n",
      "Iteration 622, loss = 2.93702563\n",
      "Iteration 623, loss = 2.93631573\n",
      "Iteration 624, loss = 2.93554779\n",
      "Iteration 625, loss = 2.93519228\n",
      "Iteration 626, loss = 2.93463338\n",
      "Iteration 627, loss = 2.93439981\n",
      "Iteration 628, loss = 2.93423997\n",
      "Iteration 629, loss = 2.93365113\n",
      "Iteration 630, loss = 2.93384585\n",
      "Iteration 631, loss = 2.93338003\n",
      "Iteration 632, loss = 2.93303329\n",
      "Iteration 633, loss = 2.93236415\n",
      "Iteration 634, loss = 2.93206751\n",
      "Iteration 635, loss = 2.93248434\n",
      "Iteration 636, loss = 2.93104949\n",
      "Iteration 637, loss = 2.93091630\n",
      "Iteration 638, loss = 2.93058192\n",
      "Iteration 639, loss = 2.93004196\n",
      "Iteration 640, loss = 2.92908412\n",
      "Iteration 641, loss = 2.92963698\n",
      "Iteration 642, loss = 2.93004767\n",
      "Iteration 643, loss = 2.92895353\n",
      "Iteration 644, loss = 2.92794224\n",
      "Iteration 645, loss = 2.92740873\n",
      "Iteration 646, loss = 2.92711391\n",
      "Iteration 647, loss = 2.92726317\n",
      "Iteration 648, loss = 2.92684595\n",
      "Iteration 649, loss = 2.92668821\n",
      "Iteration 650, loss = 2.92556656\n",
      "Iteration 651, loss = 2.92538431\n",
      "Iteration 652, loss = 2.92526920\n",
      "Iteration 653, loss = 2.92478839\n",
      "Iteration 654, loss = 2.92435396\n",
      "Iteration 655, loss = 2.92413595\n",
      "Iteration 656, loss = 2.92343235\n",
      "Iteration 657, loss = 2.92336925\n",
      "Iteration 658, loss = 2.92295913\n",
      "Iteration 659, loss = 2.92282352\n",
      "Iteration 660, loss = 2.92229307\n",
      "Iteration 661, loss = 2.92196701\n",
      "Iteration 662, loss = 2.92159589\n",
      "Iteration 663, loss = 2.92107894\n",
      "Iteration 664, loss = 2.92054881\n",
      "Iteration 665, loss = 2.92016355\n",
      "Iteration 666, loss = 2.91990544\n",
      "Iteration 667, loss = 2.92028578\n",
      "Iteration 668, loss = 2.91922362\n",
      "Iteration 669, loss = 2.91911039\n",
      "Iteration 670, loss = 2.91848370\n",
      "Iteration 671, loss = 2.91821645\n",
      "Iteration 672, loss = 2.91798245\n",
      "Iteration 673, loss = 2.91744799\n",
      "Iteration 674, loss = 2.91706550\n",
      "Iteration 675, loss = 2.91648682\n",
      "Iteration 676, loss = 2.91654815\n",
      "Iteration 677, loss = 2.91559962\n",
      "Iteration 678, loss = 2.91543711\n",
      "Iteration 679, loss = 2.91548980\n",
      "Iteration 680, loss = 2.91536479\n",
      "Iteration 681, loss = 2.91486073\n",
      "Iteration 682, loss = 2.91452610\n",
      "Iteration 683, loss = 2.91467646\n",
      "Iteration 684, loss = 2.91407535\n",
      "Iteration 685, loss = 2.91276071\n",
      "Iteration 686, loss = 2.91252047\n",
      "Iteration 687, loss = 2.91260727\n",
      "Iteration 688, loss = 2.91327485\n",
      "Iteration 689, loss = 2.91281619\n",
      "Iteration 690, loss = 2.91075481\n",
      "Iteration 691, loss = 2.91097265\n",
      "Iteration 692, loss = 2.90995664\n",
      "Iteration 693, loss = 2.90977151\n",
      "Iteration 694, loss = 2.90984159\n",
      "Iteration 695, loss = 2.90987636\n",
      "Iteration 696, loss = 2.90939527\n",
      "Iteration 697, loss = 2.90895006\n",
      "Iteration 698, loss = 2.90871485\n",
      "Iteration 699, loss = 2.90820753\n",
      "Iteration 700, loss = 2.90792462\n",
      "Iteration 701, loss = 2.90725637\n",
      "Iteration 702, loss = 2.90680584\n",
      "Iteration 703, loss = 2.90649776\n",
      "Iteration 704, loss = 2.90620481\n",
      "Iteration 705, loss = 2.90552548\n",
      "Iteration 706, loss = 2.90543078\n",
      "Iteration 707, loss = 2.90506408\n",
      "Iteration 708, loss = 2.90439896\n",
      "Iteration 709, loss = 2.90399903\n",
      "Iteration 710, loss = 2.90388035\n",
      "Iteration 711, loss = 2.90366825\n",
      "Iteration 712, loss = 2.90300129\n",
      "Iteration 713, loss = 2.90318848\n",
      "Iteration 714, loss = 2.90216379\n",
      "Iteration 715, loss = 2.90223517\n",
      "Iteration 716, loss = 2.90304968\n",
      "Iteration 717, loss = 2.90207607\n",
      "Iteration 718, loss = 2.90098742\n",
      "Iteration 719, loss = 2.90086854\n",
      "Iteration 720, loss = 2.90070919\n",
      "Iteration 721, loss = 2.90040598\n",
      "Iteration 722, loss = 2.89949978\n",
      "Iteration 723, loss = 2.89946057\n",
      "Iteration 724, loss = 2.89928412\n",
      "Iteration 725, loss = 2.89899805\n",
      "Iteration 726, loss = 2.89822748\n",
      "Iteration 727, loss = 2.89771740\n",
      "Iteration 728, loss = 2.89758152\n",
      "Iteration 729, loss = 2.89807169\n",
      "Iteration 730, loss = 2.89720134\n",
      "Iteration 731, loss = 2.89608043\n",
      "Iteration 732, loss = 2.89578731\n",
      "Iteration 733, loss = 2.89534295\n",
      "Iteration 734, loss = 2.89574559\n",
      "Iteration 735, loss = 2.89548847\n",
      "Iteration 736, loss = 2.89530467\n",
      "Iteration 737, loss = 2.89504946\n",
      "Iteration 738, loss = 2.89514975\n",
      "Iteration 739, loss = 2.89388253\n",
      "Iteration 740, loss = 2.89335382\n",
      "Iteration 741, loss = 2.89275069\n",
      "Iteration 742, loss = 2.89249209\n",
      "Iteration 743, loss = 2.89228529\n",
      "Iteration 744, loss = 2.89187761\n",
      "Iteration 745, loss = 2.89074515\n",
      "Iteration 746, loss = 2.89094226\n",
      "Iteration 747, loss = 2.89071110\n",
      "Iteration 748, loss = 2.89106498\n",
      "Iteration 749, loss = 2.89072447\n",
      "Iteration 750, loss = 2.88997349\n",
      "Iteration 751, loss = 2.88968027\n",
      "Iteration 752, loss = 2.88923884\n",
      "Iteration 753, loss = 2.88896899\n",
      "Iteration 754, loss = 2.88857473\n",
      "Iteration 755, loss = 2.88778791\n",
      "Iteration 756, loss = 2.88776917\n",
      "Iteration 757, loss = 2.88724845\n",
      "Iteration 758, loss = 2.88680242\n",
      "Iteration 759, loss = 2.88693380\n",
      "Iteration 760, loss = 2.88657182\n",
      "Iteration 761, loss = 2.88585762\n",
      "Iteration 762, loss = 2.88591634\n",
      "Iteration 763, loss = 2.88541088\n",
      "Iteration 764, loss = 2.88501395\n",
      "Iteration 765, loss = 2.88420271\n",
      "Iteration 766, loss = 2.88402864\n",
      "Iteration 767, loss = 2.88359316\n",
      "Iteration 768, loss = 2.88357661\n",
      "Iteration 769, loss = 2.88295951\n",
      "Iteration 770, loss = 2.88339455\n",
      "Iteration 771, loss = 2.88243491\n",
      "Iteration 772, loss = 2.88198761\n",
      "Iteration 773, loss = 2.88163044\n",
      "Iteration 774, loss = 2.88125983\n",
      "Iteration 775, loss = 2.88131288\n",
      "Iteration 776, loss = 2.88091527\n",
      "Iteration 777, loss = 2.88057459\n",
      "Iteration 778, loss = 2.88000750\n",
      "Iteration 779, loss = 2.87917763\n",
      "Iteration 780, loss = 2.87884608\n",
      "Iteration 781, loss = 2.87902422\n",
      "Iteration 782, loss = 2.87865042\n",
      "Iteration 783, loss = 2.87854773\n",
      "Iteration 784, loss = 2.87775310\n",
      "Iteration 785, loss = 2.87758873\n",
      "Iteration 786, loss = 2.87774935\n",
      "Iteration 787, loss = 2.87741043\n",
      "Iteration 788, loss = 2.87717432\n",
      "Iteration 789, loss = 2.87690064\n",
      "Iteration 790, loss = 2.87663988\n",
      "Iteration 791, loss = 2.87639708\n",
      "Iteration 792, loss = 2.87542607\n",
      "Iteration 793, loss = 2.87522305\n",
      "Iteration 794, loss = 2.87493904\n",
      "Iteration 795, loss = 2.87418582\n",
      "Iteration 796, loss = 2.87384361\n",
      "Iteration 797, loss = 2.87433700\n",
      "Iteration 798, loss = 2.87357882\n",
      "Iteration 799, loss = 2.87314841\n",
      "Iteration 800, loss = 2.87255937\n",
      "Iteration 801, loss = 2.87279505\n",
      "Iteration 802, loss = 2.87275910\n",
      "Iteration 803, loss = 2.87165740\n",
      "Iteration 804, loss = 2.87134048\n",
      "Iteration 805, loss = 2.87113563\n",
      "Iteration 806, loss = 2.87043525\n",
      "Iteration 807, loss = 2.87014720\n",
      "Iteration 808, loss = 2.86999480\n",
      "Iteration 809, loss = 2.86923000\n",
      "Iteration 810, loss = 2.86915688\n",
      "Iteration 811, loss = 2.86882516\n",
      "Iteration 812, loss = 2.86860281\n",
      "Iteration 813, loss = 2.86900330\n",
      "Iteration 814, loss = 2.86833450\n",
      "Iteration 815, loss = 2.86780234\n",
      "Iteration 816, loss = 2.86742157\n",
      "Iteration 817, loss = 2.86700513\n",
      "Iteration 818, loss = 2.86699064\n",
      "Iteration 819, loss = 2.86652211\n",
      "Iteration 820, loss = 2.86605634\n",
      "Iteration 821, loss = 2.86569250\n",
      "Iteration 822, loss = 2.86640440\n",
      "Iteration 823, loss = 2.86526208\n",
      "Iteration 824, loss = 2.86493904\n",
      "Iteration 825, loss = 2.86480563\n",
      "Iteration 826, loss = 2.86396301\n",
      "Iteration 827, loss = 2.86357790\n",
      "Iteration 828, loss = 2.86331710\n",
      "Iteration 829, loss = 2.86339930\n",
      "Iteration 830, loss = 2.86254480\n",
      "Iteration 831, loss = 2.86220897\n",
      "Iteration 832, loss = 2.86209184\n",
      "Iteration 833, loss = 2.86177833\n",
      "Iteration 834, loss = 2.86137190\n",
      "Iteration 835, loss = 2.86075041\n",
      "Iteration 836, loss = 2.86088977\n",
      "Iteration 837, loss = 2.86062064\n",
      "Iteration 838, loss = 2.86009563\n",
      "Iteration 839, loss = 2.85971303\n",
      "Iteration 840, loss = 2.85939663\n",
      "Iteration 841, loss = 2.85910978\n",
      "Iteration 842, loss = 2.85893219\n",
      "Iteration 843, loss = 2.85891255\n",
      "Iteration 844, loss = 2.85864850\n",
      "Iteration 845, loss = 2.85797630\n",
      "Iteration 846, loss = 2.85762111\n",
      "Iteration 847, loss = 2.85731917\n",
      "Iteration 848, loss = 2.85688015\n",
      "Iteration 849, loss = 2.85631498\n",
      "Iteration 850, loss = 2.85570376\n",
      "Iteration 851, loss = 2.85637315\n",
      "Iteration 852, loss = 2.85633558\n",
      "Iteration 853, loss = 2.85610728\n",
      "Iteration 854, loss = 2.85545795\n",
      "Iteration 855, loss = 2.85456101\n",
      "Iteration 856, loss = 2.85377089\n",
      "Iteration 857, loss = 2.85391683\n",
      "Iteration 858, loss = 2.85338929\n",
      "Iteration 859, loss = 2.85321554\n",
      "Iteration 860, loss = 2.85290039\n",
      "Iteration 861, loss = 2.85240404\n",
      "Iteration 862, loss = 2.85261208\n",
      "Iteration 863, loss = 2.85213528\n",
      "Iteration 864, loss = 2.85250474\n",
      "Iteration 865, loss = 2.85337766\n",
      "Iteration 866, loss = 2.85212245\n",
      "Iteration 867, loss = 2.85127584\n",
      "Iteration 868, loss = 2.85098484\n",
      "Iteration 869, loss = 2.85045868\n",
      "Iteration 870, loss = 2.85038930\n",
      "Iteration 871, loss = 2.85017957\n",
      "Iteration 872, loss = 2.84934143\n",
      "Iteration 873, loss = 2.84909963\n",
      "Iteration 874, loss = 2.84858793\n",
      "Iteration 875, loss = 2.84833692\n",
      "Iteration 876, loss = 2.84761152\n",
      "Iteration 877, loss = 2.84703272\n",
      "Iteration 878, loss = 2.84697539\n",
      "Iteration 879, loss = 2.84755041\n",
      "Iteration 880, loss = 2.84681124\n",
      "Iteration 881, loss = 2.84646021\n",
      "Iteration 882, loss = 2.84611356\n",
      "Iteration 883, loss = 2.84541703\n",
      "Iteration 884, loss = 2.84538508\n",
      "Iteration 885, loss = 2.84496757\n",
      "Iteration 886, loss = 2.84457303\n",
      "Iteration 887, loss = 2.84464659\n",
      "Iteration 888, loss = 2.84424164\n",
      "Iteration 889, loss = 2.84406967\n",
      "Iteration 890, loss = 2.84348434\n",
      "Iteration 891, loss = 2.84370309\n",
      "Iteration 892, loss = 2.84296644\n",
      "Iteration 893, loss = 2.84242734\n",
      "Iteration 894, loss = 2.84235662\n",
      "Iteration 895, loss = 2.84216530\n",
      "Iteration 896, loss = 2.84239141\n",
      "Iteration 897, loss = 2.84130415\n",
      "Iteration 898, loss = 2.84130819\n",
      "Iteration 899, loss = 2.84078352\n",
      "Iteration 900, loss = 2.84044959\n",
      "Iteration 901, loss = 2.84012557\n",
      "Iteration 902, loss = 2.83975019\n",
      "Iteration 903, loss = 2.83956696\n",
      "Iteration 904, loss = 2.83880668\n",
      "Iteration 905, loss = 2.83892833\n",
      "Iteration 906, loss = 2.83845758\n",
      "Iteration 907, loss = 2.83817430\n",
      "Iteration 908, loss = 2.83788137\n",
      "Iteration 909, loss = 2.83812972\n",
      "Iteration 910, loss = 2.83714694\n",
      "Iteration 911, loss = 2.83704622\n",
      "Iteration 912, loss = 2.83703334\n",
      "Iteration 913, loss = 2.83703545\n",
      "Iteration 914, loss = 2.83688631\n",
      "Iteration 915, loss = 2.83591684\n",
      "Iteration 916, loss = 2.83543088\n",
      "Iteration 917, loss = 2.83534314\n",
      "Iteration 918, loss = 2.83497941\n",
      "Iteration 919, loss = 2.83468718\n",
      "Iteration 920, loss = 2.83427441\n",
      "Iteration 921, loss = 2.83416824\n",
      "Iteration 922, loss = 2.83341626\n",
      "Iteration 923, loss = 2.83327690\n",
      "Iteration 924, loss = 2.83290245\n",
      "Iteration 925, loss = 2.83322929\n",
      "Iteration 926, loss = 2.83286220\n",
      "Iteration 927, loss = 2.83266286\n",
      "Iteration 928, loss = 2.83193141\n",
      "Iteration 929, loss = 2.83153605\n",
      "Iteration 930, loss = 2.83157384\n",
      "Iteration 931, loss = 2.83137553\n",
      "Iteration 932, loss = 2.83057144\n",
      "Iteration 933, loss = 2.83013006\n",
      "Iteration 934, loss = 2.83086218\n",
      "Iteration 935, loss = 2.83038289\n",
      "Iteration 936, loss = 2.82968968\n",
      "Iteration 937, loss = 2.82937813\n",
      "Iteration 938, loss = 2.82882331\n",
      "Iteration 939, loss = 2.82846617\n",
      "Iteration 940, loss = 2.82853631\n",
      "Iteration 941, loss = 2.82778425\n",
      "Iteration 942, loss = 2.82776216\n",
      "Iteration 943, loss = 2.82781534\n",
      "Iteration 944, loss = 2.82765028\n",
      "Iteration 945, loss = 2.82671346\n",
      "Iteration 946, loss = 2.82647096\n",
      "Iteration 947, loss = 2.82645915\n",
      "Iteration 948, loss = 2.82619853\n",
      "Iteration 949, loss = 2.82553547\n",
      "Iteration 950, loss = 2.82545936\n",
      "Iteration 951, loss = 2.82544490\n",
      "Iteration 952, loss = 2.82489242\n",
      "Iteration 953, loss = 2.82509483\n",
      "Iteration 954, loss = 2.82465281\n",
      "Iteration 955, loss = 2.82383205\n",
      "Iteration 956, loss = 2.82359536\n",
      "Iteration 957, loss = 2.82357942\n",
      "Iteration 958, loss = 2.82292425\n",
      "Iteration 959, loss = 2.82253271\n",
      "Iteration 960, loss = 2.82215644\n",
      "Iteration 961, loss = 2.82174912\n",
      "Iteration 962, loss = 2.82178713\n",
      "Iteration 963, loss = 2.82183067\n",
      "Iteration 964, loss = 2.82136814\n",
      "Iteration 965, loss = 2.82097600\n",
      "Iteration 966, loss = 2.82035040\n",
      "Iteration 967, loss = 2.82045340\n",
      "Iteration 968, loss = 2.81984497\n",
      "Iteration 969, loss = 2.81978931\n",
      "Iteration 970, loss = 2.81959077\n",
      "Iteration 971, loss = 2.81920912\n",
      "Iteration 972, loss = 2.81877955\n",
      "Iteration 973, loss = 2.81861141\n",
      "Iteration 974, loss = 2.81834809\n",
      "Iteration 975, loss = 2.81751664\n",
      "Iteration 976, loss = 2.81741561\n",
      "Iteration 977, loss = 2.81716087\n",
      "Iteration 978, loss = 2.81678248\n",
      "Iteration 979, loss = 2.81672628\n",
      "Iteration 980, loss = 2.81608660\n",
      "Iteration 981, loss = 2.81605510\n",
      "Iteration 982, loss = 2.81578486\n",
      "Iteration 983, loss = 2.81579199\n",
      "Iteration 984, loss = 2.81567677\n",
      "Iteration 985, loss = 2.81575572\n",
      "Iteration 986, loss = 2.81476090\n",
      "Iteration 987, loss = 2.81455970\n",
      "Iteration 988, loss = 2.81441541\n",
      "Iteration 989, loss = 2.81407237\n",
      "Iteration 990, loss = 2.81305678\n",
      "Iteration 991, loss = 2.81312599\n",
      "Iteration 992, loss = 2.81340135\n",
      "Iteration 993, loss = 2.81308091\n",
      "Iteration 994, loss = 2.81345437\n",
      "Iteration 995, loss = 2.81347690\n",
      "Iteration 996, loss = 2.81286047\n",
      "Iteration 997, loss = 2.81242932\n",
      "Iteration 998, loss = 2.81243977\n",
      "Iteration 999, loss = 2.81179925\n",
      "Iteration 1000, loss = 2.81134951\n",
      "Iteration 1001, loss = 2.81171557\n",
      "Iteration 1002, loss = 2.81139001\n",
      "Iteration 1003, loss = 2.81099437\n",
      "Iteration 1004, loss = 2.81016833\n",
      "Iteration 1005, loss = 2.80944269\n",
      "Iteration 1006, loss = 2.80826215\n",
      "Iteration 1007, loss = 2.80813212\n",
      "Iteration 1008, loss = 2.80787900\n",
      "Iteration 1009, loss = 2.80815965\n",
      "Iteration 1010, loss = 2.80751745\n",
      "Iteration 1011, loss = 2.80741356\n",
      "Iteration 1012, loss = 2.80762838\n",
      "Iteration 1013, loss = 2.80755126\n",
      "Iteration 1014, loss = 2.80691714\n",
      "Iteration 1015, loss = 2.80697262\n",
      "Iteration 1016, loss = 2.80638837\n",
      "Iteration 1017, loss = 2.80598612\n",
      "Iteration 1018, loss = 2.80611187\n",
      "Iteration 1019, loss = 2.80584569\n",
      "Iteration 1020, loss = 2.80574048\n",
      "Iteration 1021, loss = 2.80471484\n",
      "Iteration 1022, loss = 2.80460715\n",
      "Iteration 1023, loss = 2.80454988\n",
      "Iteration 1024, loss = 2.80373255\n",
      "Iteration 1025, loss = 2.80379685\n",
      "Iteration 1026, loss = 2.80334497\n",
      "Iteration 1027, loss = 2.80266070\n",
      "Iteration 1028, loss = 2.80183818\n",
      "Iteration 1029, loss = 2.80253357\n",
      "Iteration 1030, loss = 2.80221607\n",
      "Iteration 1031, loss = 2.80209871\n",
      "Iteration 1032, loss = 2.80123985\n",
      "Iteration 1033, loss = 2.80109900\n",
      "Iteration 1034, loss = 2.80098701\n",
      "Iteration 1035, loss = 2.80093995\n",
      "Iteration 1036, loss = 2.80088446\n",
      "Iteration 1037, loss = 2.80056002\n",
      "Iteration 1038, loss = 2.79983023\n",
      "Iteration 1039, loss = 2.79971340\n",
      "Iteration 1040, loss = 2.80023593\n",
      "Iteration 1041, loss = 2.79898060\n",
      "Iteration 1042, loss = 2.79879580\n",
      "Iteration 1043, loss = 2.79917245\n",
      "Iteration 1044, loss = 2.79849068\n",
      "Iteration 1045, loss = 2.79812299\n",
      "Iteration 1046, loss = 2.79882928\n",
      "Iteration 1047, loss = 2.79803307\n",
      "Iteration 1048, loss = 2.79712344\n",
      "Iteration 1049, loss = 2.79659329\n",
      "Iteration 1050, loss = 2.79654949\n",
      "Iteration 1051, loss = 2.79631414\n",
      "Iteration 1052, loss = 2.79600773\n",
      "Iteration 1053, loss = 2.79520388\n",
      "Iteration 1054, loss = 2.79513604\n",
      "Iteration 1055, loss = 2.79513535\n",
      "Iteration 1056, loss = 2.79465544\n",
      "Iteration 1057, loss = 2.79459798\n",
      "Iteration 1058, loss = 2.79403022\n",
      "Iteration 1059, loss = 2.79405003\n",
      "Iteration 1060, loss = 2.79362094\n",
      "Iteration 1061, loss = 2.79328693\n",
      "Iteration 1062, loss = 2.79310724\n",
      "Iteration 1063, loss = 2.79297428\n",
      "Iteration 1064, loss = 2.79250508\n",
      "Iteration 1065, loss = 2.79213562\n",
      "Iteration 1066, loss = 2.79183153\n",
      "Iteration 1067, loss = 2.79141930\n",
      "Iteration 1068, loss = 2.79155640\n",
      "Iteration 1069, loss = 2.79159722\n",
      "Iteration 1070, loss = 2.79153049\n",
      "Iteration 1071, loss = 2.79039291\n",
      "Iteration 1072, loss = 2.79042700\n",
      "Iteration 1073, loss = 2.79010520\n",
      "Iteration 1074, loss = 2.79004280\n",
      "Iteration 1075, loss = 2.78988547\n",
      "Iteration 1076, loss = 2.78902743\n",
      "Iteration 1077, loss = 2.78925168\n",
      "Iteration 1078, loss = 2.79085414\n",
      "Iteration 1079, loss = 2.78988050\n",
      "Iteration 1080, loss = 2.78796077\n",
      "Iteration 1081, loss = 2.78740827\n",
      "Iteration 1082, loss = 2.78764174\n",
      "Iteration 1083, loss = 2.78703652\n",
      "Iteration 1084, loss = 2.78754331\n",
      "Iteration 1085, loss = 2.78733560\n",
      "Iteration 1086, loss = 2.78659846\n",
      "Iteration 1087, loss = 2.78577267\n",
      "Iteration 1088, loss = 2.78559289\n",
      "Iteration 1089, loss = 2.78561937\n",
      "Iteration 1090, loss = 2.78542003\n",
      "Iteration 1091, loss = 2.78517275\n",
      "Iteration 1092, loss = 2.78451889\n",
      "Iteration 1093, loss = 2.78467446\n",
      "Iteration 1094, loss = 2.78437704\n",
      "Iteration 1095, loss = 2.78424426\n",
      "Iteration 1096, loss = 2.78380307\n",
      "Iteration 1097, loss = 2.78359793\n",
      "Iteration 1098, loss = 2.78326954\n",
      "Iteration 1099, loss = 2.78328949\n",
      "Iteration 1100, loss = 2.78256643\n",
      "Iteration 1101, loss = 2.78259395\n",
      "Iteration 1102, loss = 2.78261716\n",
      "Iteration 1103, loss = 2.78224142\n",
      "Iteration 1104, loss = 2.78200650\n",
      "Iteration 1105, loss = 2.78276439\n",
      "Iteration 1106, loss = 2.78193651\n",
      "Iteration 1107, loss = 2.78106309\n",
      "Iteration 1108, loss = 2.78032587\n",
      "Iteration 1109, loss = 2.78011160\n",
      "Iteration 1110, loss = 2.78004702\n",
      "Iteration 1111, loss = 2.77988491\n",
      "Iteration 1112, loss = 2.78017480\n",
      "Iteration 1113, loss = 2.77961375\n",
      "Iteration 1114, loss = 2.77881763\n",
      "Iteration 1115, loss = 2.77865418\n",
      "Iteration 1116, loss = 2.77853416\n",
      "Iteration 1117, loss = 2.77864398\n",
      "Iteration 1118, loss = 2.77780941\n",
      "Iteration 1119, loss = 2.77761698\n",
      "Iteration 1120, loss = 2.77762020\n",
      "Iteration 1121, loss = 2.77686643\n",
      "Iteration 1122, loss = 2.77664918\n",
      "Iteration 1123, loss = 2.77645651\n",
      "Iteration 1124, loss = 2.77637949\n",
      "Iteration 1125, loss = 2.77597545\n",
      "Iteration 1126, loss = 2.77590964\n",
      "Iteration 1127, loss = 2.77608892\n",
      "Iteration 1128, loss = 2.77556101\n",
      "Iteration 1129, loss = 2.77500419\n",
      "Iteration 1130, loss = 2.77468965\n",
      "Iteration 1131, loss = 2.77425137\n",
      "Iteration 1132, loss = 2.77396757\n",
      "Iteration 1133, loss = 2.77356166\n",
      "Iteration 1134, loss = 2.77349297\n",
      "Iteration 1135, loss = 2.77378083\n",
      "Iteration 1136, loss = 2.77366138\n",
      "Iteration 1137, loss = 2.77284138\n",
      "Iteration 1138, loss = 2.77234188\n",
      "Iteration 1139, loss = 2.77225021\n",
      "Iteration 1140, loss = 2.77253752\n",
      "Iteration 1141, loss = 2.77228231\n",
      "Iteration 1142, loss = 2.77125872\n",
      "Iteration 1143, loss = 2.77196520\n",
      "Iteration 1144, loss = 2.77171925\n",
      "Iteration 1145, loss = 2.77074199\n",
      "Iteration 1146, loss = 2.77091793\n",
      "Iteration 1147, loss = 2.77114939\n",
      "Iteration 1148, loss = 2.77042115\n",
      "Iteration 1149, loss = 2.76961046\n",
      "Iteration 1150, loss = 2.76919320\n",
      "Iteration 1151, loss = 2.76905055\n",
      "Iteration 1152, loss = 2.76913465\n",
      "Iteration 1153, loss = 2.76883942\n",
      "Iteration 1154, loss = 2.76868723\n",
      "Iteration 1155, loss = 2.76865452\n",
      "Iteration 1156, loss = 2.76834516\n",
      "Iteration 1157, loss = 2.76786047\n",
      "Iteration 1158, loss = 2.76678629\n",
      "Iteration 1159, loss = 2.76680375\n",
      "Iteration 1160, loss = 2.76705099\n",
      "Iteration 1161, loss = 2.76641407\n",
      "Iteration 1162, loss = 2.76584653\n",
      "Iteration 1163, loss = 2.76577028\n",
      "Iteration 1164, loss = 2.76540904\n",
      "Iteration 1165, loss = 2.76525997\n",
      "Iteration 1166, loss = 2.76509927\n",
      "Iteration 1167, loss = 2.76503819\n",
      "Iteration 1168, loss = 2.76417653\n",
      "Iteration 1169, loss = 2.76569091\n",
      "Iteration 1170, loss = 2.76459757\n",
      "Iteration 1171, loss = 2.76328607\n",
      "Iteration 1172, loss = 2.76363026\n",
      "Iteration 1173, loss = 2.76358143\n",
      "Iteration 1174, loss = 2.76269025\n",
      "Iteration 1175, loss = 2.76314558\n",
      "Iteration 1176, loss = 2.76340052\n",
      "Iteration 1177, loss = 2.76261820\n",
      "Iteration 1178, loss = 2.76226158\n",
      "Iteration 1179, loss = 2.76207953\n",
      "Iteration 1180, loss = 2.76106785\n",
      "Iteration 1181, loss = 2.76115918\n",
      "Iteration 1182, loss = 2.76066589\n",
      "Iteration 1183, loss = 2.76021999\n",
      "Iteration 1184, loss = 2.75979079\n",
      "Iteration 1185, loss = 2.76025417\n",
      "Iteration 1186, loss = 2.75990466\n",
      "Iteration 1187, loss = 2.75943249\n",
      "Iteration 1188, loss = 2.75927094\n",
      "Iteration 1189, loss = 2.75918343\n",
      "Iteration 1190, loss = 2.75910374\n",
      "Iteration 1191, loss = 2.75829953\n",
      "Iteration 1192, loss = 2.75793414\n",
      "Iteration 1193, loss = 2.75740975\n",
      "Iteration 1194, loss = 2.75866069\n",
      "Iteration 1195, loss = 2.75789906\n",
      "Iteration 1196, loss = 2.75758464\n",
      "Iteration 1197, loss = 2.75749584\n",
      "Iteration 1198, loss = 2.75736410\n",
      "Iteration 1199, loss = 2.75714140\n",
      "Iteration 1200, loss = 2.75667792\n",
      "Iteration 1201, loss = 2.75624710\n",
      "Iteration 1202, loss = 2.75609320\n",
      "Iteration 1203, loss = 2.75562442\n",
      "Iteration 1204, loss = 2.75546728\n",
      "Iteration 1205, loss = 2.75473882\n",
      "Iteration 1206, loss = 2.75475456\n",
      "Iteration 1207, loss = 2.75506322\n",
      "Iteration 1208, loss = 2.75463325\n",
      "Iteration 1209, loss = 2.75409977\n",
      "Iteration 1210, loss = 2.75482965\n",
      "Iteration 1211, loss = 2.75426461\n",
      "Iteration 1212, loss = 2.75346129\n",
      "Iteration 1213, loss = 2.75288124\n",
      "Iteration 1214, loss = 2.75306690\n",
      "Iteration 1215, loss = 2.75268088\n",
      "Iteration 1216, loss = 2.75255057\n",
      "Iteration 1217, loss = 2.75185016\n",
      "Iteration 1218, loss = 2.75178016\n",
      "Iteration 1219, loss = 2.75148280\n",
      "Iteration 1220, loss = 2.75135031\n",
      "Iteration 1221, loss = 2.75142214\n",
      "Iteration 1222, loss = 2.75229184\n",
      "Iteration 1223, loss = 2.75045591\n",
      "Iteration 1224, loss = 2.75039700\n",
      "Iteration 1225, loss = 2.74980915\n",
      "Iteration 1226, loss = 2.74984823\n",
      "Iteration 1227, loss = 2.74980859\n",
      "Iteration 1228, loss = 2.74931126\n",
      "Iteration 1229, loss = 2.74914257\n",
      "Iteration 1230, loss = 2.74835858\n",
      "Iteration 1231, loss = 2.74848161\n",
      "Iteration 1232, loss = 2.74792980\n",
      "Iteration 1233, loss = 2.74758897\n",
      "Iteration 1234, loss = 2.74784399\n",
      "Iteration 1235, loss = 2.74750622\n",
      "Iteration 1236, loss = 2.74697647\n",
      "Iteration 1237, loss = 2.74635488\n",
      "Iteration 1238, loss = 2.74618749\n",
      "Iteration 1239, loss = 2.74591965\n",
      "Iteration 1240, loss = 2.74578451\n",
      "Iteration 1241, loss = 2.74573699\n",
      "Iteration 1242, loss = 2.74535592\n",
      "Iteration 1243, loss = 2.74478894\n",
      "Iteration 1244, loss = 2.74489679\n",
      "Iteration 1245, loss = 2.74439036\n",
      "Iteration 1246, loss = 2.74445624\n",
      "Iteration 1247, loss = 2.74393223\n",
      "Iteration 1248, loss = 2.74389838\n",
      "Iteration 1249, loss = 2.74390797\n",
      "Iteration 1250, loss = 2.74373133\n",
      "Iteration 1251, loss = 2.74335369\n",
      "Iteration 1252, loss = 2.74316131\n",
      "Iteration 1253, loss = 2.74344844\n",
      "Iteration 1254, loss = 2.74264509\n",
      "Iteration 1255, loss = 2.74295179\n",
      "Iteration 1256, loss = 2.74315194\n",
      "Iteration 1257, loss = 2.74333693\n",
      "Iteration 1258, loss = 2.74168969\n",
      "Iteration 1259, loss = 2.74156740\n",
      "Iteration 1260, loss = 2.74135796\n",
      "Iteration 1261, loss = 2.74055204\n",
      "Iteration 1262, loss = 2.74074956\n",
      "Iteration 1263, loss = 2.73989262\n",
      "Iteration 1264, loss = 2.74044315\n",
      "Iteration 1265, loss = 2.73983442\n",
      "Iteration 1266, loss = 2.73948555\n",
      "Iteration 1267, loss = 2.74050363\n",
      "Iteration 1268, loss = 2.73905904\n",
      "Iteration 1269, loss = 2.73937645\n",
      "Iteration 1, loss = 4.15989222\n",
      "Iteration 2, loss = 4.11919935\n",
      "Iteration 3, loss = 4.07921361\n",
      "Iteration 4, loss = 4.03866868\n",
      "Iteration 5, loss = 3.99637050\n",
      "Iteration 6, loss = 3.95222025\n",
      "Iteration 7, loss = 3.90514950\n",
      "Iteration 8, loss = 3.85589678\n",
      "Iteration 9, loss = 3.80487040\n",
      "Iteration 10, loss = 3.75024530\n",
      "Iteration 11, loss = 3.69436777\n",
      "Iteration 12, loss = 3.63744445\n",
      "Iteration 13, loss = 3.58201956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 3.52525615\n",
      "Iteration 15, loss = 3.47184417\n",
      "Iteration 16, loss = 3.42092781\n",
      "Iteration 17, loss = 3.37469822\n",
      "Iteration 18, loss = 3.33670897\n",
      "Iteration 19, loss = 3.30513924\n",
      "Iteration 20, loss = 3.28104972\n",
      "Iteration 21, loss = 3.26034972\n",
      "Iteration 22, loss = 3.24754996\n",
      "Iteration 23, loss = 3.23764919\n",
      "Iteration 24, loss = 3.23157050\n",
      "Iteration 25, loss = 3.22826862\n",
      "Iteration 26, loss = 3.22602637\n",
      "Iteration 27, loss = 3.22440624\n",
      "Iteration 28, loss = 3.22341408\n",
      "Iteration 29, loss = 3.22202964\n",
      "Iteration 30, loss = 3.22129552\n",
      "Iteration 31, loss = 3.22051017\n",
      "Iteration 32, loss = 3.21979681\n",
      "Iteration 33, loss = 3.21928093\n",
      "Iteration 34, loss = 3.21893911\n",
      "Iteration 35, loss = 3.21801356\n",
      "Iteration 36, loss = 3.21743271\n",
      "Iteration 37, loss = 3.21697205\n",
      "Iteration 38, loss = 3.21633298\n",
      "Iteration 39, loss = 3.21579445\n",
      "Iteration 40, loss = 3.21545539\n",
      "Iteration 41, loss = 3.21488949\n",
      "Iteration 42, loss = 3.21453145\n",
      "Iteration 43, loss = 3.21387270\n",
      "Iteration 44, loss = 3.21316916\n",
      "Iteration 45, loss = 3.21270021\n",
      "Iteration 46, loss = 3.21197712\n",
      "Iteration 47, loss = 3.21158375\n",
      "Iteration 48, loss = 3.21122240\n",
      "Iteration 49, loss = 3.21066551\n",
      "Iteration 50, loss = 3.21002705\n",
      "Iteration 51, loss = 3.20925914\n",
      "Iteration 52, loss = 3.20880878\n",
      "Iteration 53, loss = 3.20828195\n",
      "Iteration 54, loss = 3.20779135\n",
      "Iteration 55, loss = 3.20740272\n",
      "Iteration 56, loss = 3.20686140\n",
      "Iteration 57, loss = 3.20631568\n",
      "Iteration 58, loss = 3.20556843\n",
      "Iteration 59, loss = 3.20478606\n",
      "Iteration 60, loss = 3.20438732\n",
      "Iteration 61, loss = 3.20403416\n",
      "Iteration 62, loss = 3.20378450\n",
      "Iteration 63, loss = 3.20315286\n",
      "Iteration 64, loss = 3.20289764\n",
      "Iteration 65, loss = 3.20250526\n",
      "Iteration 66, loss = 3.20180456\n",
      "Iteration 67, loss = 3.20084587\n",
      "Iteration 68, loss = 3.20030918\n",
      "Iteration 69, loss = 3.19994034\n",
      "Iteration 70, loss = 3.19957885\n",
      "Iteration 71, loss = 3.19876377\n",
      "Iteration 72, loss = 3.19840108\n",
      "Iteration 73, loss = 3.19789026\n",
      "Iteration 74, loss = 3.19762114\n",
      "Iteration 75, loss = 3.19704082\n",
      "Iteration 76, loss = 3.19635579\n",
      "Iteration 77, loss = 3.19582239\n",
      "Iteration 78, loss = 3.19501464\n",
      "Iteration 79, loss = 3.19459177\n",
      "Iteration 80, loss = 3.19390208\n",
      "Iteration 81, loss = 3.19298928\n",
      "Iteration 82, loss = 3.19251099\n",
      "Iteration 83, loss = 3.19160792\n",
      "Iteration 84, loss = 3.19097259\n",
      "Iteration 85, loss = 3.19059344\n",
      "Iteration 86, loss = 3.19012569\n",
      "Iteration 87, loss = 3.18966024\n",
      "Iteration 88, loss = 3.18928086\n",
      "Iteration 89, loss = 3.18837288\n",
      "Iteration 90, loss = 3.18767027\n",
      "Iteration 91, loss = 3.18702741\n",
      "Iteration 92, loss = 3.18649534\n",
      "Iteration 93, loss = 3.18573521\n",
      "Iteration 94, loss = 3.18552634\n",
      "Iteration 95, loss = 3.18474888\n",
      "Iteration 96, loss = 3.18413089\n",
      "Iteration 97, loss = 3.18338891\n",
      "Iteration 98, loss = 3.18286937\n",
      "Iteration 99, loss = 3.18243027\n",
      "Iteration 100, loss = 3.18168831\n",
      "Iteration 101, loss = 3.18105603\n",
      "Iteration 102, loss = 3.18034278\n",
      "Iteration 103, loss = 3.17961193\n",
      "Iteration 104, loss = 3.17894338\n",
      "Iteration 105, loss = 3.17843299\n",
      "Iteration 106, loss = 3.17788149\n",
      "Iteration 107, loss = 3.17770420\n",
      "Iteration 108, loss = 3.17682010\n",
      "Iteration 109, loss = 3.17616854\n",
      "Iteration 110, loss = 3.17535571\n",
      "Iteration 111, loss = 3.17464774\n",
      "Iteration 112, loss = 3.17390415\n",
      "Iteration 113, loss = 3.17314989\n",
      "Iteration 114, loss = 3.17255152\n",
      "Iteration 115, loss = 3.17170660\n",
      "Iteration 116, loss = 3.17130812\n",
      "Iteration 117, loss = 3.17056527\n",
      "Iteration 118, loss = 3.16995923\n",
      "Iteration 119, loss = 3.16920985\n",
      "Iteration 120, loss = 3.16874593\n",
      "Iteration 121, loss = 3.16827756\n",
      "Iteration 122, loss = 3.16737815\n",
      "Iteration 123, loss = 3.16719479\n",
      "Iteration 124, loss = 3.16702351\n",
      "Iteration 125, loss = 3.16581275\n",
      "Iteration 126, loss = 3.16510254\n",
      "Iteration 127, loss = 3.16438524\n",
      "Iteration 128, loss = 3.16391168\n",
      "Iteration 129, loss = 3.16293738\n",
      "Iteration 130, loss = 3.16260608\n",
      "Iteration 131, loss = 3.16188180\n",
      "Iteration 132, loss = 3.16098180\n",
      "Iteration 133, loss = 3.16049830\n",
      "Iteration 134, loss = 3.16023326\n",
      "Iteration 135, loss = 3.15960414\n",
      "Iteration 136, loss = 3.15887616\n",
      "Iteration 137, loss = 3.15772101\n",
      "Iteration 138, loss = 3.15704243\n",
      "Iteration 139, loss = 3.15652047\n",
      "Iteration 140, loss = 3.15673942\n",
      "Iteration 141, loss = 3.15588724\n",
      "Iteration 142, loss = 3.15524465\n",
      "Iteration 143, loss = 3.15465457\n",
      "Iteration 144, loss = 3.15420528\n",
      "Iteration 145, loss = 3.15321600\n",
      "Iteration 146, loss = 3.15230705\n",
      "Iteration 147, loss = 3.15163212\n",
      "Iteration 148, loss = 3.15100228\n",
      "Iteration 149, loss = 3.15062453\n",
      "Iteration 150, loss = 3.14988925\n",
      "Iteration 151, loss = 3.14919739\n",
      "Iteration 152, loss = 3.14904381\n",
      "Iteration 153, loss = 3.14814423\n",
      "Iteration 154, loss = 3.14750520\n",
      "Iteration 155, loss = 3.14685362\n",
      "Iteration 156, loss = 3.14646559\n",
      "Iteration 157, loss = 3.14650154\n",
      "Iteration 158, loss = 3.14588262\n",
      "Iteration 159, loss = 3.14456003\n",
      "Iteration 160, loss = 3.14426125\n",
      "Iteration 161, loss = 3.14404394\n",
      "Iteration 162, loss = 3.14342557\n",
      "Iteration 163, loss = 3.14239027\n",
      "Iteration 164, loss = 3.14194544\n",
      "Iteration 165, loss = 3.14175791\n",
      "Iteration 166, loss = 3.14103971\n",
      "Iteration 167, loss = 3.14018485\n",
      "Iteration 168, loss = 3.13947211\n",
      "Iteration 169, loss = 3.13918839\n",
      "Iteration 170, loss = 3.13827029\n",
      "Iteration 171, loss = 3.13735584\n",
      "Iteration 172, loss = 3.13708325\n",
      "Iteration 173, loss = 3.13670286\n",
      "Iteration 174, loss = 3.13608947\n",
      "Iteration 175, loss = 3.13519221\n",
      "Iteration 176, loss = 3.13455857\n",
      "Iteration 177, loss = 3.13414560\n",
      "Iteration 178, loss = 3.13387303\n",
      "Iteration 179, loss = 3.13279062\n",
      "Iteration 180, loss = 3.13257417\n",
      "Iteration 181, loss = 3.13140816\n",
      "Iteration 182, loss = 3.13106093\n",
      "Iteration 183, loss = 3.12982511\n",
      "Iteration 184, loss = 3.12950314\n",
      "Iteration 185, loss = 3.12900975\n",
      "Iteration 186, loss = 3.12847833\n",
      "Iteration 187, loss = 3.12772429\n",
      "Iteration 188, loss = 3.12729447\n",
      "Iteration 189, loss = 3.12698414\n",
      "Iteration 190, loss = 3.12652437\n",
      "Iteration 191, loss = 3.12595741\n",
      "Iteration 192, loss = 3.12520411\n",
      "Iteration 193, loss = 3.12470265\n",
      "Iteration 194, loss = 3.12423098\n",
      "Iteration 195, loss = 3.12368660\n",
      "Iteration 196, loss = 3.12298612\n",
      "Iteration 197, loss = 3.12218563\n",
      "Iteration 198, loss = 3.12195357\n",
      "Iteration 199, loss = 3.12151077\n",
      "Iteration 200, loss = 3.12064487\n",
      "Iteration 201, loss = 3.12016824\n",
      "Iteration 202, loss = 3.11953028\n",
      "Iteration 203, loss = 3.11880751\n",
      "Iteration 204, loss = 3.11808134\n",
      "Iteration 205, loss = 3.11775322\n",
      "Iteration 206, loss = 3.11734525\n",
      "Iteration 207, loss = 3.11636953\n",
      "Iteration 208, loss = 3.11598966\n",
      "Iteration 209, loss = 3.11548254\n",
      "Iteration 210, loss = 3.11516376\n",
      "Iteration 211, loss = 3.11470418\n",
      "Iteration 212, loss = 3.11414107\n",
      "Iteration 213, loss = 3.11353316\n",
      "Iteration 214, loss = 3.11284247\n",
      "Iteration 215, loss = 3.11240949\n",
      "Iteration 216, loss = 3.11217179\n",
      "Iteration 217, loss = 3.11091442\n",
      "Iteration 218, loss = 3.11109186\n",
      "Iteration 219, loss = 3.11065632\n",
      "Iteration 220, loss = 3.10963870\n",
      "Iteration 221, loss = 3.10934335\n",
      "Iteration 222, loss = 3.10825011\n",
      "Iteration 223, loss = 3.10773612\n",
      "Iteration 224, loss = 3.10767262\n",
      "Iteration 225, loss = 3.10772778\n",
      "Iteration 226, loss = 3.10726832\n",
      "Iteration 227, loss = 3.10674055\n",
      "Iteration 228, loss = 3.10556919\n",
      "Iteration 229, loss = 3.10508618\n",
      "Iteration 230, loss = 3.10419286\n",
      "Iteration 231, loss = 3.10341038\n",
      "Iteration 232, loss = 3.10305032\n",
      "Iteration 233, loss = 3.10221967\n",
      "Iteration 234, loss = 3.10179828\n",
      "Iteration 235, loss = 3.10113433\n",
      "Iteration 236, loss = 3.10060243\n",
      "Iteration 237, loss = 3.10009276\n",
      "Iteration 238, loss = 3.09934963\n",
      "Iteration 239, loss = 3.09903168\n",
      "Iteration 240, loss = 3.09838530\n",
      "Iteration 241, loss = 3.09774445\n",
      "Iteration 242, loss = 3.09761083\n",
      "Iteration 243, loss = 3.09707745\n",
      "Iteration 244, loss = 3.09642253\n",
      "Iteration 245, loss = 3.09593149\n",
      "Iteration 246, loss = 3.09603670\n",
      "Iteration 247, loss = 3.09552711\n",
      "Iteration 248, loss = 3.09472252\n",
      "Iteration 249, loss = 3.09395303\n",
      "Iteration 250, loss = 3.09365926\n",
      "Iteration 251, loss = 3.09284310\n",
      "Iteration 252, loss = 3.09208058\n",
      "Iteration 253, loss = 3.09204552\n",
      "Iteration 254, loss = 3.09137114\n",
      "Iteration 255, loss = 3.09067971\n",
      "Iteration 256, loss = 3.09010058\n",
      "Iteration 257, loss = 3.08960172\n",
      "Iteration 258, loss = 3.08916287\n",
      "Iteration 259, loss = 3.08842944\n",
      "Iteration 260, loss = 3.08806492\n",
      "Iteration 261, loss = 3.08806240\n",
      "Iteration 262, loss = 3.08745733\n",
      "Iteration 263, loss = 3.08707012\n",
      "Iteration 264, loss = 3.08634806\n",
      "Iteration 265, loss = 3.08600828\n",
      "Iteration 266, loss = 3.08576182\n",
      "Iteration 267, loss = 3.08516698\n",
      "Iteration 268, loss = 3.08448763\n",
      "Iteration 269, loss = 3.08401019\n",
      "Iteration 270, loss = 3.08388125\n",
      "Iteration 271, loss = 3.08328344\n",
      "Iteration 272, loss = 3.08219900\n",
      "Iteration 273, loss = 3.08172716\n",
      "Iteration 274, loss = 3.08167465\n",
      "Iteration 275, loss = 3.08156987\n",
      "Iteration 276, loss = 3.08073537\n",
      "Iteration 277, loss = 3.08014974\n",
      "Iteration 278, loss = 3.07939787\n",
      "Iteration 279, loss = 3.07882412\n",
      "Iteration 280, loss = 3.07839339\n",
      "Iteration 281, loss = 3.07768919\n",
      "Iteration 282, loss = 3.07713677\n",
      "Iteration 283, loss = 3.07660459\n",
      "Iteration 284, loss = 3.07618018\n",
      "Iteration 285, loss = 3.07572514\n",
      "Iteration 286, loss = 3.07495375\n",
      "Iteration 287, loss = 3.07405905\n",
      "Iteration 288, loss = 3.07382313\n",
      "Iteration 289, loss = 3.07375993\n",
      "Iteration 290, loss = 3.07303645\n",
      "Iteration 291, loss = 3.07252022\n",
      "Iteration 292, loss = 3.07200038\n",
      "Iteration 293, loss = 3.07151495\n",
      "Iteration 294, loss = 3.07110077\n",
      "Iteration 295, loss = 3.07126203\n",
      "Iteration 296, loss = 3.07093737\n",
      "Iteration 297, loss = 3.07010937\n",
      "Iteration 298, loss = 3.06976699\n",
      "Iteration 299, loss = 3.06953973\n",
      "Iteration 300, loss = 3.06901358\n",
      "Iteration 301, loss = 3.06777485\n",
      "Iteration 302, loss = 3.06787849\n",
      "Iteration 303, loss = 3.06755803\n",
      "Iteration 304, loss = 3.06673490\n",
      "Iteration 305, loss = 3.06596983\n",
      "Iteration 306, loss = 3.06540125\n",
      "Iteration 307, loss = 3.06483497\n",
      "Iteration 308, loss = 3.06437910\n",
      "Iteration 309, loss = 3.06379477\n",
      "Iteration 310, loss = 3.06312267\n",
      "Iteration 311, loss = 3.06278669\n",
      "Iteration 312, loss = 3.06294625\n",
      "Iteration 313, loss = 3.06240739\n",
      "Iteration 314, loss = 3.06173993\n",
      "Iteration 315, loss = 3.06151142\n",
      "Iteration 316, loss = 3.06122906\n",
      "Iteration 317, loss = 3.06050283\n",
      "Iteration 318, loss = 3.05980812\n",
      "Iteration 319, loss = 3.05969955\n",
      "Iteration 320, loss = 3.05869704\n",
      "Iteration 321, loss = 3.05815887\n",
      "Iteration 322, loss = 3.05768141\n",
      "Iteration 323, loss = 3.05757037\n",
      "Iteration 324, loss = 3.05702821\n",
      "Iteration 325, loss = 3.05636262\n",
      "Iteration 326, loss = 3.05612339\n",
      "Iteration 327, loss = 3.05520182\n",
      "Iteration 328, loss = 3.05492312\n",
      "Iteration 329, loss = 3.05462298\n",
      "Iteration 330, loss = 3.05436058\n",
      "Iteration 331, loss = 3.05383547\n",
      "Iteration 332, loss = 3.05293308\n",
      "Iteration 333, loss = 3.05209809\n",
      "Iteration 334, loss = 3.05190497\n",
      "Iteration 335, loss = 3.05177548\n",
      "Iteration 336, loss = 3.05114046\n",
      "Iteration 337, loss = 3.05017394\n",
      "Iteration 338, loss = 3.05017757\n",
      "Iteration 339, loss = 3.04981298\n",
      "Iteration 340, loss = 3.04960991\n",
      "Iteration 341, loss = 3.04928253\n",
      "Iteration 342, loss = 3.04942848\n",
      "Iteration 343, loss = 3.04871327\n",
      "Iteration 344, loss = 3.04806240\n",
      "Iteration 345, loss = 3.04745618\n",
      "Iteration 346, loss = 3.04665070\n",
      "Iteration 347, loss = 3.04676173\n",
      "Iteration 348, loss = 3.04697801\n",
      "Iteration 349, loss = 3.04672795\n",
      "Iteration 350, loss = 3.04620181\n",
      "Iteration 351, loss = 3.04455951\n",
      "Iteration 352, loss = 3.04393155\n",
      "Iteration 353, loss = 3.04341317\n",
      "Iteration 354, loss = 3.04354625\n",
      "Iteration 355, loss = 3.04259395\n",
      "Iteration 356, loss = 3.04222909\n",
      "Iteration 357, loss = 3.04168841\n",
      "Iteration 358, loss = 3.04109894\n",
      "Iteration 359, loss = 3.04085751\n",
      "Iteration 360, loss = 3.04017649\n",
      "Iteration 361, loss = 3.04009322\n",
      "Iteration 362, loss = 3.03978076\n",
      "Iteration 363, loss = 3.03897937\n",
      "Iteration 364, loss = 3.03862405\n",
      "Iteration 365, loss = 3.03816562\n",
      "Iteration 366, loss = 3.03767454\n",
      "Iteration 367, loss = 3.03729355\n",
      "Iteration 368, loss = 3.03677107\n",
      "Iteration 369, loss = 3.03629535\n",
      "Iteration 370, loss = 3.03606836\n",
      "Iteration 371, loss = 3.03562119\n",
      "Iteration 372, loss = 3.03521050\n",
      "Iteration 373, loss = 3.03455342\n",
      "Iteration 374, loss = 3.03380532\n",
      "Iteration 375, loss = 3.03357081\n",
      "Iteration 376, loss = 3.03536399\n",
      "Iteration 377, loss = 3.03467695\n",
      "Iteration 378, loss = 3.03218657\n",
      "Iteration 379, loss = 3.03213558\n",
      "Iteration 380, loss = 3.03166009\n",
      "Iteration 381, loss = 3.03064732\n",
      "Iteration 382, loss = 3.03001099\n",
      "Iteration 383, loss = 3.02949408\n",
      "Iteration 384, loss = 3.02908941\n",
      "Iteration 385, loss = 3.02902777\n",
      "Iteration 386, loss = 3.02855471\n",
      "Iteration 387, loss = 3.02795737\n",
      "Iteration 388, loss = 3.02774978\n",
      "Iteration 389, loss = 3.02720109\n",
      "Iteration 390, loss = 3.02667570\n",
      "Iteration 391, loss = 3.02628039\n",
      "Iteration 392, loss = 3.02569597\n",
      "Iteration 393, loss = 3.02548849\n",
      "Iteration 394, loss = 3.02479411\n",
      "Iteration 395, loss = 3.02431883\n",
      "Iteration 396, loss = 3.02436989\n",
      "Iteration 397, loss = 3.02403343\n",
      "Iteration 398, loss = 3.02332933\n",
      "Iteration 399, loss = 3.02429521\n",
      "Iteration 400, loss = 3.02367121\n",
      "Iteration 401, loss = 3.02294641\n",
      "Iteration 402, loss = 3.02170371\n",
      "Iteration 403, loss = 3.02136575\n",
      "Iteration 404, loss = 3.02132702\n",
      "Iteration 405, loss = 3.02082996\n",
      "Iteration 406, loss = 3.02046462\n",
      "Iteration 407, loss = 3.02024975\n",
      "Iteration 408, loss = 3.01961084\n",
      "Iteration 409, loss = 3.01899935\n",
      "Iteration 410, loss = 3.01832916\n",
      "Iteration 411, loss = 3.01815091\n",
      "Iteration 412, loss = 3.01769602\n",
      "Iteration 413, loss = 3.01710355\n",
      "Iteration 414, loss = 3.01644851\n",
      "Iteration 415, loss = 3.01613743\n",
      "Iteration 416, loss = 3.01581326\n",
      "Iteration 417, loss = 3.01506557\n",
      "Iteration 418, loss = 3.01488217\n",
      "Iteration 419, loss = 3.01442025\n",
      "Iteration 420, loss = 3.01376760\n",
      "Iteration 421, loss = 3.01399622\n",
      "Iteration 422, loss = 3.01373106\n",
      "Iteration 423, loss = 3.01293671\n",
      "Iteration 424, loss = 3.01261636\n",
      "Iteration 425, loss = 3.01246377\n",
      "Iteration 426, loss = 3.01197593\n",
      "Iteration 427, loss = 3.01148183\n",
      "Iteration 428, loss = 3.01081150\n",
      "Iteration 429, loss = 3.01018905\n",
      "Iteration 430, loss = 3.00958505\n",
      "Iteration 431, loss = 3.00913651\n",
      "Iteration 432, loss = 3.00903133\n",
      "Iteration 433, loss = 3.00837406\n",
      "Iteration 434, loss = 3.00836527\n",
      "Iteration 435, loss = 3.00784038\n",
      "Iteration 436, loss = 3.00699997\n",
      "Iteration 437, loss = 3.00663471\n",
      "Iteration 438, loss = 3.00622412\n",
      "Iteration 439, loss = 3.00587314\n",
      "Iteration 440, loss = 3.00530586\n",
      "Iteration 441, loss = 3.00494542\n",
      "Iteration 442, loss = 3.00438530\n",
      "Iteration 443, loss = 3.00413610\n",
      "Iteration 444, loss = 3.00391283\n",
      "Iteration 445, loss = 3.00340758\n",
      "Iteration 446, loss = 3.00325626\n",
      "Iteration 447, loss = 3.00237794\n",
      "Iteration 448, loss = 3.00214142\n",
      "Iteration 449, loss = 3.00179138\n",
      "Iteration 450, loss = 3.00137457\n",
      "Iteration 451, loss = 3.00072506\n",
      "Iteration 452, loss = 3.00019582\n",
      "Iteration 453, loss = 2.99997427\n",
      "Iteration 454, loss = 2.99932779\n",
      "Iteration 455, loss = 2.99869008\n",
      "Iteration 456, loss = 2.99896004\n",
      "Iteration 457, loss = 2.99852796\n",
      "Iteration 458, loss = 2.99801963\n",
      "Iteration 459, loss = 2.99744388\n",
      "Iteration 460, loss = 2.99729734\n",
      "Iteration 461, loss = 2.99638497\n",
      "Iteration 462, loss = 2.99655926\n",
      "Iteration 463, loss = 2.99641141\n",
      "Iteration 464, loss = 2.99551221\n",
      "Iteration 465, loss = 2.99528229\n",
      "Iteration 466, loss = 2.99495339\n",
      "Iteration 467, loss = 2.99441493\n",
      "Iteration 468, loss = 2.99376178\n",
      "Iteration 469, loss = 2.99335927\n",
      "Iteration 470, loss = 2.99279987\n",
      "Iteration 471, loss = 2.99259693\n",
      "Iteration 472, loss = 2.99245383\n",
      "Iteration 473, loss = 2.99213752\n",
      "Iteration 474, loss = 2.99152766\n",
      "Iteration 475, loss = 2.99104006\n",
      "Iteration 476, loss = 2.99084585\n",
      "Iteration 477, loss = 2.99038611\n",
      "Iteration 478, loss = 2.98973701\n",
      "Iteration 479, loss = 2.98910961\n",
      "Iteration 480, loss = 2.98896214\n",
      "Iteration 481, loss = 2.98834269\n",
      "Iteration 482, loss = 2.98797619\n",
      "Iteration 483, loss = 2.98790169\n",
      "Iteration 484, loss = 2.98760835\n",
      "Iteration 485, loss = 2.98715321\n",
      "Iteration 486, loss = 2.98644876\n",
      "Iteration 487, loss = 2.98631587\n",
      "Iteration 488, loss = 2.98624045\n",
      "Iteration 489, loss = 2.98524216\n",
      "Iteration 490, loss = 2.98490400\n",
      "Iteration 491, loss = 2.98519252\n",
      "Iteration 492, loss = 2.98436478\n",
      "Iteration 493, loss = 2.98372629\n",
      "Iteration 494, loss = 2.98319783\n",
      "Iteration 495, loss = 2.98253267\n",
      "Iteration 496, loss = 2.98207488\n",
      "Iteration 497, loss = 2.98194943\n",
      "Iteration 498, loss = 2.98167297\n",
      "Iteration 499, loss = 2.98098919\n",
      "Iteration 500, loss = 2.98098286\n",
      "Iteration 501, loss = 2.98060566\n",
      "Iteration 502, loss = 2.98032110\n",
      "Iteration 503, loss = 2.97937904\n",
      "Iteration 504, loss = 2.97905903\n",
      "Iteration 505, loss = 2.98116770\n",
      "Iteration 506, loss = 2.98047352\n",
      "Iteration 507, loss = 2.97866710\n",
      "Iteration 508, loss = 2.97766182\n",
      "Iteration 509, loss = 2.97744495\n",
      "Iteration 510, loss = 2.97683461\n",
      "Iteration 511, loss = 2.97597999\n",
      "Iteration 512, loss = 2.97617352\n",
      "Iteration 513, loss = 2.97524176\n",
      "Iteration 514, loss = 2.97506423\n",
      "Iteration 515, loss = 2.97465323\n",
      "Iteration 516, loss = 2.97424738\n",
      "Iteration 517, loss = 2.97412176\n",
      "Iteration 518, loss = 2.97380932\n",
      "Iteration 519, loss = 2.97344564\n",
      "Iteration 520, loss = 2.97286048\n",
      "Iteration 521, loss = 2.97191995\n",
      "Iteration 522, loss = 2.97179844\n",
      "Iteration 523, loss = 2.97139615\n",
      "Iteration 524, loss = 2.97109162\n",
      "Iteration 525, loss = 2.97062954\n",
      "Iteration 526, loss = 2.97013950\n",
      "Iteration 527, loss = 2.96956656\n",
      "Iteration 528, loss = 2.96911249\n",
      "Iteration 529, loss = 2.96882837\n",
      "Iteration 530, loss = 2.96830313\n",
      "Iteration 531, loss = 2.96782914\n",
      "Iteration 532, loss = 2.96741244\n",
      "Iteration 533, loss = 2.96685174\n",
      "Iteration 534, loss = 2.96665329\n",
      "Iteration 535, loss = 2.96620019\n",
      "Iteration 536, loss = 2.96594477\n",
      "Iteration 537, loss = 2.96544983\n",
      "Iteration 538, loss = 2.96533732\n",
      "Iteration 539, loss = 2.96461127\n",
      "Iteration 540, loss = 2.96426918\n",
      "Iteration 541, loss = 2.96410527\n",
      "Iteration 542, loss = 2.96338831\n",
      "Iteration 543, loss = 2.96316177\n",
      "Iteration 544, loss = 2.96347940\n",
      "Iteration 545, loss = 2.96238892\n",
      "Iteration 546, loss = 2.96233213\n",
      "Iteration 547, loss = 2.96203991\n",
      "Iteration 548, loss = 2.96080795\n",
      "Iteration 549, loss = 2.96157193\n",
      "Iteration 550, loss = 2.96050657\n",
      "Iteration 551, loss = 2.96020968\n",
      "Iteration 552, loss = 2.95962578\n",
      "Iteration 553, loss = 2.95915824\n",
      "Iteration 554, loss = 2.95871146\n",
      "Iteration 555, loss = 2.95813428\n",
      "Iteration 556, loss = 2.95803870\n",
      "Iteration 557, loss = 2.95761012\n",
      "Iteration 558, loss = 2.95671996\n",
      "Iteration 559, loss = 2.95651817\n",
      "Iteration 560, loss = 2.95647537\n",
      "Iteration 561, loss = 2.95584948\n",
      "Iteration 562, loss = 2.95570462\n",
      "Iteration 563, loss = 2.95521598\n",
      "Iteration 564, loss = 2.95459547\n",
      "Iteration 565, loss = 2.95439643\n",
      "Iteration 566, loss = 2.95394404\n",
      "Iteration 567, loss = 2.95337441\n",
      "Iteration 568, loss = 2.95300388\n",
      "Iteration 569, loss = 2.95240716\n",
      "Iteration 570, loss = 2.95235244\n",
      "Iteration 571, loss = 2.95287285\n",
      "Iteration 572, loss = 2.95203449\n",
      "Iteration 573, loss = 2.95177093\n",
      "Iteration 574, loss = 2.95111811\n",
      "Iteration 575, loss = 2.95055049\n",
      "Iteration 576, loss = 2.94996632\n",
      "Iteration 577, loss = 2.94944879\n",
      "Iteration 578, loss = 2.94915729\n",
      "Iteration 579, loss = 2.94880240\n",
      "Iteration 580, loss = 2.94833262\n",
      "Iteration 581, loss = 2.94817041\n",
      "Iteration 582, loss = 2.94783524\n",
      "Iteration 583, loss = 2.94710908\n",
      "Iteration 584, loss = 2.94673980\n",
      "Iteration 585, loss = 2.94663123\n",
      "Iteration 586, loss = 2.94595683\n",
      "Iteration 587, loss = 2.94564973\n",
      "Iteration 588, loss = 2.94577986\n",
      "Iteration 589, loss = 2.94517757\n",
      "Iteration 590, loss = 2.94471899\n",
      "Iteration 591, loss = 2.94483681\n",
      "Iteration 592, loss = 2.94432833\n",
      "Iteration 593, loss = 2.94370394\n",
      "Iteration 594, loss = 2.94336187\n",
      "Iteration 595, loss = 2.94290706\n",
      "Iteration 596, loss = 2.94247072\n",
      "Iteration 597, loss = 2.94203945\n",
      "Iteration 598, loss = 2.94183135\n",
      "Iteration 599, loss = 2.94162794\n",
      "Iteration 600, loss = 2.94112737\n",
      "Iteration 601, loss = 2.94046495\n",
      "Iteration 602, loss = 2.94016004\n",
      "Iteration 603, loss = 2.93967271\n",
      "Iteration 604, loss = 2.93942462\n",
      "Iteration 605, loss = 2.93946330\n",
      "Iteration 606, loss = 2.93874251\n",
      "Iteration 607, loss = 2.93826160\n",
      "Iteration 608, loss = 2.93802036\n",
      "Iteration 609, loss = 2.93768250\n",
      "Iteration 610, loss = 2.93679403\n",
      "Iteration 611, loss = 2.93694512\n",
      "Iteration 612, loss = 2.93662629\n",
      "Iteration 613, loss = 2.93664948\n",
      "Iteration 614, loss = 2.93553912\n",
      "Iteration 615, loss = 2.93522188\n",
      "Iteration 616, loss = 2.93470116\n",
      "Iteration 617, loss = 2.93492229\n",
      "Iteration 618, loss = 2.93385010\n",
      "Iteration 619, loss = 2.93360329\n",
      "Iteration 620, loss = 2.93273072\n",
      "Iteration 621, loss = 2.93323858\n",
      "Iteration 622, loss = 2.93275568\n",
      "Iteration 623, loss = 2.93175191\n",
      "Iteration 624, loss = 2.93160729\n",
      "Iteration 625, loss = 2.93161425\n",
      "Iteration 626, loss = 2.93106245\n",
      "Iteration 627, loss = 2.93059743\n",
      "Iteration 628, loss = 2.93013035\n",
      "Iteration 629, loss = 2.92974254\n",
      "Iteration 630, loss = 2.93002712\n",
      "Iteration 631, loss = 2.92984739\n",
      "Iteration 632, loss = 2.92921449\n",
      "Iteration 633, loss = 2.92855666\n",
      "Iteration 634, loss = 2.92853181\n",
      "Iteration 635, loss = 2.92754161\n",
      "Iteration 636, loss = 2.92725490\n",
      "Iteration 637, loss = 2.92635654\n",
      "Iteration 638, loss = 2.92574315\n",
      "Iteration 639, loss = 2.92572146\n",
      "Iteration 640, loss = 2.92573333\n",
      "Iteration 641, loss = 2.92568829\n",
      "Iteration 642, loss = 2.92505405\n",
      "Iteration 643, loss = 2.92449816\n",
      "Iteration 644, loss = 2.92442942\n",
      "Iteration 645, loss = 2.92414442\n",
      "Iteration 646, loss = 2.92349412\n",
      "Iteration 647, loss = 2.92277574\n",
      "Iteration 648, loss = 2.92274762\n",
      "Iteration 649, loss = 2.92255943\n",
      "Iteration 650, loss = 2.92180292\n",
      "Iteration 651, loss = 2.92171744\n",
      "Iteration 652, loss = 2.92085477\n",
      "Iteration 653, loss = 2.92073731\n",
      "Iteration 654, loss = 2.92035111\n",
      "Iteration 655, loss = 2.92018843\n",
      "Iteration 656, loss = 2.91975936\n",
      "Iteration 657, loss = 2.91952345\n",
      "Iteration 658, loss = 2.91909019\n",
      "Iteration 659, loss = 2.91842564\n",
      "Iteration 660, loss = 2.91834399\n",
      "Iteration 661, loss = 2.91774824\n",
      "Iteration 662, loss = 2.91743918\n",
      "Iteration 663, loss = 2.91739771\n",
      "Iteration 664, loss = 2.91683965\n",
      "Iteration 665, loss = 2.91623834\n",
      "Iteration 666, loss = 2.91577525\n",
      "Iteration 667, loss = 2.91517030\n",
      "Iteration 668, loss = 2.91543142\n",
      "Iteration 669, loss = 2.91570307\n",
      "Iteration 670, loss = 2.91485441\n",
      "Iteration 671, loss = 2.91427064\n",
      "Iteration 672, loss = 2.91377793\n",
      "Iteration 673, loss = 2.91347297\n",
      "Iteration 674, loss = 2.91329354\n",
      "Iteration 675, loss = 2.91251362\n",
      "Iteration 676, loss = 2.91329611\n",
      "Iteration 677, loss = 2.91254969\n",
      "Iteration 678, loss = 2.91203269\n",
      "Iteration 679, loss = 2.91122939\n",
      "Iteration 680, loss = 2.91159065\n",
      "Iteration 681, loss = 2.91071781\n",
      "Iteration 682, loss = 2.91002471\n",
      "Iteration 683, loss = 2.90973467\n",
      "Iteration 684, loss = 2.90929008\n",
      "Iteration 685, loss = 2.90936935\n",
      "Iteration 686, loss = 2.90851123\n",
      "Iteration 687, loss = 2.90825216\n",
      "Iteration 688, loss = 2.90808524\n",
      "Iteration 689, loss = 2.90757089\n",
      "Iteration 690, loss = 2.90720633\n",
      "Iteration 691, loss = 2.90795808\n",
      "Iteration 692, loss = 2.90671478\n",
      "Iteration 693, loss = 2.90626949\n",
      "Iteration 694, loss = 2.90603526\n",
      "Iteration 695, loss = 2.90582196\n",
      "Iteration 696, loss = 2.90540152\n",
      "Iteration 697, loss = 2.90478124\n",
      "Iteration 698, loss = 2.90421009\n",
      "Iteration 699, loss = 2.90433173\n",
      "Iteration 700, loss = 2.90359173\n",
      "Iteration 701, loss = 2.90333679\n",
      "Iteration 702, loss = 2.90283947\n",
      "Iteration 703, loss = 2.90230852\n",
      "Iteration 704, loss = 2.90210857\n",
      "Iteration 705, loss = 2.90234439\n",
      "Iteration 706, loss = 2.90151921\n",
      "Iteration 707, loss = 2.90132577\n",
      "Iteration 708, loss = 2.90112764\n",
      "Iteration 709, loss = 2.90064601\n",
      "Iteration 710, loss = 2.89967535\n",
      "Iteration 711, loss = 2.89946149\n",
      "Iteration 712, loss = 2.89917185\n",
      "Iteration 713, loss = 2.89923381\n",
      "Iteration 714, loss = 2.89882956\n",
      "Iteration 715, loss = 2.89779438\n",
      "Iteration 716, loss = 2.89831349\n",
      "Iteration 717, loss = 2.89777212\n",
      "Iteration 718, loss = 2.89769116\n",
      "Iteration 719, loss = 2.89642970\n",
      "Iteration 720, loss = 2.89649723\n",
      "Iteration 721, loss = 2.89599539\n",
      "Iteration 722, loss = 2.89596118\n",
      "Iteration 723, loss = 2.89568390\n",
      "Iteration 724, loss = 2.89506400\n",
      "Iteration 725, loss = 2.89445938\n",
      "Iteration 726, loss = 2.89442023\n",
      "Iteration 727, loss = 2.89403587\n",
      "Iteration 728, loss = 2.89353512\n",
      "Iteration 729, loss = 2.89377110\n",
      "Iteration 730, loss = 2.89308530\n",
      "Iteration 731, loss = 2.89248864\n",
      "Iteration 732, loss = 2.89217108\n",
      "Iteration 733, loss = 2.89223754\n",
      "Iteration 734, loss = 2.89144097\n",
      "Iteration 735, loss = 2.89116018\n",
      "Iteration 736, loss = 2.89074699\n",
      "Iteration 737, loss = 2.89030730\n",
      "Iteration 738, loss = 2.88996000\n",
      "Iteration 739, loss = 2.89003325\n",
      "Iteration 740, loss = 2.88988722\n",
      "Iteration 741, loss = 2.88954119\n",
      "Iteration 742, loss = 2.88883032\n",
      "Iteration 743, loss = 2.88829187\n",
      "Iteration 744, loss = 2.88757413\n",
      "Iteration 745, loss = 2.88750689\n",
      "Iteration 746, loss = 2.88708033\n",
      "Iteration 747, loss = 2.88691664\n",
      "Iteration 748, loss = 2.88661666\n",
      "Iteration 749, loss = 2.88613262\n",
      "Iteration 750, loss = 2.88567005\n",
      "Iteration 751, loss = 2.88571773\n",
      "Iteration 752, loss = 2.88507292\n",
      "Iteration 753, loss = 2.88526636\n",
      "Iteration 754, loss = 2.88472153\n",
      "Iteration 755, loss = 2.88430463\n",
      "Iteration 756, loss = 2.88346610\n",
      "Iteration 757, loss = 2.88300485\n",
      "Iteration 758, loss = 2.88284721\n",
      "Iteration 759, loss = 2.88268934\n",
      "Iteration 760, loss = 2.88280292\n",
      "Iteration 761, loss = 2.88190055\n",
      "Iteration 762, loss = 2.88144509\n",
      "Iteration 763, loss = 2.88149451\n",
      "Iteration 764, loss = 2.88147139\n",
      "Iteration 765, loss = 2.88075843\n",
      "Iteration 766, loss = 2.88007185\n",
      "Iteration 767, loss = 2.88017656\n",
      "Iteration 768, loss = 2.87950115\n",
      "Iteration 769, loss = 2.87863795\n",
      "Iteration 770, loss = 2.87848400\n",
      "Iteration 771, loss = 2.87846557\n",
      "Iteration 772, loss = 2.87821738\n",
      "Iteration 773, loss = 2.87809761\n",
      "Iteration 774, loss = 2.87740842\n",
      "Iteration 775, loss = 2.87682790\n",
      "Iteration 776, loss = 2.87632427\n",
      "Iteration 777, loss = 2.87622542\n",
      "Iteration 778, loss = 2.87543759\n",
      "Iteration 779, loss = 2.87527114\n",
      "Iteration 780, loss = 2.87531986\n",
      "Iteration 781, loss = 2.87503889\n",
      "Iteration 782, loss = 2.87429832\n",
      "Iteration 783, loss = 2.87388020\n",
      "Iteration 784, loss = 2.87393629\n",
      "Iteration 785, loss = 2.87348307\n",
      "Iteration 786, loss = 2.87298726\n",
      "Iteration 787, loss = 2.87309876\n",
      "Iteration 788, loss = 2.87252168\n",
      "Iteration 789, loss = 2.87310075\n",
      "Iteration 790, loss = 2.87152436\n",
      "Iteration 791, loss = 2.87079996\n",
      "Iteration 792, loss = 2.87077115\n",
      "Iteration 793, loss = 2.87057587\n",
      "Iteration 794, loss = 2.87023877\n",
      "Iteration 795, loss = 2.87003804\n",
      "Iteration 796, loss = 2.86945081\n",
      "Iteration 797, loss = 2.86859652\n",
      "Iteration 798, loss = 2.86835803\n",
      "Iteration 799, loss = 2.86828878\n",
      "Iteration 800, loss = 2.86782364\n",
      "Iteration 801, loss = 2.86778297\n",
      "Iteration 802, loss = 2.86772152\n",
      "Iteration 803, loss = 2.86730030\n",
      "Iteration 804, loss = 2.86679470\n",
      "Iteration 805, loss = 2.86610812\n",
      "Iteration 806, loss = 2.86638259\n",
      "Iteration 807, loss = 2.86576225\n",
      "Iteration 808, loss = 2.86503650\n",
      "Iteration 809, loss = 2.86464241\n",
      "Iteration 810, loss = 2.86403225\n",
      "Iteration 811, loss = 2.86417520\n",
      "Iteration 812, loss = 2.86353026\n",
      "Iteration 813, loss = 2.86348544\n",
      "Iteration 814, loss = 2.86341574\n",
      "Iteration 815, loss = 2.86352858\n",
      "Iteration 816, loss = 2.86275142\n",
      "Iteration 817, loss = 2.86246399\n",
      "Iteration 818, loss = 2.86207546\n",
      "Iteration 819, loss = 2.86167139\n",
      "Iteration 820, loss = 2.86119241\n",
      "Iteration 821, loss = 2.86102780\n",
      "Iteration 822, loss = 2.86021545\n",
      "Iteration 823, loss = 2.86003170\n",
      "Iteration 824, loss = 2.85985449\n",
      "Iteration 825, loss = 2.85957066\n",
      "Iteration 826, loss = 2.85974683\n",
      "Iteration 827, loss = 2.85916042\n",
      "Iteration 828, loss = 2.85850282\n",
      "Iteration 829, loss = 2.85812052\n",
      "Iteration 830, loss = 2.85787315\n",
      "Iteration 831, loss = 2.85684168\n",
      "Iteration 832, loss = 2.85657769\n",
      "Iteration 833, loss = 2.85627234\n",
      "Iteration 834, loss = 2.85625595\n",
      "Iteration 835, loss = 2.85596254\n",
      "Iteration 836, loss = 2.85612306\n",
      "Iteration 837, loss = 2.85583254\n",
      "Iteration 838, loss = 2.85512665\n",
      "Iteration 839, loss = 2.85474585\n",
      "Iteration 840, loss = 2.85461690\n",
      "Iteration 841, loss = 2.85346741\n",
      "Iteration 842, loss = 2.85327198\n",
      "Iteration 843, loss = 2.85287667\n",
      "Iteration 844, loss = 2.85292069\n",
      "Iteration 845, loss = 2.85253554\n",
      "Iteration 846, loss = 2.85209449\n",
      "Iteration 847, loss = 2.85170116\n",
      "Iteration 848, loss = 2.85146718\n",
      "Iteration 849, loss = 2.85132870\n",
      "Iteration 850, loss = 2.85093094\n",
      "Iteration 851, loss = 2.85061180\n",
      "Iteration 852, loss = 2.85024005\n",
      "Iteration 853, loss = 2.84999370\n",
      "Iteration 854, loss = 2.84987465\n",
      "Iteration 855, loss = 2.84932979\n",
      "Iteration 856, loss = 2.84891683\n",
      "Iteration 857, loss = 2.84850394\n",
      "Iteration 858, loss = 2.84769592\n",
      "Iteration 859, loss = 2.84733675\n",
      "Iteration 860, loss = 2.84688194\n",
      "Iteration 861, loss = 2.84680359\n",
      "Iteration 862, loss = 2.84676999\n",
      "Iteration 863, loss = 2.84649931\n",
      "Iteration 864, loss = 2.84614426\n",
      "Iteration 865, loss = 2.84579023\n",
      "Iteration 866, loss = 2.84568727\n",
      "Iteration 867, loss = 2.84542098\n",
      "Iteration 868, loss = 2.84498022\n",
      "Iteration 869, loss = 2.84462452\n",
      "Iteration 870, loss = 2.84398942\n",
      "Iteration 871, loss = 2.84382732\n",
      "Iteration 872, loss = 2.84327081\n",
      "Iteration 873, loss = 2.84305262\n",
      "Iteration 874, loss = 2.84316678\n",
      "Iteration 875, loss = 2.84264601\n",
      "Iteration 876, loss = 2.84202751\n",
      "Iteration 877, loss = 2.84210756\n",
      "Iteration 878, loss = 2.84187194\n",
      "Iteration 879, loss = 2.84114252\n",
      "Iteration 880, loss = 2.84057127\n",
      "Iteration 881, loss = 2.84008206\n",
      "Iteration 882, loss = 2.83977216\n",
      "Iteration 883, loss = 2.83973585\n",
      "Iteration 884, loss = 2.83926997\n",
      "Iteration 885, loss = 2.83906927\n",
      "Iteration 886, loss = 2.83868308\n",
      "Iteration 887, loss = 2.83866546\n",
      "Iteration 888, loss = 2.83819592\n",
      "Iteration 889, loss = 2.83756624\n",
      "Iteration 890, loss = 2.83735486\n",
      "Iteration 891, loss = 2.83704906\n",
      "Iteration 892, loss = 2.83641374\n",
      "Iteration 893, loss = 2.83615276\n",
      "Iteration 894, loss = 2.83619822\n",
      "Iteration 895, loss = 2.83614763\n",
      "Iteration 896, loss = 2.83570991\n",
      "Iteration 897, loss = 2.83513973\n",
      "Iteration 898, loss = 2.83590473\n",
      "Iteration 899, loss = 2.83555668\n",
      "Iteration 900, loss = 2.83460762\n",
      "Iteration 901, loss = 2.83392395\n",
      "Iteration 902, loss = 2.83358079\n",
      "Iteration 903, loss = 2.83335186\n",
      "Iteration 904, loss = 2.83239784\n",
      "Iteration 905, loss = 2.83235137\n",
      "Iteration 906, loss = 2.83204204\n",
      "Iteration 907, loss = 2.83194115\n",
      "Iteration 908, loss = 2.83116588\n",
      "Iteration 909, loss = 2.83075274\n",
      "Iteration 910, loss = 2.83055170\n",
      "Iteration 911, loss = 2.83003130\n",
      "Iteration 912, loss = 2.83028972\n",
      "Iteration 913, loss = 2.83010686\n",
      "Iteration 914, loss = 2.82975326\n",
      "Iteration 915, loss = 2.82960357\n",
      "Iteration 916, loss = 2.82909302\n",
      "Iteration 917, loss = 2.82856164\n",
      "Iteration 918, loss = 2.82794061\n",
      "Iteration 919, loss = 2.82747693\n",
      "Iteration 920, loss = 2.82767372\n",
      "Iteration 921, loss = 2.82680159\n",
      "Iteration 922, loss = 2.82689342\n",
      "Iteration 923, loss = 2.82699415\n",
      "Iteration 924, loss = 2.82579011\n",
      "Iteration 925, loss = 2.82642236\n",
      "Iteration 926, loss = 2.82621516\n",
      "Iteration 927, loss = 2.82538363\n",
      "Iteration 928, loss = 2.82482634\n",
      "Iteration 929, loss = 2.82469480\n",
      "Iteration 930, loss = 2.82500239\n",
      "Iteration 931, loss = 2.82416686\n",
      "Iteration 932, loss = 2.82414495\n",
      "Iteration 933, loss = 2.82391140\n",
      "Iteration 934, loss = 2.82355002\n",
      "Iteration 935, loss = 2.82345933\n",
      "Iteration 936, loss = 2.82290315\n",
      "Iteration 937, loss = 2.82201078\n",
      "Iteration 938, loss = 2.82153522\n",
      "Iteration 939, loss = 2.82114524\n",
      "Iteration 940, loss = 2.82149033\n",
      "Iteration 941, loss = 2.82126983\n",
      "Iteration 942, loss = 2.82068939\n",
      "Iteration 943, loss = 2.82017729\n",
      "Iteration 944, loss = 2.81965902\n",
      "Iteration 945, loss = 2.81944737\n",
      "Iteration 946, loss = 2.81909893\n",
      "Iteration 947, loss = 2.81952092\n",
      "Iteration 948, loss = 2.81849239\n",
      "Iteration 949, loss = 2.81768642\n",
      "Iteration 950, loss = 2.81737940\n",
      "Iteration 951, loss = 2.81733407\n",
      "Iteration 952, loss = 2.81690344\n",
      "Iteration 953, loss = 2.81681018\n",
      "Iteration 954, loss = 2.81642430\n",
      "Iteration 955, loss = 2.81597229\n",
      "Iteration 956, loss = 2.81546470\n",
      "Iteration 957, loss = 2.81504133\n",
      "Iteration 958, loss = 2.81496954\n",
      "Iteration 959, loss = 2.81478387\n",
      "Iteration 960, loss = 2.81439434\n",
      "Iteration 961, loss = 2.81447973\n",
      "Iteration 962, loss = 2.81393519\n",
      "Iteration 963, loss = 2.81404268\n",
      "Iteration 964, loss = 2.81332439\n",
      "Iteration 965, loss = 2.81316315\n",
      "Iteration 966, loss = 2.81264676\n",
      "Iteration 967, loss = 2.81198110\n",
      "Iteration 968, loss = 2.81188500\n",
      "Iteration 969, loss = 2.81198936\n",
      "Iteration 970, loss = 2.81209655\n",
      "Iteration 971, loss = 2.81145235\n",
      "Iteration 972, loss = 2.81089847\n",
      "Iteration 973, loss = 2.81003001\n",
      "Iteration 974, loss = 2.81110456\n",
      "Iteration 975, loss = 2.80969705\n",
      "Iteration 976, loss = 2.80913130\n",
      "Iteration 977, loss = 2.80877918\n",
      "Iteration 978, loss = 2.80938664\n",
      "Iteration 979, loss = 2.80837992\n",
      "Iteration 980, loss = 2.80929491\n",
      "Iteration 981, loss = 2.80818029\n",
      "Iteration 982, loss = 2.80762942\n",
      "Iteration 983, loss = 2.80779404\n",
      "Iteration 984, loss = 2.80686174\n",
      "Iteration 985, loss = 2.80648795\n",
      "Iteration 986, loss = 2.80634990\n",
      "Iteration 987, loss = 2.80595085\n",
      "Iteration 988, loss = 2.80596190\n",
      "Iteration 989, loss = 2.80615510\n",
      "Iteration 990, loss = 2.80519044\n",
      "Iteration 991, loss = 2.80488825\n",
      "Iteration 992, loss = 2.80469180\n",
      "Iteration 993, loss = 2.80400245\n",
      "Iteration 994, loss = 2.80372833\n",
      "Iteration 995, loss = 2.80372084\n",
      "Iteration 996, loss = 2.80343686\n",
      "Iteration 997, loss = 2.80293407\n",
      "Iteration 998, loss = 2.80295506\n",
      "Iteration 999, loss = 2.80244923\n",
      "Iteration 1000, loss = 2.80203087\n",
      "Iteration 1001, loss = 2.80194504\n",
      "Iteration 1002, loss = 2.80140520\n",
      "Iteration 1003, loss = 2.80093703\n",
      "Iteration 1004, loss = 2.80066733\n",
      "Iteration 1005, loss = 2.80059476\n",
      "Iteration 1006, loss = 2.80032999\n",
      "Iteration 1007, loss = 2.79998428\n",
      "Iteration 1008, loss = 2.79931147\n",
      "Iteration 1009, loss = 2.79923190\n",
      "Iteration 1010, loss = 2.79921342\n",
      "Iteration 1011, loss = 2.79886703\n",
      "Iteration 1012, loss = 2.79833802\n",
      "Iteration 1013, loss = 2.79757359\n",
      "Iteration 1014, loss = 2.79720786\n",
      "Iteration 1015, loss = 2.79738873\n",
      "Iteration 1016, loss = 2.79726188\n",
      "Iteration 1017, loss = 2.79659155\n",
      "Iteration 1018, loss = 2.79676635\n",
      "Iteration 1019, loss = 2.79628409\n",
      "Iteration 1020, loss = 2.79631822\n",
      "Iteration 1021, loss = 2.79581204\n",
      "Iteration 1022, loss = 2.79543302\n",
      "Iteration 1023, loss = 2.79541493\n",
      "Iteration 1024, loss = 2.79464822\n",
      "Iteration 1025, loss = 2.79467084\n",
      "Iteration 1026, loss = 2.79451109\n",
      "Iteration 1027, loss = 2.79366078\n",
      "Iteration 1028, loss = 2.79302599\n",
      "Iteration 1029, loss = 2.79277367\n",
      "Iteration 1030, loss = 2.79244659\n",
      "Iteration 1031, loss = 2.79188563\n",
      "Iteration 1032, loss = 2.79227698\n",
      "Iteration 1033, loss = 2.79165398\n",
      "Iteration 1034, loss = 2.79111647\n",
      "Iteration 1035, loss = 2.79084900\n",
      "Iteration 1036, loss = 2.79065621\n",
      "Iteration 1037, loss = 2.79060313\n",
      "Iteration 1038, loss = 2.79034600\n",
      "Iteration 1039, loss = 2.78964630\n",
      "Iteration 1040, loss = 2.78939017\n",
      "Iteration 1041, loss = 2.78948495\n",
      "Iteration 1042, loss = 2.78922496\n",
      "Iteration 1043, loss = 2.78966809\n",
      "Iteration 1044, loss = 2.78907277\n",
      "Iteration 1045, loss = 2.78871503\n",
      "Iteration 1046, loss = 2.78857116\n",
      "Iteration 1047, loss = 2.78744455\n",
      "Iteration 1048, loss = 2.78705978\n",
      "Iteration 1049, loss = 2.78682189\n",
      "Iteration 1050, loss = 2.78697138\n",
      "Iteration 1051, loss = 2.78653375\n",
      "Iteration 1052, loss = 2.78574921\n",
      "Iteration 1053, loss = 2.78572203\n",
      "Iteration 1054, loss = 2.78544665\n",
      "Iteration 1055, loss = 2.78514312\n",
      "Iteration 1056, loss = 2.78505620\n",
      "Iteration 1057, loss = 2.78465686\n",
      "Iteration 1058, loss = 2.78410042\n",
      "Iteration 1059, loss = 2.78394349\n",
      "Iteration 1060, loss = 2.78363677\n",
      "Iteration 1061, loss = 2.78293680\n",
      "Iteration 1062, loss = 2.78267576\n",
      "Iteration 1063, loss = 2.78248875\n",
      "Iteration 1064, loss = 2.78268356\n",
      "Iteration 1065, loss = 2.78171566\n",
      "Iteration 1066, loss = 2.78136582\n",
      "Iteration 1067, loss = 2.78118877\n",
      "Iteration 1068, loss = 2.78076959\n",
      "Iteration 1069, loss = 2.78105999\n",
      "Iteration 1070, loss = 2.78083611\n",
      "Iteration 1071, loss = 2.78051978\n",
      "Iteration 1072, loss = 2.78036688\n",
      "Iteration 1073, loss = 2.77976202\n",
      "Iteration 1074, loss = 2.77895414\n",
      "Iteration 1075, loss = 2.77871223\n",
      "Iteration 1076, loss = 2.77911099\n",
      "Iteration 1077, loss = 2.77912744\n",
      "Iteration 1078, loss = 2.77841334\n",
      "Iteration 1079, loss = 2.77854900\n",
      "Iteration 1080, loss = 2.77832809\n",
      "Iteration 1081, loss = 2.77766786\n",
      "Iteration 1082, loss = 2.77697760\n",
      "Iteration 1083, loss = 2.77715444\n",
      "Iteration 1084, loss = 2.77645180\n",
      "Iteration 1085, loss = 2.77646318\n",
      "Iteration 1086, loss = 2.77587952\n",
      "Iteration 1087, loss = 2.77536229\n",
      "Iteration 1088, loss = 2.77488029\n",
      "Iteration 1089, loss = 2.77465490\n",
      "Iteration 1090, loss = 2.77464123\n",
      "Iteration 1091, loss = 2.77481274\n",
      "Iteration 1092, loss = 2.77426237\n",
      "Iteration 1093, loss = 2.77430763\n",
      "Iteration 1094, loss = 2.77394351\n",
      "Iteration 1095, loss = 2.77297337\n",
      "Iteration 1096, loss = 2.77290527\n",
      "Iteration 1097, loss = 2.77257225\n",
      "Iteration 1098, loss = 2.77195124\n",
      "Iteration 1099, loss = 2.77140894\n",
      "Iteration 1100, loss = 2.77152096\n",
      "Iteration 1101, loss = 2.77200082\n",
      "Iteration 1102, loss = 2.77157203\n",
      "Iteration 1103, loss = 2.77112929\n",
      "Iteration 1104, loss = 2.77064685\n",
      "Iteration 1105, loss = 2.77056832\n",
      "Iteration 1106, loss = 2.77089843\n",
      "Iteration 1107, loss = 2.77004771\n",
      "Iteration 1108, loss = 2.76946651\n",
      "Iteration 1109, loss = 2.76958677\n",
      "Iteration 1110, loss = 2.76944052\n",
      "Iteration 1111, loss = 2.76841380\n",
      "Iteration 1112, loss = 2.76841524\n",
      "Iteration 1113, loss = 2.76871839\n",
      "Iteration 1114, loss = 2.76839671\n",
      "Iteration 1115, loss = 2.76805982\n",
      "Iteration 1116, loss = 2.76769346\n",
      "Iteration 1117, loss = 2.76778429\n",
      "Iteration 1118, loss = 2.76724350\n",
      "Iteration 1119, loss = 2.76700348\n",
      "Iteration 1120, loss = 2.76592734\n",
      "Iteration 1121, loss = 2.76546096\n",
      "Iteration 1122, loss = 2.76559331\n",
      "Iteration 1123, loss = 2.76467102\n",
      "Iteration 1124, loss = 2.76489129\n",
      "Iteration 1125, loss = 2.76457007\n",
      "Iteration 1126, loss = 2.76465759\n",
      "Iteration 1127, loss = 2.76401650\n",
      "Iteration 1128, loss = 2.76349003\n",
      "Iteration 1129, loss = 2.76296290\n",
      "Iteration 1130, loss = 2.76263914\n",
      "Iteration 1131, loss = 2.76267566\n",
      "Iteration 1132, loss = 2.76254572\n",
      "Iteration 1133, loss = 2.76218986\n",
      "Iteration 1134, loss = 2.76216942\n",
      "Iteration 1135, loss = 2.76130717\n",
      "Iteration 1136, loss = 2.76138392\n",
      "Iteration 1137, loss = 2.76108455\n",
      "Iteration 1138, loss = 2.76067920\n",
      "Iteration 1139, loss = 2.76087162\n",
      "Iteration 1140, loss = 2.76041624\n",
      "Iteration 1141, loss = 2.76056319\n",
      "Iteration 1142, loss = 2.75960233\n",
      "Iteration 1143, loss = 2.76077725\n",
      "Iteration 1144, loss = 2.75994574\n",
      "Iteration 1145, loss = 2.75874463\n",
      "Iteration 1146, loss = 2.75881871\n",
      "Iteration 1147, loss = 2.75848383\n",
      "Iteration 1148, loss = 2.75777722\n",
      "Iteration 1149, loss = 2.75778177\n",
      "Iteration 1150, loss = 2.75724639\n",
      "Iteration 1151, loss = 2.75744446\n",
      "Iteration 1152, loss = 2.75706333\n",
      "Iteration 1153, loss = 2.75691288\n",
      "Iteration 1154, loss = 2.75683627\n",
      "Iteration 1155, loss = 2.75669600\n",
      "Iteration 1156, loss = 2.75593357\n",
      "Iteration 1157, loss = 2.75533514\n",
      "Iteration 1158, loss = 2.75559929\n",
      "Iteration 1159, loss = 2.75506149\n",
      "Iteration 1160, loss = 2.75431620\n",
      "Iteration 1161, loss = 2.75459213\n",
      "Iteration 1162, loss = 2.75408778\n",
      "Iteration 1163, loss = 2.75391029\n",
      "Iteration 1164, loss = 2.75360900\n",
      "Iteration 1165, loss = 2.75290332\n",
      "Iteration 1166, loss = 2.75316465\n",
      "Iteration 1167, loss = 2.75279027\n",
      "Iteration 1168, loss = 2.75267722\n",
      "Iteration 1169, loss = 2.75245862\n",
      "Iteration 1170, loss = 2.75232957\n",
      "Iteration 1171, loss = 2.75147303\n",
      "Iteration 1172, loss = 2.75158469\n",
      "Iteration 1173, loss = 2.75208850\n",
      "Iteration 1174, loss = 2.75133111\n",
      "Iteration 1175, loss = 2.75132924\n",
      "Iteration 1176, loss = 2.75074560\n",
      "Iteration 1177, loss = 2.75039326\n",
      "Iteration 1178, loss = 2.74971639\n",
      "Iteration 1179, loss = 2.74880211\n",
      "Iteration 1180, loss = 2.74901940\n",
      "Iteration 1181, loss = 2.74925067\n",
      "Iteration 1182, loss = 2.74879719\n",
      "Iteration 1183, loss = 2.74840977\n",
      "Iteration 1184, loss = 2.74808779\n",
      "Iteration 1185, loss = 2.74733537\n",
      "Iteration 1186, loss = 2.74738915\n",
      "Iteration 1187, loss = 2.74686929\n",
      "Iteration 1188, loss = 2.74669137\n",
      "Iteration 1189, loss = 2.74619239\n",
      "Iteration 1190, loss = 2.74646176\n",
      "Iteration 1191, loss = 2.74649094\n",
      "Iteration 1192, loss = 2.74590028\n",
      "Iteration 1193, loss = 2.74549735\n",
      "Iteration 1194, loss = 2.74581072\n",
      "Iteration 1195, loss = 2.74529455\n",
      "Iteration 1196, loss = 2.74489910\n",
      "Iteration 1197, loss = 2.74495659\n",
      "Iteration 1198, loss = 2.74459876\n",
      "Iteration 1199, loss = 2.74418747\n",
      "Iteration 1200, loss = 2.74388315\n",
      "Iteration 1201, loss = 2.74368792\n",
      "Iteration 1202, loss = 2.74330830\n",
      "Iteration 1203, loss = 2.74348824\n",
      "Iteration 1204, loss = 2.74310946\n",
      "Iteration 1205, loss = 2.74207710\n",
      "Iteration 1206, loss = 2.74189270\n",
      "Iteration 1207, loss = 2.74153947\n",
      "Iteration 1208, loss = 2.74160830\n",
      "Iteration 1209, loss = 2.74122838\n",
      "Iteration 1210, loss = 2.74102229\n",
      "Iteration 1211, loss = 2.74043380\n",
      "Iteration 1212, loss = 2.74144998\n",
      "Iteration 1213, loss = 2.74202339\n",
      "Iteration 1214, loss = 2.74008679\n",
      "Iteration 1215, loss = 2.73922563\n",
      "Iteration 1216, loss = 2.73906109\n",
      "Iteration 1217, loss = 2.73889202\n",
      "Iteration 1218, loss = 2.73863451\n",
      "Iteration 1219, loss = 2.73797072\n",
      "Iteration 1220, loss = 2.73760222\n",
      "Iteration 1221, loss = 2.73936451\n",
      "Iteration 1222, loss = 2.73798538\n",
      "Iteration 1223, loss = 2.73710376\n",
      "Iteration 1224, loss = 2.73678498\n",
      "Iteration 1225, loss = 2.73629191\n",
      "Iteration 1226, loss = 2.73641983\n",
      "Iteration 1227, loss = 2.73608426\n",
      "Iteration 1228, loss = 2.73592662\n",
      "Iteration 1229, loss = 2.73608950\n",
      "Iteration 1230, loss = 2.73497721\n",
      "Iteration 1231, loss = 2.73484381\n",
      "Iteration 1232, loss = 2.73554184\n",
      "Iteration 1233, loss = 2.73463494\n",
      "Iteration 1234, loss = 2.73431047\n",
      "Iteration 1235, loss = 2.73454528\n",
      "Iteration 1236, loss = 2.73363134\n",
      "Iteration 1237, loss = 2.73321667\n",
      "Iteration 1238, loss = 2.73302461\n",
      "Iteration 1239, loss = 2.73256169\n",
      "Iteration 1240, loss = 2.73177833\n",
      "Iteration 1241, loss = 2.73196936\n",
      "Iteration 1242, loss = 2.73264082\n",
      "Iteration 1243, loss = 2.73250907\n",
      "Iteration 1244, loss = 2.73215266\n",
      "Iteration 1245, loss = 2.73215741\n",
      "Iteration 1246, loss = 2.73148577\n",
      "Iteration 1247, loss = 2.73130368\n",
      "Iteration 1248, loss = 2.73067277\n",
      "Iteration 1249, loss = 2.73021359\n",
      "Iteration 1250, loss = 2.73004553\n",
      "Iteration 1251, loss = 2.72959111\n",
      "Iteration 1252, loss = 2.72962641\n",
      "Iteration 1253, loss = 2.72974800\n",
      "Iteration 1254, loss = 2.72940859\n",
      "Iteration 1255, loss = 2.72919492\n",
      "Iteration 1256, loss = 2.72890599\n",
      "Iteration 1257, loss = 2.72821609\n",
      "Iteration 1258, loss = 2.72753883\n",
      "Iteration 1259, loss = 2.72754733\n",
      "Iteration 1260, loss = 2.72767890\n",
      "Iteration 1261, loss = 2.72708695\n",
      "Iteration 1262, loss = 2.72710801\n",
      "Iteration 1263, loss = 2.72701184\n",
      "Iteration 1264, loss = 2.72620735\n",
      "Iteration 1265, loss = 2.72618456\n",
      "Iteration 1266, loss = 2.72579920\n",
      "Iteration 1267, loss = 2.72537724\n",
      "Iteration 1268, loss = 2.72477014\n",
      "Iteration 1269, loss = 2.72475490\n",
      "Iteration 1, loss = 4.16041840\n",
      "Iteration 2, loss = 4.12039653\n",
      "Iteration 3, loss = 4.08220757\n",
      "Iteration 4, loss = 4.04253168\n",
      "Iteration 5, loss = 4.00142775\n",
      "Iteration 6, loss = 3.95777004\n",
      "Iteration 7, loss = 3.91119852\n",
      "Iteration 8, loss = 3.86242177\n",
      "Iteration 9, loss = 3.81125737\n",
      "Iteration 10, loss = 3.75758454\n",
      "Iteration 11, loss = 3.70163415\n",
      "Iteration 12, loss = 3.64750103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 3.59182302\n",
      "Iteration 14, loss = 3.53625487\n",
      "Iteration 15, loss = 3.48261654\n",
      "Iteration 16, loss = 3.43381601\n",
      "Iteration 17, loss = 3.39013933\n",
      "Iteration 18, loss = 3.35191095\n",
      "Iteration 19, loss = 3.32036890\n",
      "Iteration 20, loss = 3.29372217\n",
      "Iteration 21, loss = 3.27446064\n",
      "Iteration 22, loss = 3.25939765\n",
      "Iteration 23, loss = 3.24905382\n",
      "Iteration 24, loss = 3.24213721\n",
      "Iteration 25, loss = 3.23686709\n",
      "Iteration 26, loss = 3.23417523\n",
      "Iteration 27, loss = 3.23157160\n",
      "Iteration 28, loss = 3.23046103\n",
      "Iteration 29, loss = 3.22930737\n",
      "Iteration 30, loss = 3.22835300\n",
      "Iteration 31, loss = 3.22769164\n",
      "Iteration 32, loss = 3.22700093\n",
      "Iteration 33, loss = 3.22656646\n",
      "Iteration 34, loss = 3.22565630\n",
      "Iteration 35, loss = 3.22528990\n",
      "Iteration 36, loss = 3.22472016\n",
      "Iteration 37, loss = 3.22432974\n",
      "Iteration 38, loss = 3.22348748\n",
      "Iteration 39, loss = 3.22333445\n",
      "Iteration 40, loss = 3.22248818\n",
      "Iteration 41, loss = 3.22214966\n",
      "Iteration 42, loss = 3.22167064\n",
      "Iteration 43, loss = 3.22089334\n",
      "Iteration 44, loss = 3.22072786\n",
      "Iteration 45, loss = 3.22025862\n",
      "Iteration 46, loss = 3.21971432\n",
      "Iteration 47, loss = 3.21940578\n",
      "Iteration 48, loss = 3.21863058\n",
      "Iteration 49, loss = 3.21829109\n",
      "Iteration 50, loss = 3.21754039\n",
      "Iteration 51, loss = 3.21719427\n",
      "Iteration 52, loss = 3.21636027\n",
      "Iteration 53, loss = 3.21609293\n",
      "Iteration 54, loss = 3.21569525\n",
      "Iteration 55, loss = 3.21495280\n",
      "Iteration 56, loss = 3.21475795\n",
      "Iteration 57, loss = 3.21434267\n",
      "Iteration 58, loss = 3.21378639\n",
      "Iteration 59, loss = 3.21319277\n",
      "Iteration 60, loss = 3.21249050\n",
      "Iteration 61, loss = 3.21235923\n",
      "Iteration 62, loss = 3.21167255\n",
      "Iteration 63, loss = 3.21134899\n",
      "Iteration 64, loss = 3.21085003\n",
      "Iteration 65, loss = 3.21016489\n",
      "Iteration 66, loss = 3.20955243\n",
      "Iteration 67, loss = 3.20905657\n",
      "Iteration 68, loss = 3.20859858\n",
      "Iteration 69, loss = 3.20820500\n",
      "Iteration 70, loss = 3.20738745\n",
      "Iteration 71, loss = 3.20704064\n",
      "Iteration 72, loss = 3.20661928\n",
      "Iteration 73, loss = 3.20630094\n",
      "Iteration 74, loss = 3.20588324\n",
      "Iteration 75, loss = 3.20525258\n",
      "Iteration 76, loss = 3.20447740\n",
      "Iteration 77, loss = 3.20416980\n",
      "Iteration 78, loss = 3.20366700\n",
      "Iteration 79, loss = 3.20321445\n",
      "Iteration 80, loss = 3.20265959\n",
      "Iteration 81, loss = 3.20214496\n",
      "Iteration 82, loss = 3.20152711\n",
      "Iteration 83, loss = 3.20109069\n",
      "Iteration 84, loss = 3.20073039\n",
      "Iteration 85, loss = 3.20010601\n",
      "Iteration 86, loss = 3.19946919\n",
      "Iteration 87, loss = 3.19878493\n",
      "Iteration 88, loss = 3.19827375\n",
      "Iteration 89, loss = 3.19756025\n",
      "Iteration 90, loss = 3.19713071\n",
      "Iteration 91, loss = 3.19655164\n",
      "Iteration 92, loss = 3.19584434\n",
      "Iteration 93, loss = 3.19521321\n",
      "Iteration 94, loss = 3.19454637\n",
      "Iteration 95, loss = 3.19402306\n",
      "Iteration 96, loss = 3.19338615\n",
      "Iteration 97, loss = 3.19273632\n",
      "Iteration 98, loss = 3.19231926\n",
      "Iteration 99, loss = 3.19144001\n",
      "Iteration 100, loss = 3.19084293\n",
      "Iteration 101, loss = 3.19038640\n",
      "Iteration 102, loss = 3.19021667\n",
      "Iteration 103, loss = 3.18928519\n",
      "Iteration 104, loss = 3.18871155\n",
      "Iteration 105, loss = 3.18786547\n",
      "Iteration 106, loss = 3.18732946\n",
      "Iteration 107, loss = 3.18648491\n",
      "Iteration 108, loss = 3.18603138\n",
      "Iteration 109, loss = 3.18514338\n",
      "Iteration 110, loss = 3.18441725\n",
      "Iteration 111, loss = 3.18408830\n",
      "Iteration 112, loss = 3.18358123\n",
      "Iteration 113, loss = 3.18275868\n",
      "Iteration 114, loss = 3.18231251\n",
      "Iteration 115, loss = 3.18184296\n",
      "Iteration 116, loss = 3.18086821\n",
      "Iteration 117, loss = 3.18029952\n",
      "Iteration 118, loss = 3.17955399\n",
      "Iteration 119, loss = 3.17851493\n",
      "Iteration 120, loss = 3.17779709\n",
      "Iteration 121, loss = 3.17711866\n",
      "Iteration 122, loss = 3.17649789\n",
      "Iteration 123, loss = 3.17632239\n",
      "Iteration 124, loss = 3.17543418\n",
      "Iteration 125, loss = 3.17450853\n",
      "Iteration 126, loss = 3.17392860\n",
      "Iteration 127, loss = 3.17309195\n",
      "Iteration 128, loss = 3.17248876\n",
      "Iteration 129, loss = 3.17190832\n",
      "Iteration 130, loss = 3.17114144\n",
      "Iteration 131, loss = 3.17042708\n",
      "Iteration 132, loss = 3.16986855\n",
      "Iteration 133, loss = 3.16921135\n",
      "Iteration 134, loss = 3.16917543\n",
      "Iteration 135, loss = 3.16829216\n",
      "Iteration 136, loss = 3.16743033\n",
      "Iteration 137, loss = 3.16678273\n",
      "Iteration 138, loss = 3.16588509\n",
      "Iteration 139, loss = 3.16520945\n",
      "Iteration 140, loss = 3.16431016\n",
      "Iteration 141, loss = 3.16369654\n",
      "Iteration 142, loss = 3.16289959\n",
      "Iteration 143, loss = 3.16285591\n",
      "Iteration 144, loss = 3.16182417\n",
      "Iteration 145, loss = 3.16113172\n",
      "Iteration 146, loss = 3.16018905\n",
      "Iteration 147, loss = 3.15964661\n",
      "Iteration 148, loss = 3.15954128\n",
      "Iteration 149, loss = 3.15870811\n",
      "Iteration 150, loss = 3.15768842\n",
      "Iteration 151, loss = 3.15741200\n",
      "Iteration 152, loss = 3.15666592\n",
      "Iteration 153, loss = 3.15571263\n",
      "Iteration 154, loss = 3.15519784\n",
      "Iteration 155, loss = 3.15404301\n",
      "Iteration 156, loss = 3.15355036\n",
      "Iteration 157, loss = 3.15310081\n",
      "Iteration 158, loss = 3.15233520\n",
      "Iteration 159, loss = 3.15167698\n",
      "Iteration 160, loss = 3.15103271\n",
      "Iteration 161, loss = 3.15032743\n",
      "Iteration 162, loss = 3.14998209\n",
      "Iteration 163, loss = 3.14923876\n",
      "Iteration 164, loss = 3.14871257\n",
      "Iteration 165, loss = 3.14798343\n",
      "Iteration 166, loss = 3.14746330\n",
      "Iteration 167, loss = 3.14687280\n",
      "Iteration 168, loss = 3.14597165\n",
      "Iteration 169, loss = 3.14518834\n",
      "Iteration 170, loss = 3.14414655\n",
      "Iteration 171, loss = 3.14389371\n",
      "Iteration 172, loss = 3.14306009\n",
      "Iteration 173, loss = 3.14259760\n",
      "Iteration 174, loss = 3.14214771\n",
      "Iteration 175, loss = 3.14134019\n",
      "Iteration 176, loss = 3.14078797\n",
      "Iteration 177, loss = 3.14008710\n",
      "Iteration 178, loss = 3.13937943\n",
      "Iteration 179, loss = 3.13858158\n",
      "Iteration 180, loss = 3.13817799\n",
      "Iteration 181, loss = 3.13728202\n",
      "Iteration 182, loss = 3.13652364\n",
      "Iteration 183, loss = 3.13610424\n",
      "Iteration 184, loss = 3.13546710\n",
      "Iteration 185, loss = 3.13481387\n",
      "Iteration 186, loss = 3.13413449\n",
      "Iteration 187, loss = 3.13354442\n",
      "Iteration 188, loss = 3.13294239\n",
      "Iteration 189, loss = 3.13218949\n",
      "Iteration 190, loss = 3.13170862\n",
      "Iteration 191, loss = 3.13118625\n",
      "Iteration 192, loss = 3.13050681\n",
      "Iteration 193, loss = 3.12982544\n",
      "Iteration 194, loss = 3.12939228\n",
      "Iteration 195, loss = 3.12921573\n",
      "Iteration 196, loss = 3.12814431\n",
      "Iteration 197, loss = 3.12739971\n",
      "Iteration 198, loss = 3.12738936\n",
      "Iteration 199, loss = 3.12670380\n",
      "Iteration 200, loss = 3.12566683\n",
      "Iteration 201, loss = 3.12490138\n",
      "Iteration 202, loss = 3.12435185\n",
      "Iteration 203, loss = 3.12377606\n",
      "Iteration 204, loss = 3.12301359\n",
      "Iteration 205, loss = 3.12265509\n",
      "Iteration 206, loss = 3.12220843\n",
      "Iteration 207, loss = 3.12172853\n",
      "Iteration 208, loss = 3.12082010\n",
      "Iteration 209, loss = 3.12013683\n",
      "Iteration 210, loss = 3.11945221\n",
      "Iteration 211, loss = 3.11890820\n",
      "Iteration 212, loss = 3.11828928\n",
      "Iteration 213, loss = 3.11761093\n",
      "Iteration 214, loss = 3.11712152\n",
      "Iteration 215, loss = 3.11656302\n",
      "Iteration 216, loss = 3.11594867\n",
      "Iteration 217, loss = 3.11566432\n",
      "Iteration 218, loss = 3.11482098\n",
      "Iteration 219, loss = 3.11451299\n",
      "Iteration 220, loss = 3.11427300\n",
      "Iteration 221, loss = 3.11377652\n",
      "Iteration 222, loss = 3.11258209\n",
      "Iteration 223, loss = 3.11197939\n",
      "Iteration 224, loss = 3.11140278\n",
      "Iteration 225, loss = 3.11115065\n",
      "Iteration 226, loss = 3.11026901\n",
      "Iteration 227, loss = 3.10969534\n",
      "Iteration 228, loss = 3.10933341\n",
      "Iteration 229, loss = 3.10878825\n",
      "Iteration 230, loss = 3.10847320\n",
      "Iteration 231, loss = 3.10791677\n",
      "Iteration 232, loss = 3.10789698\n",
      "Iteration 233, loss = 3.10687994\n",
      "Iteration 234, loss = 3.10559600\n",
      "Iteration 235, loss = 3.10511958\n",
      "Iteration 236, loss = 3.10451448\n",
      "Iteration 237, loss = 3.10336963\n",
      "Iteration 238, loss = 3.10296165\n",
      "Iteration 239, loss = 3.10248076\n",
      "Iteration 240, loss = 3.10231064\n",
      "Iteration 241, loss = 3.10194654\n",
      "Iteration 242, loss = 3.10103346\n",
      "Iteration 243, loss = 3.10031455\n",
      "Iteration 244, loss = 3.09937752\n",
      "Iteration 245, loss = 3.09890416\n",
      "Iteration 246, loss = 3.09856495\n",
      "Iteration 247, loss = 3.09803593\n",
      "Iteration 248, loss = 3.09764585\n",
      "Iteration 249, loss = 3.09691969\n",
      "Iteration 250, loss = 3.09650371\n",
      "Iteration 251, loss = 3.09600255\n",
      "Iteration 252, loss = 3.09531025\n",
      "Iteration 253, loss = 3.09490064\n",
      "Iteration 254, loss = 3.09493368\n",
      "Iteration 255, loss = 3.09364912\n",
      "Iteration 256, loss = 3.09259799\n",
      "Iteration 257, loss = 3.09265698\n",
      "Iteration 258, loss = 3.09209507\n",
      "Iteration 259, loss = 3.09148521\n",
      "Iteration 260, loss = 3.09100648\n",
      "Iteration 261, loss = 3.09037268\n",
      "Iteration 262, loss = 3.08990628\n",
      "Iteration 263, loss = 3.08882441\n",
      "Iteration 264, loss = 3.08825857\n",
      "Iteration 265, loss = 3.08770547\n",
      "Iteration 266, loss = 3.08754658\n",
      "Iteration 267, loss = 3.08705416\n",
      "Iteration 268, loss = 3.08615129\n",
      "Iteration 269, loss = 3.08574182\n",
      "Iteration 270, loss = 3.08521057\n",
      "Iteration 271, loss = 3.08465186\n",
      "Iteration 272, loss = 3.08438181\n",
      "Iteration 273, loss = 3.08382845\n",
      "Iteration 274, loss = 3.08347912\n",
      "Iteration 275, loss = 3.08253404\n",
      "Iteration 276, loss = 3.08231061\n",
      "Iteration 277, loss = 3.08119898\n",
      "Iteration 278, loss = 3.08083646\n",
      "Iteration 279, loss = 3.08070252\n",
      "Iteration 280, loss = 3.07963974\n",
      "Iteration 281, loss = 3.07947854\n",
      "Iteration 282, loss = 3.07877099\n",
      "Iteration 283, loss = 3.07826352\n",
      "Iteration 284, loss = 3.07742118\n",
      "Iteration 285, loss = 3.07723466\n",
      "Iteration 286, loss = 3.07718203\n",
      "Iteration 287, loss = 3.07636531\n",
      "Iteration 288, loss = 3.07590985\n",
      "Iteration 289, loss = 3.07513507\n",
      "Iteration 290, loss = 3.07439722\n",
      "Iteration 291, loss = 3.07375402\n",
      "Iteration 292, loss = 3.07301937\n",
      "Iteration 293, loss = 3.07301081\n",
      "Iteration 294, loss = 3.07255260\n",
      "Iteration 295, loss = 3.07213688\n",
      "Iteration 296, loss = 3.07106847\n",
      "Iteration 297, loss = 3.07092538\n",
      "Iteration 298, loss = 3.07020993\n",
      "Iteration 299, loss = 3.06991370\n",
      "Iteration 300, loss = 3.06980690\n",
      "Iteration 301, loss = 3.06873605\n",
      "Iteration 302, loss = 3.06844691\n",
      "Iteration 303, loss = 3.06801419\n",
      "Iteration 304, loss = 3.06812207\n",
      "Iteration 305, loss = 3.06673385\n",
      "Iteration 306, loss = 3.06611386\n",
      "Iteration 307, loss = 3.06547051\n",
      "Iteration 308, loss = 3.06480912\n",
      "Iteration 309, loss = 3.06471111\n",
      "Iteration 310, loss = 3.06402337\n",
      "Iteration 311, loss = 3.06371848\n",
      "Iteration 312, loss = 3.06295469\n",
      "Iteration 313, loss = 3.06289812\n",
      "Iteration 314, loss = 3.06236174\n",
      "Iteration 315, loss = 3.06218375\n",
      "Iteration 316, loss = 3.06152373\n",
      "Iteration 317, loss = 3.06125335\n",
      "Iteration 318, loss = 3.06077889\n",
      "Iteration 319, loss = 3.05994509\n",
      "Iteration 320, loss = 3.05892817\n",
      "Iteration 321, loss = 3.05843455\n",
      "Iteration 322, loss = 3.05870911\n",
      "Iteration 323, loss = 3.05816698\n",
      "Iteration 324, loss = 3.05707546\n",
      "Iteration 325, loss = 3.05605876\n",
      "Iteration 326, loss = 3.05572266\n",
      "Iteration 327, loss = 3.05469005\n",
      "Iteration 328, loss = 3.05431587\n",
      "Iteration 329, loss = 3.05439560\n",
      "Iteration 330, loss = 3.05351180\n",
      "Iteration 331, loss = 3.05317639\n",
      "Iteration 332, loss = 3.05245651\n",
      "Iteration 333, loss = 3.05212635\n",
      "Iteration 334, loss = 3.05166699\n",
      "Iteration 335, loss = 3.05061498\n",
      "Iteration 336, loss = 3.05012361\n",
      "Iteration 337, loss = 3.04989414\n",
      "Iteration 338, loss = 3.04944070\n",
      "Iteration 339, loss = 3.04897621\n",
      "Iteration 340, loss = 3.04846820\n",
      "Iteration 341, loss = 3.04770543\n",
      "Iteration 342, loss = 3.04727247\n",
      "Iteration 343, loss = 3.04695622\n",
      "Iteration 344, loss = 3.04644542\n",
      "Iteration 345, loss = 3.04536539\n",
      "Iteration 346, loss = 3.04535077\n",
      "Iteration 347, loss = 3.04508103\n",
      "Iteration 348, loss = 3.04451918\n",
      "Iteration 349, loss = 3.04345740\n",
      "Iteration 350, loss = 3.04338352\n",
      "Iteration 351, loss = 3.04249422\n",
      "Iteration 352, loss = 3.04210741\n",
      "Iteration 353, loss = 3.04251402\n",
      "Iteration 354, loss = 3.04171884\n",
      "Iteration 355, loss = 3.04067286\n",
      "Iteration 356, loss = 3.04007481\n",
      "Iteration 357, loss = 3.04049766\n",
      "Iteration 358, loss = 3.04012159\n",
      "Iteration 359, loss = 3.03916582\n",
      "Iteration 360, loss = 3.03784575\n",
      "Iteration 361, loss = 3.03763654\n",
      "Iteration 362, loss = 3.03706957\n",
      "Iteration 363, loss = 3.03672012\n",
      "Iteration 364, loss = 3.03586960\n",
      "Iteration 365, loss = 3.03553434\n",
      "Iteration 366, loss = 3.03514051\n",
      "Iteration 367, loss = 3.03461042\n",
      "Iteration 368, loss = 3.03417175\n",
      "Iteration 369, loss = 3.03378043\n",
      "Iteration 370, loss = 3.03334768\n",
      "Iteration 371, loss = 3.03277546\n",
      "Iteration 372, loss = 3.03175279\n",
      "Iteration 373, loss = 3.03126077\n",
      "Iteration 374, loss = 3.03082092\n",
      "Iteration 375, loss = 3.03072477\n",
      "Iteration 376, loss = 3.03042989\n",
      "Iteration 377, loss = 3.03012205\n",
      "Iteration 378, loss = 3.02895426\n",
      "Iteration 379, loss = 3.02833365\n",
      "Iteration 380, loss = 3.02810447\n",
      "Iteration 381, loss = 3.02756069\n",
      "Iteration 382, loss = 3.02718173\n",
      "Iteration 383, loss = 3.02641398\n",
      "Iteration 384, loss = 3.02610570\n",
      "Iteration 385, loss = 3.02601257\n",
      "Iteration 386, loss = 3.02480399\n",
      "Iteration 387, loss = 3.02435777\n",
      "Iteration 388, loss = 3.02398906\n",
      "Iteration 389, loss = 3.02321328\n",
      "Iteration 390, loss = 3.02260572\n",
      "Iteration 391, loss = 3.02311863\n",
      "Iteration 392, loss = 3.02216521\n",
      "Iteration 393, loss = 3.02128978\n",
      "Iteration 394, loss = 3.02059543\n",
      "Iteration 395, loss = 3.02031870\n",
      "Iteration 396, loss = 3.01984149\n",
      "Iteration 397, loss = 3.01937858\n",
      "Iteration 398, loss = 3.01929408\n",
      "Iteration 399, loss = 3.01868283\n",
      "Iteration 400, loss = 3.01832249\n",
      "Iteration 401, loss = 3.01737625\n",
      "Iteration 402, loss = 3.01696383\n",
      "Iteration 403, loss = 3.01683771\n",
      "Iteration 404, loss = 3.01663898\n",
      "Iteration 405, loss = 3.01632602\n",
      "Iteration 406, loss = 3.01525198\n",
      "Iteration 407, loss = 3.01470053\n",
      "Iteration 408, loss = 3.01427614\n",
      "Iteration 409, loss = 3.01354689\n",
      "Iteration 410, loss = 3.01300237\n",
      "Iteration 411, loss = 3.01280180\n",
      "Iteration 412, loss = 3.01229168\n",
      "Iteration 413, loss = 3.01163673\n",
      "Iteration 414, loss = 3.01116328\n",
      "Iteration 415, loss = 3.01048967\n",
      "Iteration 416, loss = 3.01025762\n",
      "Iteration 417, loss = 3.00952399\n",
      "Iteration 418, loss = 3.00901681\n",
      "Iteration 419, loss = 3.00868162\n",
      "Iteration 420, loss = 3.00816914\n",
      "Iteration 421, loss = 3.00774928\n",
      "Iteration 422, loss = 3.00719628\n",
      "Iteration 423, loss = 3.00666622\n",
      "Iteration 424, loss = 3.00625624\n",
      "Iteration 425, loss = 3.00563538\n",
      "Iteration 426, loss = 3.00497168\n",
      "Iteration 427, loss = 3.00463899\n",
      "Iteration 428, loss = 3.00393269\n",
      "Iteration 429, loss = 3.00379674\n",
      "Iteration 430, loss = 3.00355052\n",
      "Iteration 431, loss = 3.00334613\n",
      "Iteration 432, loss = 3.00229496\n",
      "Iteration 433, loss = 3.00141061\n",
      "Iteration 434, loss = 3.00100852\n",
      "Iteration 435, loss = 3.00021318\n",
      "Iteration 436, loss = 2.99981061\n",
      "Iteration 437, loss = 2.99942578\n",
      "Iteration 438, loss = 2.99893365\n",
      "Iteration 439, loss = 2.99875543\n",
      "Iteration 440, loss = 2.99837438\n",
      "Iteration 441, loss = 2.99750447\n",
      "Iteration 442, loss = 2.99695108\n",
      "Iteration 443, loss = 2.99742320\n",
      "Iteration 444, loss = 2.99631122\n",
      "Iteration 445, loss = 2.99546135\n",
      "Iteration 446, loss = 2.99515475\n",
      "Iteration 447, loss = 2.99488612\n",
      "Iteration 448, loss = 2.99493713\n",
      "Iteration 449, loss = 2.99413279\n",
      "Iteration 450, loss = 2.99334498\n",
      "Iteration 451, loss = 2.99308314\n",
      "Iteration 452, loss = 2.99302016\n",
      "Iteration 453, loss = 2.99236195\n",
      "Iteration 454, loss = 2.99250218\n",
      "Iteration 455, loss = 2.99208333\n",
      "Iteration 456, loss = 2.99076510\n",
      "Iteration 457, loss = 2.99038927\n",
      "Iteration 458, loss = 2.98962919\n",
      "Iteration 459, loss = 2.98912421\n",
      "Iteration 460, loss = 2.98910832\n",
      "Iteration 461, loss = 2.98821483\n",
      "Iteration 462, loss = 2.98773347\n",
      "Iteration 463, loss = 2.98759039\n",
      "Iteration 464, loss = 2.98677625\n",
      "Iteration 465, loss = 2.98643476\n",
      "Iteration 466, loss = 2.98606351\n",
      "Iteration 467, loss = 2.98593833\n",
      "Iteration 468, loss = 2.98533519\n",
      "Iteration 469, loss = 2.98499792\n",
      "Iteration 470, loss = 2.98402953\n",
      "Iteration 471, loss = 2.98372885\n",
      "Iteration 472, loss = 2.98322088\n",
      "Iteration 473, loss = 2.98292962\n",
      "Iteration 474, loss = 2.98274141\n",
      "Iteration 475, loss = 2.98204972\n",
      "Iteration 476, loss = 2.98183197\n",
      "Iteration 477, loss = 2.98108690\n",
      "Iteration 478, loss = 2.98062760\n",
      "Iteration 479, loss = 2.98017332\n",
      "Iteration 480, loss = 2.97959440\n",
      "Iteration 481, loss = 2.97900189\n",
      "Iteration 482, loss = 2.97873734\n",
      "Iteration 483, loss = 2.97845842\n",
      "Iteration 484, loss = 2.97824789\n",
      "Iteration 485, loss = 2.97724116\n",
      "Iteration 486, loss = 2.97730691\n",
      "Iteration 487, loss = 2.97687356\n",
      "Iteration 488, loss = 2.97594781\n",
      "Iteration 489, loss = 2.97544926\n",
      "Iteration 490, loss = 2.97478534\n",
      "Iteration 491, loss = 2.97473456\n",
      "Iteration 492, loss = 2.97475083\n",
      "Iteration 493, loss = 2.97360781\n",
      "Iteration 494, loss = 2.97319674\n",
      "Iteration 495, loss = 2.97290952\n",
      "Iteration 496, loss = 2.97297715\n",
      "Iteration 497, loss = 2.97308567\n",
      "Iteration 498, loss = 2.97210018\n",
      "Iteration 499, loss = 2.97127932\n",
      "Iteration 500, loss = 2.97095716\n",
      "Iteration 501, loss = 2.97065495\n",
      "Iteration 502, loss = 2.96990856\n",
      "Iteration 503, loss = 2.96957862\n",
      "Iteration 504, loss = 2.96913534\n",
      "Iteration 505, loss = 2.96873446\n",
      "Iteration 506, loss = 2.96806193\n",
      "Iteration 507, loss = 2.96780954\n",
      "Iteration 508, loss = 2.96705918\n",
      "Iteration 509, loss = 2.96634178\n",
      "Iteration 510, loss = 2.96700033\n",
      "Iteration 511, loss = 2.96613457\n",
      "Iteration 512, loss = 2.96537206\n",
      "Iteration 513, loss = 2.96510881\n",
      "Iteration 514, loss = 2.96425781\n",
      "Iteration 515, loss = 2.96440047\n",
      "Iteration 516, loss = 2.96352852\n",
      "Iteration 517, loss = 2.96277623\n",
      "Iteration 518, loss = 2.96241679\n",
      "Iteration 519, loss = 2.96250183\n",
      "Iteration 520, loss = 2.96304706\n",
      "Iteration 521, loss = 2.96230996\n",
      "Iteration 522, loss = 2.96112101\n",
      "Iteration 523, loss = 2.96193468\n",
      "Iteration 524, loss = 2.96126338\n",
      "Iteration 525, loss = 2.96034649\n",
      "Iteration 526, loss = 2.95936116\n",
      "Iteration 527, loss = 2.95903545\n",
      "Iteration 528, loss = 2.95823328\n",
      "Iteration 529, loss = 2.95771046\n",
      "Iteration 530, loss = 2.95764217\n",
      "Iteration 531, loss = 2.95710795\n",
      "Iteration 532, loss = 2.95658356\n",
      "Iteration 533, loss = 2.95648090\n",
      "Iteration 534, loss = 2.95634246\n",
      "Iteration 535, loss = 2.95612056\n",
      "Iteration 536, loss = 2.95566968\n",
      "Iteration 537, loss = 2.95473274\n",
      "Iteration 538, loss = 2.95386415\n",
      "Iteration 539, loss = 2.95366127\n",
      "Iteration 540, loss = 2.95305550\n",
      "Iteration 541, loss = 2.95251684\n",
      "Iteration 542, loss = 2.95236600\n",
      "Iteration 543, loss = 2.95202775\n",
      "Iteration 544, loss = 2.95126499\n",
      "Iteration 545, loss = 2.95118256\n",
      "Iteration 546, loss = 2.95080429\n",
      "Iteration 547, loss = 2.95002531\n",
      "Iteration 548, loss = 2.95038852\n",
      "Iteration 549, loss = 2.94972950\n",
      "Iteration 550, loss = 2.94876893\n",
      "Iteration 551, loss = 2.94848579\n",
      "Iteration 552, loss = 2.94808494\n",
      "Iteration 553, loss = 2.94775886\n",
      "Iteration 554, loss = 2.94767920\n",
      "Iteration 555, loss = 2.94666093\n",
      "Iteration 556, loss = 2.94628239\n",
      "Iteration 557, loss = 2.94553245\n",
      "Iteration 558, loss = 2.94560406\n",
      "Iteration 559, loss = 2.94505680\n",
      "Iteration 560, loss = 2.94508038\n",
      "Iteration 561, loss = 2.94409951\n",
      "Iteration 562, loss = 2.94405656\n",
      "Iteration 563, loss = 2.94340263\n",
      "Iteration 564, loss = 2.94302290\n",
      "Iteration 565, loss = 2.94286573\n",
      "Iteration 566, loss = 2.94201711\n",
      "Iteration 567, loss = 2.94162439\n",
      "Iteration 568, loss = 2.94137318\n",
      "Iteration 569, loss = 2.94072763\n",
      "Iteration 570, loss = 2.94029961\n",
      "Iteration 571, loss = 2.94008573\n",
      "Iteration 572, loss = 2.93951711\n",
      "Iteration 573, loss = 2.93932400\n",
      "Iteration 574, loss = 2.93860438\n",
      "Iteration 575, loss = 2.93857965\n",
      "Iteration 576, loss = 2.93738451\n",
      "Iteration 577, loss = 2.93747187\n",
      "Iteration 578, loss = 2.93689436\n",
      "Iteration 579, loss = 2.93625775\n",
      "Iteration 580, loss = 2.93638103\n",
      "Iteration 581, loss = 2.93572881\n",
      "Iteration 582, loss = 2.93489899\n",
      "Iteration 583, loss = 2.93464358\n",
      "Iteration 584, loss = 2.93423686\n",
      "Iteration 585, loss = 2.93399726\n",
      "Iteration 586, loss = 2.93332916\n",
      "Iteration 587, loss = 2.93338871\n",
      "Iteration 588, loss = 2.93319327\n",
      "Iteration 589, loss = 2.93219387\n",
      "Iteration 590, loss = 2.93225345\n",
      "Iteration 591, loss = 2.93252413\n",
      "Iteration 592, loss = 2.93168689\n",
      "Iteration 593, loss = 2.93114804\n",
      "Iteration 594, loss = 2.93034178\n",
      "Iteration 595, loss = 2.92980340\n",
      "Iteration 596, loss = 2.93052192\n",
      "Iteration 597, loss = 2.93068034\n",
      "Iteration 598, loss = 2.92874079\n",
      "Iteration 599, loss = 2.92779385\n",
      "Iteration 600, loss = 2.92744283\n",
      "Iteration 601, loss = 2.92744991\n",
      "Iteration 602, loss = 2.92693855\n",
      "Iteration 603, loss = 2.92648961\n",
      "Iteration 604, loss = 2.92610772\n",
      "Iteration 605, loss = 2.92552264\n",
      "Iteration 606, loss = 2.92572533\n",
      "Iteration 607, loss = 2.92530556\n",
      "Iteration 608, loss = 2.92516486\n",
      "Iteration 609, loss = 2.92447324\n",
      "Iteration 610, loss = 2.92410003\n",
      "Iteration 611, loss = 2.92361075\n",
      "Iteration 612, loss = 2.92318945\n",
      "Iteration 613, loss = 2.92258807\n",
      "Iteration 614, loss = 2.92224289\n",
      "Iteration 615, loss = 2.92159043\n",
      "Iteration 616, loss = 2.92112298\n",
      "Iteration 617, loss = 2.92093124\n",
      "Iteration 618, loss = 2.92019418\n",
      "Iteration 619, loss = 2.92001494\n",
      "Iteration 620, loss = 2.91952800\n",
      "Iteration 621, loss = 2.91914980\n",
      "Iteration 622, loss = 2.91892504\n",
      "Iteration 623, loss = 2.91837449\n",
      "Iteration 624, loss = 2.91811436\n",
      "Iteration 625, loss = 2.91751818\n",
      "Iteration 626, loss = 2.91719820\n",
      "Iteration 627, loss = 2.91687338\n",
      "Iteration 628, loss = 2.91616806\n",
      "Iteration 629, loss = 2.91568322\n",
      "Iteration 630, loss = 2.91498080\n",
      "Iteration 631, loss = 2.91449951\n",
      "Iteration 632, loss = 2.91490974\n",
      "Iteration 633, loss = 2.91448001\n",
      "Iteration 634, loss = 2.91399695\n",
      "Iteration 635, loss = 2.91365706\n",
      "Iteration 636, loss = 2.91341703\n",
      "Iteration 637, loss = 2.91255562\n",
      "Iteration 638, loss = 2.91270770\n",
      "Iteration 639, loss = 2.91225795\n",
      "Iteration 640, loss = 2.91150561\n",
      "Iteration 641, loss = 2.91090536\n",
      "Iteration 642, loss = 2.91060429\n",
      "Iteration 643, loss = 2.91059509\n",
      "Iteration 644, loss = 2.90986752\n",
      "Iteration 645, loss = 2.91004967\n",
      "Iteration 646, loss = 2.91021202\n",
      "Iteration 647, loss = 2.90935084\n",
      "Iteration 648, loss = 2.90838369\n",
      "Iteration 649, loss = 2.90811930\n",
      "Iteration 650, loss = 2.90759956\n",
      "Iteration 651, loss = 2.90701566\n",
      "Iteration 652, loss = 2.90721931\n",
      "Iteration 653, loss = 2.90675446\n",
      "Iteration 654, loss = 2.90620237\n",
      "Iteration 655, loss = 2.90562759\n",
      "Iteration 656, loss = 2.90512997\n",
      "Iteration 657, loss = 2.90481208\n",
      "Iteration 658, loss = 2.90416050\n",
      "Iteration 659, loss = 2.90393593\n",
      "Iteration 660, loss = 2.90405308\n",
      "Iteration 661, loss = 2.90321579\n",
      "Iteration 662, loss = 2.90265569\n",
      "Iteration 663, loss = 2.90256469\n",
      "Iteration 664, loss = 2.90275441\n",
      "Iteration 665, loss = 2.90213616\n",
      "Iteration 666, loss = 2.90138239\n",
      "Iteration 667, loss = 2.90115780\n",
      "Iteration 668, loss = 2.90059850\n",
      "Iteration 669, loss = 2.90107245\n",
      "Iteration 670, loss = 2.90045514\n",
      "Iteration 671, loss = 2.89975497\n",
      "Iteration 672, loss = 2.89961498\n",
      "Iteration 673, loss = 2.89920888\n",
      "Iteration 674, loss = 2.89868084\n",
      "Iteration 675, loss = 2.89820649\n",
      "Iteration 676, loss = 2.89763631\n",
      "Iteration 677, loss = 2.89708102\n",
      "Iteration 678, loss = 2.89694351\n",
      "Iteration 679, loss = 2.89653540\n",
      "Iteration 680, loss = 2.89636896\n",
      "Iteration 681, loss = 2.89552972\n",
      "Iteration 682, loss = 2.89519067\n",
      "Iteration 683, loss = 2.89511647\n",
      "Iteration 684, loss = 2.89483942\n",
      "Iteration 685, loss = 2.89435163\n",
      "Iteration 686, loss = 2.89389724\n",
      "Iteration 687, loss = 2.89350958\n",
      "Iteration 688, loss = 2.89311385\n",
      "Iteration 689, loss = 2.89319765\n",
      "Iteration 690, loss = 2.89294782\n",
      "Iteration 691, loss = 2.89249410\n",
      "Iteration 692, loss = 2.89153480\n",
      "Iteration 693, loss = 2.89140391\n",
      "Iteration 694, loss = 2.89068647\n",
      "Iteration 695, loss = 2.89091244\n",
      "Iteration 696, loss = 2.89000576\n",
      "Iteration 697, loss = 2.88952759\n",
      "Iteration 698, loss = 2.88882133\n",
      "Iteration 699, loss = 2.88944022\n",
      "Iteration 700, loss = 2.88839522\n",
      "Iteration 701, loss = 2.88898812\n",
      "Iteration 702, loss = 2.88922594\n",
      "Iteration 703, loss = 2.88811984\n",
      "Iteration 704, loss = 2.88691098\n",
      "Iteration 705, loss = 2.88691893\n",
      "Iteration 706, loss = 2.88694889\n",
      "Iteration 707, loss = 2.88648509\n",
      "Iteration 708, loss = 2.88590241\n",
      "Iteration 709, loss = 2.88559639\n",
      "Iteration 710, loss = 2.88469975\n",
      "Iteration 711, loss = 2.88482637\n",
      "Iteration 712, loss = 2.88431731\n",
      "Iteration 713, loss = 2.88355467\n",
      "Iteration 714, loss = 2.88377290\n",
      "Iteration 715, loss = 2.88352019\n",
      "Iteration 716, loss = 2.88337857\n",
      "Iteration 717, loss = 2.88238817\n",
      "Iteration 718, loss = 2.88211672\n",
      "Iteration 719, loss = 2.88159856\n",
      "Iteration 720, loss = 2.88143913\n",
      "Iteration 721, loss = 2.88125684\n",
      "Iteration 722, loss = 2.88099984\n",
      "Iteration 723, loss = 2.88056327\n",
      "Iteration 724, loss = 2.87980795\n",
      "Iteration 725, loss = 2.87973744\n",
      "Iteration 726, loss = 2.87912566\n",
      "Iteration 727, loss = 2.87889300\n",
      "Iteration 728, loss = 2.87880159\n",
      "Iteration 729, loss = 2.87852945\n",
      "Iteration 730, loss = 2.87732526\n",
      "Iteration 731, loss = 2.87723353\n",
      "Iteration 732, loss = 2.87733318\n",
      "Iteration 733, loss = 2.87699353\n",
      "Iteration 734, loss = 2.87654669\n",
      "Iteration 735, loss = 2.87635040\n",
      "Iteration 736, loss = 2.87592912\n",
      "Iteration 737, loss = 2.87554559\n",
      "Iteration 738, loss = 2.87533525\n",
      "Iteration 739, loss = 2.87465708\n",
      "Iteration 740, loss = 2.87486529\n",
      "Iteration 741, loss = 2.87365615\n",
      "Iteration 742, loss = 2.87362266\n",
      "Iteration 743, loss = 2.87322006\n",
      "Iteration 744, loss = 2.87330490\n",
      "Iteration 745, loss = 2.87308957\n",
      "Iteration 746, loss = 2.87249583\n",
      "Iteration 747, loss = 2.87204633\n",
      "Iteration 748, loss = 2.87192665\n",
      "Iteration 749, loss = 2.87182712\n",
      "Iteration 750, loss = 2.87148502\n",
      "Iteration 751, loss = 2.87062067\n",
      "Iteration 752, loss = 2.86977980\n",
      "Iteration 753, loss = 2.86960987\n",
      "Iteration 754, loss = 2.86983675\n",
      "Iteration 755, loss = 2.86924485\n",
      "Iteration 756, loss = 2.86931454\n",
      "Iteration 757, loss = 2.86846552\n",
      "Iteration 758, loss = 2.86889540\n",
      "Iteration 759, loss = 2.86814352\n",
      "Iteration 760, loss = 2.86724383\n",
      "Iteration 761, loss = 2.86738520\n",
      "Iteration 762, loss = 2.86703111\n",
      "Iteration 763, loss = 2.86667333\n",
      "Iteration 764, loss = 2.86616706\n",
      "Iteration 765, loss = 2.86534888\n",
      "Iteration 766, loss = 2.86488610\n",
      "Iteration 767, loss = 2.86466170\n",
      "Iteration 768, loss = 2.86405525\n",
      "Iteration 769, loss = 2.86376316\n",
      "Iteration 770, loss = 2.86407425\n",
      "Iteration 771, loss = 2.86295490\n",
      "Iteration 772, loss = 2.86335641\n",
      "Iteration 773, loss = 2.86192903\n",
      "Iteration 774, loss = 2.86288123\n",
      "Iteration 775, loss = 2.86280192\n",
      "Iteration 776, loss = 2.86185298\n",
      "Iteration 777, loss = 2.86096827\n",
      "Iteration 778, loss = 2.86027819\n",
      "Iteration 779, loss = 2.86007097\n",
      "Iteration 780, loss = 2.86009989\n",
      "Iteration 781, loss = 2.85970575\n",
      "Iteration 782, loss = 2.85938005\n",
      "Iteration 783, loss = 2.85850249\n",
      "Iteration 784, loss = 2.85848017\n",
      "Iteration 785, loss = 2.85824773\n",
      "Iteration 786, loss = 2.85776261\n",
      "Iteration 787, loss = 2.85705547\n",
      "Iteration 788, loss = 2.85714540\n",
      "Iteration 789, loss = 2.85668268\n",
      "Iteration 790, loss = 2.85612892\n",
      "Iteration 791, loss = 2.85612288\n",
      "Iteration 792, loss = 2.85565463\n",
      "Iteration 793, loss = 2.85538296\n",
      "Iteration 794, loss = 2.85531541\n",
      "Iteration 795, loss = 2.85476073\n",
      "Iteration 796, loss = 2.85500513\n",
      "Iteration 797, loss = 2.85439184\n",
      "Iteration 798, loss = 2.85418777\n",
      "Iteration 799, loss = 2.85335900\n",
      "Iteration 800, loss = 2.85313184\n",
      "Iteration 801, loss = 2.85309276\n",
      "Iteration 802, loss = 2.85265981\n",
      "Iteration 803, loss = 2.85202627\n",
      "Iteration 804, loss = 2.85181033\n",
      "Iteration 805, loss = 2.85112106\n",
      "Iteration 806, loss = 2.85127743\n",
      "Iteration 807, loss = 2.85123313\n",
      "Iteration 808, loss = 2.85156468\n",
      "Iteration 809, loss = 2.85068747\n",
      "Iteration 810, loss = 2.85063287\n",
      "Iteration 811, loss = 2.85002748\n",
      "Iteration 812, loss = 2.84880856\n",
      "Iteration 813, loss = 2.84852477\n",
      "Iteration 814, loss = 2.84806461\n",
      "Iteration 815, loss = 2.84829252\n",
      "Iteration 816, loss = 2.84823550\n",
      "Iteration 817, loss = 2.84744473\n",
      "Iteration 818, loss = 2.84725028\n",
      "Iteration 819, loss = 2.84673074\n",
      "Iteration 820, loss = 2.84653902\n",
      "Iteration 821, loss = 2.84627063\n",
      "Iteration 822, loss = 2.84605237\n",
      "Iteration 823, loss = 2.84544406\n",
      "Iteration 824, loss = 2.84500724\n",
      "Iteration 825, loss = 2.84466016\n",
      "Iteration 826, loss = 2.84510162\n",
      "Iteration 827, loss = 2.84397600\n",
      "Iteration 828, loss = 2.84368927\n",
      "Iteration 829, loss = 2.84376883\n",
      "Iteration 830, loss = 2.84332720\n",
      "Iteration 831, loss = 2.84313591\n",
      "Iteration 832, loss = 2.84286927\n",
      "Iteration 833, loss = 2.84229944\n",
      "Iteration 834, loss = 2.84209578\n",
      "Iteration 835, loss = 2.84157151\n",
      "Iteration 836, loss = 2.84166863\n",
      "Iteration 837, loss = 2.84189248\n",
      "Iteration 838, loss = 2.84171617\n",
      "Iteration 839, loss = 2.84132117\n",
      "Iteration 840, loss = 2.84070158\n",
      "Iteration 841, loss = 2.84010289\n",
      "Iteration 842, loss = 2.83971380\n",
      "Iteration 843, loss = 2.83970581\n",
      "Iteration 844, loss = 2.83979061\n",
      "Iteration 845, loss = 2.83931274\n",
      "Iteration 846, loss = 2.83912552\n",
      "Iteration 847, loss = 2.83804227\n",
      "Iteration 848, loss = 2.83718960\n",
      "Iteration 849, loss = 2.83759282\n",
      "Iteration 850, loss = 2.83808284\n",
      "Iteration 851, loss = 2.83795941\n",
      "Iteration 852, loss = 2.83669772\n",
      "Iteration 853, loss = 2.83628166\n",
      "Iteration 854, loss = 2.83568796\n",
      "Iteration 855, loss = 2.83594609\n",
      "Iteration 856, loss = 2.83534634\n",
      "Iteration 857, loss = 2.83412479\n",
      "Iteration 858, loss = 2.83409146\n",
      "Iteration 859, loss = 2.83405530\n",
      "Iteration 860, loss = 2.83362129\n",
      "Iteration 861, loss = 2.83307564\n",
      "Iteration 862, loss = 2.83274235\n",
      "Iteration 863, loss = 2.83223726\n",
      "Iteration 864, loss = 2.83247206\n",
      "Iteration 865, loss = 2.83204243\n",
      "Iteration 866, loss = 2.83140209\n",
      "Iteration 867, loss = 2.83139825\n",
      "Iteration 868, loss = 2.83168769\n",
      "Iteration 869, loss = 2.83094738\n",
      "Iteration 870, loss = 2.83013448\n",
      "Iteration 871, loss = 2.82989478\n",
      "Iteration 872, loss = 2.82970022\n",
      "Iteration 873, loss = 2.82919449\n",
      "Iteration 874, loss = 2.82897421\n",
      "Iteration 875, loss = 2.82819323\n",
      "Iteration 876, loss = 2.82812594\n",
      "Iteration 877, loss = 2.82790415\n",
      "Iteration 878, loss = 2.82752021\n",
      "Iteration 879, loss = 2.82736050\n",
      "Iteration 880, loss = 2.82703998\n",
      "Iteration 881, loss = 2.82681121\n",
      "Iteration 882, loss = 2.82660091\n",
      "Iteration 883, loss = 2.82573461\n",
      "Iteration 884, loss = 2.82546859\n",
      "Iteration 885, loss = 2.82542985\n",
      "Iteration 886, loss = 2.82471305\n",
      "Iteration 887, loss = 2.82461571\n",
      "Iteration 888, loss = 2.82460073\n",
      "Iteration 889, loss = 2.82428938\n",
      "Iteration 890, loss = 2.82396874\n",
      "Iteration 891, loss = 2.82334743\n",
      "Iteration 892, loss = 2.82346037\n",
      "Iteration 893, loss = 2.82312979\n",
      "Iteration 894, loss = 2.82284411\n",
      "Iteration 895, loss = 2.82235753\n",
      "Iteration 896, loss = 2.82206729\n",
      "Iteration 897, loss = 2.82149348\n",
      "Iteration 898, loss = 2.82128425\n",
      "Iteration 899, loss = 2.82167392\n",
      "Iteration 900, loss = 2.82174567\n",
      "Iteration 901, loss = 2.82092731\n",
      "Iteration 902, loss = 2.81990543\n",
      "Iteration 903, loss = 2.81983274\n",
      "Iteration 904, loss = 2.81924235\n",
      "Iteration 905, loss = 2.81853929\n",
      "Iteration 906, loss = 2.81866738\n",
      "Iteration 907, loss = 2.81813317\n",
      "Iteration 908, loss = 2.81796590\n",
      "Iteration 909, loss = 2.81744648\n",
      "Iteration 910, loss = 2.81713985\n",
      "Iteration 911, loss = 2.81737495\n",
      "Iteration 912, loss = 2.81753847\n",
      "Iteration 913, loss = 2.81796660\n",
      "Iteration 914, loss = 2.81659388\n",
      "Iteration 915, loss = 2.81573034\n",
      "Iteration 916, loss = 2.81559435\n",
      "Iteration 917, loss = 2.81502605\n",
      "Iteration 918, loss = 2.81487518\n",
      "Iteration 919, loss = 2.81453645\n",
      "Iteration 920, loss = 2.81463345\n",
      "Iteration 921, loss = 2.81477123\n",
      "Iteration 922, loss = 2.81406747\n",
      "Iteration 923, loss = 2.81316101\n",
      "Iteration 924, loss = 2.81304909\n",
      "Iteration 925, loss = 2.81273374\n",
      "Iteration 926, loss = 2.81200680\n",
      "Iteration 927, loss = 2.81196756\n",
      "Iteration 928, loss = 2.81143238\n",
      "Iteration 929, loss = 2.81145411\n",
      "Iteration 930, loss = 2.81166018\n",
      "Iteration 931, loss = 2.81097067\n",
      "Iteration 932, loss = 2.81022547\n",
      "Iteration 933, loss = 2.81025580\n",
      "Iteration 934, loss = 2.80980226\n",
      "Iteration 935, loss = 2.81028396\n",
      "Iteration 936, loss = 2.80945859\n",
      "Iteration 937, loss = 2.80953312\n",
      "Iteration 938, loss = 2.80913130\n",
      "Iteration 939, loss = 2.80895818\n",
      "Iteration 940, loss = 2.80868022\n",
      "Iteration 941, loss = 2.80851991\n",
      "Iteration 942, loss = 2.80751474\n",
      "Iteration 943, loss = 2.80719124\n",
      "Iteration 944, loss = 2.80647201\n",
      "Iteration 945, loss = 2.80656542\n",
      "Iteration 946, loss = 2.80629222\n",
      "Iteration 947, loss = 2.80599387\n",
      "Iteration 948, loss = 2.80578621\n",
      "Iteration 949, loss = 2.80561619\n",
      "Iteration 950, loss = 2.80518921\n",
      "Iteration 951, loss = 2.80451000\n",
      "Iteration 952, loss = 2.80396199\n",
      "Iteration 953, loss = 2.80504563\n",
      "Iteration 954, loss = 2.80438007\n",
      "Iteration 955, loss = 2.80374307\n",
      "Iteration 956, loss = 2.80330630\n",
      "Iteration 957, loss = 2.80283076\n",
      "Iteration 958, loss = 2.80248351\n",
      "Iteration 959, loss = 2.80233414\n",
      "Iteration 960, loss = 2.80194617\n",
      "Iteration 961, loss = 2.80235231\n",
      "Iteration 962, loss = 2.80200084\n",
      "Iteration 963, loss = 2.80131734\n",
      "Iteration 964, loss = 2.80074549\n",
      "Iteration 965, loss = 2.80087047\n",
      "Iteration 966, loss = 2.80056564\n",
      "Iteration 967, loss = 2.80048517\n",
      "Iteration 968, loss = 2.80012526\n",
      "Iteration 969, loss = 2.79969068\n",
      "Iteration 970, loss = 2.79921916\n",
      "Iteration 971, loss = 2.79891371\n",
      "Iteration 972, loss = 2.79901378\n",
      "Iteration 973, loss = 2.79859017\n",
      "Iteration 974, loss = 2.79813866\n",
      "Iteration 975, loss = 2.79773525\n",
      "Iteration 976, loss = 2.79746011\n",
      "Iteration 977, loss = 2.79691067\n",
      "Iteration 978, loss = 2.79662274\n",
      "Iteration 979, loss = 2.79669046\n",
      "Iteration 980, loss = 2.79590778\n",
      "Iteration 981, loss = 2.79529251\n",
      "Iteration 982, loss = 2.79575262\n",
      "Iteration 983, loss = 2.79582441\n",
      "Iteration 984, loss = 2.79450123\n",
      "Iteration 985, loss = 2.79461425\n",
      "Iteration 986, loss = 2.79410582\n",
      "Iteration 987, loss = 2.79385063\n",
      "Iteration 988, loss = 2.79386599\n",
      "Iteration 989, loss = 2.79368613\n",
      "Iteration 990, loss = 2.79308945\n",
      "Iteration 991, loss = 2.79290004\n",
      "Iteration 992, loss = 2.79276739\n",
      "Iteration 993, loss = 2.79234191\n",
      "Iteration 994, loss = 2.79222374\n",
      "Iteration 995, loss = 2.79211347\n",
      "Iteration 996, loss = 2.79198236\n",
      "Iteration 997, loss = 2.79119763\n",
      "Iteration 998, loss = 2.79125627\n",
      "Iteration 999, loss = 2.79069579\n",
      "Iteration 1000, loss = 2.79018034\n",
      "Iteration 1001, loss = 2.78934747\n",
      "Iteration 1002, loss = 2.78989890\n",
      "Iteration 1003, loss = 2.78928867\n",
      "Iteration 1004, loss = 2.78899770\n",
      "Iteration 1005, loss = 2.78918183\n",
      "Iteration 1006, loss = 2.78840463\n",
      "Iteration 1007, loss = 2.78795691\n",
      "Iteration 1008, loss = 2.78836224\n",
      "Iteration 1009, loss = 2.78774784\n",
      "Iteration 1010, loss = 2.78700022\n",
      "Iteration 1011, loss = 2.78734708\n",
      "Iteration 1012, loss = 2.78686588\n",
      "Iteration 1013, loss = 2.78652454\n",
      "Iteration 1014, loss = 2.78585931\n",
      "Iteration 1015, loss = 2.78571709\n",
      "Iteration 1016, loss = 2.78549741\n",
      "Iteration 1017, loss = 2.78531817\n",
      "Iteration 1018, loss = 2.78543489\n",
      "Iteration 1019, loss = 2.78443386\n",
      "Iteration 1020, loss = 2.78396319\n",
      "Iteration 1021, loss = 2.78342450\n",
      "Iteration 1022, loss = 2.78412311\n",
      "Iteration 1023, loss = 2.78380101\n",
      "Iteration 1024, loss = 2.78348581\n",
      "Iteration 1025, loss = 2.78490367\n",
      "Iteration 1026, loss = 2.78378905\n",
      "Iteration 1027, loss = 2.78248027\n",
      "Iteration 1028, loss = 2.78233430\n",
      "Iteration 1029, loss = 2.78152637\n",
      "Iteration 1030, loss = 2.78117844\n",
      "Iteration 1031, loss = 2.78120641\n",
      "Iteration 1032, loss = 2.78094425\n",
      "Iteration 1033, loss = 2.78070253\n",
      "Iteration 1034, loss = 2.78035018\n",
      "Iteration 1035, loss = 2.78039232\n",
      "Iteration 1036, loss = 2.78023925\n",
      "Iteration 1037, loss = 2.77941727\n",
      "Iteration 1038, loss = 2.77962735\n",
      "Iteration 1039, loss = 2.77894755\n",
      "Iteration 1040, loss = 2.77864224\n",
      "Iteration 1041, loss = 2.77855977\n",
      "Iteration 1042, loss = 2.77796201\n",
      "Iteration 1043, loss = 2.77828492\n",
      "Iteration 1044, loss = 2.77764007\n",
      "Iteration 1045, loss = 2.77721973\n",
      "Iteration 1046, loss = 2.77729515\n",
      "Iteration 1047, loss = 2.77687189\n",
      "Iteration 1048, loss = 2.77588081\n",
      "Iteration 1049, loss = 2.77639091\n",
      "Iteration 1050, loss = 2.77587654\n",
      "Iteration 1051, loss = 2.77567177\n",
      "Iteration 1052, loss = 2.77526217\n",
      "Iteration 1053, loss = 2.77474392\n",
      "Iteration 1054, loss = 2.77562484\n",
      "Iteration 1055, loss = 2.77521305\n",
      "Iteration 1056, loss = 2.77454591\n",
      "Iteration 1057, loss = 2.77406319\n",
      "Iteration 1058, loss = 2.77340656\n",
      "Iteration 1059, loss = 2.77321327\n",
      "Iteration 1060, loss = 2.77330965\n",
      "Iteration 1061, loss = 2.77271352\n",
      "Iteration 1062, loss = 2.77245388\n",
      "Iteration 1063, loss = 2.77231614\n",
      "Iteration 1064, loss = 2.77224648\n",
      "Iteration 1065, loss = 2.77174157\n",
      "Iteration 1066, loss = 2.77142870\n",
      "Iteration 1067, loss = 2.77116125\n",
      "Iteration 1068, loss = 2.77095161\n",
      "Iteration 1069, loss = 2.77034095\n",
      "Iteration 1070, loss = 2.76977361\n",
      "Iteration 1071, loss = 2.76924433\n",
      "Iteration 1072, loss = 2.76912992\n",
      "Iteration 1073, loss = 2.76836645\n",
      "Iteration 1074, loss = 2.76931635\n",
      "Iteration 1075, loss = 2.76839521\n",
      "Iteration 1076, loss = 2.76904457\n",
      "Iteration 1077, loss = 2.76819910\n",
      "Iteration 1078, loss = 2.76785600\n",
      "Iteration 1079, loss = 2.76741420\n",
      "Iteration 1080, loss = 2.76693327\n",
      "Iteration 1081, loss = 2.76667664\n",
      "Iteration 1082, loss = 2.76688421\n",
      "Iteration 1083, loss = 2.76719256\n",
      "Iteration 1084, loss = 2.76635785\n",
      "Iteration 1085, loss = 2.76604028\n",
      "Iteration 1086, loss = 2.76538116\n",
      "Iteration 1087, loss = 2.76514943\n",
      "Iteration 1088, loss = 2.76482348\n",
      "Iteration 1089, loss = 2.76478991\n",
      "Iteration 1090, loss = 2.76446517\n",
      "Iteration 1091, loss = 2.76417820\n",
      "Iteration 1092, loss = 2.76360058\n",
      "Iteration 1093, loss = 2.76364884\n",
      "Iteration 1094, loss = 2.76309715\n",
      "Iteration 1095, loss = 2.76325975\n",
      "Iteration 1096, loss = 2.76390002\n",
      "Iteration 1097, loss = 2.76364404\n",
      "Iteration 1098, loss = 2.76251246\n",
      "Iteration 1099, loss = 2.76163837\n",
      "Iteration 1100, loss = 2.76156290\n",
      "Iteration 1101, loss = 2.76135410\n",
      "Iteration 1102, loss = 2.76091593\n",
      "Iteration 1103, loss = 2.76048657\n",
      "Iteration 1104, loss = 2.76032124\n",
      "Iteration 1105, loss = 2.76001661\n",
      "Iteration 1106, loss = 2.75948622\n",
      "Iteration 1107, loss = 2.75943999\n",
      "Iteration 1108, loss = 2.75937091\n",
      "Iteration 1109, loss = 2.75951808\n",
      "Iteration 1110, loss = 2.75923286\n",
      "Iteration 1111, loss = 2.75820881\n",
      "Iteration 1112, loss = 2.75836600\n",
      "Iteration 1113, loss = 2.75821286\n",
      "Iteration 1114, loss = 2.75760451\n",
      "Iteration 1115, loss = 2.75767310\n",
      "Iteration 1116, loss = 2.75711075\n",
      "Iteration 1117, loss = 2.75710823\n",
      "Iteration 1118, loss = 2.75657817\n",
      "Iteration 1119, loss = 2.75612803\n",
      "Iteration 1120, loss = 2.75563007\n",
      "Iteration 1121, loss = 2.75611656\n",
      "Iteration 1122, loss = 2.75573757\n",
      "Iteration 1123, loss = 2.75478417\n",
      "Iteration 1124, loss = 2.75469884\n",
      "Iteration 1125, loss = 2.75451248\n",
      "Iteration 1126, loss = 2.75462181\n",
      "Iteration 1127, loss = 2.75396658\n",
      "Iteration 1128, loss = 2.75387283\n",
      "Iteration 1129, loss = 2.75364229\n",
      "Iteration 1130, loss = 2.75336409\n",
      "Iteration 1131, loss = 2.75260916\n",
      "Iteration 1132, loss = 2.75292838\n",
      "Iteration 1133, loss = 2.75322127\n",
      "Iteration 1134, loss = 2.75255336\n",
      "Iteration 1135, loss = 2.75150242\n",
      "Iteration 1136, loss = 2.75133162\n",
      "Iteration 1137, loss = 2.75102120\n",
      "Iteration 1138, loss = 2.75138635\n",
      "Iteration 1139, loss = 2.75111053\n",
      "Iteration 1140, loss = 2.75070675\n",
      "Iteration 1141, loss = 2.74963773\n",
      "Iteration 1142, loss = 2.74961377\n",
      "Iteration 1143, loss = 2.74931230\n",
      "Iteration 1144, loss = 2.74965364\n",
      "Iteration 1145, loss = 2.74905934\n",
      "Iteration 1146, loss = 2.74914611\n",
      "Iteration 1147, loss = 2.74845421\n",
      "Iteration 1148, loss = 2.74801114\n",
      "Iteration 1149, loss = 2.74866933\n",
      "Iteration 1150, loss = 2.74817918\n",
      "Iteration 1151, loss = 2.74730791\n",
      "Iteration 1152, loss = 2.74734346\n",
      "Iteration 1153, loss = 2.74696272\n",
      "Iteration 1154, loss = 2.74651497\n",
      "Iteration 1155, loss = 2.74617747\n",
      "Iteration 1156, loss = 2.74582421\n",
      "Iteration 1157, loss = 2.74602754\n",
      "Iteration 1158, loss = 2.74554123\n",
      "Iteration 1159, loss = 2.74511514\n",
      "Iteration 1160, loss = 2.74517727\n",
      "Iteration 1161, loss = 2.74470991\n",
      "Iteration 1162, loss = 2.74482417\n",
      "Iteration 1163, loss = 2.74488392\n",
      "Iteration 1164, loss = 2.74440979\n",
      "Iteration 1165, loss = 2.74461258\n",
      "Iteration 1166, loss = 2.74438494\n",
      "Iteration 1167, loss = 2.74333962\n",
      "Iteration 1168, loss = 2.74236462\n",
      "Iteration 1169, loss = 2.74233689\n",
      "Iteration 1170, loss = 2.74219281\n",
      "Iteration 1171, loss = 2.74213733\n",
      "Iteration 1172, loss = 2.74219938\n",
      "Iteration 1173, loss = 2.74146468\n",
      "Iteration 1174, loss = 2.74101334\n",
      "Iteration 1175, loss = 2.74080967\n",
      "Iteration 1176, loss = 2.74085555\n",
      "Iteration 1177, loss = 2.74018945\n",
      "Iteration 1178, loss = 2.74022704\n",
      "Iteration 1179, loss = 2.73967859\n",
      "Iteration 1180, loss = 2.73957985\n",
      "Iteration 1181, loss = 2.73940276\n",
      "Iteration 1182, loss = 2.73889531\n",
      "Iteration 1183, loss = 2.73998499\n",
      "Iteration 1184, loss = 2.73912201\n",
      "Iteration 1185, loss = 2.73822822\n",
      "Iteration 1186, loss = 2.73777285\n",
      "Iteration 1187, loss = 2.73756680\n",
      "Iteration 1188, loss = 2.73754675\n",
      "Iteration 1189, loss = 2.73705389\n",
      "Iteration 1190, loss = 2.73660111\n",
      "Iteration 1191, loss = 2.73619013\n",
      "Iteration 1192, loss = 2.73628125\n",
      "Iteration 1193, loss = 2.73673397\n",
      "Iteration 1194, loss = 2.73610416\n",
      "Iteration 1195, loss = 2.73547913\n",
      "Iteration 1196, loss = 2.73532894\n",
      "Iteration 1197, loss = 2.73467494\n",
      "Iteration 1198, loss = 2.73466426\n",
      "Iteration 1199, loss = 2.73441142\n",
      "Iteration 1200, loss = 2.73457288\n",
      "Iteration 1201, loss = 2.73410895\n",
      "Iteration 1202, loss = 2.73368689\n",
      "Iteration 1203, loss = 2.73347509\n",
      "Iteration 1204, loss = 2.73330606\n",
      "Iteration 1205, loss = 2.73261692\n",
      "Iteration 1206, loss = 2.73274142\n",
      "Iteration 1207, loss = 2.73261761\n",
      "Iteration 1208, loss = 2.73194016\n",
      "Iteration 1209, loss = 2.73178403\n",
      "Iteration 1210, loss = 2.73182903\n",
      "Iteration 1211, loss = 2.73197539\n",
      "Iteration 1212, loss = 2.73164110\n",
      "Iteration 1213, loss = 2.73049661\n",
      "Iteration 1214, loss = 2.73022373\n",
      "Iteration 1215, loss = 2.73026315\n",
      "Iteration 1216, loss = 2.73015218\n",
      "Iteration 1217, loss = 2.72994921\n",
      "Iteration 1218, loss = 2.73006677\n",
      "Iteration 1219, loss = 2.72995556\n",
      "Iteration 1220, loss = 2.72890141\n",
      "Iteration 1221, loss = 2.72890031\n",
      "Iteration 1222, loss = 2.72874611\n",
      "Iteration 1223, loss = 2.72777115\n",
      "Iteration 1224, loss = 2.72755425\n",
      "Iteration 1225, loss = 2.72760204\n",
      "Iteration 1226, loss = 2.72737311\n",
      "Iteration 1227, loss = 2.72674641\n",
      "Iteration 1228, loss = 2.72624293\n",
      "Iteration 1229, loss = 2.72624112\n",
      "Iteration 1230, loss = 2.72681849\n",
      "Iteration 1231, loss = 2.72647273\n",
      "Iteration 1232, loss = 2.72521325\n",
      "Iteration 1233, loss = 2.72537779\n",
      "Iteration 1234, loss = 2.72505362\n",
      "Iteration 1235, loss = 2.72573322\n",
      "Iteration 1236, loss = 2.72566872\n",
      "Iteration 1237, loss = 2.72501235\n",
      "Iteration 1238, loss = 2.72437607\n",
      "Iteration 1239, loss = 2.72405323\n",
      "Iteration 1240, loss = 2.72359657\n",
      "Iteration 1241, loss = 2.72320892\n",
      "Iteration 1242, loss = 2.72281532\n",
      "Iteration 1243, loss = 2.72271997\n",
      "Iteration 1244, loss = 2.72244897\n",
      "Iteration 1245, loss = 2.72256181\n",
      "Iteration 1246, loss = 2.72188267\n",
      "Iteration 1247, loss = 2.72175931\n",
      "Iteration 1248, loss = 2.72162260\n",
      "Iteration 1249, loss = 2.72108115\n",
      "Iteration 1250, loss = 2.72097188\n",
      "Iteration 1251, loss = 2.72067593\n",
      "Iteration 1252, loss = 2.72066771\n",
      "Iteration 1253, loss = 2.72024380\n",
      "Iteration 1254, loss = 2.71982133\n",
      "Iteration 1255, loss = 2.71934526\n",
      "Iteration 1256, loss = 2.71959271\n",
      "Iteration 1257, loss = 2.71960246\n",
      "Iteration 1258, loss = 2.71919030\n",
      "Iteration 1259, loss = 2.71869057\n",
      "Iteration 1260, loss = 2.71854610\n",
      "Iteration 1261, loss = 2.71829796\n",
      "Iteration 1262, loss = 2.71852276\n",
      "Iteration 1263, loss = 2.71848234\n",
      "Iteration 1264, loss = 2.71778602\n",
      "Iteration 1265, loss = 2.71730687\n",
      "Iteration 1266, loss = 2.71634892\n",
      "Iteration 1267, loss = 2.71684480\n",
      "Iteration 1268, loss = 2.71675642\n",
      "Iteration 1269, loss = 2.71659485\n",
      "Iteration 1, loss = 4.14639814\n",
      "Iteration 2, loss = 4.01797471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1269) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 3.89974036\n",
      "Iteration 4, loss = 3.78775356\n",
      "Iteration 5, loss = 3.68106680\n",
      "Iteration 6, loss = 3.57883899\n",
      "Iteration 7, loss = 3.48811041\n",
      "Iteration 8, loss = 3.40713785\n",
      "Iteration 9, loss = 3.33972119\n",
      "Iteration 10, loss = 3.29134097\n",
      "Iteration 11, loss = 3.25772155\n",
      "Iteration 12, loss = 3.24001964\n",
      "Iteration 13, loss = 3.23330977\n",
      "Iteration 14, loss = 3.23082836\n",
      "Iteration 15, loss = 3.22932242\n",
      "Iteration 16, loss = 3.22780340\n",
      "Iteration 17, loss = 3.22663096\n",
      "Iteration 18, loss = 3.22473693\n",
      "Iteration 19, loss = 3.22352672\n",
      "Iteration 20, loss = 3.22286199\n",
      "Iteration 21, loss = 3.22246528\n",
      "Iteration 22, loss = 3.22157401\n",
      "Iteration 23, loss = 3.22076890\n",
      "Iteration 24, loss = 3.22033094\n",
      "Iteration 25, loss = 3.21990812\n",
      "Iteration 26, loss = 3.21895438\n",
      "Iteration 27, loss = 3.21890088\n",
      "Iteration 28, loss = 3.21877822\n",
      "Iteration 29, loss = 3.21802451\n",
      "Iteration 30, loss = 3.21682312\n",
      "Iteration 31, loss = 3.21579584\n",
      "Iteration 32, loss = 3.21549181\n",
      "Iteration 33, loss = 3.21572858\n",
      "Iteration 34, loss = 3.21476922\n",
      "Iteration 35, loss = 3.21416205\n",
      "Iteration 36, loss = 3.21327107\n",
      "Iteration 37, loss = 3.21307665\n",
      "Iteration 38, loss = 3.21277066\n",
      "Iteration 39, loss = 3.21207678\n",
      "Iteration 40, loss = 3.21159349\n",
      "Iteration 41, loss = 3.21065877\n",
      "Iteration 42, loss = 3.21075699\n",
      "Iteration 43, loss = 3.21047300\n",
      "Iteration 44, loss = 3.20992530\n",
      "Iteration 45, loss = 3.20900262\n",
      "Iteration 46, loss = 3.20895404\n",
      "Iteration 47, loss = 3.20915263\n",
      "Iteration 48, loss = 3.20874489\n",
      "Iteration 49, loss = 3.20803649\n",
      "Iteration 50, loss = 3.20733216\n",
      "Iteration 51, loss = 3.20713152\n",
      "Iteration 52, loss = 3.20611442\n",
      "Iteration 53, loss = 3.20605269\n",
      "Iteration 54, loss = 3.20547137\n",
      "Iteration 55, loss = 3.20483677\n",
      "Iteration 56, loss = 3.20470402\n",
      "Iteration 57, loss = 3.20448073\n",
      "Iteration 58, loss = 3.20428589\n",
      "Iteration 59, loss = 3.20350873\n",
      "Iteration 60, loss = 3.20289102\n",
      "Iteration 61, loss = 3.20290858\n",
      "Iteration 62, loss = 3.20308765\n",
      "Iteration 63, loss = 3.20349889\n",
      "Iteration 64, loss = 3.20289531\n",
      "Iteration 65, loss = 3.20180231\n",
      "Iteration 66, loss = 3.20094980\n",
      "Iteration 67, loss = 3.20098757\n",
      "Iteration 68, loss = 3.20055076\n",
      "Iteration 69, loss = 3.20015473\n",
      "Iteration 70, loss = 3.19949375\n",
      "Iteration 71, loss = 3.19889125\n",
      "Iteration 72, loss = 3.19821797\n",
      "Iteration 73, loss = 3.19824606\n",
      "Iteration 74, loss = 3.19742145\n",
      "Iteration 75, loss = 3.19779906\n",
      "Iteration 76, loss = 3.19699636\n",
      "Iteration 77, loss = 3.19703006\n",
      "Iteration 78, loss = 3.19695883\n",
      "Iteration 79, loss = 3.19656635\n",
      "Iteration 80, loss = 3.19648111\n",
      "Iteration 81, loss = 3.19629411\n",
      "Iteration 82, loss = 3.19571239\n",
      "Iteration 83, loss = 3.19532464\n",
      "Iteration 84, loss = 3.19539768\n",
      "Iteration 85, loss = 3.19468848\n",
      "Iteration 86, loss = 3.19406994\n",
      "Iteration 87, loss = 3.19376686\n",
      "Iteration 88, loss = 3.19321843\n",
      "Iteration 89, loss = 3.19278939\n",
      "Iteration 90, loss = 3.19265836\n",
      "Iteration 91, loss = 3.19273797\n",
      "Iteration 92, loss = 3.19194511\n",
      "Iteration 93, loss = 3.19190716\n",
      "Iteration 94, loss = 3.19212701\n",
      "Iteration 95, loss = 3.19131253\n",
      "Iteration 96, loss = 3.19012292\n",
      "Iteration 97, loss = 3.19027480\n",
      "Iteration 98, loss = 3.18969388\n",
      "Iteration 99, loss = 3.18984059\n",
      "Iteration 100, loss = 3.18897904\n",
      "Iteration 101, loss = 3.18830795\n",
      "Iteration 102, loss = 3.18846190\n",
      "Iteration 103, loss = 3.18804630\n",
      "Iteration 104, loss = 3.18758373\n",
      "Iteration 105, loss = 3.18734893\n",
      "Iteration 106, loss = 3.18703903\n",
      "Iteration 107, loss = 3.18703377\n",
      "Iteration 108, loss = 3.18655896\n",
      "Iteration 109, loss = 3.18594433\n",
      "Iteration 110, loss = 3.18572780\n",
      "Iteration 111, loss = 3.18538480\n",
      "Iteration 112, loss = 3.18493888\n",
      "Iteration 113, loss = 3.18499047\n",
      "Iteration 114, loss = 3.18433293\n",
      "Iteration 115, loss = 3.18351497\n",
      "Iteration 116, loss = 3.18366502\n",
      "Iteration 117, loss = 3.18393952\n",
      "Iteration 118, loss = 3.18330459\n",
      "Iteration 119, loss = 3.18193561\n",
      "Iteration 120, loss = 3.18122591\n",
      "Iteration 121, loss = 3.18140556\n",
      "Iteration 122, loss = 3.18063354\n",
      "Iteration 123, loss = 3.18061348\n",
      "Iteration 124, loss = 3.17966858\n",
      "Iteration 125, loss = 3.17976475\n",
      "Iteration 126, loss = 3.18013243\n",
      "Iteration 127, loss = 3.17944273\n",
      "Iteration 128, loss = 3.17878692\n",
      "Iteration 129, loss = 3.17885574\n",
      "Iteration 130, loss = 3.17841748\n",
      "Iteration 131, loss = 3.17780479\n",
      "Iteration 132, loss = 3.17723443\n",
      "Iteration 133, loss = 3.17678915\n",
      "Iteration 134, loss = 3.17653221\n",
      "Iteration 135, loss = 3.17584127\n",
      "Iteration 136, loss = 3.17489875\n",
      "Iteration 137, loss = 3.17499437\n",
      "Iteration 138, loss = 3.17456376\n",
      "Iteration 139, loss = 3.17429324\n",
      "Iteration 140, loss = 3.17382995\n",
      "Iteration 141, loss = 3.17329530\n",
      "Iteration 142, loss = 3.17299964\n",
      "Iteration 143, loss = 3.17316514\n",
      "Iteration 144, loss = 3.17277419\n",
      "Iteration 145, loss = 3.17181879\n",
      "Iteration 146, loss = 3.17103514\n",
      "Iteration 147, loss = 3.17104423\n",
      "Iteration 148, loss = 3.17033096\n",
      "Iteration 149, loss = 3.17025094\n",
      "Iteration 150, loss = 3.16967279\n",
      "Iteration 151, loss = 3.16929547\n",
      "Iteration 152, loss = 3.16879805\n",
      "Iteration 153, loss = 3.16764385\n",
      "Iteration 154, loss = 3.16754875\n",
      "Iteration 155, loss = 3.16700623\n",
      "Iteration 156, loss = 3.16784059\n",
      "Iteration 157, loss = 3.16712043\n",
      "Iteration 158, loss = 3.16647295\n",
      "Iteration 159, loss = 3.16574835\n",
      "Iteration 160, loss = 3.16505997\n",
      "Iteration 161, loss = 3.16443513\n",
      "Iteration 162, loss = 3.16376995\n",
      "Iteration 163, loss = 3.16342811\n",
      "Iteration 164, loss = 3.16303504\n",
      "Iteration 165, loss = 3.16269973\n",
      "Iteration 166, loss = 3.16195074\n",
      "Iteration 167, loss = 3.16171357\n",
      "Iteration 168, loss = 3.16173877\n",
      "Iteration 169, loss = 3.16122809\n",
      "Iteration 170, loss = 3.16088183\n",
      "Iteration 171, loss = 3.16065977\n",
      "Iteration 172, loss = 3.16039078\n",
      "Iteration 173, loss = 3.15966865\n",
      "Iteration 174, loss = 3.15864286\n",
      "Iteration 175, loss = 3.15821990\n",
      "Iteration 176, loss = 3.15766144\n",
      "Iteration 177, loss = 3.15659348\n",
      "Iteration 178, loss = 3.15661976\n",
      "Iteration 179, loss = 3.15610263\n",
      "Iteration 180, loss = 3.15552419\n",
      "Iteration 181, loss = 3.15550779\n",
      "Iteration 182, loss = 3.15508224\n",
      "Iteration 183, loss = 3.15419720\n",
      "Iteration 184, loss = 3.15372410\n",
      "Iteration 185, loss = 3.15283052\n",
      "Iteration 186, loss = 3.15275962\n",
      "Iteration 187, loss = 3.15209379\n",
      "Iteration 188, loss = 3.15124013\n",
      "Iteration 189, loss = 3.15084514\n",
      "Iteration 190, loss = 3.15046507\n",
      "Iteration 191, loss = 3.15041843\n",
      "Iteration 192, loss = 3.14966484\n",
      "Iteration 193, loss = 3.14845802\n",
      "Iteration 194, loss = 3.14865267\n",
      "Iteration 195, loss = 3.14766277\n",
      "Iteration 196, loss = 3.14688401\n",
      "Iteration 197, loss = 3.14592060\n",
      "Iteration 198, loss = 3.14567621\n",
      "Iteration 199, loss = 3.14538529\n",
      "Iteration 200, loss = 3.14521678\n",
      "Iteration 201, loss = 3.14429838\n",
      "Iteration 202, loss = 3.14372799\n",
      "Iteration 203, loss = 3.14351479\n",
      "Iteration 204, loss = 3.14267641\n",
      "Iteration 205, loss = 3.14163927\n",
      "Iteration 206, loss = 3.14197297\n",
      "Iteration 207, loss = 3.14210491\n",
      "Iteration 208, loss = 3.14112851\n",
      "Iteration 209, loss = 3.13959148\n",
      "Iteration 210, loss = 3.13881143\n",
      "Iteration 211, loss = 3.13901440\n",
      "Iteration 212, loss = 3.13884384\n",
      "Iteration 213, loss = 3.13829517\n",
      "Iteration 214, loss = 3.13761715\n",
      "Iteration 215, loss = 3.13727255\n",
      "Iteration 216, loss = 3.13606818\n",
      "Iteration 217, loss = 3.13560393\n",
      "Iteration 218, loss = 3.13501838\n",
      "Iteration 219, loss = 3.13472999\n",
      "Iteration 220, loss = 3.13360714\n",
      "Iteration 221, loss = 3.13392203\n",
      "Iteration 222, loss = 3.13325019\n",
      "Iteration 223, loss = 3.13394975\n",
      "Iteration 224, loss = 3.13235913\n",
      "Iteration 225, loss = 3.13045959\n",
      "Iteration 226, loss = 3.13018726\n",
      "Iteration 227, loss = 3.13002304\n",
      "Iteration 228, loss = 3.12908475\n",
      "Iteration 229, loss = 3.12834104\n",
      "Iteration 230, loss = 3.12849160\n",
      "Iteration 231, loss = 3.12783654\n",
      "Iteration 232, loss = 3.12666774\n",
      "Iteration 233, loss = 3.12583943\n",
      "Iteration 234, loss = 3.12542849\n",
      "Iteration 235, loss = 3.12506940\n",
      "Iteration 236, loss = 3.12518321\n",
      "Iteration 237, loss = 3.12443077\n",
      "Iteration 238, loss = 3.12421398\n",
      "Iteration 239, loss = 3.12303894\n",
      "Iteration 240, loss = 3.12269826\n",
      "Iteration 241, loss = 3.12267182\n",
      "Iteration 242, loss = 3.12198480\n",
      "Iteration 243, loss = 3.12131501\n",
      "Iteration 244, loss = 3.12048917\n",
      "Iteration 245, loss = 3.11982727\n",
      "Iteration 246, loss = 3.11894639\n",
      "Iteration 247, loss = 3.11780915\n",
      "Iteration 248, loss = 3.11813462\n",
      "Iteration 249, loss = 3.11815870\n",
      "Iteration 250, loss = 3.11695506\n",
      "Iteration 251, loss = 3.11657281\n",
      "Iteration 252, loss = 3.11641355\n",
      "Iteration 253, loss = 3.11580038\n",
      "Iteration 254, loss = 3.11495510\n",
      "Iteration 255, loss = 3.11412374\n",
      "Iteration 256, loss = 3.11337426\n",
      "Iteration 257, loss = 3.11345219\n",
      "Iteration 258, loss = 3.11240007\n",
      "Iteration 259, loss = 3.11202251\n",
      "Iteration 260, loss = 3.11112448\n",
      "Iteration 261, loss = 3.11063143\n",
      "Iteration 262, loss = 3.11015736\n",
      "Iteration 263, loss = 3.11054713\n",
      "Iteration 264, loss = 3.10905027\n",
      "Iteration 265, loss = 3.10837875\n",
      "Iteration 266, loss = 3.10903582\n",
      "Iteration 267, loss = 3.10825261\n",
      "Iteration 268, loss = 3.10754849\n",
      "Iteration 269, loss = 3.10595168\n",
      "Iteration 270, loss = 3.10624532\n",
      "Iteration 271, loss = 3.10581064\n",
      "Iteration 272, loss = 3.10465556\n",
      "Iteration 273, loss = 3.10371099\n",
      "Iteration 274, loss = 3.10368498\n",
      "Iteration 275, loss = 3.10288820\n",
      "Iteration 276, loss = 3.10302993\n",
      "Iteration 277, loss = 3.10288692\n",
      "Iteration 278, loss = 3.10157568\n",
      "Iteration 279, loss = 3.10006816\n",
      "Iteration 280, loss = 3.10032810\n",
      "Iteration 281, loss = 3.09942575\n",
      "Iteration 282, loss = 3.09871370\n",
      "Iteration 283, loss = 3.09808421\n",
      "Iteration 284, loss = 3.09788517\n",
      "Iteration 285, loss = 3.09721070\n",
      "Iteration 286, loss = 3.09679921\n",
      "Iteration 287, loss = 3.09594745\n",
      "Iteration 288, loss = 3.09521507\n",
      "Iteration 289, loss = 3.09468028\n",
      "Iteration 290, loss = 3.09425525\n",
      "Iteration 291, loss = 3.09359788\n",
      "Iteration 292, loss = 3.09394667\n",
      "Iteration 293, loss = 3.09340682\n",
      "Iteration 294, loss = 3.09282008\n",
      "Iteration 295, loss = 3.09222060\n",
      "Iteration 296, loss = 3.09140121\n",
      "Iteration 297, loss = 3.09104583\n",
      "Iteration 298, loss = 3.09093475\n",
      "Iteration 299, loss = 3.09063542\n",
      "Iteration 300, loss = 3.08997182\n",
      "Iteration 301, loss = 3.08901658\n",
      "Iteration 302, loss = 3.08842085\n",
      "Iteration 303, loss = 3.08777016\n",
      "Iteration 304, loss = 3.08761441\n",
      "Iteration 305, loss = 3.08644553\n",
      "Iteration 306, loss = 3.08593507\n",
      "Iteration 307, loss = 3.08542633\n",
      "Iteration 308, loss = 3.08539506\n",
      "Iteration 309, loss = 3.08416686\n",
      "Iteration 310, loss = 3.08310216\n",
      "Iteration 311, loss = 3.08248712\n",
      "Iteration 312, loss = 3.08201243\n",
      "Iteration 313, loss = 3.08170535\n",
      "Iteration 314, loss = 3.08096019\n",
      "Iteration 315, loss = 3.08067812\n",
      "Iteration 316, loss = 3.08059763\n",
      "Iteration 317, loss = 3.08003265\n",
      "Iteration 318, loss = 3.07903932\n",
      "Iteration 319, loss = 3.07868131\n",
      "Iteration 320, loss = 3.07751007\n",
      "Iteration 321, loss = 3.07739386\n",
      "Iteration 322, loss = 3.07783391\n",
      "Iteration 323, loss = 3.07716545\n",
      "Iteration 324, loss = 3.07609389\n",
      "Iteration 325, loss = 3.07585573\n",
      "Iteration 326, loss = 3.07511148\n",
      "Iteration 327, loss = 3.07443993\n",
      "Iteration 328, loss = 3.07482549\n",
      "Iteration 329, loss = 3.07454732\n",
      "Iteration 330, loss = 3.07321549\n",
      "Iteration 331, loss = 3.07219241\n",
      "Iteration 332, loss = 3.07191705\n",
      "Iteration 333, loss = 3.07136427\n",
      "Iteration 334, loss = 3.07069348\n",
      "Iteration 335, loss = 3.06983143\n",
      "Iteration 336, loss = 3.06925266\n",
      "Iteration 337, loss = 3.06881386\n",
      "Iteration 338, loss = 3.06810181\n",
      "Iteration 339, loss = 3.06748949\n",
      "Iteration 340, loss = 3.06714254\n",
      "Iteration 341, loss = 3.06678204\n",
      "Iteration 342, loss = 3.06537399\n",
      "Iteration 343, loss = 3.06513705\n",
      "Iteration 344, loss = 3.06520071\n",
      "Iteration 345, loss = 3.06496921\n",
      "Iteration 346, loss = 3.06481457\n",
      "Iteration 347, loss = 3.06399857\n",
      "Iteration 348, loss = 3.06314026\n",
      "Iteration 349, loss = 3.06266752\n",
      "Iteration 350, loss = 3.06230345\n",
      "Iteration 351, loss = 3.06182037\n",
      "Iteration 352, loss = 3.06063510\n",
      "Iteration 353, loss = 3.06013960\n",
      "Iteration 354, loss = 3.05968955\n",
      "Iteration 355, loss = 3.05985140\n",
      "Iteration 356, loss = 3.05922370\n",
      "Iteration 357, loss = 3.05878819\n",
      "Iteration 358, loss = 3.05784991\n",
      "Iteration 359, loss = 3.05789046\n",
      "Iteration 360, loss = 3.05715054\n",
      "Iteration 361, loss = 3.05747254\n",
      "Iteration 362, loss = 3.05671882\n",
      "Iteration 363, loss = 3.05444836\n",
      "Iteration 364, loss = 3.05424146\n",
      "Iteration 365, loss = 3.05400709\n",
      "Iteration 366, loss = 3.05338675\n",
      "Iteration 367, loss = 3.05299566\n",
      "Iteration 368, loss = 3.05255862\n",
      "Iteration 369, loss = 3.05184048\n",
      "Iteration 370, loss = 3.05095982\n",
      "Iteration 371, loss = 3.05077173\n",
      "Iteration 372, loss = 3.05064594\n",
      "Iteration 373, loss = 3.04962862\n",
      "Iteration 374, loss = 3.04866483\n",
      "Iteration 375, loss = 3.04869152\n",
      "Iteration 376, loss = 3.04868321\n",
      "Iteration 377, loss = 3.04850910\n",
      "Iteration 378, loss = 3.04739176\n",
      "Iteration 379, loss = 3.04727228\n",
      "Iteration 380, loss = 3.04591282\n",
      "Iteration 381, loss = 3.04568331\n",
      "Iteration 382, loss = 3.04551907\n",
      "Iteration 383, loss = 3.04487376\n",
      "Iteration 384, loss = 3.04462163\n",
      "Iteration 385, loss = 3.04411845\n",
      "Iteration 386, loss = 3.04293399\n",
      "Iteration 387, loss = 3.04266044\n",
      "Iteration 388, loss = 3.04287399\n",
      "Iteration 389, loss = 3.04200897\n",
      "Iteration 390, loss = 3.04183293\n",
      "Iteration 391, loss = 3.04139072\n",
      "Iteration 392, loss = 3.04159165\n",
      "Iteration 393, loss = 3.04060399\n",
      "Iteration 394, loss = 3.03904715\n",
      "Iteration 395, loss = 3.03897258\n",
      "Iteration 396, loss = 3.03863767\n",
      "Iteration 397, loss = 3.03834041\n",
      "Iteration 398, loss = 3.03760320\n",
      "Iteration 399, loss = 3.03709192\n",
      "Iteration 400, loss = 3.03615778\n",
      "Iteration 401, loss = 3.03611461\n",
      "Iteration 402, loss = 3.03609101\n",
      "Iteration 403, loss = 3.03547109\n",
      "Iteration 404, loss = 3.03400437\n",
      "Iteration 405, loss = 3.03362922\n",
      "Iteration 406, loss = 3.03311049\n",
      "Iteration 407, loss = 3.03269916\n",
      "Iteration 408, loss = 3.03300447\n",
      "Iteration 409, loss = 3.03328819\n",
      "Iteration 410, loss = 3.03234546\n",
      "Iteration 411, loss = 3.03121429\n",
      "Iteration 412, loss = 3.03087858\n",
      "Iteration 413, loss = 3.03002850\n",
      "Iteration 414, loss = 3.02901693\n",
      "Iteration 415, loss = 3.02893158\n",
      "Iteration 416, loss = 3.02888541\n",
      "Iteration 417, loss = 3.02904781\n",
      "Iteration 418, loss = 3.02765057\n",
      "Iteration 419, loss = 3.02721290\n",
      "Iteration 420, loss = 3.02634864\n",
      "Iteration 421, loss = 3.02586008\n",
      "Iteration 422, loss = 3.02525673\n",
      "Iteration 423, loss = 3.02433315\n",
      "Iteration 424, loss = 3.02472661\n",
      "Iteration 425, loss = 3.02422545\n",
      "Iteration 426, loss = 3.02252401\n",
      "Iteration 427, loss = 3.02297338\n",
      "Iteration 428, loss = 3.02328511\n",
      "Iteration 429, loss = 3.02148819\n",
      "Iteration 430, loss = 3.02098751\n",
      "Iteration 431, loss = 3.02123164\n",
      "Iteration 432, loss = 3.02010720\n",
      "Iteration 433, loss = 3.01985274\n",
      "Iteration 434, loss = 3.01956907\n",
      "Iteration 435, loss = 3.01852815\n",
      "Iteration 436, loss = 3.01834519\n",
      "Iteration 437, loss = 3.01795924\n",
      "Iteration 438, loss = 3.01736834\n",
      "Iteration 439, loss = 3.01676882\n",
      "Iteration 440, loss = 3.01614582\n",
      "Iteration 441, loss = 3.01566992\n",
      "Iteration 442, loss = 3.01642212\n",
      "Iteration 443, loss = 3.01575350\n",
      "Iteration 444, loss = 3.01594299\n",
      "Iteration 445, loss = 3.01525030\n",
      "Iteration 446, loss = 3.01410691\n",
      "Iteration 447, loss = 3.01324193\n",
      "Iteration 448, loss = 3.01286449\n",
      "Iteration 449, loss = 3.01238820\n",
      "Iteration 450, loss = 3.01184426\n",
      "Iteration 451, loss = 3.01141552\n",
      "Iteration 452, loss = 3.01111103\n",
      "Iteration 453, loss = 3.01043423\n",
      "Iteration 454, loss = 3.00963579\n",
      "Iteration 455, loss = 3.00872882\n",
      "Iteration 456, loss = 3.00841742\n",
      "Iteration 457, loss = 3.00847401\n",
      "Iteration 458, loss = 3.00842561\n",
      "Iteration 459, loss = 3.00774060\n",
      "Iteration 460, loss = 3.00707735\n",
      "Iteration 461, loss = 3.00783445\n",
      "Iteration 462, loss = 3.00636136\n",
      "Iteration 463, loss = 3.00578604\n",
      "Iteration 464, loss = 3.00488169\n",
      "Iteration 465, loss = 3.00494146\n",
      "Iteration 466, loss = 3.00479821\n",
      "Iteration 467, loss = 3.00420580\n",
      "Iteration 468, loss = 3.00404781\n",
      "Iteration 469, loss = 3.00389672\n",
      "Iteration 470, loss = 3.00235966\n",
      "Iteration 471, loss = 3.00232168\n",
      "Iteration 472, loss = 3.00198302\n",
      "Iteration 473, loss = 3.00251572\n",
      "Iteration 474, loss = 3.00147113\n",
      "Iteration 475, loss = 3.00035983\n",
      "Iteration 476, loss = 3.00040064\n",
      "Iteration 477, loss = 2.99967471\n",
      "Iteration 478, loss = 2.99890293\n",
      "Iteration 479, loss = 2.99918225\n",
      "Iteration 480, loss = 2.99854650\n",
      "Iteration 481, loss = 2.99704927\n",
      "Iteration 482, loss = 2.99700651\n",
      "Iteration 483, loss = 2.99684392\n",
      "Iteration 484, loss = 2.99646482\n",
      "Iteration 485, loss = 2.99620531\n",
      "Iteration 486, loss = 2.99576871\n",
      "Iteration 487, loss = 2.99561543\n",
      "Iteration 488, loss = 2.99472111\n",
      "Iteration 489, loss = 2.99489433\n",
      "Iteration 490, loss = 2.99373761\n",
      "Iteration 491, loss = 2.99382850\n",
      "Iteration 492, loss = 2.99354300\n",
      "Iteration 493, loss = 2.99347179\n",
      "Iteration 494, loss = 2.99203458\n",
      "Iteration 495, loss = 2.99261329\n",
      "Iteration 496, loss = 2.99113249\n",
      "Iteration 497, loss = 2.99049842\n",
      "Iteration 498, loss = 2.99011107\n",
      "Iteration 499, loss = 2.99005204\n",
      "Iteration 500, loss = 2.99019703\n",
      "Iteration 501, loss = 2.99004727\n",
      "Iteration 502, loss = 2.98987774\n",
      "Iteration 503, loss = 2.98906166\n",
      "Iteration 504, loss = 2.98833025\n",
      "Iteration 505, loss = 2.98784543\n",
      "Iteration 506, loss = 2.98734623\n",
      "Iteration 507, loss = 2.98736302\n",
      "Iteration 508, loss = 2.98694895\n",
      "Iteration 509, loss = 2.98663646\n",
      "Iteration 510, loss = 2.98591455\n",
      "Iteration 511, loss = 2.98595583\n",
      "Iteration 512, loss = 2.98538993\n",
      "Iteration 513, loss = 2.98314144\n",
      "Iteration 514, loss = 2.98419935\n",
      "Iteration 515, loss = 2.98442383\n",
      "Iteration 516, loss = 2.98354884\n",
      "Iteration 517, loss = 2.98261426\n",
      "Iteration 518, loss = 2.98183411\n",
      "Iteration 519, loss = 2.98126384\n",
      "Iteration 520, loss = 2.98079637\n",
      "Iteration 521, loss = 2.98058068\n",
      "Iteration 522, loss = 2.98063968\n",
      "Iteration 523, loss = 2.98025344\n",
      "Iteration 524, loss = 2.97958028\n",
      "Iteration 525, loss = 2.97968785\n",
      "Iteration 526, loss = 2.97828181\n",
      "Iteration 527, loss = 2.97716392\n",
      "Iteration 528, loss = 2.97793369\n",
      "Iteration 529, loss = 2.97711587\n",
      "Iteration 530, loss = 2.97659263\n",
      "Iteration 531, loss = 2.97686268\n",
      "Iteration 532, loss = 2.97686280\n",
      "Iteration 533, loss = 2.97506840\n",
      "Iteration 534, loss = 2.97511663\n",
      "Iteration 535, loss = 2.97518355\n",
      "Iteration 536, loss = 2.97410587\n",
      "Iteration 537, loss = 2.97431869\n",
      "Iteration 538, loss = 2.97404054\n",
      "Iteration 539, loss = 2.97329784\n",
      "Iteration 540, loss = 2.97266273\n",
      "Iteration 541, loss = 2.97241326\n",
      "Iteration 542, loss = 2.97298771\n",
      "Iteration 543, loss = 2.97216060\n",
      "Iteration 544, loss = 2.97179533\n",
      "Iteration 545, loss = 2.97073547\n",
      "Iteration 546, loss = 2.97057171\n",
      "Iteration 547, loss = 2.97001685\n",
      "Iteration 548, loss = 2.96992653\n",
      "Iteration 549, loss = 2.96917013\n",
      "Iteration 550, loss = 2.96933773\n",
      "Iteration 551, loss = 2.96910672\n",
      "Iteration 552, loss = 2.96886829\n",
      "Iteration 553, loss = 2.96839555\n",
      "Iteration 554, loss = 2.96826955\n",
      "Iteration 555, loss = 2.96750902\n",
      "Iteration 556, loss = 2.96672189\n",
      "Iteration 557, loss = 2.96557986\n",
      "Iteration 558, loss = 2.96602707\n",
      "Iteration 559, loss = 2.96565646\n",
      "Iteration 560, loss = 2.96467782\n",
      "Iteration 561, loss = 2.96393744\n",
      "Iteration 562, loss = 2.96425219\n",
      "Iteration 563, loss = 2.96357380\n",
      "Iteration 564, loss = 2.96285950\n",
      "Iteration 565, loss = 2.96262315\n",
      "Iteration 566, loss = 2.96173783\n",
      "Iteration 567, loss = 2.96255468\n",
      "Iteration 568, loss = 2.96086002\n",
      "Iteration 569, loss = 2.96060668\n",
      "Iteration 570, loss = 2.96099182\n",
      "Iteration 571, loss = 2.96031111\n",
      "Iteration 572, loss = 2.95967700\n",
      "Iteration 573, loss = 2.95928125\n",
      "Iteration 574, loss = 2.95916466\n",
      "Iteration 575, loss = 2.95880557\n",
      "Iteration 576, loss = 2.95775898\n",
      "Iteration 577, loss = 2.95726577\n",
      "Iteration 578, loss = 2.95709467\n",
      "Iteration 579, loss = 2.95714961\n",
      "Iteration 580, loss = 2.95626302\n",
      "Iteration 581, loss = 2.95660206\n",
      "Iteration 582, loss = 2.95574042\n",
      "Iteration 583, loss = 2.95504192\n",
      "Iteration 584, loss = 2.95539719\n",
      "Iteration 585, loss = 2.95560087\n",
      "Iteration 586, loss = 2.95509036\n",
      "Iteration 587, loss = 2.95435837\n",
      "Iteration 588, loss = 2.95393493\n",
      "Iteration 589, loss = 2.95313320\n",
      "Iteration 590, loss = 2.95309485\n",
      "Iteration 591, loss = 2.95334801\n",
      "Iteration 592, loss = 2.95259819\n",
      "Iteration 593, loss = 2.95211756\n",
      "Iteration 594, loss = 2.95210454\n",
      "Iteration 595, loss = 2.95130105\n",
      "Iteration 596, loss = 2.95233367\n",
      "Iteration 597, loss = 2.95167507\n",
      "Iteration 598, loss = 2.95167877\n",
      "Iteration 599, loss = 2.95029973\n",
      "Iteration 600, loss = 2.94937293\n",
      "Iteration 601, loss = 2.94864922\n",
      "Iteration 602, loss = 2.94776755\n",
      "Iteration 603, loss = 2.94980931\n",
      "Iteration 604, loss = 2.94882878\n",
      "Iteration 605, loss = 2.94797724\n",
      "Iteration 606, loss = 2.94709172\n",
      "Iteration 607, loss = 2.94666803\n",
      "Iteration 608, loss = 2.94675258\n",
      "Iteration 609, loss = 2.94691761\n",
      "Iteration 610, loss = 2.94616429\n",
      "Iteration 611, loss = 2.94534816\n",
      "Iteration 612, loss = 2.94534657\n",
      "Iteration 613, loss = 2.94459454\n",
      "Iteration 614, loss = 2.94421940\n",
      "Iteration 615, loss = 2.94392120\n",
      "Iteration 616, loss = 2.94325673\n",
      "Iteration 617, loss = 2.94310757\n",
      "Iteration 618, loss = 2.94320224\n",
      "Iteration 619, loss = 2.94190931\n",
      "Iteration 620, loss = 2.94148674\n",
      "Iteration 621, loss = 2.94094567\n",
      "Iteration 622, loss = 2.94076436\n",
      "Iteration 623, loss = 2.94060880\n",
      "Iteration 624, loss = 2.93996006\n",
      "Iteration 625, loss = 2.94003728\n",
      "Iteration 626, loss = 2.93959057\n",
      "Iteration 627, loss = 2.93970018\n",
      "Iteration 628, loss = 2.93892604\n",
      "Iteration 629, loss = 2.93882385\n",
      "Iteration 630, loss = 2.93851739\n",
      "Iteration 631, loss = 2.93819515\n",
      "Iteration 632, loss = 2.93736498\n",
      "Iteration 633, loss = 2.93730467\n",
      "Iteration 634, loss = 2.93666836\n",
      "Iteration 635, loss = 2.93605285\n",
      "Iteration 636, loss = 2.93624902\n",
      "Iteration 637, loss = 2.93528711\n",
      "Iteration 638, loss = 2.93686470\n",
      "Iteration 639, loss = 2.93529945\n",
      "Iteration 640, loss = 2.93432804\n",
      "Iteration 641, loss = 2.93409371\n",
      "Iteration 642, loss = 2.93339096\n",
      "Iteration 643, loss = 2.93440775\n",
      "Iteration 644, loss = 2.93322754\n",
      "Iteration 645, loss = 2.93265920\n",
      "Iteration 646, loss = 2.93221944\n",
      "Iteration 647, loss = 2.93188583\n",
      "Iteration 648, loss = 2.93193476\n",
      "Iteration 649, loss = 2.93134781\n",
      "Iteration 650, loss = 2.93069331\n",
      "Iteration 651, loss = 2.93046482\n",
      "Iteration 652, loss = 2.93001455\n",
      "Iteration 653, loss = 2.92966318\n",
      "Iteration 654, loss = 2.92886539\n",
      "Iteration 655, loss = 2.92871410\n",
      "Iteration 656, loss = 2.92785166\n",
      "Iteration 657, loss = 2.92863035\n",
      "Iteration 658, loss = 2.92963780\n",
      "Iteration 659, loss = 2.92858343\n",
      "Iteration 660, loss = 2.92763532\n",
      "Iteration 661, loss = 2.92701332\n",
      "Iteration 662, loss = 2.92697971\n",
      "Iteration 663, loss = 2.92605279\n",
      "Iteration 664, loss = 2.92567965\n",
      "Iteration 665, loss = 2.92574915\n",
      "Iteration 666, loss = 2.92601125\n",
      "Iteration 667, loss = 2.92544272\n",
      "Iteration 668, loss = 2.92516338\n",
      "Iteration 669, loss = 2.92504749\n",
      "Iteration 670, loss = 2.92413257\n",
      "Iteration 671, loss = 2.92425919\n",
      "Iteration 672, loss = 2.92459872\n",
      "Iteration 673, loss = 2.92283912\n",
      "Iteration 674, loss = 2.92279713\n",
      "Iteration 675, loss = 2.92179906\n",
      "Iteration 676, loss = 2.92178758\n",
      "Iteration 677, loss = 2.92177512\n",
      "Iteration 678, loss = 2.92096426\n",
      "Iteration 679, loss = 2.92098442\n",
      "Iteration 680, loss = 2.92020251\n",
      "Iteration 681, loss = 2.91999489\n",
      "Iteration 682, loss = 2.91995025\n",
      "Iteration 683, loss = 2.91934062\n",
      "Iteration 684, loss = 2.91851356\n",
      "Iteration 685, loss = 2.91838015\n",
      "Iteration 686, loss = 2.91909864\n",
      "Iteration 687, loss = 2.91793876\n",
      "Iteration 688, loss = 2.91766209\n",
      "Iteration 689, loss = 2.91809697\n",
      "Iteration 690, loss = 2.91736748\n",
      "Iteration 691, loss = 2.91642597\n",
      "Iteration 692, loss = 2.91600752\n",
      "Iteration 693, loss = 2.91630306\n",
      "Iteration 694, loss = 2.91585598\n",
      "Iteration 695, loss = 2.91496734\n",
      "Iteration 696, loss = 2.91429778\n",
      "Iteration 697, loss = 2.91428612\n",
      "Iteration 698, loss = 2.91414806\n",
      "Iteration 699, loss = 2.91397538\n",
      "Iteration 700, loss = 2.91374923\n",
      "Iteration 701, loss = 2.91328419\n",
      "Iteration 702, loss = 2.91357199\n",
      "Iteration 703, loss = 2.91282859\n",
      "Iteration 704, loss = 2.91282032\n",
      "Iteration 705, loss = 2.91183938\n",
      "Iteration 706, loss = 2.91139086\n",
      "Iteration 707, loss = 2.91230399\n",
      "Iteration 708, loss = 2.91152622\n",
      "Iteration 709, loss = 2.91063792\n",
      "Iteration 710, loss = 2.90990106\n",
      "Iteration 711, loss = 2.90996804\n",
      "Iteration 712, loss = 2.90923859\n",
      "Iteration 713, loss = 2.91235425\n",
      "Iteration 714, loss = 2.91108780\n",
      "Iteration 715, loss = 2.90882887\n",
      "Iteration 716, loss = 2.90831829\n",
      "Iteration 717, loss = 2.90798807\n",
      "Iteration 718, loss = 2.90765065\n",
      "Iteration 719, loss = 2.90725097\n",
      "Iteration 720, loss = 2.90649060\n",
      "Iteration 721, loss = 2.90605354\n",
      "Iteration 722, loss = 2.90582036\n",
      "Iteration 723, loss = 2.90532087\n",
      "Iteration 724, loss = 2.90554301\n",
      "Iteration 725, loss = 2.90483922\n",
      "Iteration 726, loss = 2.90448238\n",
      "Iteration 727, loss = 2.90419291\n",
      "Iteration 728, loss = 2.90459703\n",
      "Iteration 729, loss = 2.90481921\n",
      "Iteration 730, loss = 2.90349637\n",
      "Iteration 731, loss = 2.90309274\n",
      "Iteration 732, loss = 2.90394969\n",
      "Iteration 733, loss = 2.90233218\n",
      "Iteration 734, loss = 2.90212311\n",
      "Iteration 735, loss = 2.90159379\n",
      "Iteration 736, loss = 2.90198978\n",
      "Iteration 737, loss = 2.90189005\n",
      "Iteration 738, loss = 2.90159317\n",
      "Iteration 739, loss = 2.90109755\n",
      "Iteration 740, loss = 2.90197035\n",
      "Iteration 741, loss = 2.89998777\n",
      "Iteration 742, loss = 2.89952258\n",
      "Iteration 743, loss = 2.89993268\n",
      "Iteration 744, loss = 2.89886811\n",
      "Iteration 745, loss = 2.89840169\n",
      "Iteration 746, loss = 2.89849882\n",
      "Iteration 747, loss = 2.89771625\n",
      "Iteration 748, loss = 2.89800717\n",
      "Iteration 749, loss = 2.89727517\n",
      "Iteration 750, loss = 2.89644157\n",
      "Iteration 751, loss = 2.89673901\n",
      "Iteration 752, loss = 2.89825829\n",
      "Iteration 753, loss = 2.89693481\n",
      "Iteration 754, loss = 2.89598237\n",
      "Iteration 755, loss = 2.89555248\n",
      "Iteration 756, loss = 2.89506425\n",
      "Iteration 757, loss = 2.89449056\n",
      "Iteration 758, loss = 2.89567022\n",
      "Iteration 759, loss = 2.89455045\n",
      "Iteration 760, loss = 2.89447791\n",
      "Iteration 761, loss = 2.89349293\n",
      "Iteration 762, loss = 2.89364012\n",
      "Iteration 763, loss = 2.89365504\n",
      "Iteration 764, loss = 2.89307093\n",
      "Iteration 765, loss = 2.89249224\n",
      "Iteration 766, loss = 2.89201873\n",
      "Iteration 767, loss = 2.89199602\n",
      "Iteration 768, loss = 2.89095436\n",
      "Iteration 769, loss = 2.89126052\n",
      "Iteration 770, loss = 2.89108531\n",
      "Iteration 771, loss = 2.89092826\n",
      "Iteration 772, loss = 2.88995461\n",
      "Iteration 773, loss = 2.88985087\n",
      "Iteration 774, loss = 2.88988659\n",
      "Iteration 775, loss = 2.88964523\n",
      "Iteration 776, loss = 2.88955396\n",
      "Iteration 777, loss = 2.88896827\n",
      "Iteration 778, loss = 2.88902319\n",
      "Iteration 779, loss = 2.88846292\n",
      "Iteration 780, loss = 2.88792144\n",
      "Iteration 781, loss = 2.88819015\n",
      "Iteration 782, loss = 2.88829194\n",
      "Iteration 783, loss = 2.88718419\n",
      "Iteration 784, loss = 2.88657714\n",
      "Iteration 785, loss = 2.88653300\n",
      "Iteration 786, loss = 2.88683399\n",
      "Iteration 787, loss = 2.88683240\n",
      "Iteration 788, loss = 2.88653757\n",
      "Iteration 789, loss = 2.88529194\n",
      "Iteration 790, loss = 2.88537436\n",
      "Iteration 791, loss = 2.88544894\n",
      "Iteration 792, loss = 2.88506746\n",
      "Iteration 793, loss = 2.88445788\n",
      "Iteration 794, loss = 2.88473170\n",
      "Iteration 795, loss = 2.88398257\n",
      "Iteration 796, loss = 2.88425535\n",
      "Iteration 797, loss = 2.88356870\n",
      "Iteration 798, loss = 2.88299266\n",
      "Iteration 799, loss = 2.88286934\n",
      "Iteration 800, loss = 2.88156815\n",
      "Iteration 801, loss = 2.88225880\n",
      "Iteration 802, loss = 2.88198022\n",
      "Iteration 803, loss = 2.88174956\n",
      "Iteration 804, loss = 2.88140450\n",
      "Iteration 805, loss = 2.88089607\n",
      "Iteration 806, loss = 2.88065960\n",
      "Iteration 807, loss = 2.88013670\n",
      "Iteration 808, loss = 2.87979511\n",
      "Iteration 809, loss = 2.88044518\n",
      "Iteration 810, loss = 2.87923527\n",
      "Iteration 811, loss = 2.87789697\n",
      "Iteration 812, loss = 2.87856094\n",
      "Iteration 813, loss = 2.87865662\n",
      "Iteration 814, loss = 2.87804994\n",
      "Iteration 815, loss = 2.87792077\n",
      "Iteration 816, loss = 2.87834843\n",
      "Iteration 817, loss = 2.87773942\n",
      "Iteration 818, loss = 2.87731233\n",
      "Iteration 819, loss = 2.87608023\n",
      "Iteration 820, loss = 2.87637750\n",
      "Iteration 821, loss = 2.87614546\n",
      "Iteration 822, loss = 2.87621322\n",
      "Iteration 823, loss = 2.87661565\n",
      "Iteration 824, loss = 2.87562174\n",
      "Iteration 825, loss = 2.87621625\n",
      "Iteration 826, loss = 2.87424205\n",
      "Iteration 827, loss = 2.87416105\n",
      "Iteration 828, loss = 2.87457146\n",
      "Iteration 829, loss = 2.87279891\n",
      "Iteration 830, loss = 2.87416118\n",
      "Iteration 831, loss = 2.87404175\n",
      "Iteration 832, loss = 2.87239409\n",
      "Iteration 833, loss = 2.87282845\n",
      "Iteration 834, loss = 2.87223272\n",
      "Iteration 835, loss = 2.87138879\n",
      "Iteration 836, loss = 2.87101661\n",
      "Iteration 837, loss = 2.87089950\n",
      "Iteration 838, loss = 2.87112019\n",
      "Iteration 839, loss = 2.87070952\n",
      "Iteration 840, loss = 2.87078907\n",
      "Iteration 841, loss = 2.87084812\n",
      "Iteration 842, loss = 2.87005795\n",
      "Iteration 843, loss = 2.86967432\n",
      "Iteration 844, loss = 2.86972356\n",
      "Iteration 845, loss = 2.86977274\n",
      "Iteration 846, loss = 2.86888775\n",
      "Iteration 847, loss = 2.86887406\n",
      "Iteration 848, loss = 2.86845018\n",
      "Iteration 849, loss = 2.86851550\n",
      "Iteration 850, loss = 2.86819145\n",
      "Iteration 851, loss = 2.86830795\n",
      "Iteration 852, loss = 2.86739825\n",
      "Iteration 853, loss = 2.86677086\n",
      "Iteration 854, loss = 2.86643767\n",
      "Iteration 855, loss = 2.86616478\n",
      "Iteration 856, loss = 2.86639772\n",
      "Iteration 857, loss = 2.86651276\n",
      "Iteration 858, loss = 2.86613562\n",
      "Iteration 859, loss = 2.86557661\n",
      "Iteration 860, loss = 2.86517018\n",
      "Iteration 861, loss = 2.86528558\n",
      "Iteration 862, loss = 2.86475146\n",
      "Iteration 863, loss = 2.86414983\n",
      "Iteration 864, loss = 2.86335052\n",
      "Iteration 865, loss = 2.86358067\n",
      "Iteration 866, loss = 2.86301346\n",
      "Iteration 867, loss = 2.86289820\n",
      "Iteration 868, loss = 2.86262736\n",
      "Iteration 869, loss = 2.86206663\n",
      "Iteration 870, loss = 2.86213258\n",
      "Iteration 871, loss = 2.86350364\n",
      "Iteration 872, loss = 2.86192753\n",
      "Iteration 873, loss = 2.86219856\n",
      "Iteration 874, loss = 2.86151037\n",
      "Iteration 875, loss = 2.86035701\n",
      "Iteration 876, loss = 2.86001856\n",
      "Iteration 877, loss = 2.85997841\n",
      "Iteration 878, loss = 2.85921724\n",
      "Iteration 879, loss = 2.86017760\n",
      "Iteration 880, loss = 2.85987293\n",
      "Iteration 881, loss = 2.85964142\n",
      "Iteration 882, loss = 2.85875715\n",
      "Iteration 883, loss = 2.85863377\n",
      "Iteration 884, loss = 2.85977978\n",
      "Iteration 885, loss = 2.85865532\n",
      "Iteration 886, loss = 2.85828260\n",
      "Iteration 887, loss = 2.85886538\n",
      "Iteration 888, loss = 2.85915905\n",
      "Iteration 889, loss = 2.85674858\n",
      "Iteration 890, loss = 2.85638334\n",
      "Iteration 891, loss = 2.85694288\n",
      "Iteration 892, loss = 2.85667170\n",
      "Iteration 893, loss = 2.85597839\n",
      "Iteration 894, loss = 2.85605637\n",
      "Iteration 895, loss = 2.85533328\n",
      "Iteration 896, loss = 2.85474443\n",
      "Iteration 897, loss = 2.85592671\n",
      "Iteration 898, loss = 2.85538591\n",
      "Iteration 899, loss = 2.85460035\n",
      "Iteration 900, loss = 2.85460162\n",
      "Iteration 901, loss = 2.85490809\n",
      "Iteration 902, loss = 2.85394676\n",
      "Iteration 903, loss = 2.85304270\n",
      "Iteration 904, loss = 2.85311765\n",
      "Iteration 905, loss = 2.85277700\n",
      "Iteration 906, loss = 2.85396540\n",
      "Iteration 907, loss = 2.85351627\n",
      "Iteration 908, loss = 2.85298905\n",
      "Iteration 909, loss = 2.85380581\n",
      "Iteration 910, loss = 2.85301933\n",
      "Iteration 911, loss = 2.85148860\n",
      "Iteration 912, loss = 2.85059080\n",
      "Iteration 913, loss = 2.85196443\n",
      "Iteration 914, loss = 2.85005654\n",
      "Iteration 915, loss = 2.84995159\n",
      "Iteration 916, loss = 2.85012326\n",
      "Iteration 917, loss = 2.84914332\n",
      "Iteration 918, loss = 2.85042977\n",
      "Iteration 919, loss = 2.85018930\n",
      "Iteration 920, loss = 2.84913838\n",
      "Iteration 921, loss = 2.84851064\n",
      "Iteration 922, loss = 2.84813007\n",
      "Iteration 923, loss = 2.84756267\n",
      "Iteration 924, loss = 2.84831312\n",
      "Iteration 925, loss = 2.84801242\n",
      "Iteration 926, loss = 2.84780100\n",
      "Iteration 927, loss = 2.84674385\n",
      "Iteration 928, loss = 2.84677887\n",
      "Iteration 929, loss = 2.84749723\n",
      "Iteration 930, loss = 2.84713536\n",
      "Iteration 931, loss = 2.84643554\n",
      "Iteration 932, loss = 2.84534633\n",
      "Iteration 933, loss = 2.84590052\n",
      "Iteration 934, loss = 2.84528136\n",
      "Iteration 935, loss = 2.84518156\n",
      "Iteration 936, loss = 2.84487365\n",
      "Iteration 937, loss = 2.84537204\n",
      "Iteration 938, loss = 2.84577204\n",
      "Iteration 939, loss = 2.84382161\n",
      "Iteration 940, loss = 2.84698085\n",
      "Iteration 941, loss = 2.84450998\n",
      "Iteration 942, loss = 2.84415667\n",
      "Iteration 943, loss = 2.84377235\n",
      "Iteration 944, loss = 2.84353947\n",
      "Iteration 945, loss = 2.84403743\n",
      "Iteration 946, loss = 2.84214566\n",
      "Iteration 947, loss = 2.84252446\n",
      "Iteration 948, loss = 2.84263574\n",
      "Iteration 949, loss = 2.84204054\n",
      "Iteration 950, loss = 2.84225971\n",
      "Iteration 951, loss = 2.84197114\n",
      "Iteration 952, loss = 2.84120571\n",
      "Iteration 953, loss = 2.84069041\n",
      "Iteration 954, loss = 2.84090867\n",
      "Iteration 955, loss = 2.84096465\n",
      "Iteration 956, loss = 2.84268207\n",
      "Iteration 957, loss = 2.84287513\n",
      "Iteration 958, loss = 2.83954573\n",
      "Iteration 959, loss = 2.83960088\n",
      "Iteration 960, loss = 2.83950288\n",
      "Iteration 961, loss = 2.83967718\n",
      "Iteration 962, loss = 2.83911697\n",
      "Iteration 963, loss = 2.83860200\n",
      "Iteration 964, loss = 2.83876918\n",
      "Iteration 965, loss = 2.83836488\n",
      "Iteration 966, loss = 2.83853543\n",
      "Iteration 967, loss = 2.83929094\n",
      "Iteration 968, loss = 2.83864747\n",
      "Iteration 969, loss = 2.83762655\n",
      "Iteration 970, loss = 2.83778475\n",
      "Iteration 971, loss = 2.83751234\n",
      "Iteration 972, loss = 2.83735278\n",
      "Iteration 973, loss = 2.83592294\n",
      "Iteration 974, loss = 2.83548438\n",
      "Iteration 975, loss = 2.83574052\n",
      "Iteration 976, loss = 2.83559372\n",
      "Iteration 977, loss = 2.83513228\n",
      "Iteration 978, loss = 2.83509220\n",
      "Iteration 979, loss = 2.83444623\n",
      "Iteration 980, loss = 2.83433506\n",
      "Iteration 981, loss = 2.83493859\n",
      "Iteration 982, loss = 2.83464061\n",
      "Iteration 983, loss = 2.83471466\n",
      "Iteration 984, loss = 2.83375063\n",
      "Iteration 985, loss = 2.83367791\n",
      "Iteration 986, loss = 2.83270133\n",
      "Iteration 987, loss = 2.83277162\n",
      "Iteration 988, loss = 2.83236606\n",
      "Iteration 989, loss = 2.83269486\n",
      "Iteration 990, loss = 2.83226798\n",
      "Iteration 991, loss = 2.83225856\n",
      "Iteration 992, loss = 2.83279705\n",
      "Iteration 993, loss = 2.83221519\n",
      "Iteration 994, loss = 2.83129492\n",
      "Iteration 995, loss = 2.83086371\n",
      "Iteration 996, loss = 2.83075809\n",
      "Iteration 997, loss = 2.83070740\n",
      "Iteration 998, loss = 2.82964337\n",
      "Iteration 999, loss = 2.82903387\n",
      "Iteration 1000, loss = 2.82955414\n",
      "Iteration 1001, loss = 2.82911014\n",
      "Iteration 1002, loss = 2.82888346\n",
      "Iteration 1003, loss = 2.82971837\n",
      "Iteration 1004, loss = 2.82889666\n",
      "Iteration 1005, loss = 2.82820250\n",
      "Iteration 1006, loss = 2.82915829\n",
      "Iteration 1007, loss = 2.82798889\n",
      "Iteration 1008, loss = 2.82898304\n",
      "Iteration 1009, loss = 2.82796653\n",
      "Iteration 1010, loss = 2.82824588\n",
      "Iteration 1011, loss = 2.82714875\n",
      "Iteration 1012, loss = 2.82661187\n",
      "Iteration 1013, loss = 2.82590080\n",
      "Iteration 1014, loss = 2.82734873\n",
      "Iteration 1015, loss = 2.82643587\n",
      "Iteration 1016, loss = 2.82794176\n",
      "Iteration 1017, loss = 2.82583427\n",
      "Iteration 1018, loss = 2.82594897\n",
      "Iteration 1019, loss = 2.82563109\n",
      "Iteration 1020, loss = 2.82597795\n",
      "Iteration 1021, loss = 2.82593073\n",
      "Iteration 1022, loss = 2.82477819\n",
      "Iteration 1023, loss = 2.82534582\n",
      "Iteration 1024, loss = 2.82483655\n",
      "Iteration 1025, loss = 2.82436711\n",
      "Iteration 1026, loss = 2.82523309\n",
      "Iteration 1027, loss = 2.82408084\n",
      "Iteration 1028, loss = 2.82314108\n",
      "Iteration 1029, loss = 2.82269803\n",
      "Iteration 1030, loss = 2.82250950\n",
      "Iteration 1031, loss = 2.82209018\n",
      "Iteration 1032, loss = 2.82194908\n",
      "Iteration 1033, loss = 2.82206215\n",
      "Iteration 1034, loss = 2.82205348\n",
      "Iteration 1035, loss = 2.82131186\n",
      "Iteration 1036, loss = 2.82143406\n",
      "Iteration 1037, loss = 2.82184644\n",
      "Iteration 1038, loss = 2.82101937\n",
      "Iteration 1039, loss = 2.82176400\n",
      "Iteration 1040, loss = 2.82154019\n",
      "Iteration 1041, loss = 2.82070227\n",
      "Iteration 1042, loss = 2.81982826\n",
      "Iteration 1043, loss = 2.82038754\n",
      "Iteration 1044, loss = 2.82006275\n",
      "Iteration 1045, loss = 2.81974060\n",
      "Iteration 1046, loss = 2.82063076\n",
      "Iteration 1047, loss = 2.82171481\n",
      "Iteration 1048, loss = 2.81925362\n",
      "Iteration 1049, loss = 2.81838253\n",
      "Iteration 1050, loss = 2.81983420\n",
      "Iteration 1051, loss = 2.81888074\n",
      "Iteration 1052, loss = 2.81848919\n",
      "Iteration 1053, loss = 2.81832050\n",
      "Iteration 1054, loss = 2.81782943\n",
      "Iteration 1055, loss = 2.81841532\n",
      "Iteration 1056, loss = 2.81737957\n",
      "Iteration 1057, loss = 2.81876660\n",
      "Iteration 1058, loss = 2.81818422\n",
      "Iteration 1059, loss = 2.81715715\n",
      "Iteration 1060, loss = 2.81600022\n",
      "Iteration 1061, loss = 2.81688845\n",
      "Iteration 1062, loss = 2.81700913\n",
      "Iteration 1063, loss = 2.81614614\n",
      "Iteration 1064, loss = 2.81667003\n",
      "Iteration 1065, loss = 2.81575284\n",
      "Iteration 1066, loss = 2.81578067\n",
      "Iteration 1067, loss = 2.81551973\n",
      "Iteration 1068, loss = 2.81534033\n",
      "Iteration 1069, loss = 2.81529064\n",
      "Iteration 1070, loss = 2.81457488\n",
      "Iteration 1071, loss = 2.81444065\n",
      "Iteration 1072, loss = 2.81411774\n",
      "Iteration 1073, loss = 2.81416578\n",
      "Iteration 1074, loss = 2.81358109\n",
      "Iteration 1075, loss = 2.81345174\n",
      "Iteration 1076, loss = 2.81386671\n",
      "Iteration 1077, loss = 2.81412892\n",
      "Iteration 1078, loss = 2.81302843\n",
      "Iteration 1079, loss = 2.81323482\n",
      "Iteration 1080, loss = 2.81281090\n",
      "Iteration 1081, loss = 2.81240029\n",
      "Iteration 1082, loss = 2.81305306\n",
      "Iteration 1083, loss = 2.81237132\n",
      "Iteration 1084, loss = 2.81173697\n",
      "Iteration 1085, loss = 2.81139691\n",
      "Iteration 1086, loss = 2.81083365\n",
      "Iteration 1087, loss = 2.81136214\n",
      "Iteration 1088, loss = 2.81187008\n",
      "Iteration 1089, loss = 2.81196711\n",
      "Iteration 1090, loss = 2.81051009\n",
      "Iteration 1091, loss = 2.80983699\n",
      "Iteration 1092, loss = 2.81014221\n",
      "Iteration 1093, loss = 2.80947237\n",
      "Iteration 1094, loss = 2.80983078\n",
      "Iteration 1095, loss = 2.81053247\n",
      "Iteration 1096, loss = 2.81003019\n",
      "Iteration 1097, loss = 2.80924824\n",
      "Iteration 1098, loss = 2.80909594\n",
      "Iteration 1099, loss = 2.80816767\n",
      "Iteration 1100, loss = 2.80816366\n",
      "Iteration 1101, loss = 2.80850379\n",
      "Iteration 1102, loss = 2.80813356\n",
      "Iteration 1103, loss = 2.80784374\n",
      "Iteration 1104, loss = 2.80746851\n",
      "Iteration 1105, loss = 2.80868304\n",
      "Iteration 1106, loss = 2.80724203\n",
      "Iteration 1107, loss = 2.80748794\n",
      "Iteration 1108, loss = 2.80659834\n",
      "Iteration 1109, loss = 2.80695562\n",
      "Iteration 1110, loss = 2.80665690\n",
      "Iteration 1111, loss = 2.80667992\n",
      "Iteration 1112, loss = 2.80683749\n",
      "Iteration 1113, loss = 2.80625565\n",
      "Iteration 1114, loss = 2.80590043\n",
      "Iteration 1115, loss = 2.80581352\n",
      "Iteration 1116, loss = 2.80557776\n",
      "Iteration 1117, loss = 2.80521097\n",
      "Iteration 1118, loss = 2.80452755\n",
      "Iteration 1119, loss = 2.80536277\n",
      "Iteration 1120, loss = 2.80490135\n",
      "Iteration 1121, loss = 2.80400759\n",
      "Iteration 1122, loss = 2.80396193\n",
      "Iteration 1123, loss = 2.80316772\n",
      "Iteration 1124, loss = 2.80325891\n",
      "Iteration 1125, loss = 2.80379024\n",
      "Iteration 1126, loss = 2.80399424\n",
      "Iteration 1127, loss = 2.80393249\n",
      "Iteration 1128, loss = 2.80399260\n",
      "Iteration 1129, loss = 2.80268908\n",
      "Iteration 1130, loss = 2.80376377\n",
      "Iteration 1131, loss = 2.80286767\n",
      "Iteration 1132, loss = 2.80213985\n",
      "Iteration 1133, loss = 2.80211893\n",
      "Iteration 1134, loss = 2.80152063\n",
      "Iteration 1135, loss = 2.80104053\n",
      "Iteration 1136, loss = 2.80190137\n",
      "Iteration 1137, loss = 2.80096998\n",
      "Iteration 1138, loss = 2.80087993\n",
      "Iteration 1139, loss = 2.80089770\n",
      "Iteration 1140, loss = 2.80038997\n",
      "Iteration 1141, loss = 2.80038456\n",
      "Iteration 1142, loss = 2.80047033\n",
      "Iteration 1143, loss = 2.80043852\n",
      "Iteration 1144, loss = 2.80087336\n",
      "Iteration 1145, loss = 2.79951576\n",
      "Iteration 1146, loss = 2.79917727\n",
      "Iteration 1147, loss = 2.79939640\n",
      "Iteration 1148, loss = 2.79866639\n",
      "Iteration 1149, loss = 2.79914991\n",
      "Iteration 1150, loss = 2.79910697\n",
      "Iteration 1151, loss = 2.79849715\n",
      "Iteration 1152, loss = 2.79819028\n",
      "Iteration 1153, loss = 2.79772715\n",
      "Iteration 1154, loss = 2.79712587\n",
      "Iteration 1155, loss = 2.79744159\n",
      "Iteration 1156, loss = 2.79711315\n",
      "Iteration 1157, loss = 2.79676717\n",
      "Iteration 1158, loss = 2.79697396\n",
      "Iteration 1159, loss = 2.79665500\n",
      "Iteration 1160, loss = 2.79633647\n",
      "Iteration 1161, loss = 2.79601845\n",
      "Iteration 1162, loss = 2.79575594\n",
      "Iteration 1163, loss = 2.79608508\n",
      "Iteration 1164, loss = 2.79596682\n",
      "Iteration 1165, loss = 2.79581324\n",
      "Iteration 1166, loss = 2.79506763\n",
      "Iteration 1167, loss = 2.79515968\n",
      "Iteration 1168, loss = 2.79440671\n",
      "Iteration 1169, loss = 2.79469129\n",
      "Iteration 1170, loss = 2.79520846\n",
      "Iteration 1171, loss = 2.79461113\n",
      "Iteration 1172, loss = 2.79525450\n",
      "Iteration 1173, loss = 2.79552602\n",
      "Iteration 1174, loss = 2.79494390\n",
      "Iteration 1175, loss = 2.79397580\n",
      "Iteration 1176, loss = 2.79390213\n",
      "Iteration 1177, loss = 2.79425833\n",
      "Iteration 1178, loss = 2.79429259\n",
      "Iteration 1179, loss = 2.79384011\n",
      "Iteration 1180, loss = 2.79365249\n",
      "Iteration 1181, loss = 2.79288964\n",
      "Iteration 1182, loss = 2.79243850\n",
      "Iteration 1183, loss = 2.79226608\n",
      "Iteration 1184, loss = 2.79182678\n",
      "Iteration 1185, loss = 2.79284512\n",
      "Iteration 1186, loss = 2.79246967\n",
      "Iteration 1187, loss = 2.79208219\n",
      "Iteration 1188, loss = 2.79242337\n",
      "Iteration 1189, loss = 2.79225153\n",
      "Iteration 1190, loss = 2.79116694\n",
      "Iteration 1191, loss = 2.79179875\n",
      "Iteration 1192, loss = 2.79193156\n",
      "Iteration 1193, loss = 2.79151666\n",
      "Iteration 1194, loss = 2.79154125\n",
      "Iteration 1195, loss = 2.79173821\n",
      "Iteration 1196, loss = 2.79090548\n",
      "Iteration 1197, loss = 2.79022226\n",
      "Iteration 1198, loss = 2.78946457\n",
      "Iteration 1199, loss = 2.78953450\n",
      "Iteration 1200, loss = 2.78876529\n",
      "Iteration 1201, loss = 2.78906795\n",
      "Iteration 1202, loss = 2.78891038\n",
      "Iteration 1203, loss = 2.78786753\n",
      "Iteration 1204, loss = 2.78812033\n",
      "Iteration 1205, loss = 2.78815552\n",
      "Iteration 1206, loss = 2.78768391\n",
      "Iteration 1207, loss = 2.78746615\n",
      "Iteration 1208, loss = 2.78789991\n",
      "Iteration 1209, loss = 2.78877793\n",
      "Iteration 1210, loss = 2.78872704\n",
      "Iteration 1211, loss = 2.78776173\n",
      "Iteration 1212, loss = 2.78699640\n",
      "Iteration 1213, loss = 2.78723195\n",
      "Iteration 1214, loss = 2.78678743\n",
      "Iteration 1215, loss = 2.78583281\n",
      "Iteration 1216, loss = 2.78647592\n",
      "Iteration 1217, loss = 2.78626411\n",
      "Iteration 1218, loss = 2.78711334\n",
      "Iteration 1219, loss = 2.78655172\n",
      "Iteration 1220, loss = 2.78546543\n",
      "Iteration 1221, loss = 2.78472811\n",
      "Iteration 1222, loss = 2.78492919\n",
      "Iteration 1223, loss = 2.78522835\n",
      "Iteration 1224, loss = 2.78517987\n",
      "Iteration 1225, loss = 2.78451423\n",
      "Iteration 1226, loss = 2.78471727\n",
      "Iteration 1227, loss = 2.78456026\n",
      "Iteration 1228, loss = 2.78373339\n",
      "Iteration 1229, loss = 2.78380876\n",
      "Iteration 1230, loss = 2.78368862\n",
      "Iteration 1231, loss = 2.78292338\n",
      "Iteration 1232, loss = 2.78294639\n",
      "Iteration 1233, loss = 2.78389590\n",
      "Iteration 1234, loss = 2.78324869\n",
      "Iteration 1235, loss = 2.78272094\n",
      "Iteration 1236, loss = 2.78224772\n",
      "Iteration 1237, loss = 2.78225081\n",
      "Iteration 1238, loss = 2.78260105\n",
      "Iteration 1239, loss = 2.78248034\n",
      "Iteration 1240, loss = 2.78237109\n",
      "Iteration 1241, loss = 2.78166968\n",
      "Iteration 1242, loss = 2.78196920\n",
      "Iteration 1243, loss = 2.78163496\n",
      "Iteration 1244, loss = 2.78138290\n",
      "Iteration 1245, loss = 2.78106899\n",
      "Iteration 1246, loss = 2.78166733\n",
      "Iteration 1247, loss = 2.78206083\n",
      "Iteration 1248, loss = 2.78062837\n",
      "Iteration 1249, loss = 2.78160791\n",
      "Iteration 1250, loss = 2.78114742\n",
      "Iteration 1251, loss = 2.78084855\n",
      "Iteration 1252, loss = 2.78128272\n",
      "Iteration 1253, loss = 2.78030456\n",
      "Iteration 1254, loss = 2.78004219\n",
      "Iteration 1255, loss = 2.77906900\n",
      "Iteration 1256, loss = 2.77883157\n",
      "Iteration 1257, loss = 2.77958612\n",
      "Iteration 1258, loss = 2.77899806\n",
      "Iteration 1259, loss = 2.77824916\n",
      "Iteration 1260, loss = 2.77914485\n",
      "Iteration 1261, loss = 2.77859924\n",
      "Iteration 1262, loss = 2.77934798\n",
      "Iteration 1263, loss = 2.77904887\n",
      "Iteration 1264, loss = 2.77819408\n",
      "Iteration 1265, loss = 2.77719953\n",
      "Iteration 1266, loss = 2.77744369\n",
      "Iteration 1267, loss = 2.77650548\n",
      "Iteration 1268, loss = 2.77669650\n",
      "Iteration 1269, loss = 2.77716992\n",
      "Iteration 1270, loss = 2.77801907\n",
      "Iteration 1271, loss = 2.77706130\n",
      "Iteration 1272, loss = 2.77661124\n",
      "Iteration 1273, loss = 2.77651391\n",
      "Iteration 1274, loss = 2.77633616\n",
      "Iteration 1275, loss = 2.77614413\n",
      "Iteration 1276, loss = 2.77618671\n",
      "Iteration 1277, loss = 2.77607973\n",
      "Iteration 1278, loss = 2.77586791\n",
      "Iteration 1279, loss = 2.77522030\n",
      "Iteration 1280, loss = 2.77528627\n",
      "Iteration 1281, loss = 2.77503996\n",
      "Iteration 1282, loss = 2.77515247\n",
      "Iteration 1283, loss = 2.77473271\n",
      "Iteration 1284, loss = 2.77462907\n",
      "Iteration 1285, loss = 2.77393836\n",
      "Iteration 1286, loss = 2.77514345\n",
      "Iteration 1287, loss = 2.77652257\n",
      "Iteration 1288, loss = 2.77483281\n",
      "Iteration 1289, loss = 2.77484291\n",
      "Iteration 1290, loss = 2.77471203\n",
      "Iteration 1291, loss = 2.77646807\n",
      "Iteration 1292, loss = 2.77405434\n",
      "Iteration 1293, loss = 2.77350922\n",
      "Iteration 1294, loss = 2.77573810\n",
      "Iteration 1295, loss = 2.77342306\n",
      "Iteration 1296, loss = 2.77260411\n",
      "Iteration 1297, loss = 2.77231205\n",
      "Iteration 1298, loss = 2.77180653\n",
      "Iteration 1299, loss = 2.77184404\n",
      "Iteration 1300, loss = 2.77163833\n",
      "Iteration 1301, loss = 2.77177980\n",
      "Iteration 1302, loss = 2.77276702\n",
      "Iteration 1303, loss = 2.77212800\n",
      "Iteration 1304, loss = 2.77208032\n",
      "Iteration 1305, loss = 2.77122419\n",
      "Iteration 1306, loss = 2.77141309\n",
      "Iteration 1307, loss = 2.77138513\n",
      "Iteration 1308, loss = 2.77171771\n",
      "Iteration 1309, loss = 2.77084988\n",
      "Iteration 1310, loss = 2.77060112\n",
      "Iteration 1311, loss = 2.76985046\n",
      "Iteration 1312, loss = 2.76982017\n",
      "Iteration 1313, loss = 2.76954484\n",
      "Iteration 1314, loss = 2.77057182\n",
      "Iteration 1315, loss = 2.77003916\n",
      "Iteration 1316, loss = 2.77012198\n",
      "Iteration 1317, loss = 2.76947091\n",
      "Iteration 1318, loss = 2.76961552\n",
      "Iteration 1319, loss = 2.76868432\n",
      "Iteration 1320, loss = 2.76803161\n",
      "Iteration 1321, loss = 2.76882993\n",
      "Iteration 1322, loss = 2.76928380\n",
      "Iteration 1323, loss = 2.76911369\n",
      "Iteration 1324, loss = 2.76794635\n",
      "Iteration 1325, loss = 2.76842176\n",
      "Iteration 1326, loss = 2.76937867\n",
      "Iteration 1327, loss = 2.77029058\n",
      "Iteration 1328, loss = 2.76766616\n",
      "Iteration 1329, loss = 2.76696002\n",
      "Iteration 1330, loss = 2.76710580\n",
      "Iteration 1331, loss = 2.76713368\n",
      "Iteration 1332, loss = 2.76694198\n",
      "Iteration 1333, loss = 2.76707735\n",
      "Iteration 1334, loss = 2.76735639\n",
      "Iteration 1335, loss = 2.76804214\n",
      "Iteration 1336, loss = 2.76709060\n",
      "Iteration 1337, loss = 2.76630631\n",
      "Iteration 1338, loss = 2.76579940\n",
      "Iteration 1339, loss = 2.76540886\n",
      "Iteration 1340, loss = 2.76535680\n",
      "Iteration 1341, loss = 2.76575144\n",
      "Iteration 1342, loss = 2.76562004\n",
      "Iteration 1343, loss = 2.76613581\n",
      "Iteration 1344, loss = 2.76517642\n",
      "Iteration 1345, loss = 2.76493929\n",
      "Iteration 1346, loss = 2.76411014\n",
      "Iteration 1347, loss = 2.76437549\n",
      "Iteration 1348, loss = 2.76446171\n",
      "Iteration 1349, loss = 2.76417021\n",
      "Iteration 1350, loss = 2.76438698\n",
      "Iteration 1351, loss = 2.76405910\n",
      "Iteration 1352, loss = 2.76347767\n",
      "Iteration 1353, loss = 2.76383507\n",
      "Iteration 1354, loss = 2.76337578\n",
      "Iteration 1355, loss = 2.76242343\n",
      "Iteration 1356, loss = 2.76392284\n",
      "Iteration 1357, loss = 2.76387650\n",
      "Iteration 1358, loss = 2.76281312\n",
      "Iteration 1359, loss = 2.76266882\n",
      "Iteration 1360, loss = 2.76227553\n",
      "Iteration 1361, loss = 2.76292348\n",
      "Iteration 1362, loss = 2.76217073\n",
      "Iteration 1363, loss = 2.76145832\n",
      "Iteration 1364, loss = 2.76213761\n",
      "Iteration 1365, loss = 2.76168247\n",
      "Iteration 1366, loss = 2.76223713\n",
      "Iteration 1367, loss = 2.76175059\n",
      "Iteration 1368, loss = 2.76284338\n",
      "Iteration 1369, loss = 2.76205346\n",
      "Iteration 1370, loss = 2.76271485\n",
      "Iteration 1371, loss = 2.76159540\n",
      "Iteration 1372, loss = 2.76035540\n",
      "Iteration 1373, loss = 2.75987225\n",
      "Iteration 1374, loss = 2.76016334\n",
      "Iteration 1375, loss = 2.76023751\n",
      "Iteration 1376, loss = 2.75991608\n",
      "Iteration 1377, loss = 2.76005053\n",
      "Iteration 1378, loss = 2.75964518\n",
      "Iteration 1379, loss = 2.76004233\n",
      "Iteration 1380, loss = 2.76029424\n",
      "Iteration 1381, loss = 2.75892235\n",
      "Iteration 1382, loss = 2.75957703\n",
      "Iteration 1383, loss = 2.75967555\n",
      "Iteration 1384, loss = 2.75921343\n",
      "Iteration 1385, loss = 2.75830303\n",
      "Iteration 1386, loss = 2.75805457\n",
      "Iteration 1387, loss = 2.75770918\n",
      "Iteration 1388, loss = 2.75751051\n",
      "Iteration 1389, loss = 2.75755800\n",
      "Iteration 1390, loss = 2.75844513\n",
      "Iteration 1391, loss = 2.75773495\n",
      "Iteration 1392, loss = 2.75820028\n",
      "Iteration 1393, loss = 2.75847767\n",
      "Iteration 1394, loss = 2.76012205\n",
      "Iteration 1395, loss = 2.75854128\n",
      "Iteration 1396, loss = 2.75721417\n",
      "Iteration 1397, loss = 2.75767990\n",
      "Iteration 1398, loss = 2.75740984\n",
      "Iteration 1399, loss = 2.75664250\n",
      "Iteration 1400, loss = 2.75594712\n",
      "Iteration 1401, loss = 2.75627862\n",
      "Iteration 1402, loss = 2.75716559\n",
      "Iteration 1403, loss = 2.75835019\n",
      "Iteration 1404, loss = 2.75730538\n",
      "Iteration 1405, loss = 2.75652601\n",
      "Iteration 1406, loss = 2.75590142\n",
      "Iteration 1407, loss = 2.75561075\n",
      "Iteration 1408, loss = 2.75494878\n",
      "Iteration 1409, loss = 2.75545804\n",
      "Iteration 1410, loss = 2.75647671\n",
      "Iteration 1411, loss = 2.75507702\n",
      "Iteration 1412, loss = 2.75478134\n",
      "Iteration 1413, loss = 2.75520520\n",
      "Iteration 1414, loss = 2.75531785\n",
      "Iteration 1415, loss = 2.75406652\n",
      "Iteration 1416, loss = 2.75355071\n",
      "Iteration 1417, loss = 2.75391793\n",
      "Iteration 1418, loss = 2.75619079\n",
      "Iteration 1419, loss = 2.75502700\n",
      "Iteration 1420, loss = 2.75342528\n",
      "Iteration 1421, loss = 2.75410837\n",
      "Iteration 1422, loss = 2.75381311\n",
      "Iteration 1423, loss = 2.75261412\n",
      "Iteration 1424, loss = 2.75343212\n",
      "Iteration 1425, loss = 2.75258299\n",
      "Iteration 1426, loss = 2.75238137\n",
      "Iteration 1427, loss = 2.75297406\n",
      "Iteration 1428, loss = 2.75242970\n",
      "Iteration 1429, loss = 2.75231029\n",
      "Iteration 1430, loss = 2.75188786\n",
      "Iteration 1431, loss = 2.75290727\n",
      "Iteration 1432, loss = 2.75162634\n",
      "Iteration 1433, loss = 2.75135369\n",
      "Iteration 1434, loss = 2.75102622\n",
      "Iteration 1435, loss = 2.75098793\n",
      "Iteration 1436, loss = 2.75126770\n",
      "Iteration 1437, loss = 2.75065570\n",
      "Iteration 1438, loss = 2.75113576\n",
      "Iteration 1439, loss = 2.75061968\n",
      "Iteration 1440, loss = 2.75045583\n",
      "Iteration 1441, loss = 2.75073469\n",
      "Iteration 1442, loss = 2.75054421\n",
      "Iteration 1443, loss = 2.75091309\n",
      "Iteration 1444, loss = 2.75024671\n",
      "Iteration 1445, loss = 2.74997980\n",
      "Iteration 1446, loss = 2.75027640\n",
      "Iteration 1447, loss = 2.74991076\n",
      "Iteration 1448, loss = 2.74865850\n",
      "Iteration 1449, loss = 2.74918142\n",
      "Iteration 1450, loss = 2.74884280\n",
      "Iteration 1451, loss = 2.74957748\n",
      "Iteration 1452, loss = 2.74896894\n",
      "Iteration 1453, loss = 2.74874178\n",
      "Iteration 1454, loss = 2.74815197\n",
      "Iteration 1455, loss = 2.74796645\n",
      "Iteration 1456, loss = 2.74782105\n",
      "Iteration 1457, loss = 2.74778859\n",
      "Iteration 1458, loss = 2.74826589\n",
      "Iteration 1459, loss = 2.74966468\n",
      "Iteration 1460, loss = 2.74849707\n",
      "Iteration 1461, loss = 2.74791353\n",
      "Iteration 1462, loss = 2.74818150\n",
      "Iteration 1463, loss = 2.74782627\n",
      "Iteration 1464, loss = 2.74743422\n",
      "Iteration 1465, loss = 2.74757723\n",
      "Iteration 1466, loss = 2.74720236\n",
      "Iteration 1467, loss = 2.74642929\n",
      "Iteration 1468, loss = 2.74679169\n",
      "Iteration 1469, loss = 2.74770073\n",
      "Iteration 1470, loss = 2.74683734\n",
      "Iteration 1471, loss = 2.74764008\n",
      "Iteration 1472, loss = 2.74482748\n",
      "Iteration 1473, loss = 2.74650751\n",
      "Iteration 1474, loss = 2.74684161\n",
      "Iteration 1475, loss = 2.74692315\n",
      "Iteration 1476, loss = 2.74690655\n",
      "Iteration 1477, loss = 2.74589012\n",
      "Iteration 1478, loss = 2.74540949\n",
      "Iteration 1479, loss = 2.74542051\n",
      "Iteration 1480, loss = 2.74485350\n",
      "Iteration 1481, loss = 2.74419415\n",
      "Iteration 1482, loss = 2.74397681\n",
      "Iteration 1483, loss = 2.74453116\n",
      "Iteration 1484, loss = 2.74387838\n",
      "Iteration 1485, loss = 2.74372813\n",
      "Iteration 1486, loss = 2.74317431\n",
      "Iteration 1487, loss = 2.74367499\n",
      "Iteration 1488, loss = 2.74304909\n",
      "Iteration 1489, loss = 2.74374210\n",
      "Iteration 1490, loss = 2.74379112\n",
      "Iteration 1491, loss = 2.74269226\n",
      "Iteration 1492, loss = 2.74406809\n",
      "Iteration 1493, loss = 2.74344015\n",
      "Iteration 1494, loss = 2.74348459\n",
      "Iteration 1495, loss = 2.74304354\n",
      "Iteration 1496, loss = 2.74280027\n",
      "Iteration 1497, loss = 2.74240279\n",
      "Iteration 1498, loss = 2.74171828\n",
      "Iteration 1499, loss = 2.74092560\n",
      "Iteration 1500, loss = 2.74193052\n",
      "Iteration 1501, loss = 2.74164826\n",
      "Iteration 1502, loss = 2.74160939\n",
      "Iteration 1503, loss = 2.74159137\n",
      "Iteration 1504, loss = 2.74202631\n",
      "Iteration 1505, loss = 2.74229372\n",
      "Iteration 1506, loss = 2.74210275\n",
      "Iteration 1507, loss = 2.74086066\n",
      "Iteration 1508, loss = 2.74038132\n",
      "Iteration 1509, loss = 2.74075140\n",
      "Iteration 1510, loss = 2.74179453\n",
      "Iteration 1511, loss = 2.74195539\n",
      "Iteration 1512, loss = 2.74037403\n",
      "Iteration 1513, loss = 2.73940569\n",
      "Iteration 1514, loss = 2.74025249\n",
      "Iteration 1515, loss = 2.73978834\n",
      "Iteration 1516, loss = 2.73913303\n",
      "Iteration 1517, loss = 2.73954523\n",
      "Iteration 1518, loss = 2.74066779\n",
      "Iteration 1519, loss = 2.73998392\n",
      "Iteration 1520, loss = 2.74044329\n",
      "Iteration 1521, loss = 2.73971310\n",
      "Iteration 1522, loss = 2.73911505\n",
      "Iteration 1523, loss = 2.73997363\n",
      "Iteration 1524, loss = 2.73899008\n",
      "Iteration 1525, loss = 2.73955915\n",
      "Iteration 1526, loss = 2.73957668\n",
      "Iteration 1527, loss = 2.73840161\n",
      "Iteration 1528, loss = 2.73745229\n",
      "Iteration 1529, loss = 2.73852430\n",
      "Iteration 1530, loss = 2.73870215\n",
      "Iteration 1531, loss = 2.73900073\n",
      "Iteration 1532, loss = 2.73842039\n",
      "Iteration 1533, loss = 2.73870028\n",
      "Iteration 1534, loss = 2.73928047\n",
      "Iteration 1535, loss = 2.73807978\n",
      "Iteration 1536, loss = 2.73685657\n",
      "Iteration 1537, loss = 2.73737561\n",
      "Iteration 1538, loss = 2.73640356\n",
      "Iteration 1539, loss = 2.73659508\n",
      "Iteration 1540, loss = 2.73616729\n",
      "Iteration 1541, loss = 2.73593355\n",
      "Iteration 1542, loss = 2.73611625\n",
      "Iteration 1543, loss = 2.73555769\n",
      "Iteration 1544, loss = 2.73571192\n",
      "Iteration 1545, loss = 2.73582078\n",
      "Iteration 1546, loss = 2.73534317\n",
      "Iteration 1547, loss = 2.73620590\n",
      "Iteration 1548, loss = 2.73556973\n",
      "Iteration 1549, loss = 2.73488670\n",
      "Iteration 1550, loss = 2.73485599\n",
      "Iteration 1551, loss = 2.73529783\n",
      "Iteration 1552, loss = 2.73543625\n",
      "Iteration 1553, loss = 2.73542683\n",
      "Iteration 1554, loss = 2.73733115\n",
      "Iteration 1555, loss = 2.73552840\n",
      "Iteration 1556, loss = 2.73478651\n",
      "Iteration 1557, loss = 2.73448777\n",
      "Iteration 1558, loss = 2.73459704\n",
      "Iteration 1559, loss = 2.73456123\n",
      "Iteration 1560, loss = 2.73397179\n",
      "Iteration 1561, loss = 2.73391792\n",
      "Iteration 1562, loss = 2.73465519\n",
      "Iteration 1563, loss = 2.73326056\n",
      "Iteration 1564, loss = 2.73308919\n",
      "Iteration 1565, loss = 2.73243610\n",
      "Iteration 1566, loss = 2.73284843\n",
      "Iteration 1567, loss = 2.73297002\n",
      "Iteration 1568, loss = 2.73283318\n",
      "Iteration 1569, loss = 2.73299331\n",
      "Iteration 1570, loss = 2.73291964\n",
      "Iteration 1571, loss = 2.73286837\n",
      "Iteration 1572, loss = 2.73194275\n",
      "Iteration 1573, loss = 2.73283369\n",
      "Iteration 1574, loss = 2.73238992\n",
      "Iteration 1575, loss = 2.73185155\n",
      "Iteration 1576, loss = 2.73227395\n",
      "Iteration 1577, loss = 2.73176850\n",
      "Iteration 1578, loss = 2.73351187\n",
      "Iteration 1579, loss = 2.73204342\n",
      "Iteration 1580, loss = 2.73194302\n",
      "Iteration 1581, loss = 2.73112574\n",
      "Iteration 1582, loss = 2.73159234\n",
      "Iteration 1583, loss = 2.72997019\n",
      "Iteration 1584, loss = 2.73044449\n",
      "Iteration 1585, loss = 2.73042287\n",
      "Iteration 1586, loss = 2.73028320\n",
      "Iteration 1587, loss = 2.73066429\n",
      "Iteration 1588, loss = 2.73068127\n",
      "Iteration 1589, loss = 2.72962789\n",
      "Iteration 1590, loss = 2.72958679\n",
      "Iteration 1591, loss = 2.72958273\n",
      "Iteration 1592, loss = 2.73026369\n",
      "Iteration 1593, loss = 2.73030881\n",
      "Iteration 1594, loss = 2.72924327\n",
      "Iteration 1595, loss = 2.72974317\n",
      "Iteration 1596, loss = 2.73006846\n",
      "Iteration 1597, loss = 2.72893290\n",
      "Iteration 1598, loss = 2.72834499\n",
      "Iteration 1599, loss = 2.72870699\n",
      "Iteration 1600, loss = 2.72866260\n",
      "Iteration 1601, loss = 2.72908742\n",
      "Iteration 1602, loss = 2.72938529\n",
      "Iteration 1603, loss = 2.72830918\n",
      "Iteration 1604, loss = 2.72847800\n",
      "Iteration 1605, loss = 2.72934720\n",
      "Iteration 1606, loss = 2.72890899\n",
      "Iteration 1607, loss = 2.72999372\n",
      "Iteration 1608, loss = 2.72855868\n",
      "Iteration 1609, loss = 2.72737646\n",
      "Iteration 1610, loss = 2.72898952\n",
      "Iteration 1611, loss = 2.72810118\n",
      "Iteration 1612, loss = 2.72790624\n",
      "Iteration 1613, loss = 2.72850133\n",
      "Iteration 1614, loss = 2.72645199\n",
      "Iteration 1615, loss = 2.72912130\n",
      "Iteration 1616, loss = 2.72737921\n",
      "Iteration 1617, loss = 2.72661116\n",
      "Iteration 1618, loss = 2.72603773\n",
      "Iteration 1619, loss = 2.72648279\n",
      "Iteration 1620, loss = 2.72579955\n",
      "Iteration 1621, loss = 2.72605101\n",
      "Iteration 1622, loss = 2.72648969\n",
      "Iteration 1623, loss = 2.72635422\n",
      "Iteration 1624, loss = 2.72584189\n",
      "Iteration 1625, loss = 2.72678661\n",
      "Iteration 1626, loss = 2.72506239\n",
      "Iteration 1627, loss = 2.72654548\n",
      "Iteration 1628, loss = 2.72516539\n",
      "Iteration 1629, loss = 2.72552966\n",
      "Iteration 1630, loss = 2.72597456\n",
      "Iteration 1631, loss = 2.72406238\n",
      "Iteration 1632, loss = 2.72466093\n",
      "Iteration 1633, loss = 2.72514639\n",
      "Iteration 1634, loss = 2.72432188\n",
      "Iteration 1635, loss = 2.72448319\n",
      "Iteration 1636, loss = 2.72577904\n",
      "Iteration 1637, loss = 2.72409273\n",
      "Iteration 1638, loss = 2.72448374\n",
      "Iteration 1639, loss = 2.72490094\n",
      "Iteration 1640, loss = 2.72378621\n",
      "Iteration 1641, loss = 2.72263917\n",
      "Iteration 1642, loss = 2.72408633\n",
      "Iteration 1643, loss = 2.72338777\n",
      "Iteration 1644, loss = 2.72441306\n",
      "Iteration 1645, loss = 2.72334583\n",
      "Iteration 1646, loss = 2.72309903\n",
      "Iteration 1647, loss = 2.72289987\n",
      "Iteration 1648, loss = 2.72149908\n",
      "Iteration 1649, loss = 2.72352112\n",
      "Iteration 1650, loss = 2.72266720\n",
      "Iteration 1651, loss = 2.72321189\n",
      "Iteration 1652, loss = 2.72250548\n",
      "Iteration 1653, loss = 2.72298023\n",
      "Iteration 1654, loss = 2.72295966\n",
      "Iteration 1655, loss = 2.72249901\n",
      "Iteration 1656, loss = 2.72161382\n",
      "Iteration 1657, loss = 2.72224170\n",
      "Iteration 1658, loss = 2.72196489\n",
      "Iteration 1659, loss = 2.72059306\n",
      "Iteration 1660, loss = 2.72112446\n",
      "Iteration 1661, loss = 2.72134634\n",
      "Iteration 1662, loss = 2.72067888\n",
      "Iteration 1663, loss = 2.72177232\n",
      "Iteration 1664, loss = 2.72075867\n",
      "Iteration 1665, loss = 2.72104151\n",
      "Iteration 1666, loss = 2.72003890\n",
      "Iteration 1667, loss = 2.72036335\n",
      "Iteration 1668, loss = 2.71997951\n",
      "Iteration 1669, loss = 2.71978513\n",
      "Iteration 1670, loss = 2.71955272\n",
      "Iteration 1671, loss = 2.71992329\n",
      "Iteration 1672, loss = 2.72011674\n",
      "Iteration 1673, loss = 2.72014857\n",
      "Iteration 1674, loss = 2.71984714\n",
      "Iteration 1675, loss = 2.72007987\n",
      "Iteration 1676, loss = 2.71918704\n",
      "Iteration 1677, loss = 2.71977326\n",
      "Iteration 1678, loss = 2.71960034\n",
      "Iteration 1679, loss = 2.71863911\n",
      "Iteration 1680, loss = 2.71933699\n",
      "Iteration 1681, loss = 2.71844036\n",
      "Iteration 1682, loss = 2.71855190\n",
      "Iteration 1683, loss = 2.71840329\n",
      "Iteration 1684, loss = 2.71823608\n",
      "Iteration 1, loss = 4.14766872\n",
      "Iteration 2, loss = 4.01980552\n",
      "Iteration 3, loss = 3.90060770\n",
      "Iteration 4, loss = 3.78902255\n",
      "Iteration 5, loss = 3.67972416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1684) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 3.57882787\n",
      "Iteration 7, loss = 3.48675387\n",
      "Iteration 8, loss = 3.40782680\n",
      "Iteration 9, loss = 3.33874205\n",
      "Iteration 10, loss = 3.28947436\n",
      "Iteration 11, loss = 3.25913704\n",
      "Iteration 12, loss = 3.24447041\n",
      "Iteration 13, loss = 3.23772429\n",
      "Iteration 14, loss = 3.23462957\n",
      "Iteration 15, loss = 3.23312890\n",
      "Iteration 16, loss = 3.23179031\n",
      "Iteration 17, loss = 3.22998402\n",
      "Iteration 18, loss = 3.22843938\n",
      "Iteration 19, loss = 3.22817852\n",
      "Iteration 20, loss = 3.22661119\n",
      "Iteration 21, loss = 3.22629076\n",
      "Iteration 22, loss = 3.22629722\n",
      "Iteration 23, loss = 3.22569475\n",
      "Iteration 24, loss = 3.22480634\n",
      "Iteration 25, loss = 3.22469068\n",
      "Iteration 26, loss = 3.22393277\n",
      "Iteration 27, loss = 3.22273671\n",
      "Iteration 28, loss = 3.22245310\n",
      "Iteration 29, loss = 3.22175031\n",
      "Iteration 30, loss = 3.22171015\n",
      "Iteration 31, loss = 3.22081298\n",
      "Iteration 32, loss = 3.22042982\n",
      "Iteration 33, loss = 3.22059800\n",
      "Iteration 34, loss = 3.22005804\n",
      "Iteration 35, loss = 3.21943802\n",
      "Iteration 36, loss = 3.21844057\n",
      "Iteration 37, loss = 3.21739158\n",
      "Iteration 38, loss = 3.21680523\n",
      "Iteration 39, loss = 3.21614708\n",
      "Iteration 40, loss = 3.21595350\n",
      "Iteration 41, loss = 3.21546651\n",
      "Iteration 42, loss = 3.21487568\n",
      "Iteration 43, loss = 3.21431542\n",
      "Iteration 44, loss = 3.21401434\n",
      "Iteration 45, loss = 3.21398231\n",
      "Iteration 46, loss = 3.21298327\n",
      "Iteration 47, loss = 3.21272819\n",
      "Iteration 48, loss = 3.21251289\n",
      "Iteration 49, loss = 3.21214296\n",
      "Iteration 50, loss = 3.21154731\n",
      "Iteration 51, loss = 3.21149077\n",
      "Iteration 52, loss = 3.21090170\n",
      "Iteration 53, loss = 3.21085875\n",
      "Iteration 54, loss = 3.21008745\n",
      "Iteration 55, loss = 3.21000526\n",
      "Iteration 56, loss = 3.20940136\n",
      "Iteration 57, loss = 3.20824600\n",
      "Iteration 58, loss = 3.20772257\n",
      "Iteration 59, loss = 3.20731100\n",
      "Iteration 60, loss = 3.20666949\n",
      "Iteration 61, loss = 3.20623098\n",
      "Iteration 62, loss = 3.20592783\n",
      "Iteration 63, loss = 3.20578786\n",
      "Iteration 64, loss = 3.20503944\n",
      "Iteration 65, loss = 3.20532985\n",
      "Iteration 66, loss = 3.20418159\n",
      "Iteration 67, loss = 3.20358243\n",
      "Iteration 68, loss = 3.20316999\n",
      "Iteration 69, loss = 3.20279199\n",
      "Iteration 70, loss = 3.20291750\n",
      "Iteration 71, loss = 3.20233366\n",
      "Iteration 72, loss = 3.20231964\n",
      "Iteration 73, loss = 3.20167782\n",
      "Iteration 74, loss = 3.20220333\n",
      "Iteration 75, loss = 3.20114040\n",
      "Iteration 76, loss = 3.20049657\n",
      "Iteration 77, loss = 3.19982457\n",
      "Iteration 78, loss = 3.19994558\n",
      "Iteration 79, loss = 3.19971846\n",
      "Iteration 80, loss = 3.19888260\n",
      "Iteration 81, loss = 3.19848346\n",
      "Iteration 82, loss = 3.19820110\n",
      "Iteration 83, loss = 3.19781187\n",
      "Iteration 84, loss = 3.19719091\n",
      "Iteration 85, loss = 3.19673399\n",
      "Iteration 86, loss = 3.19631423\n",
      "Iteration 87, loss = 3.19612877\n",
      "Iteration 88, loss = 3.19566414\n",
      "Iteration 89, loss = 3.19505402\n",
      "Iteration 90, loss = 3.19476719\n",
      "Iteration 91, loss = 3.19415207\n",
      "Iteration 92, loss = 3.19417773\n",
      "Iteration 93, loss = 3.19437728\n",
      "Iteration 94, loss = 3.19424388\n",
      "Iteration 95, loss = 3.19313471\n",
      "Iteration 96, loss = 3.19259658\n",
      "Iteration 97, loss = 3.19201270\n",
      "Iteration 98, loss = 3.19118114\n",
      "Iteration 99, loss = 3.19132219\n",
      "Iteration 100, loss = 3.19115577\n",
      "Iteration 101, loss = 3.19031835\n",
      "Iteration 102, loss = 3.18977847\n",
      "Iteration 103, loss = 3.18924175\n",
      "Iteration 104, loss = 3.18847816\n",
      "Iteration 105, loss = 3.18833176\n",
      "Iteration 106, loss = 3.18830408\n",
      "Iteration 107, loss = 3.18743539\n",
      "Iteration 108, loss = 3.18708433\n",
      "Iteration 109, loss = 3.18698031\n",
      "Iteration 110, loss = 3.18653810\n",
      "Iteration 111, loss = 3.18665267\n",
      "Iteration 112, loss = 3.18544732\n",
      "Iteration 113, loss = 3.18470809\n",
      "Iteration 114, loss = 3.18481798\n",
      "Iteration 115, loss = 3.18430078\n",
      "Iteration 116, loss = 3.18387075\n",
      "Iteration 117, loss = 3.18356366\n",
      "Iteration 118, loss = 3.18345826\n",
      "Iteration 119, loss = 3.18300663\n",
      "Iteration 120, loss = 3.18270478\n",
      "Iteration 121, loss = 3.18172319\n",
      "Iteration 122, loss = 3.18135751\n",
      "Iteration 123, loss = 3.18083807\n",
      "Iteration 124, loss = 3.18126968\n",
      "Iteration 125, loss = 3.18105584\n",
      "Iteration 126, loss = 3.17993026\n",
      "Iteration 127, loss = 3.17886317\n",
      "Iteration 128, loss = 3.17888825\n",
      "Iteration 129, loss = 3.17886954\n",
      "Iteration 130, loss = 3.17761286\n",
      "Iteration 131, loss = 3.17712497\n",
      "Iteration 132, loss = 3.17705110\n",
      "Iteration 133, loss = 3.17634378\n",
      "Iteration 134, loss = 3.17590979\n",
      "Iteration 135, loss = 3.17479755\n",
      "Iteration 136, loss = 3.17472838\n",
      "Iteration 137, loss = 3.17502356\n",
      "Iteration 138, loss = 3.17427623\n",
      "Iteration 139, loss = 3.17429535\n",
      "Iteration 140, loss = 3.17357168\n",
      "Iteration 141, loss = 3.17259380\n",
      "Iteration 142, loss = 3.17196327\n",
      "Iteration 143, loss = 3.17155392\n",
      "Iteration 144, loss = 3.17124001\n",
      "Iteration 145, loss = 3.17073084\n",
      "Iteration 146, loss = 3.17047357\n",
      "Iteration 147, loss = 3.16946129\n",
      "Iteration 148, loss = 3.16885281\n",
      "Iteration 149, loss = 3.16867715\n",
      "Iteration 150, loss = 3.16821901\n",
      "Iteration 151, loss = 3.16781017\n",
      "Iteration 152, loss = 3.16759744\n",
      "Iteration 153, loss = 3.16665454\n",
      "Iteration 154, loss = 3.16629400\n",
      "Iteration 155, loss = 3.16589973\n",
      "Iteration 156, loss = 3.16521235\n",
      "Iteration 157, loss = 3.16503991\n",
      "Iteration 158, loss = 3.16446106\n",
      "Iteration 159, loss = 3.16416516\n",
      "Iteration 160, loss = 3.16350176\n",
      "Iteration 161, loss = 3.16283398\n",
      "Iteration 162, loss = 3.16220426\n",
      "Iteration 163, loss = 3.16140035\n",
      "Iteration 164, loss = 3.16044501\n",
      "Iteration 165, loss = 3.16013798\n",
      "Iteration 166, loss = 3.16019157\n",
      "Iteration 167, loss = 3.15931522\n",
      "Iteration 168, loss = 3.15861602\n",
      "Iteration 169, loss = 3.15777609\n",
      "Iteration 170, loss = 3.15732632\n",
      "Iteration 171, loss = 3.15737742\n",
      "Iteration 172, loss = 3.15739943\n",
      "Iteration 173, loss = 3.15602861\n",
      "Iteration 174, loss = 3.15658506\n",
      "Iteration 175, loss = 3.15541431\n",
      "Iteration 176, loss = 3.15489498\n",
      "Iteration 177, loss = 3.15443544\n",
      "Iteration 178, loss = 3.15319815\n",
      "Iteration 179, loss = 3.15232176\n",
      "Iteration 180, loss = 3.15222732\n",
      "Iteration 181, loss = 3.15128432\n",
      "Iteration 182, loss = 3.15019273\n",
      "Iteration 183, loss = 3.14978587\n",
      "Iteration 184, loss = 3.14923688\n",
      "Iteration 185, loss = 3.14893615\n",
      "Iteration 186, loss = 3.14840979\n",
      "Iteration 187, loss = 3.14763300\n",
      "Iteration 188, loss = 3.14696198\n",
      "Iteration 189, loss = 3.14673641\n",
      "Iteration 190, loss = 3.14676280\n",
      "Iteration 191, loss = 3.14659518\n",
      "Iteration 192, loss = 3.14476213\n",
      "Iteration 193, loss = 3.14569167\n",
      "Iteration 194, loss = 3.14541917\n",
      "Iteration 195, loss = 3.14413412\n",
      "Iteration 196, loss = 3.14323989\n",
      "Iteration 197, loss = 3.14269286\n",
      "Iteration 198, loss = 3.14131929\n",
      "Iteration 199, loss = 3.14125238\n",
      "Iteration 200, loss = 3.14081113\n",
      "Iteration 201, loss = 3.14081768\n",
      "Iteration 202, loss = 3.14021319\n",
      "Iteration 203, loss = 3.14018677\n",
      "Iteration 204, loss = 3.13850513\n",
      "Iteration 205, loss = 3.13769717\n",
      "Iteration 206, loss = 3.13717314\n",
      "Iteration 207, loss = 3.13691217\n",
      "Iteration 208, loss = 3.13649030\n",
      "Iteration 209, loss = 3.13536934\n",
      "Iteration 210, loss = 3.13462092\n",
      "Iteration 211, loss = 3.13420982\n",
      "Iteration 212, loss = 3.13494035\n",
      "Iteration 213, loss = 3.13369651\n",
      "Iteration 214, loss = 3.13290814\n",
      "Iteration 215, loss = 3.13236572\n",
      "Iteration 216, loss = 3.13260651\n",
      "Iteration 217, loss = 3.13154419\n",
      "Iteration 218, loss = 3.13088067\n",
      "Iteration 219, loss = 3.13005430\n",
      "Iteration 220, loss = 3.12906505\n",
      "Iteration 221, loss = 3.12780858\n",
      "Iteration 222, loss = 3.12712244\n",
      "Iteration 223, loss = 3.12711042\n",
      "Iteration 224, loss = 3.12620456\n",
      "Iteration 225, loss = 3.12607865\n",
      "Iteration 226, loss = 3.12564475\n",
      "Iteration 227, loss = 3.12504683\n",
      "Iteration 228, loss = 3.12407594\n",
      "Iteration 229, loss = 3.12490690\n",
      "Iteration 230, loss = 3.12336527\n",
      "Iteration 231, loss = 3.12263266\n",
      "Iteration 232, loss = 3.12245614\n",
      "Iteration 233, loss = 3.12067345\n",
      "Iteration 234, loss = 3.12014634\n",
      "Iteration 235, loss = 3.11979181\n",
      "Iteration 236, loss = 3.11942171\n",
      "Iteration 237, loss = 3.11852892\n",
      "Iteration 238, loss = 3.11792634\n",
      "Iteration 239, loss = 3.11743300\n",
      "Iteration 240, loss = 3.11671540\n",
      "Iteration 241, loss = 3.11608732\n",
      "Iteration 242, loss = 3.11534917\n",
      "Iteration 243, loss = 3.11492296\n",
      "Iteration 244, loss = 3.11378244\n",
      "Iteration 245, loss = 3.11344378\n",
      "Iteration 246, loss = 3.11273279\n",
      "Iteration 247, loss = 3.11221475\n",
      "Iteration 248, loss = 3.11164500\n",
      "Iteration 249, loss = 3.11124390\n",
      "Iteration 250, loss = 3.11028358\n",
      "Iteration 251, loss = 3.10978908\n",
      "Iteration 252, loss = 3.10827791\n",
      "Iteration 253, loss = 3.10774359\n",
      "Iteration 254, loss = 3.10712917\n",
      "Iteration 255, loss = 3.10663079\n",
      "Iteration 256, loss = 3.10655563\n",
      "Iteration 257, loss = 3.10573307\n",
      "Iteration 258, loss = 3.10462216\n",
      "Iteration 259, loss = 3.10386436\n",
      "Iteration 260, loss = 3.10389530\n",
      "Iteration 261, loss = 3.10262078\n",
      "Iteration 262, loss = 3.10197403\n",
      "Iteration 263, loss = 3.10237021\n",
      "Iteration 264, loss = 3.10160511\n",
      "Iteration 265, loss = 3.10062334\n",
      "Iteration 266, loss = 3.10065989\n",
      "Iteration 267, loss = 3.09934057\n",
      "Iteration 268, loss = 3.09843306\n",
      "Iteration 269, loss = 3.09954394\n",
      "Iteration 270, loss = 3.09885290\n",
      "Iteration 271, loss = 3.09644104\n",
      "Iteration 272, loss = 3.09569139\n",
      "Iteration 273, loss = 3.09521504\n",
      "Iteration 274, loss = 3.09503923\n",
      "Iteration 275, loss = 3.09415421\n",
      "Iteration 276, loss = 3.09391616\n",
      "Iteration 277, loss = 3.09296698\n",
      "Iteration 278, loss = 3.09188797\n",
      "Iteration 279, loss = 3.09133783\n",
      "Iteration 280, loss = 3.09079360\n",
      "Iteration 281, loss = 3.09099174\n",
      "Iteration 282, loss = 3.09008638\n",
      "Iteration 283, loss = 3.08940105\n",
      "Iteration 284, loss = 3.08858581\n",
      "Iteration 285, loss = 3.08769316\n",
      "Iteration 286, loss = 3.08701101\n",
      "Iteration 287, loss = 3.08613077\n",
      "Iteration 288, loss = 3.08594233\n",
      "Iteration 289, loss = 3.08502608\n",
      "Iteration 290, loss = 3.08495403\n",
      "Iteration 291, loss = 3.08402569\n",
      "Iteration 292, loss = 3.08344630\n",
      "Iteration 293, loss = 3.08233015\n",
      "Iteration 294, loss = 3.08189024\n",
      "Iteration 295, loss = 3.08202072\n",
      "Iteration 296, loss = 3.08159732\n",
      "Iteration 297, loss = 3.08103794\n",
      "Iteration 298, loss = 3.08000006\n",
      "Iteration 299, loss = 3.07954222\n",
      "Iteration 300, loss = 3.07884509\n",
      "Iteration 301, loss = 3.07805651\n",
      "Iteration 302, loss = 3.07763858\n",
      "Iteration 303, loss = 3.07649489\n",
      "Iteration 304, loss = 3.07597163\n",
      "Iteration 305, loss = 3.07518050\n",
      "Iteration 306, loss = 3.07422180\n",
      "Iteration 307, loss = 3.07421797\n",
      "Iteration 308, loss = 3.07315923\n",
      "Iteration 309, loss = 3.07228290\n",
      "Iteration 310, loss = 3.07218278\n",
      "Iteration 311, loss = 3.07230926\n",
      "Iteration 312, loss = 3.07126952\n",
      "Iteration 313, loss = 3.07043768\n",
      "Iteration 314, loss = 3.06987890\n",
      "Iteration 315, loss = 3.06885084\n",
      "Iteration 316, loss = 3.06785153\n",
      "Iteration 317, loss = 3.06721328\n",
      "Iteration 318, loss = 3.06681881\n",
      "Iteration 319, loss = 3.06610434\n",
      "Iteration 320, loss = 3.06562567\n",
      "Iteration 321, loss = 3.06489498\n",
      "Iteration 322, loss = 3.06419465\n",
      "Iteration 323, loss = 3.06368552\n",
      "Iteration 324, loss = 3.06349750\n",
      "Iteration 325, loss = 3.06250743\n",
      "Iteration 326, loss = 3.06164175\n",
      "Iteration 327, loss = 3.06083393\n",
      "Iteration 328, loss = 3.06027899\n",
      "Iteration 329, loss = 3.06049129\n",
      "Iteration 330, loss = 3.06057350\n",
      "Iteration 331, loss = 3.06003714\n",
      "Iteration 332, loss = 3.05924593\n",
      "Iteration 333, loss = 3.05894404\n",
      "Iteration 334, loss = 3.05762585\n",
      "Iteration 335, loss = 3.05641066\n",
      "Iteration 336, loss = 3.05608767\n",
      "Iteration 337, loss = 3.05545997\n",
      "Iteration 338, loss = 3.05486065\n",
      "Iteration 339, loss = 3.05423018\n",
      "Iteration 340, loss = 3.05330980\n",
      "Iteration 341, loss = 3.05266202\n",
      "Iteration 342, loss = 3.05187027\n",
      "Iteration 343, loss = 3.05256664\n",
      "Iteration 344, loss = 3.05182530\n",
      "Iteration 345, loss = 3.05072847\n",
      "Iteration 346, loss = 3.04988999\n",
      "Iteration 347, loss = 3.04918954\n",
      "Iteration 348, loss = 3.04846610\n",
      "Iteration 349, loss = 3.04835116\n",
      "Iteration 350, loss = 3.04756216\n",
      "Iteration 351, loss = 3.04719784\n",
      "Iteration 352, loss = 3.04665652\n",
      "Iteration 353, loss = 3.04571962\n",
      "Iteration 354, loss = 3.04488019\n",
      "Iteration 355, loss = 3.04440424\n",
      "Iteration 356, loss = 3.04418759\n",
      "Iteration 357, loss = 3.04334630\n",
      "Iteration 358, loss = 3.04203993\n",
      "Iteration 359, loss = 3.04166715\n",
      "Iteration 360, loss = 3.04131990\n",
      "Iteration 361, loss = 3.04045232\n",
      "Iteration 362, loss = 3.04034026\n",
      "Iteration 363, loss = 3.03970369\n",
      "Iteration 364, loss = 3.03933821\n",
      "Iteration 365, loss = 3.03860802\n",
      "Iteration 366, loss = 3.03776535\n",
      "Iteration 367, loss = 3.03748430\n",
      "Iteration 368, loss = 3.03749819\n",
      "Iteration 369, loss = 3.03642732\n",
      "Iteration 370, loss = 3.03620234\n",
      "Iteration 371, loss = 3.03617260\n",
      "Iteration 372, loss = 3.03499846\n",
      "Iteration 373, loss = 3.03455305\n",
      "Iteration 374, loss = 3.03344735\n",
      "Iteration 375, loss = 3.03271049\n",
      "Iteration 376, loss = 3.03372786\n",
      "Iteration 377, loss = 3.03172870\n",
      "Iteration 378, loss = 3.03097151\n",
      "Iteration 379, loss = 3.03109072\n",
      "Iteration 380, loss = 3.03069952\n",
      "Iteration 381, loss = 3.02979033\n",
      "Iteration 382, loss = 3.02968808\n",
      "Iteration 383, loss = 3.02789192\n",
      "Iteration 384, loss = 3.02778108\n",
      "Iteration 385, loss = 3.02714144\n",
      "Iteration 386, loss = 3.02697884\n",
      "Iteration 387, loss = 3.02708704\n",
      "Iteration 388, loss = 3.02645018\n",
      "Iteration 389, loss = 3.02534665\n",
      "Iteration 390, loss = 3.02451562\n",
      "Iteration 391, loss = 3.02501327\n",
      "Iteration 392, loss = 3.02401837\n",
      "Iteration 393, loss = 3.02256397\n",
      "Iteration 394, loss = 3.02253465\n",
      "Iteration 395, loss = 3.02187232\n",
      "Iteration 396, loss = 3.02171995\n",
      "Iteration 397, loss = 3.02146597\n",
      "Iteration 398, loss = 3.02173809\n",
      "Iteration 399, loss = 3.02007190\n",
      "Iteration 400, loss = 3.01979525\n",
      "Iteration 401, loss = 3.01970079\n",
      "Iteration 402, loss = 3.01916025\n",
      "Iteration 403, loss = 3.01771608\n",
      "Iteration 404, loss = 3.01781308\n",
      "Iteration 405, loss = 3.01735606\n",
      "Iteration 406, loss = 3.01652809\n",
      "Iteration 407, loss = 3.01564864\n",
      "Iteration 408, loss = 3.01726151\n",
      "Iteration 409, loss = 3.01562090\n",
      "Iteration 410, loss = 3.01393662\n",
      "Iteration 411, loss = 3.01355378\n",
      "Iteration 412, loss = 3.01327925\n",
      "Iteration 413, loss = 3.01252841\n",
      "Iteration 414, loss = 3.01153185\n",
      "Iteration 415, loss = 3.01083300\n",
      "Iteration 416, loss = 3.01073499\n",
      "Iteration 417, loss = 3.01125053\n",
      "Iteration 418, loss = 3.01022298\n",
      "Iteration 419, loss = 3.00944483\n",
      "Iteration 420, loss = 3.00883365\n",
      "Iteration 421, loss = 3.00799245\n",
      "Iteration 422, loss = 3.00695745\n",
      "Iteration 423, loss = 3.00702064\n",
      "Iteration 424, loss = 3.00621444\n",
      "Iteration 425, loss = 3.00548931\n",
      "Iteration 426, loss = 3.00562541\n",
      "Iteration 427, loss = 3.00504902\n",
      "Iteration 428, loss = 3.00428145\n",
      "Iteration 429, loss = 3.00361517\n",
      "Iteration 430, loss = 3.00341113\n",
      "Iteration 431, loss = 3.00328383\n",
      "Iteration 432, loss = 3.00420783\n",
      "Iteration 433, loss = 3.00286613\n",
      "Iteration 434, loss = 3.00178679\n",
      "Iteration 435, loss = 3.00098668\n",
      "Iteration 436, loss = 3.00045877\n",
      "Iteration 437, loss = 3.00032217\n",
      "Iteration 438, loss = 2.99981404\n",
      "Iteration 439, loss = 2.99967933\n",
      "Iteration 440, loss = 2.99900597\n",
      "Iteration 441, loss = 2.99827228\n",
      "Iteration 442, loss = 2.99774849\n",
      "Iteration 443, loss = 2.99845521\n",
      "Iteration 444, loss = 2.99752398\n",
      "Iteration 445, loss = 2.99824273\n",
      "Iteration 446, loss = 2.99987483\n",
      "Iteration 447, loss = 2.99670539\n",
      "Iteration 448, loss = 2.99489601\n",
      "Iteration 449, loss = 2.99424056\n",
      "Iteration 450, loss = 2.99405154\n",
      "Iteration 451, loss = 2.99582569\n",
      "Iteration 452, loss = 2.99414430\n",
      "Iteration 453, loss = 2.99201499\n",
      "Iteration 454, loss = 2.99169036\n",
      "Iteration 455, loss = 2.99110862\n",
      "Iteration 456, loss = 2.99032635\n",
      "Iteration 457, loss = 2.99030054\n",
      "Iteration 458, loss = 2.98949030\n",
      "Iteration 459, loss = 2.99003841\n",
      "Iteration 460, loss = 2.98940999\n",
      "Iteration 461, loss = 2.98923021\n",
      "Iteration 462, loss = 2.98898263\n",
      "Iteration 463, loss = 2.98814207\n",
      "Iteration 464, loss = 2.98846271\n",
      "Iteration 465, loss = 2.98877855\n",
      "Iteration 466, loss = 2.98680941\n",
      "Iteration 467, loss = 2.98673669\n",
      "Iteration 468, loss = 2.98593927\n",
      "Iteration 469, loss = 2.98481985\n",
      "Iteration 470, loss = 2.98421437\n",
      "Iteration 471, loss = 2.98426776\n",
      "Iteration 472, loss = 2.98388775\n",
      "Iteration 473, loss = 2.98297671\n",
      "Iteration 474, loss = 2.98289064\n",
      "Iteration 475, loss = 2.98219192\n",
      "Iteration 476, loss = 2.98143997\n",
      "Iteration 477, loss = 2.98284514\n",
      "Iteration 478, loss = 2.98191219\n",
      "Iteration 479, loss = 2.98020691\n",
      "Iteration 480, loss = 2.98075073\n",
      "Iteration 481, loss = 2.97977368\n",
      "Iteration 482, loss = 2.97924330\n",
      "Iteration 483, loss = 2.97925985\n",
      "Iteration 484, loss = 2.97779051\n",
      "Iteration 485, loss = 2.97703051\n",
      "Iteration 486, loss = 2.97771041\n",
      "Iteration 487, loss = 2.97634820\n",
      "Iteration 488, loss = 2.97636248\n",
      "Iteration 489, loss = 2.97571216\n",
      "Iteration 490, loss = 2.97523034\n",
      "Iteration 491, loss = 2.97467560\n",
      "Iteration 492, loss = 2.97443507\n",
      "Iteration 493, loss = 2.97388817\n",
      "Iteration 494, loss = 2.97416638\n",
      "Iteration 495, loss = 2.97376206\n",
      "Iteration 496, loss = 2.97227179\n",
      "Iteration 497, loss = 2.97230338\n",
      "Iteration 498, loss = 2.97170813\n",
      "Iteration 499, loss = 2.97045260\n",
      "Iteration 500, loss = 2.97082958\n",
      "Iteration 501, loss = 2.97070209\n",
      "Iteration 502, loss = 2.97059509\n",
      "Iteration 503, loss = 2.96961160\n",
      "Iteration 504, loss = 2.96849066\n",
      "Iteration 505, loss = 2.96846903\n",
      "Iteration 506, loss = 2.96866593\n",
      "Iteration 507, loss = 2.96783044\n",
      "Iteration 508, loss = 2.96806982\n",
      "Iteration 509, loss = 2.96761996\n",
      "Iteration 510, loss = 2.96719752\n",
      "Iteration 511, loss = 2.96615827\n",
      "Iteration 512, loss = 2.96579616\n",
      "Iteration 513, loss = 2.96521528\n",
      "Iteration 514, loss = 2.96454705\n",
      "Iteration 515, loss = 2.96463793\n",
      "Iteration 516, loss = 2.96353548\n",
      "Iteration 517, loss = 2.96342353\n",
      "Iteration 518, loss = 2.96278897\n",
      "Iteration 519, loss = 2.96238708\n",
      "Iteration 520, loss = 2.96220844\n",
      "Iteration 521, loss = 2.96180009\n",
      "Iteration 522, loss = 2.96148592\n",
      "Iteration 523, loss = 2.96088773\n",
      "Iteration 524, loss = 2.96090935\n",
      "Iteration 525, loss = 2.96189543\n",
      "Iteration 526, loss = 2.96032134\n",
      "Iteration 527, loss = 2.95969348\n",
      "Iteration 528, loss = 2.95885668\n",
      "Iteration 529, loss = 2.96120947\n",
      "Iteration 530, loss = 2.95930721\n",
      "Iteration 531, loss = 2.95821225\n",
      "Iteration 532, loss = 2.95832137\n",
      "Iteration 533, loss = 2.95701356\n",
      "Iteration 534, loss = 2.95679937\n",
      "Iteration 535, loss = 2.95553276\n",
      "Iteration 536, loss = 2.95534221\n",
      "Iteration 537, loss = 2.95516904\n",
      "Iteration 538, loss = 2.95494962\n",
      "Iteration 539, loss = 2.95525691\n",
      "Iteration 540, loss = 2.95561149\n",
      "Iteration 541, loss = 2.95538057\n",
      "Iteration 542, loss = 2.95376585\n",
      "Iteration 543, loss = 2.95311051\n",
      "Iteration 544, loss = 2.95268838\n",
      "Iteration 545, loss = 2.95113496\n",
      "Iteration 546, loss = 2.95112742\n",
      "Iteration 547, loss = 2.95036793\n",
      "Iteration 548, loss = 2.94985834\n",
      "Iteration 549, loss = 2.94943654\n",
      "Iteration 550, loss = 2.94854886\n",
      "Iteration 551, loss = 2.94870597\n",
      "Iteration 552, loss = 2.94915647\n",
      "Iteration 553, loss = 2.94900081\n",
      "Iteration 554, loss = 2.94728467\n",
      "Iteration 555, loss = 2.94715463\n",
      "Iteration 556, loss = 2.94724167\n",
      "Iteration 557, loss = 2.94643422\n",
      "Iteration 558, loss = 2.94596386\n",
      "Iteration 559, loss = 2.94573003\n",
      "Iteration 560, loss = 2.94582743\n",
      "Iteration 561, loss = 2.94443021\n",
      "Iteration 562, loss = 2.94464132\n",
      "Iteration 563, loss = 2.94539824\n",
      "Iteration 564, loss = 2.94502353\n",
      "Iteration 565, loss = 2.94407916\n",
      "Iteration 566, loss = 2.94365354\n",
      "Iteration 567, loss = 2.94266006\n",
      "Iteration 568, loss = 2.94242098\n",
      "Iteration 569, loss = 2.94127829\n",
      "Iteration 570, loss = 2.94133508\n",
      "Iteration 571, loss = 2.94088865\n",
      "Iteration 572, loss = 2.94230539\n",
      "Iteration 573, loss = 2.94087795\n",
      "Iteration 574, loss = 2.94123223\n",
      "Iteration 575, loss = 2.93991318\n",
      "Iteration 576, loss = 2.93926319\n",
      "Iteration 577, loss = 2.93863481\n",
      "Iteration 578, loss = 2.93839179\n",
      "Iteration 579, loss = 2.93732556\n",
      "Iteration 580, loss = 2.93677253\n",
      "Iteration 581, loss = 2.93655595\n",
      "Iteration 582, loss = 2.93605396\n",
      "Iteration 583, loss = 2.93515331\n",
      "Iteration 584, loss = 2.93508700\n",
      "Iteration 585, loss = 2.93500768\n",
      "Iteration 586, loss = 2.93508111\n",
      "Iteration 587, loss = 2.93461155\n",
      "Iteration 588, loss = 2.93288236\n",
      "Iteration 589, loss = 2.93430284\n",
      "Iteration 590, loss = 2.93359999\n",
      "Iteration 591, loss = 2.93259086\n",
      "Iteration 592, loss = 2.93151882\n",
      "Iteration 593, loss = 2.93177411\n",
      "Iteration 594, loss = 2.93163145\n",
      "Iteration 595, loss = 2.93106387\n",
      "Iteration 596, loss = 2.93125060\n",
      "Iteration 597, loss = 2.93260915\n",
      "Iteration 598, loss = 2.92988860\n",
      "Iteration 599, loss = 2.92972572\n",
      "Iteration 600, loss = 2.93009162\n",
      "Iteration 601, loss = 2.92876117\n",
      "Iteration 602, loss = 2.92906237\n",
      "Iteration 603, loss = 2.92810971\n",
      "Iteration 604, loss = 2.92731709\n",
      "Iteration 605, loss = 2.92701822\n",
      "Iteration 606, loss = 2.92652701\n",
      "Iteration 607, loss = 2.92617246\n",
      "Iteration 608, loss = 2.92632027\n",
      "Iteration 609, loss = 2.92572865\n",
      "Iteration 610, loss = 2.92573916\n",
      "Iteration 611, loss = 2.92490289\n",
      "Iteration 612, loss = 2.92431607\n",
      "Iteration 613, loss = 2.92394212\n",
      "Iteration 614, loss = 2.92462021\n",
      "Iteration 615, loss = 2.92388078\n",
      "Iteration 616, loss = 2.92285572\n",
      "Iteration 617, loss = 2.92251885\n",
      "Iteration 618, loss = 2.92218486\n",
      "Iteration 619, loss = 2.92213559\n",
      "Iteration 620, loss = 2.92168410\n",
      "Iteration 621, loss = 2.92091533\n",
      "Iteration 622, loss = 2.92011553\n",
      "Iteration 623, loss = 2.91995601\n",
      "Iteration 624, loss = 2.91899700\n",
      "Iteration 625, loss = 2.91929684\n",
      "Iteration 626, loss = 2.91877084\n",
      "Iteration 627, loss = 2.91865657\n",
      "Iteration 628, loss = 2.91901268\n",
      "Iteration 629, loss = 2.91825212\n",
      "Iteration 630, loss = 2.91777263\n",
      "Iteration 631, loss = 2.91717050\n",
      "Iteration 632, loss = 2.91668790\n",
      "Iteration 633, loss = 2.91689754\n",
      "Iteration 634, loss = 2.91621006\n",
      "Iteration 635, loss = 2.91759322\n",
      "Iteration 636, loss = 2.91620482\n",
      "Iteration 637, loss = 2.91507869\n",
      "Iteration 638, loss = 2.91656992\n",
      "Iteration 639, loss = 2.91470346\n",
      "Iteration 640, loss = 2.91397205\n",
      "Iteration 641, loss = 2.91339930\n",
      "Iteration 642, loss = 2.91325265\n",
      "Iteration 643, loss = 2.91280840\n",
      "Iteration 644, loss = 2.91258024\n",
      "Iteration 645, loss = 2.91229849\n",
      "Iteration 646, loss = 2.91118360\n",
      "Iteration 647, loss = 2.91065997\n",
      "Iteration 648, loss = 2.91063685\n",
      "Iteration 649, loss = 2.91069567\n",
      "Iteration 650, loss = 2.90983513\n",
      "Iteration 651, loss = 2.90913679\n",
      "Iteration 652, loss = 2.90935681\n",
      "Iteration 653, loss = 2.90928745\n",
      "Iteration 654, loss = 2.90864571\n",
      "Iteration 655, loss = 2.90847179\n",
      "Iteration 656, loss = 2.90781264\n",
      "Iteration 657, loss = 2.90739545\n",
      "Iteration 658, loss = 2.90676141\n",
      "Iteration 659, loss = 2.90674579\n",
      "Iteration 660, loss = 2.90701518\n",
      "Iteration 661, loss = 2.90625759\n",
      "Iteration 662, loss = 2.90633950\n",
      "Iteration 663, loss = 2.90528883\n",
      "Iteration 664, loss = 2.90491827\n",
      "Iteration 665, loss = 2.90475781\n",
      "Iteration 666, loss = 2.90427317\n",
      "Iteration 667, loss = 2.90452504\n",
      "Iteration 668, loss = 2.90327585\n",
      "Iteration 669, loss = 2.90395679\n",
      "Iteration 670, loss = 2.90351479\n",
      "Iteration 671, loss = 2.90256212\n",
      "Iteration 672, loss = 2.90239507\n",
      "Iteration 673, loss = 2.90203482\n",
      "Iteration 674, loss = 2.90171009\n",
      "Iteration 675, loss = 2.90189951\n",
      "Iteration 676, loss = 2.90069969\n",
      "Iteration 677, loss = 2.90015713\n",
      "Iteration 678, loss = 2.89944472\n",
      "Iteration 679, loss = 2.89953308\n",
      "Iteration 680, loss = 2.89899449\n",
      "Iteration 681, loss = 2.89858126\n",
      "Iteration 682, loss = 2.89849414\n",
      "Iteration 683, loss = 2.89858953\n",
      "Iteration 684, loss = 2.89868395\n",
      "Iteration 685, loss = 2.89827144\n",
      "Iteration 686, loss = 2.89809250\n",
      "Iteration 687, loss = 2.89723570\n",
      "Iteration 688, loss = 2.89686582\n",
      "Iteration 689, loss = 2.89633294\n",
      "Iteration 690, loss = 2.89566983\n",
      "Iteration 691, loss = 2.89522344\n",
      "Iteration 692, loss = 2.89430512\n",
      "Iteration 693, loss = 2.89479817\n",
      "Iteration 694, loss = 2.89471329\n",
      "Iteration 695, loss = 2.89468758\n",
      "Iteration 696, loss = 2.89448167\n",
      "Iteration 697, loss = 2.89293552\n",
      "Iteration 698, loss = 2.89278659\n",
      "Iteration 699, loss = 2.89265593\n",
      "Iteration 700, loss = 2.89224494\n",
      "Iteration 701, loss = 2.89135310\n",
      "Iteration 702, loss = 2.89125281\n",
      "Iteration 703, loss = 2.89100425\n",
      "Iteration 704, loss = 2.89022141\n",
      "Iteration 705, loss = 2.89043529\n",
      "Iteration 706, loss = 2.89007258\n",
      "Iteration 707, loss = 2.88989545\n",
      "Iteration 708, loss = 2.88937894\n",
      "Iteration 709, loss = 2.88965777\n",
      "Iteration 710, loss = 2.88879928\n",
      "Iteration 711, loss = 2.88879858\n",
      "Iteration 712, loss = 2.88830692\n",
      "Iteration 713, loss = 2.88760379\n",
      "Iteration 714, loss = 2.88782368\n",
      "Iteration 715, loss = 2.88733462\n",
      "Iteration 716, loss = 2.88668671\n",
      "Iteration 717, loss = 2.88576729\n",
      "Iteration 718, loss = 2.88552226\n",
      "Iteration 719, loss = 2.88600719\n",
      "Iteration 720, loss = 2.88575479\n",
      "Iteration 721, loss = 2.88579456\n",
      "Iteration 722, loss = 2.88401103\n",
      "Iteration 723, loss = 2.88460744\n",
      "Iteration 724, loss = 2.88373608\n",
      "Iteration 725, loss = 2.88353110\n",
      "Iteration 726, loss = 2.88294380\n",
      "Iteration 727, loss = 2.88257157\n",
      "Iteration 728, loss = 2.88330322\n",
      "Iteration 729, loss = 2.88344234\n",
      "Iteration 730, loss = 2.88268040\n",
      "Iteration 731, loss = 2.88137848\n",
      "Iteration 732, loss = 2.88075955\n",
      "Iteration 733, loss = 2.88036935\n",
      "Iteration 734, loss = 2.88023758\n",
      "Iteration 735, loss = 2.87996154\n",
      "Iteration 736, loss = 2.88004517\n",
      "Iteration 737, loss = 2.87955367\n",
      "Iteration 738, loss = 2.87935036\n",
      "Iteration 739, loss = 2.87890176\n",
      "Iteration 740, loss = 2.87859734\n",
      "Iteration 741, loss = 2.87951354\n",
      "Iteration 742, loss = 2.87782828\n",
      "Iteration 743, loss = 2.87693038\n",
      "Iteration 744, loss = 2.87755212\n",
      "Iteration 745, loss = 2.87767574\n",
      "Iteration 746, loss = 2.87647594\n",
      "Iteration 747, loss = 2.87605219\n",
      "Iteration 748, loss = 2.87528525\n",
      "Iteration 749, loss = 2.87599841\n",
      "Iteration 750, loss = 2.87502277\n",
      "Iteration 751, loss = 2.87411198\n",
      "Iteration 752, loss = 2.87357919\n",
      "Iteration 753, loss = 2.87335537\n",
      "Iteration 754, loss = 2.87297062\n",
      "Iteration 755, loss = 2.87276774\n",
      "Iteration 756, loss = 2.87323260\n",
      "Iteration 757, loss = 2.87289704\n",
      "Iteration 758, loss = 2.87305510\n",
      "Iteration 759, loss = 2.87248189\n",
      "Iteration 760, loss = 2.87162469\n",
      "Iteration 761, loss = 2.87233997\n",
      "Iteration 762, loss = 2.87183121\n",
      "Iteration 763, loss = 2.87078442\n",
      "Iteration 764, loss = 2.87099456\n",
      "Iteration 765, loss = 2.87101061\n",
      "Iteration 766, loss = 2.87088933\n",
      "Iteration 767, loss = 2.87094584\n",
      "Iteration 768, loss = 2.86977219\n",
      "Iteration 769, loss = 2.86866146\n",
      "Iteration 770, loss = 2.86840867\n",
      "Iteration 771, loss = 2.86909791\n",
      "Iteration 772, loss = 2.86909943\n",
      "Iteration 773, loss = 2.86784259\n",
      "Iteration 774, loss = 2.86689568\n",
      "Iteration 775, loss = 2.86895969\n",
      "Iteration 776, loss = 2.86711865\n",
      "Iteration 777, loss = 2.86650155\n",
      "Iteration 778, loss = 2.86639099\n",
      "Iteration 779, loss = 2.86628517\n",
      "Iteration 780, loss = 2.86583222\n",
      "Iteration 781, loss = 2.86579033\n",
      "Iteration 782, loss = 2.86514310\n",
      "Iteration 783, loss = 2.86436239\n",
      "Iteration 784, loss = 2.86445564\n",
      "Iteration 785, loss = 2.86417341\n",
      "Iteration 786, loss = 2.86427837\n",
      "Iteration 787, loss = 2.86529087\n",
      "Iteration 788, loss = 2.86385951\n",
      "Iteration 789, loss = 2.86306670\n",
      "Iteration 790, loss = 2.86344552\n",
      "Iteration 791, loss = 2.86265870\n",
      "Iteration 792, loss = 2.86219437\n",
      "Iteration 793, loss = 2.86183792\n",
      "Iteration 794, loss = 2.86215543\n",
      "Iteration 795, loss = 2.86276643\n",
      "Iteration 796, loss = 2.86252378\n",
      "Iteration 797, loss = 2.86123884\n",
      "Iteration 798, loss = 2.86048596\n",
      "Iteration 799, loss = 2.86245934\n",
      "Iteration 800, loss = 2.86196022\n",
      "Iteration 801, loss = 2.86063963\n",
      "Iteration 802, loss = 2.86070160\n",
      "Iteration 803, loss = 2.85930971\n",
      "Iteration 804, loss = 2.85863514\n",
      "Iteration 805, loss = 2.85799982\n",
      "Iteration 806, loss = 2.85798621\n",
      "Iteration 807, loss = 2.85753422\n",
      "Iteration 808, loss = 2.85768598\n",
      "Iteration 809, loss = 2.85790946\n",
      "Iteration 810, loss = 2.85702740\n",
      "Iteration 811, loss = 2.85834853\n",
      "Iteration 812, loss = 2.85726845\n",
      "Iteration 813, loss = 2.85641319\n",
      "Iteration 814, loss = 2.85541855\n",
      "Iteration 815, loss = 2.85512783\n",
      "Iteration 816, loss = 2.85488250\n",
      "Iteration 817, loss = 2.85469031\n",
      "Iteration 818, loss = 2.85486847\n",
      "Iteration 819, loss = 2.85457960\n",
      "Iteration 820, loss = 2.85631444\n",
      "Iteration 821, loss = 2.85481014\n",
      "Iteration 822, loss = 2.85400263\n",
      "Iteration 823, loss = 2.85341183\n",
      "Iteration 824, loss = 2.85446118\n",
      "Iteration 825, loss = 2.85375166\n",
      "Iteration 826, loss = 2.85319054\n",
      "Iteration 827, loss = 2.85176908\n",
      "Iteration 828, loss = 2.85134107\n",
      "Iteration 829, loss = 2.85095458\n",
      "Iteration 830, loss = 2.85043784\n",
      "Iteration 831, loss = 2.85034657\n",
      "Iteration 832, loss = 2.85019333\n",
      "Iteration 833, loss = 2.84924707\n",
      "Iteration 834, loss = 2.84917549\n",
      "Iteration 835, loss = 2.84943037\n",
      "Iteration 836, loss = 2.84993272\n",
      "Iteration 837, loss = 2.84973001\n",
      "Iteration 838, loss = 2.84914682\n",
      "Iteration 839, loss = 2.84902236\n",
      "Iteration 840, loss = 2.84810363\n",
      "Iteration 841, loss = 2.84841627\n",
      "Iteration 842, loss = 2.84787000\n",
      "Iteration 843, loss = 2.84713383\n",
      "Iteration 844, loss = 2.84711634\n",
      "Iteration 845, loss = 2.84676352\n",
      "Iteration 846, loss = 2.84734492\n",
      "Iteration 847, loss = 2.84718155\n",
      "Iteration 848, loss = 2.84609849\n",
      "Iteration 849, loss = 2.84670683\n",
      "Iteration 850, loss = 2.84631563\n",
      "Iteration 851, loss = 2.84498648\n",
      "Iteration 852, loss = 2.84534393\n",
      "Iteration 853, loss = 2.84454743\n",
      "Iteration 854, loss = 2.84483394\n",
      "Iteration 855, loss = 2.84444285\n",
      "Iteration 856, loss = 2.84428026\n",
      "Iteration 857, loss = 2.84357704\n",
      "Iteration 858, loss = 2.84304249\n",
      "Iteration 859, loss = 2.84267241\n",
      "Iteration 860, loss = 2.84267414\n",
      "Iteration 861, loss = 2.84216584\n",
      "Iteration 862, loss = 2.84141708\n",
      "Iteration 863, loss = 2.84110786\n",
      "Iteration 864, loss = 2.84054912\n",
      "Iteration 865, loss = 2.84013266\n",
      "Iteration 866, loss = 2.84034620\n",
      "Iteration 867, loss = 2.84015090\n",
      "Iteration 868, loss = 2.83978577\n",
      "Iteration 869, loss = 2.84008388\n",
      "Iteration 870, loss = 2.83957724\n",
      "Iteration 871, loss = 2.83958525\n",
      "Iteration 872, loss = 2.83962476\n",
      "Iteration 873, loss = 2.83995943\n",
      "Iteration 874, loss = 2.83954441\n",
      "Iteration 875, loss = 2.83835130\n",
      "Iteration 876, loss = 2.83848008\n",
      "Iteration 877, loss = 2.83809069\n",
      "Iteration 878, loss = 2.83790015\n",
      "Iteration 879, loss = 2.83843037\n",
      "Iteration 880, loss = 2.83754485\n",
      "Iteration 881, loss = 2.83670299\n",
      "Iteration 882, loss = 2.83643003\n",
      "Iteration 883, loss = 2.83527358\n",
      "Iteration 884, loss = 2.83570747\n",
      "Iteration 885, loss = 2.83557874\n",
      "Iteration 886, loss = 2.83532246\n",
      "Iteration 887, loss = 2.83529618\n",
      "Iteration 888, loss = 2.83480864\n",
      "Iteration 889, loss = 2.83441401\n",
      "Iteration 890, loss = 2.83419719\n",
      "Iteration 891, loss = 2.83430801\n",
      "Iteration 892, loss = 2.83394474\n",
      "Iteration 893, loss = 2.83453037\n",
      "Iteration 894, loss = 2.83346655\n",
      "Iteration 895, loss = 2.83267665\n",
      "Iteration 896, loss = 2.83226643\n",
      "Iteration 897, loss = 2.83219593\n",
      "Iteration 898, loss = 2.83263982\n",
      "Iteration 899, loss = 2.83226685\n",
      "Iteration 900, loss = 2.83146861\n",
      "Iteration 901, loss = 2.83183940\n",
      "Iteration 902, loss = 2.83150310\n",
      "Iteration 903, loss = 2.83092917\n",
      "Iteration 904, loss = 2.83112374\n",
      "Iteration 905, loss = 2.83041729\n",
      "Iteration 906, loss = 2.82936060\n",
      "Iteration 907, loss = 2.82979408\n",
      "Iteration 908, loss = 2.82897575\n",
      "Iteration 909, loss = 2.82892658\n",
      "Iteration 910, loss = 2.82907341\n",
      "Iteration 911, loss = 2.82965489\n",
      "Iteration 912, loss = 2.82998851\n",
      "Iteration 913, loss = 2.82892133\n",
      "Iteration 914, loss = 2.82792493\n",
      "Iteration 915, loss = 2.82806880\n",
      "Iteration 916, loss = 2.82742252\n",
      "Iteration 917, loss = 2.82615896\n",
      "Iteration 918, loss = 2.82659209\n",
      "Iteration 919, loss = 2.82717423\n",
      "Iteration 920, loss = 2.82553403\n",
      "Iteration 921, loss = 2.82608265\n",
      "Iteration 922, loss = 2.82579311\n",
      "Iteration 923, loss = 2.82521963\n",
      "Iteration 924, loss = 2.82490879\n",
      "Iteration 925, loss = 2.82446513\n",
      "Iteration 926, loss = 2.82521286\n",
      "Iteration 927, loss = 2.82464756\n",
      "Iteration 928, loss = 2.82393618\n",
      "Iteration 929, loss = 2.82303913\n",
      "Iteration 930, loss = 2.82388144\n",
      "Iteration 931, loss = 2.82355656\n",
      "Iteration 932, loss = 2.82269537\n",
      "Iteration 933, loss = 2.82217461\n",
      "Iteration 934, loss = 2.82194692\n",
      "Iteration 935, loss = 2.82207675\n",
      "Iteration 936, loss = 2.82181923\n",
      "Iteration 937, loss = 2.82151826\n",
      "Iteration 938, loss = 2.82169266\n",
      "Iteration 939, loss = 2.82228843\n",
      "Iteration 940, loss = 2.82184926\n",
      "Iteration 941, loss = 2.82178471\n",
      "Iteration 942, loss = 2.82088514\n",
      "Iteration 943, loss = 2.82111244\n",
      "Iteration 944, loss = 2.81926589\n",
      "Iteration 945, loss = 2.82001535\n",
      "Iteration 946, loss = 2.82030236\n",
      "Iteration 947, loss = 2.81918590\n",
      "Iteration 948, loss = 2.81940095\n",
      "Iteration 949, loss = 2.81946148\n",
      "Iteration 950, loss = 2.81905506\n",
      "Iteration 951, loss = 2.81911899\n",
      "Iteration 952, loss = 2.81783200\n",
      "Iteration 953, loss = 2.81775255\n",
      "Iteration 954, loss = 2.81737139\n",
      "Iteration 955, loss = 2.81791847\n",
      "Iteration 956, loss = 2.81699064\n",
      "Iteration 957, loss = 2.81733000\n",
      "Iteration 958, loss = 2.81662084\n",
      "Iteration 959, loss = 2.81682508\n",
      "Iteration 960, loss = 2.81607214\n",
      "Iteration 961, loss = 2.81587175\n",
      "Iteration 962, loss = 2.81559380\n",
      "Iteration 963, loss = 2.81555204\n",
      "Iteration 964, loss = 2.81533008\n",
      "Iteration 965, loss = 2.81579077\n",
      "Iteration 966, loss = 2.81533602\n",
      "Iteration 967, loss = 2.81553575\n",
      "Iteration 968, loss = 2.81475258\n",
      "Iteration 969, loss = 2.81437308\n",
      "Iteration 970, loss = 2.81389795\n",
      "Iteration 971, loss = 2.81473056\n",
      "Iteration 972, loss = 2.81391786\n",
      "Iteration 973, loss = 2.81438512\n",
      "Iteration 974, loss = 2.81289639\n",
      "Iteration 975, loss = 2.81286456\n",
      "Iteration 976, loss = 2.81200992\n",
      "Iteration 977, loss = 2.81153401\n",
      "Iteration 978, loss = 2.81167661\n",
      "Iteration 979, loss = 2.81204280\n",
      "Iteration 980, loss = 2.81186266\n",
      "Iteration 981, loss = 2.81081985\n",
      "Iteration 982, loss = 2.81095012\n",
      "Iteration 983, loss = 2.81088376\n",
      "Iteration 984, loss = 2.81043613\n",
      "Iteration 985, loss = 2.81041408\n",
      "Iteration 986, loss = 2.81009111\n",
      "Iteration 987, loss = 2.81019405\n",
      "Iteration 988, loss = 2.81076990\n",
      "Iteration 989, loss = 2.81015157\n",
      "Iteration 990, loss = 2.80913494\n",
      "Iteration 991, loss = 2.80973195\n",
      "Iteration 992, loss = 2.80980943\n",
      "Iteration 993, loss = 2.80861510\n",
      "Iteration 994, loss = 2.80912496\n",
      "Iteration 995, loss = 2.80961955\n",
      "Iteration 996, loss = 2.80895858\n",
      "Iteration 997, loss = 2.80896771\n",
      "Iteration 998, loss = 2.80756602\n",
      "Iteration 999, loss = 2.80704262\n",
      "Iteration 1000, loss = 2.80671695\n",
      "Iteration 1001, loss = 2.80671869\n",
      "Iteration 1002, loss = 2.80623142\n",
      "Iteration 1003, loss = 2.80878543\n",
      "Iteration 1004, loss = 2.80818405\n",
      "Iteration 1005, loss = 2.80670880\n",
      "Iteration 1006, loss = 2.80626412\n",
      "Iteration 1007, loss = 2.80580077\n",
      "Iteration 1008, loss = 2.80571049\n",
      "Iteration 1009, loss = 2.80512500\n",
      "Iteration 1010, loss = 2.80487999\n",
      "Iteration 1011, loss = 2.80477522\n",
      "Iteration 1012, loss = 2.80396473\n",
      "Iteration 1013, loss = 2.80330463\n",
      "Iteration 1014, loss = 2.80297272\n",
      "Iteration 1015, loss = 2.80333768\n",
      "Iteration 1016, loss = 2.80366135\n",
      "Iteration 1017, loss = 2.80324984\n",
      "Iteration 1018, loss = 2.80236866\n",
      "Iteration 1019, loss = 2.80283255\n",
      "Iteration 1020, loss = 2.80293326\n",
      "Iteration 1021, loss = 2.80378227\n",
      "Iteration 1022, loss = 2.80233433\n",
      "Iteration 1023, loss = 2.80134098\n",
      "Iteration 1024, loss = 2.80357823\n",
      "Iteration 1025, loss = 2.80248109\n",
      "Iteration 1026, loss = 2.80057606\n",
      "Iteration 1027, loss = 2.80079095\n",
      "Iteration 1028, loss = 2.80022491\n",
      "Iteration 1029, loss = 2.80015172\n",
      "Iteration 1030, loss = 2.79951142\n",
      "Iteration 1031, loss = 2.80040229\n",
      "Iteration 1032, loss = 2.79967842\n",
      "Iteration 1033, loss = 2.79944028\n",
      "Iteration 1034, loss = 2.79986548\n",
      "Iteration 1035, loss = 2.79904333\n",
      "Iteration 1036, loss = 2.79830768\n",
      "Iteration 1037, loss = 2.79789243\n",
      "Iteration 1038, loss = 2.79779076\n",
      "Iteration 1039, loss = 2.79753888\n",
      "Iteration 1040, loss = 2.79842950\n",
      "Iteration 1041, loss = 2.79943993\n",
      "Iteration 1042, loss = 2.79910490\n",
      "Iteration 1043, loss = 2.79786857\n",
      "Iteration 1044, loss = 2.79687360\n",
      "Iteration 1045, loss = 2.79617949\n",
      "Iteration 1046, loss = 2.79622435\n",
      "Iteration 1047, loss = 2.79556759\n",
      "Iteration 1048, loss = 2.79567981\n",
      "Iteration 1049, loss = 2.79506304\n",
      "Iteration 1050, loss = 2.79517109\n",
      "Iteration 1051, loss = 2.79534218\n",
      "Iteration 1052, loss = 2.79550193\n",
      "Iteration 1053, loss = 2.79521446\n",
      "Iteration 1054, loss = 2.79514322\n",
      "Iteration 1055, loss = 2.79433403\n",
      "Iteration 1056, loss = 2.79375653\n",
      "Iteration 1057, loss = 2.79436598\n",
      "Iteration 1058, loss = 2.79452713\n",
      "Iteration 1059, loss = 2.79354378\n",
      "Iteration 1060, loss = 2.79363370\n",
      "Iteration 1061, loss = 2.79397921\n",
      "Iteration 1062, loss = 2.79397805\n",
      "Iteration 1063, loss = 2.79413221\n",
      "Iteration 1064, loss = 2.79260486\n",
      "Iteration 1065, loss = 2.79243141\n",
      "Iteration 1066, loss = 2.79206666\n",
      "Iteration 1067, loss = 2.79160765\n",
      "Iteration 1068, loss = 2.79329518\n",
      "Iteration 1069, loss = 2.79219885\n",
      "Iteration 1070, loss = 2.79135320\n",
      "Iteration 1071, loss = 2.79180740\n",
      "Iteration 1072, loss = 2.79124032\n",
      "Iteration 1073, loss = 2.78981392\n",
      "Iteration 1074, loss = 2.78960102\n",
      "Iteration 1075, loss = 2.79051022\n",
      "Iteration 1076, loss = 2.79037739\n",
      "Iteration 1077, loss = 2.78926379\n",
      "Iteration 1078, loss = 2.78911745\n",
      "Iteration 1079, loss = 2.78987285\n",
      "Iteration 1080, loss = 2.78925784\n",
      "Iteration 1081, loss = 2.78897380\n",
      "Iteration 1082, loss = 2.78831316\n",
      "Iteration 1083, loss = 2.78868303\n",
      "Iteration 1084, loss = 2.78817806\n",
      "Iteration 1085, loss = 2.78735969\n",
      "Iteration 1086, loss = 2.78732873\n",
      "Iteration 1087, loss = 2.78790348\n",
      "Iteration 1088, loss = 2.78705571\n",
      "Iteration 1089, loss = 2.78815034\n",
      "Iteration 1090, loss = 2.78831911\n",
      "Iteration 1091, loss = 2.78695105\n",
      "Iteration 1092, loss = 2.78741713\n",
      "Iteration 1093, loss = 2.78576657\n",
      "Iteration 1094, loss = 2.78584796\n",
      "Iteration 1095, loss = 2.78561656\n",
      "Iteration 1096, loss = 2.78574092\n",
      "Iteration 1097, loss = 2.78550561\n",
      "Iteration 1098, loss = 2.78538467\n",
      "Iteration 1099, loss = 2.78682598\n",
      "Iteration 1100, loss = 2.78469121\n",
      "Iteration 1101, loss = 2.78501674\n",
      "Iteration 1102, loss = 2.78425973\n",
      "Iteration 1103, loss = 2.78380687\n",
      "Iteration 1104, loss = 2.78377930\n",
      "Iteration 1105, loss = 2.78307908\n",
      "Iteration 1106, loss = 2.78389541\n",
      "Iteration 1107, loss = 2.78325759\n",
      "Iteration 1108, loss = 2.78264380\n",
      "Iteration 1109, loss = 2.78350390\n",
      "Iteration 1110, loss = 2.78251769\n",
      "Iteration 1111, loss = 2.78234329\n",
      "Iteration 1112, loss = 2.78166507\n",
      "Iteration 1113, loss = 2.78151729\n",
      "Iteration 1114, loss = 2.78134406\n",
      "Iteration 1115, loss = 2.78154209\n",
      "Iteration 1116, loss = 2.78228682\n",
      "Iteration 1117, loss = 2.78159179\n",
      "Iteration 1118, loss = 2.78186175\n",
      "Iteration 1119, loss = 2.78200862\n",
      "Iteration 1120, loss = 2.78069465\n",
      "Iteration 1121, loss = 2.78026514\n",
      "Iteration 1122, loss = 2.78029943\n",
      "Iteration 1123, loss = 2.78006330\n",
      "Iteration 1124, loss = 2.78041658\n",
      "Iteration 1125, loss = 2.77995758\n",
      "Iteration 1126, loss = 2.78000073\n",
      "Iteration 1127, loss = 2.77969898\n",
      "Iteration 1128, loss = 2.77945570\n",
      "Iteration 1129, loss = 2.77909791\n",
      "Iteration 1130, loss = 2.77950205\n",
      "Iteration 1131, loss = 2.77865199\n",
      "Iteration 1132, loss = 2.77769973\n",
      "Iteration 1133, loss = 2.77843631\n",
      "Iteration 1134, loss = 2.77774709\n",
      "Iteration 1135, loss = 2.77716131\n",
      "Iteration 1136, loss = 2.77638081\n",
      "Iteration 1137, loss = 2.77667703\n",
      "Iteration 1138, loss = 2.77741042\n",
      "Iteration 1139, loss = 2.77688583\n",
      "Iteration 1140, loss = 2.77653378\n",
      "Iteration 1141, loss = 2.77596323\n",
      "Iteration 1142, loss = 2.77599519\n",
      "Iteration 1143, loss = 2.77572209\n",
      "Iteration 1144, loss = 2.77578007\n",
      "Iteration 1145, loss = 2.77553646\n",
      "Iteration 1146, loss = 2.77600764\n",
      "Iteration 1147, loss = 2.77490312\n",
      "Iteration 1148, loss = 2.77587980\n",
      "Iteration 1149, loss = 2.77519740\n",
      "Iteration 1150, loss = 2.77433489\n",
      "Iteration 1151, loss = 2.77421409\n",
      "Iteration 1152, loss = 2.77467231\n",
      "Iteration 1153, loss = 2.77373046\n",
      "Iteration 1154, loss = 2.77269548\n",
      "Iteration 1155, loss = 2.77404893\n",
      "Iteration 1156, loss = 2.77458112\n",
      "Iteration 1157, loss = 2.77373885\n",
      "Iteration 1158, loss = 2.77348288\n",
      "Iteration 1159, loss = 2.77283936\n",
      "Iteration 1160, loss = 2.77204885\n",
      "Iteration 1161, loss = 2.77253913\n",
      "Iteration 1162, loss = 2.77228426\n",
      "Iteration 1163, loss = 2.77195188\n",
      "Iteration 1164, loss = 2.77140357\n",
      "Iteration 1165, loss = 2.77103570\n",
      "Iteration 1166, loss = 2.77149273\n",
      "Iteration 1167, loss = 2.77187578\n",
      "Iteration 1168, loss = 2.77180112\n",
      "Iteration 1169, loss = 2.77096994\n",
      "Iteration 1170, loss = 2.77154941\n",
      "Iteration 1171, loss = 2.77013588\n",
      "Iteration 1172, loss = 2.77122191\n",
      "Iteration 1173, loss = 2.77226720\n",
      "Iteration 1174, loss = 2.76969613\n",
      "Iteration 1175, loss = 2.76960773\n",
      "Iteration 1176, loss = 2.77002376\n",
      "Iteration 1177, loss = 2.76948224\n",
      "Iteration 1178, loss = 2.76919497\n",
      "Iteration 1179, loss = 2.77076652\n",
      "Iteration 1180, loss = 2.76928666\n",
      "Iteration 1181, loss = 2.77182946\n",
      "Iteration 1182, loss = 2.76948478\n",
      "Iteration 1183, loss = 2.76795666\n",
      "Iteration 1184, loss = 2.76752653\n",
      "Iteration 1185, loss = 2.76699687\n",
      "Iteration 1186, loss = 2.76703188\n",
      "Iteration 1187, loss = 2.76714350\n",
      "Iteration 1188, loss = 2.76633577\n",
      "Iteration 1189, loss = 2.76673079\n",
      "Iteration 1190, loss = 2.76751032\n",
      "Iteration 1191, loss = 2.76785034\n",
      "Iteration 1192, loss = 2.76713719\n",
      "Iteration 1193, loss = 2.76639669\n",
      "Iteration 1194, loss = 2.76586603\n",
      "Iteration 1195, loss = 2.76518113\n",
      "Iteration 1196, loss = 2.76587021\n",
      "Iteration 1197, loss = 2.76687348\n",
      "Iteration 1198, loss = 2.76613854\n",
      "Iteration 1199, loss = 2.76536421\n",
      "Iteration 1200, loss = 2.76509770\n",
      "Iteration 1201, loss = 2.76430908\n",
      "Iteration 1202, loss = 2.76465925\n",
      "Iteration 1203, loss = 2.76452389\n",
      "Iteration 1204, loss = 2.76413081\n",
      "Iteration 1205, loss = 2.76339871\n",
      "Iteration 1206, loss = 2.76313101\n",
      "Iteration 1207, loss = 2.76299478\n",
      "Iteration 1208, loss = 2.76256761\n",
      "Iteration 1209, loss = 2.76291688\n",
      "Iteration 1210, loss = 2.76289238\n",
      "Iteration 1211, loss = 2.76320262\n",
      "Iteration 1212, loss = 2.76304875\n",
      "Iteration 1213, loss = 2.76273857\n",
      "Iteration 1214, loss = 2.76236524\n",
      "Iteration 1215, loss = 2.76267809\n",
      "Iteration 1216, loss = 2.76268857\n",
      "Iteration 1217, loss = 2.76177542\n",
      "Iteration 1218, loss = 2.76079011\n",
      "Iteration 1219, loss = 2.76248392\n",
      "Iteration 1220, loss = 2.76129797\n",
      "Iteration 1221, loss = 2.76089633\n",
      "Iteration 1222, loss = 2.76105229\n",
      "Iteration 1223, loss = 2.76050230\n",
      "Iteration 1224, loss = 2.76046073\n",
      "Iteration 1225, loss = 2.76082240\n",
      "Iteration 1226, loss = 2.75999068\n",
      "Iteration 1227, loss = 2.75919331\n",
      "Iteration 1228, loss = 2.75924855\n",
      "Iteration 1229, loss = 2.76050766\n",
      "Iteration 1230, loss = 2.76044557\n",
      "Iteration 1231, loss = 2.75981406\n",
      "Iteration 1232, loss = 2.75910454\n",
      "Iteration 1233, loss = 2.75923872\n",
      "Iteration 1234, loss = 2.75806542\n",
      "Iteration 1235, loss = 2.75813680\n",
      "Iteration 1236, loss = 2.75898246\n",
      "Iteration 1237, loss = 2.75932934\n",
      "Iteration 1238, loss = 2.75795669\n",
      "Iteration 1239, loss = 2.75814828\n",
      "Iteration 1240, loss = 2.75796093\n",
      "Iteration 1241, loss = 2.75714673\n",
      "Iteration 1242, loss = 2.75863714\n",
      "Iteration 1243, loss = 2.75916688\n",
      "Iteration 1244, loss = 2.75759937\n",
      "Iteration 1245, loss = 2.75726773\n",
      "Iteration 1246, loss = 2.75619318\n",
      "Iteration 1247, loss = 2.75531864\n",
      "Iteration 1248, loss = 2.75506341\n",
      "Iteration 1249, loss = 2.75573123\n",
      "Iteration 1250, loss = 2.75542093\n",
      "Iteration 1251, loss = 2.75569756\n",
      "Iteration 1252, loss = 2.75541649\n",
      "Iteration 1253, loss = 2.75536935\n",
      "Iteration 1254, loss = 2.75474363\n",
      "Iteration 1255, loss = 2.75480740\n",
      "Iteration 1256, loss = 2.75487292\n",
      "Iteration 1257, loss = 2.75473340\n",
      "Iteration 1258, loss = 2.75480255\n",
      "Iteration 1259, loss = 2.75422593\n",
      "Iteration 1260, loss = 2.75387951\n",
      "Iteration 1261, loss = 2.75272228\n",
      "Iteration 1262, loss = 2.75301549\n",
      "Iteration 1263, loss = 2.75376400\n",
      "Iteration 1264, loss = 2.75290532\n",
      "Iteration 1265, loss = 2.75539952\n",
      "Iteration 1266, loss = 2.75383801\n",
      "Iteration 1267, loss = 2.75214895\n",
      "Iteration 1268, loss = 2.75335908\n",
      "Iteration 1269, loss = 2.75261988\n",
      "Iteration 1270, loss = 2.75263144\n",
      "Iteration 1271, loss = 2.75198637\n",
      "Iteration 1272, loss = 2.75125650\n",
      "Iteration 1273, loss = 2.75192435\n",
      "Iteration 1274, loss = 2.75220915\n",
      "Iteration 1275, loss = 2.75142734\n",
      "Iteration 1276, loss = 2.75425396\n",
      "Iteration 1277, loss = 2.75049221\n",
      "Iteration 1278, loss = 2.75092435\n",
      "Iteration 1279, loss = 2.75181730\n",
      "Iteration 1280, loss = 2.75125549\n",
      "Iteration 1281, loss = 2.75102969\n",
      "Iteration 1282, loss = 2.75004176\n",
      "Iteration 1283, loss = 2.74964927\n",
      "Iteration 1284, loss = 2.74952467\n",
      "Iteration 1285, loss = 2.75056046\n",
      "Iteration 1286, loss = 2.75085813\n",
      "Iteration 1287, loss = 2.74983388\n",
      "Iteration 1288, loss = 2.74833851\n",
      "Iteration 1289, loss = 2.74822595\n",
      "Iteration 1290, loss = 2.74820300\n",
      "Iteration 1291, loss = 2.74765724\n",
      "Iteration 1292, loss = 2.74762051\n",
      "Iteration 1293, loss = 2.74756369\n",
      "Iteration 1294, loss = 2.74870582\n",
      "Iteration 1295, loss = 2.74871484\n",
      "Iteration 1296, loss = 2.74739795\n",
      "Iteration 1297, loss = 2.74802038\n",
      "Iteration 1298, loss = 2.74714081\n",
      "Iteration 1299, loss = 2.74650144\n",
      "Iteration 1300, loss = 2.74727515\n",
      "Iteration 1301, loss = 2.74683697\n",
      "Iteration 1302, loss = 2.74675385\n",
      "Iteration 1303, loss = 2.74678968\n",
      "Iteration 1304, loss = 2.74652966\n",
      "Iteration 1305, loss = 2.74812770\n",
      "Iteration 1306, loss = 2.74657697\n",
      "Iteration 1307, loss = 2.74543580\n",
      "Iteration 1308, loss = 2.74609960\n",
      "Iteration 1309, loss = 2.74555516\n",
      "Iteration 1310, loss = 2.74563291\n",
      "Iteration 1311, loss = 2.74629356\n",
      "Iteration 1312, loss = 2.74479157\n",
      "Iteration 1313, loss = 2.74525738\n",
      "Iteration 1314, loss = 2.74472604\n",
      "Iteration 1315, loss = 2.74399822\n",
      "Iteration 1316, loss = 2.74413654\n",
      "Iteration 1317, loss = 2.74406168\n",
      "Iteration 1318, loss = 2.74329597\n",
      "Iteration 1319, loss = 2.74364273\n",
      "Iteration 1320, loss = 2.74343852\n",
      "Iteration 1321, loss = 2.74411275\n",
      "Iteration 1322, loss = 2.74313295\n",
      "Iteration 1323, loss = 2.74197260\n",
      "Iteration 1324, loss = 2.74241364\n",
      "Iteration 1325, loss = 2.74275766\n",
      "Iteration 1326, loss = 2.74241496\n",
      "Iteration 1327, loss = 2.74160815\n",
      "Iteration 1328, loss = 2.74169853\n",
      "Iteration 1329, loss = 2.74281748\n",
      "Iteration 1330, loss = 2.74205044\n",
      "Iteration 1331, loss = 2.74108937\n",
      "Iteration 1332, loss = 2.74182317\n",
      "Iteration 1333, loss = 2.74171524\n",
      "Iteration 1334, loss = 2.74112687\n",
      "Iteration 1335, loss = 2.74272888\n",
      "Iteration 1336, loss = 2.74161982\n",
      "Iteration 1337, loss = 2.74103391\n",
      "Iteration 1338, loss = 2.74102392\n",
      "Iteration 1339, loss = 2.74062208\n",
      "Iteration 1340, loss = 2.74104091\n",
      "Iteration 1341, loss = 2.74062790\n",
      "Iteration 1342, loss = 2.74065280\n",
      "Iteration 1343, loss = 2.74046931\n",
      "Iteration 1344, loss = 2.73910465\n",
      "Iteration 1345, loss = 2.74013008\n",
      "Iteration 1346, loss = 2.73997426\n",
      "Iteration 1347, loss = 2.73902740\n",
      "Iteration 1348, loss = 2.73936247\n",
      "Iteration 1349, loss = 2.73960809\n",
      "Iteration 1350, loss = 2.73810405\n",
      "Iteration 1351, loss = 2.73895226\n",
      "Iteration 1352, loss = 2.73946601\n",
      "Iteration 1353, loss = 2.73890398\n",
      "Iteration 1354, loss = 2.73830397\n",
      "Iteration 1355, loss = 2.73790614\n",
      "Iteration 1356, loss = 2.73757230\n",
      "Iteration 1357, loss = 2.73828762\n",
      "Iteration 1358, loss = 2.73703073\n",
      "Iteration 1359, loss = 2.73699658\n",
      "Iteration 1360, loss = 2.73723984\n",
      "Iteration 1361, loss = 2.73666822\n",
      "Iteration 1362, loss = 2.73633113\n",
      "Iteration 1363, loss = 2.73633447\n",
      "Iteration 1364, loss = 2.73662529\n",
      "Iteration 1365, loss = 2.73593553\n",
      "Iteration 1366, loss = 2.73577507\n",
      "Iteration 1367, loss = 2.73577166\n",
      "Iteration 1368, loss = 2.73661752\n",
      "Iteration 1369, loss = 2.73599423\n",
      "Iteration 1370, loss = 2.73541904\n",
      "Iteration 1371, loss = 2.73636328\n",
      "Iteration 1372, loss = 2.73722050\n",
      "Iteration 1373, loss = 2.73519393\n",
      "Iteration 1374, loss = 2.73442924\n",
      "Iteration 1375, loss = 2.73520614\n",
      "Iteration 1376, loss = 2.73550747\n",
      "Iteration 1377, loss = 2.73489400\n",
      "Iteration 1378, loss = 2.73358545\n",
      "Iteration 1379, loss = 2.73343438\n",
      "Iteration 1380, loss = 2.73343678\n",
      "Iteration 1381, loss = 2.73348783\n",
      "Iteration 1382, loss = 2.73390826\n",
      "Iteration 1383, loss = 2.73442219\n",
      "Iteration 1384, loss = 2.73391482\n",
      "Iteration 1385, loss = 2.73276876\n",
      "Iteration 1386, loss = 2.73308361\n",
      "Iteration 1387, loss = 2.73391002\n",
      "Iteration 1388, loss = 2.73326842\n",
      "Iteration 1389, loss = 2.73252424\n",
      "Iteration 1390, loss = 2.73306576\n",
      "Iteration 1391, loss = 2.73259824\n",
      "Iteration 1392, loss = 2.73254480\n",
      "Iteration 1393, loss = 2.73265976\n",
      "Iteration 1394, loss = 2.73211309\n",
      "Iteration 1395, loss = 2.73125105\n",
      "Iteration 1396, loss = 2.73191088\n",
      "Iteration 1397, loss = 2.73141555\n",
      "Iteration 1398, loss = 2.73075268\n",
      "Iteration 1399, loss = 2.73080661\n",
      "Iteration 1400, loss = 2.73064139\n",
      "Iteration 1401, loss = 2.73101595\n",
      "Iteration 1402, loss = 2.73093626\n",
      "Iteration 1403, loss = 2.73045317\n",
      "Iteration 1404, loss = 2.73060285\n",
      "Iteration 1405, loss = 2.72966503\n",
      "Iteration 1406, loss = 2.72939348\n",
      "Iteration 1407, loss = 2.72991238\n",
      "Iteration 1408, loss = 2.73070281\n",
      "Iteration 1409, loss = 2.72955159\n",
      "Iteration 1410, loss = 2.72932179\n",
      "Iteration 1411, loss = 2.72935766\n",
      "Iteration 1412, loss = 2.72840260\n",
      "Iteration 1413, loss = 2.72911849\n",
      "Iteration 1414, loss = 2.72938924\n",
      "Iteration 1415, loss = 2.72897119\n",
      "Iteration 1416, loss = 2.72958217\n",
      "Iteration 1417, loss = 2.72807919\n",
      "Iteration 1418, loss = 2.72813739\n",
      "Iteration 1419, loss = 2.72930851\n",
      "Iteration 1420, loss = 2.72730691\n",
      "Iteration 1421, loss = 2.72740060\n",
      "Iteration 1422, loss = 2.72663078\n",
      "Iteration 1423, loss = 2.72654988\n",
      "Iteration 1424, loss = 2.72725131\n",
      "Iteration 1425, loss = 2.72626284\n",
      "Iteration 1426, loss = 2.72636433\n",
      "Iteration 1427, loss = 2.72707180\n",
      "Iteration 1428, loss = 2.72587489\n",
      "Iteration 1429, loss = 2.72761836\n",
      "Iteration 1430, loss = 2.72713261\n",
      "Iteration 1431, loss = 2.72548147\n",
      "Iteration 1432, loss = 2.72489971\n",
      "Iteration 1433, loss = 2.72546320\n",
      "Iteration 1434, loss = 2.72588711\n",
      "Iteration 1435, loss = 2.72514567\n",
      "Iteration 1436, loss = 2.72502808\n",
      "Iteration 1437, loss = 2.72433937\n",
      "Iteration 1438, loss = 2.72407094\n",
      "Iteration 1439, loss = 2.72406123\n",
      "Iteration 1440, loss = 2.72407303\n",
      "Iteration 1441, loss = 2.72418886\n",
      "Iteration 1442, loss = 2.72443438\n",
      "Iteration 1443, loss = 2.72452615\n",
      "Iteration 1444, loss = 2.72400613\n",
      "Iteration 1445, loss = 2.72496230\n",
      "Iteration 1446, loss = 2.72314755\n",
      "Iteration 1447, loss = 2.72339960\n",
      "Iteration 1448, loss = 2.72273988\n",
      "Iteration 1449, loss = 2.72297342\n",
      "Iteration 1450, loss = 2.72250594\n",
      "Iteration 1451, loss = 2.72258354\n",
      "Iteration 1452, loss = 2.72245243\n",
      "Iteration 1453, loss = 2.72239351\n",
      "Iteration 1454, loss = 2.72277209\n",
      "Iteration 1455, loss = 2.72258573\n",
      "Iteration 1456, loss = 2.72244042\n",
      "Iteration 1457, loss = 2.72289152\n",
      "Iteration 1458, loss = 2.72240604\n",
      "Iteration 1459, loss = 2.72105971\n",
      "Iteration 1460, loss = 2.72184897\n",
      "Iteration 1461, loss = 2.72111268\n",
      "Iteration 1462, loss = 2.72241454\n",
      "Iteration 1463, loss = 2.72289200\n",
      "Iteration 1464, loss = 2.72061687\n",
      "Iteration 1465, loss = 2.72110660\n",
      "Iteration 1466, loss = 2.72055687\n",
      "Iteration 1467, loss = 2.72310393\n",
      "Iteration 1468, loss = 2.72236729\n",
      "Iteration 1469, loss = 2.71976577\n",
      "Iteration 1470, loss = 2.72067334\n",
      "Iteration 1471, loss = 2.71953659\n",
      "Iteration 1472, loss = 2.71942937\n",
      "Iteration 1473, loss = 2.72000132\n",
      "Iteration 1474, loss = 2.72037125\n",
      "Iteration 1475, loss = 2.71971964\n",
      "Iteration 1476, loss = 2.71860552\n",
      "Iteration 1477, loss = 2.71789124\n",
      "Iteration 1478, loss = 2.71867141\n",
      "Iteration 1479, loss = 2.71868302\n",
      "Iteration 1480, loss = 2.71876585\n",
      "Iteration 1481, loss = 2.71814657\n",
      "Iteration 1482, loss = 2.71791677\n",
      "Iteration 1483, loss = 2.71781610\n",
      "Iteration 1484, loss = 2.71723933\n",
      "Iteration 1485, loss = 2.71713350\n",
      "Iteration 1486, loss = 2.71833899\n",
      "Iteration 1487, loss = 2.71797074\n",
      "Iteration 1488, loss = 2.71721698\n",
      "Iteration 1489, loss = 2.71617887\n",
      "Iteration 1490, loss = 2.71709673\n",
      "Iteration 1491, loss = 2.71766828\n",
      "Iteration 1492, loss = 2.71779789\n",
      "Iteration 1493, loss = 2.71739376\n",
      "Iteration 1494, loss = 2.71703137\n",
      "Iteration 1495, loss = 2.71603104\n",
      "Iteration 1496, loss = 2.71587040\n",
      "Iteration 1497, loss = 2.71633097\n",
      "Iteration 1498, loss = 2.71603717\n",
      "Iteration 1499, loss = 2.71572316\n",
      "Iteration 1500, loss = 2.71553562\n",
      "Iteration 1501, loss = 2.71537544\n",
      "Iteration 1502, loss = 2.71544742\n",
      "Iteration 1503, loss = 2.71628701\n",
      "Iteration 1504, loss = 2.71449830\n",
      "Iteration 1505, loss = 2.71541379\n",
      "Iteration 1506, loss = 2.71436089\n",
      "Iteration 1507, loss = 2.71552785\n",
      "Iteration 1508, loss = 2.71574340\n",
      "Iteration 1509, loss = 2.71332731\n",
      "Iteration 1510, loss = 2.71468035\n",
      "Iteration 1511, loss = 2.71387134\n",
      "Iteration 1512, loss = 2.71333341\n",
      "Iteration 1513, loss = 2.71404538\n",
      "Iteration 1514, loss = 2.71426657\n",
      "Iteration 1515, loss = 2.71341784\n",
      "Iteration 1516, loss = 2.71293186\n",
      "Iteration 1517, loss = 2.71494683\n",
      "Iteration 1518, loss = 2.71382980\n",
      "Iteration 1519, loss = 2.71330999\n",
      "Iteration 1520, loss = 2.71270162\n",
      "Iteration 1521, loss = 2.71215579\n",
      "Iteration 1522, loss = 2.71218101\n",
      "Iteration 1523, loss = 2.71336117\n",
      "Iteration 1524, loss = 2.71290090\n",
      "Iteration 1525, loss = 2.71301435\n",
      "Iteration 1526, loss = 2.71265026\n",
      "Iteration 1527, loss = 2.71178803\n",
      "Iteration 1528, loss = 2.71131435\n",
      "Iteration 1529, loss = 2.71068272\n",
      "Iteration 1530, loss = 2.71157347\n",
      "Iteration 1531, loss = 2.71150567\n",
      "Iteration 1532, loss = 2.71159435\n",
      "Iteration 1533, loss = 2.71142367\n",
      "Iteration 1534, loss = 2.70967840\n",
      "Iteration 1535, loss = 2.71020136\n",
      "Iteration 1536, loss = 2.71072996\n",
      "Iteration 1537, loss = 2.71117062\n",
      "Iteration 1538, loss = 2.70982570\n",
      "Iteration 1539, loss = 2.71284081\n",
      "Iteration 1540, loss = 2.71053098\n",
      "Iteration 1541, loss = 2.71007291\n",
      "Iteration 1542, loss = 2.70998059\n",
      "Iteration 1543, loss = 2.70925802\n",
      "Iteration 1544, loss = 2.70892753\n",
      "Iteration 1545, loss = 2.71027781\n",
      "Iteration 1546, loss = 2.70914081\n",
      "Iteration 1547, loss = 2.70989428\n",
      "Iteration 1548, loss = 2.71008607\n",
      "Iteration 1549, loss = 2.70817973\n",
      "Iteration 1550, loss = 2.70901507\n",
      "Iteration 1551, loss = 2.70975380\n",
      "Iteration 1552, loss = 2.70858341\n",
      "Iteration 1553, loss = 2.70766601\n",
      "Iteration 1554, loss = 2.70749284\n",
      "Iteration 1555, loss = 2.70926408\n",
      "Iteration 1556, loss = 2.70896552\n",
      "Iteration 1557, loss = 2.70701397\n",
      "Iteration 1558, loss = 2.70722507\n",
      "Iteration 1559, loss = 2.70891654\n",
      "Iteration 1560, loss = 2.70713012\n",
      "Iteration 1561, loss = 2.70732245\n",
      "Iteration 1562, loss = 2.70728436\n",
      "Iteration 1563, loss = 2.70720822\n",
      "Iteration 1564, loss = 2.70751166\n",
      "Iteration 1565, loss = 2.70694823\n",
      "Iteration 1566, loss = 2.70610513\n",
      "Iteration 1567, loss = 2.70583911\n",
      "Iteration 1568, loss = 2.70516299\n",
      "Iteration 1569, loss = 2.70552295\n",
      "Iteration 1570, loss = 2.70528866\n",
      "Iteration 1571, loss = 2.70605180\n",
      "Iteration 1572, loss = 2.70561713\n",
      "Iteration 1573, loss = 2.70473693\n",
      "Iteration 1574, loss = 2.70582598\n",
      "Iteration 1575, loss = 2.70571908\n",
      "Iteration 1576, loss = 2.70546493\n",
      "Iteration 1577, loss = 2.70522660\n",
      "Iteration 1578, loss = 2.70485792\n",
      "Iteration 1579, loss = 2.70511839\n",
      "Iteration 1580, loss = 2.70434243\n",
      "Iteration 1581, loss = 2.70709671\n",
      "Iteration 1582, loss = 2.70548126\n",
      "Iteration 1583, loss = 2.70490677\n",
      "Iteration 1584, loss = 2.70475931\n",
      "Iteration 1585, loss = 2.70534277\n",
      "Iteration 1586, loss = 2.70534339\n",
      "Iteration 1587, loss = 2.70376762\n",
      "Iteration 1588, loss = 2.70381653\n",
      "Iteration 1589, loss = 2.70322277\n",
      "Iteration 1590, loss = 2.70342298\n",
      "Iteration 1591, loss = 2.70393470\n",
      "Iteration 1592, loss = 2.70338412\n",
      "Iteration 1593, loss = 2.70335796\n",
      "Iteration 1594, loss = 2.70277508\n",
      "Iteration 1595, loss = 2.70235772\n",
      "Iteration 1596, loss = 2.70282556\n",
      "Iteration 1597, loss = 2.70347544\n",
      "Iteration 1598, loss = 2.70310953\n",
      "Iteration 1599, loss = 2.70217023\n",
      "Iteration 1600, loss = 2.70349072\n",
      "Iteration 1601, loss = 2.70214200\n",
      "Iteration 1602, loss = 2.70222761\n",
      "Iteration 1603, loss = 2.70077074\n",
      "Iteration 1604, loss = 2.70281896\n",
      "Iteration 1605, loss = 2.70261017\n",
      "Iteration 1606, loss = 2.70134453\n",
      "Iteration 1607, loss = 2.70079739\n",
      "Iteration 1608, loss = 2.70057044\n",
      "Iteration 1609, loss = 2.70101079\n",
      "Iteration 1610, loss = 2.70013140\n",
      "Iteration 1611, loss = 2.70115738\n",
      "Iteration 1612, loss = 2.69979891\n",
      "Iteration 1613, loss = 2.69968096\n",
      "Iteration 1614, loss = 2.69968363\n",
      "Iteration 1615, loss = 2.69981079\n",
      "Iteration 1616, loss = 2.69994359\n",
      "Iteration 1617, loss = 2.70058530\n",
      "Iteration 1618, loss = 2.69969434\n",
      "Iteration 1619, loss = 2.69954545\n",
      "Iteration 1620, loss = 2.69970878\n",
      "Iteration 1621, loss = 2.69891989\n",
      "Iteration 1622, loss = 2.69909982\n",
      "Iteration 1623, loss = 2.69896722\n",
      "Iteration 1624, loss = 2.69878603\n",
      "Iteration 1625, loss = 2.69875673\n",
      "Iteration 1626, loss = 2.69823917\n",
      "Iteration 1627, loss = 2.69835091\n",
      "Iteration 1628, loss = 2.69880006\n",
      "Iteration 1629, loss = 2.69861585\n",
      "Iteration 1630, loss = 2.69862414\n",
      "Iteration 1631, loss = 2.69748947\n",
      "Iteration 1632, loss = 2.69765780\n",
      "Iteration 1633, loss = 2.69746303\n",
      "Iteration 1634, loss = 2.69872677\n",
      "Iteration 1635, loss = 2.69846629\n",
      "Iteration 1636, loss = 2.69752154\n",
      "Iteration 1637, loss = 2.69667902\n",
      "Iteration 1638, loss = 2.69658574\n",
      "Iteration 1639, loss = 2.69617304\n",
      "Iteration 1640, loss = 2.69646688\n",
      "Iteration 1641, loss = 2.69605319\n",
      "Iteration 1642, loss = 2.69686198\n",
      "Iteration 1643, loss = 2.69673669\n",
      "Iteration 1644, loss = 2.69650463\n",
      "Iteration 1645, loss = 2.69606761\n",
      "Iteration 1646, loss = 2.69655845\n",
      "Iteration 1647, loss = 2.69627827\n",
      "Iteration 1648, loss = 2.69559370\n",
      "Iteration 1649, loss = 2.69603960\n",
      "Iteration 1650, loss = 2.69542102\n",
      "Iteration 1651, loss = 2.69825323\n",
      "Iteration 1652, loss = 2.69754682\n",
      "Iteration 1653, loss = 2.69536396\n",
      "Iteration 1654, loss = 2.69637733\n",
      "Iteration 1655, loss = 2.69490426\n",
      "Iteration 1656, loss = 2.69564033\n",
      "Iteration 1657, loss = 2.69506051\n",
      "Iteration 1658, loss = 2.69349204\n",
      "Iteration 1659, loss = 2.69437537\n",
      "Iteration 1660, loss = 2.69378651\n",
      "Iteration 1661, loss = 2.69401060\n",
      "Iteration 1662, loss = 2.69492431\n",
      "Iteration 1663, loss = 2.69374591\n",
      "Iteration 1664, loss = 2.69361170\n",
      "Iteration 1665, loss = 2.69519818\n",
      "Iteration 1666, loss = 2.69358874\n",
      "Iteration 1667, loss = 2.69342220\n",
      "Iteration 1668, loss = 2.69383781\n",
      "Iteration 1669, loss = 2.69333805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.14593563\n",
      "Iteration 2, loss = 4.01861722\n",
      "Iteration 3, loss = 3.89950236\n",
      "Iteration 4, loss = 3.78808361\n",
      "Iteration 5, loss = 3.68078420\n",
      "Iteration 6, loss = 3.58067041\n",
      "Iteration 7, loss = 3.48618784\n",
      "Iteration 8, loss = 3.40721763\n",
      "Iteration 9, loss = 3.33928944\n",
      "Iteration 10, loss = 3.28726686\n",
      "Iteration 11, loss = 3.25432897\n",
      "Iteration 12, loss = 3.23722558\n",
      "Iteration 13, loss = 3.23310008\n",
      "Iteration 14, loss = 3.23072135\n",
      "Iteration 15, loss = 3.22938893\n",
      "Iteration 16, loss = 3.22785448\n",
      "Iteration 17, loss = 3.22603121\n",
      "Iteration 18, loss = 3.22483757\n",
      "Iteration 19, loss = 3.22465678\n",
      "Iteration 20, loss = 3.22342528\n",
      "Iteration 21, loss = 3.22305836\n",
      "Iteration 22, loss = 3.22233146\n",
      "Iteration 23, loss = 3.22235032\n",
      "Iteration 24, loss = 3.22129230\n",
      "Iteration 25, loss = 3.22055299\n",
      "Iteration 26, loss = 3.21988296\n",
      "Iteration 27, loss = 3.21875668\n",
      "Iteration 28, loss = 3.21854053\n",
      "Iteration 29, loss = 3.21848603\n",
      "Iteration 30, loss = 3.21798607\n",
      "Iteration 31, loss = 3.21770946\n",
      "Iteration 32, loss = 3.21726628\n",
      "Iteration 33, loss = 3.21646681\n",
      "Iteration 34, loss = 3.21622399\n",
      "Iteration 35, loss = 3.21561695\n",
      "Iteration 36, loss = 3.21582278\n",
      "Iteration 37, loss = 3.21517458\n",
      "Iteration 38, loss = 3.21444438\n",
      "Iteration 39, loss = 3.21362578\n",
      "Iteration 40, loss = 3.21290584\n",
      "Iteration 41, loss = 3.21255467\n",
      "Iteration 42, loss = 3.21217947\n",
      "Iteration 43, loss = 3.21167457\n",
      "Iteration 44, loss = 3.21123087\n",
      "Iteration 45, loss = 3.21074098\n",
      "Iteration 46, loss = 3.21076878\n",
      "Iteration 47, loss = 3.21013798\n",
      "Iteration 48, loss = 3.21018506\n",
      "Iteration 49, loss = 3.20944682\n",
      "Iteration 50, loss = 3.20857824\n",
      "Iteration 51, loss = 3.20815571\n",
      "Iteration 52, loss = 3.20772317\n",
      "Iteration 53, loss = 3.20757064\n",
      "Iteration 54, loss = 3.20737666\n",
      "Iteration 55, loss = 3.20706970\n",
      "Iteration 56, loss = 3.20739344\n",
      "Iteration 57, loss = 3.20696893\n",
      "Iteration 58, loss = 3.20597533\n",
      "Iteration 59, loss = 3.20500529\n",
      "Iteration 60, loss = 3.20509197\n",
      "Iteration 61, loss = 3.20491600\n",
      "Iteration 62, loss = 3.20405869\n",
      "Iteration 63, loss = 3.20356017\n",
      "Iteration 64, loss = 3.20281123\n",
      "Iteration 65, loss = 3.20242593\n",
      "Iteration 66, loss = 3.20137004\n",
      "Iteration 67, loss = 3.20119090\n",
      "Iteration 68, loss = 3.20093998\n",
      "Iteration 69, loss = 3.19989492\n",
      "Iteration 70, loss = 3.19990192\n",
      "Iteration 71, loss = 3.19958566\n",
      "Iteration 72, loss = 3.19843097\n",
      "Iteration 73, loss = 3.19857577\n",
      "Iteration 74, loss = 3.19799190\n",
      "Iteration 75, loss = 3.19759398\n",
      "Iteration 76, loss = 3.19757495\n",
      "Iteration 77, loss = 3.19694463\n",
      "Iteration 78, loss = 3.19683807\n",
      "Iteration 79, loss = 3.19624933\n",
      "Iteration 80, loss = 3.19569609\n",
      "Iteration 81, loss = 3.19542011\n",
      "Iteration 82, loss = 3.19516932\n",
      "Iteration 83, loss = 3.19516810\n",
      "Iteration 84, loss = 3.19432796\n",
      "Iteration 85, loss = 3.19412178\n",
      "Iteration 86, loss = 3.19454673\n",
      "Iteration 87, loss = 3.19366675\n",
      "Iteration 88, loss = 3.19329640\n",
      "Iteration 89, loss = 3.19271105\n",
      "Iteration 90, loss = 3.19278239\n",
      "Iteration 91, loss = 3.19200400\n",
      "Iteration 92, loss = 3.19130735\n",
      "Iteration 93, loss = 3.19043092\n",
      "Iteration 94, loss = 3.19015028\n",
      "Iteration 95, loss = 3.18993502\n",
      "Iteration 96, loss = 3.18944206\n",
      "Iteration 97, loss = 3.18883740\n",
      "Iteration 98, loss = 3.18838910\n",
      "Iteration 99, loss = 3.18833852\n",
      "Iteration 100, loss = 3.18802158\n",
      "Iteration 101, loss = 3.18789131\n",
      "Iteration 102, loss = 3.18772589\n",
      "Iteration 103, loss = 3.18690236\n",
      "Iteration 104, loss = 3.18571624\n",
      "Iteration 105, loss = 3.18520741\n",
      "Iteration 106, loss = 3.18482997\n",
      "Iteration 107, loss = 3.18479790\n",
      "Iteration 108, loss = 3.18416298\n",
      "Iteration 109, loss = 3.18325366\n",
      "Iteration 110, loss = 3.18292196\n",
      "Iteration 111, loss = 3.18299898\n",
      "Iteration 112, loss = 3.18188481\n",
      "Iteration 113, loss = 3.18149678\n",
      "Iteration 114, loss = 3.18122117\n",
      "Iteration 115, loss = 3.18036736\n",
      "Iteration 116, loss = 3.17987499\n",
      "Iteration 117, loss = 3.17948937\n",
      "Iteration 118, loss = 3.17961843\n",
      "Iteration 119, loss = 3.17831735\n",
      "Iteration 120, loss = 3.17811749\n",
      "Iteration 121, loss = 3.17769460\n",
      "Iteration 122, loss = 3.17727559\n",
      "Iteration 123, loss = 3.17701149\n",
      "Iteration 124, loss = 3.17658420\n",
      "Iteration 125, loss = 3.17614138\n",
      "Iteration 126, loss = 3.17620336\n",
      "Iteration 127, loss = 3.17521706\n",
      "Iteration 128, loss = 3.17516374\n",
      "Iteration 129, loss = 3.17512563\n",
      "Iteration 130, loss = 3.17487674\n",
      "Iteration 131, loss = 3.17366966\n",
      "Iteration 132, loss = 3.17233632\n",
      "Iteration 133, loss = 3.17206436\n",
      "Iteration 134, loss = 3.17180979\n",
      "Iteration 135, loss = 3.17036411\n",
      "Iteration 136, loss = 3.17089699\n",
      "Iteration 137, loss = 3.16977801\n",
      "Iteration 138, loss = 3.16943516\n",
      "Iteration 139, loss = 3.16899945\n",
      "Iteration 140, loss = 3.16862567\n",
      "Iteration 141, loss = 3.16808804\n",
      "Iteration 142, loss = 3.16741601\n",
      "Iteration 143, loss = 3.16708408\n",
      "Iteration 144, loss = 3.16720416\n",
      "Iteration 145, loss = 3.16593156\n",
      "Iteration 146, loss = 3.16509936\n",
      "Iteration 147, loss = 3.16451763\n",
      "Iteration 148, loss = 3.16427326\n",
      "Iteration 149, loss = 3.16401756\n",
      "Iteration 150, loss = 3.16288597\n",
      "Iteration 151, loss = 3.16193334\n",
      "Iteration 152, loss = 3.16208579\n",
      "Iteration 153, loss = 3.16172689\n",
      "Iteration 154, loss = 3.16101781\n",
      "Iteration 155, loss = 3.16168684\n",
      "Iteration 156, loss = 3.16001213\n",
      "Iteration 157, loss = 3.15919872\n",
      "Iteration 158, loss = 3.15839941\n",
      "Iteration 159, loss = 3.15808497\n",
      "Iteration 160, loss = 3.15755251\n",
      "Iteration 161, loss = 3.15683924\n",
      "Iteration 162, loss = 3.15642237\n",
      "Iteration 163, loss = 3.15549549\n",
      "Iteration 164, loss = 3.15484354\n",
      "Iteration 165, loss = 3.15516834\n",
      "Iteration 166, loss = 3.15488440\n",
      "Iteration 167, loss = 3.15379491\n",
      "Iteration 168, loss = 3.15284051\n",
      "Iteration 169, loss = 3.15321901\n",
      "Iteration 170, loss = 3.15236727\n",
      "Iteration 171, loss = 3.15114395\n",
      "Iteration 172, loss = 3.15095100\n",
      "Iteration 173, loss = 3.15038147\n",
      "Iteration 174, loss = 3.14990984\n",
      "Iteration 175, loss = 3.14941448\n",
      "Iteration 176, loss = 3.14884441\n",
      "Iteration 177, loss = 3.14847368\n",
      "Iteration 178, loss = 3.14802936\n",
      "Iteration 179, loss = 3.14756278\n",
      "Iteration 180, loss = 3.14679941\n",
      "Iteration 181, loss = 3.14589331\n",
      "Iteration 182, loss = 3.14565678\n",
      "Iteration 183, loss = 3.14496182\n",
      "Iteration 184, loss = 3.14456279\n",
      "Iteration 185, loss = 3.14322375\n",
      "Iteration 186, loss = 3.14317215\n",
      "Iteration 187, loss = 3.14277581\n",
      "Iteration 188, loss = 3.14159112\n",
      "Iteration 189, loss = 3.14054279\n",
      "Iteration 190, loss = 3.14012877\n",
      "Iteration 191, loss = 3.14035698\n",
      "Iteration 192, loss = 3.13980631\n",
      "Iteration 193, loss = 3.13947392\n",
      "Iteration 194, loss = 3.13805168\n",
      "Iteration 195, loss = 3.13787376\n",
      "Iteration 196, loss = 3.13731260\n",
      "Iteration 197, loss = 3.13637033\n",
      "Iteration 198, loss = 3.13612449\n",
      "Iteration 199, loss = 3.13540113\n",
      "Iteration 200, loss = 3.13453326\n",
      "Iteration 201, loss = 3.13427771\n",
      "Iteration 202, loss = 3.13297725\n",
      "Iteration 203, loss = 3.13214032\n",
      "Iteration 204, loss = 3.13144186\n",
      "Iteration 205, loss = 3.13101686\n",
      "Iteration 206, loss = 3.13045399\n",
      "Iteration 207, loss = 3.13052196\n",
      "Iteration 208, loss = 3.12949559\n",
      "Iteration 209, loss = 3.12955558\n",
      "Iteration 210, loss = 3.12852899\n",
      "Iteration 211, loss = 3.12816627\n",
      "Iteration 212, loss = 3.12754886\n",
      "Iteration 213, loss = 3.12642102\n",
      "Iteration 214, loss = 3.12646496\n",
      "Iteration 215, loss = 3.12517087\n",
      "Iteration 216, loss = 3.12480138\n",
      "Iteration 217, loss = 3.12463004\n",
      "Iteration 218, loss = 3.12437006\n",
      "Iteration 219, loss = 3.12254277\n",
      "Iteration 220, loss = 3.12194495\n",
      "Iteration 221, loss = 3.12122001\n",
      "Iteration 222, loss = 3.12108243\n",
      "Iteration 223, loss = 3.12060045\n",
      "Iteration 224, loss = 3.11986437\n",
      "Iteration 225, loss = 3.11943838\n",
      "Iteration 226, loss = 3.11862763\n",
      "Iteration 227, loss = 3.11824819\n",
      "Iteration 228, loss = 3.11757369\n",
      "Iteration 229, loss = 3.11712789\n",
      "Iteration 230, loss = 3.11658007\n",
      "Iteration 231, loss = 3.11607711\n",
      "Iteration 232, loss = 3.11538767\n",
      "Iteration 233, loss = 3.11442895\n",
      "Iteration 234, loss = 3.11362828\n",
      "Iteration 235, loss = 3.11373665\n",
      "Iteration 236, loss = 3.11205962\n",
      "Iteration 237, loss = 3.11133656\n",
      "Iteration 238, loss = 3.11088310\n",
      "Iteration 239, loss = 3.11068012\n",
      "Iteration 240, loss = 3.10995735\n",
      "Iteration 241, loss = 3.10906863\n",
      "Iteration 242, loss = 3.10837839\n",
      "Iteration 243, loss = 3.10798629\n",
      "Iteration 244, loss = 3.10712215\n",
      "Iteration 245, loss = 3.10702126\n",
      "Iteration 246, loss = 3.10649888\n",
      "Iteration 247, loss = 3.10541302\n",
      "Iteration 248, loss = 3.10455822\n",
      "Iteration 249, loss = 3.10437753\n",
      "Iteration 250, loss = 3.10426773\n",
      "Iteration 251, loss = 3.10478267\n",
      "Iteration 252, loss = 3.10255733\n",
      "Iteration 253, loss = 3.10149230\n",
      "Iteration 254, loss = 3.10142813\n",
      "Iteration 255, loss = 3.10082921\n",
      "Iteration 256, loss = 3.09972769\n",
      "Iteration 257, loss = 3.09849899\n",
      "Iteration 258, loss = 3.09848205\n",
      "Iteration 259, loss = 3.09785174\n",
      "Iteration 260, loss = 3.09759952\n",
      "Iteration 261, loss = 3.09675538\n",
      "Iteration 262, loss = 3.09580350\n",
      "Iteration 263, loss = 3.09513195\n",
      "Iteration 264, loss = 3.09459280\n",
      "Iteration 265, loss = 3.09416512\n",
      "Iteration 266, loss = 3.09308039\n",
      "Iteration 267, loss = 3.09310997\n",
      "Iteration 268, loss = 3.09315689\n",
      "Iteration 269, loss = 3.09268937\n",
      "Iteration 270, loss = 3.09171270\n",
      "Iteration 271, loss = 3.09154213\n",
      "Iteration 272, loss = 3.09025033\n",
      "Iteration 273, loss = 3.08905135\n",
      "Iteration 274, loss = 3.08865054\n",
      "Iteration 275, loss = 3.08771441\n",
      "Iteration 276, loss = 3.08677415\n",
      "Iteration 277, loss = 3.08671695\n",
      "Iteration 278, loss = 3.08531159\n",
      "Iteration 279, loss = 3.08561648\n",
      "Iteration 280, loss = 3.08574243\n",
      "Iteration 281, loss = 3.08486642\n",
      "Iteration 282, loss = 3.08399250\n",
      "Iteration 283, loss = 3.08389896\n",
      "Iteration 284, loss = 3.08266416\n",
      "Iteration 285, loss = 3.08207764\n",
      "Iteration 286, loss = 3.08139483\n",
      "Iteration 287, loss = 3.08088430\n",
      "Iteration 288, loss = 3.07988131\n",
      "Iteration 289, loss = 3.07988161\n",
      "Iteration 290, loss = 3.07922226\n",
      "Iteration 291, loss = 3.07842524\n",
      "Iteration 292, loss = 3.07757882\n",
      "Iteration 293, loss = 3.07700834\n",
      "Iteration 294, loss = 3.07656837\n",
      "Iteration 295, loss = 3.07588608\n",
      "Iteration 296, loss = 3.07502380\n",
      "Iteration 297, loss = 3.07448580\n",
      "Iteration 298, loss = 3.07383766\n",
      "Iteration 299, loss = 3.07316592\n",
      "Iteration 300, loss = 3.07291448\n",
      "Iteration 301, loss = 3.07199395\n",
      "Iteration 302, loss = 3.07145506\n",
      "Iteration 303, loss = 3.07022560\n",
      "Iteration 304, loss = 3.07008562\n",
      "Iteration 305, loss = 3.06995884\n",
      "Iteration 306, loss = 3.06922450\n",
      "Iteration 307, loss = 3.06862328\n",
      "Iteration 308, loss = 3.06752198\n",
      "Iteration 309, loss = 3.06777199\n",
      "Iteration 310, loss = 3.06723753\n",
      "Iteration 311, loss = 3.06655770\n",
      "Iteration 312, loss = 3.06582456\n",
      "Iteration 313, loss = 3.06524448\n",
      "Iteration 314, loss = 3.06400700\n",
      "Iteration 315, loss = 3.06372462\n",
      "Iteration 316, loss = 3.06310376\n",
      "Iteration 317, loss = 3.06227358\n",
      "Iteration 318, loss = 3.06189534\n",
      "Iteration 319, loss = 3.06172602\n",
      "Iteration 320, loss = 3.06086373\n",
      "Iteration 321, loss = 3.06076040\n",
      "Iteration 322, loss = 3.05980189\n",
      "Iteration 323, loss = 3.05942075\n",
      "Iteration 324, loss = 3.05906312\n",
      "Iteration 325, loss = 3.05793284\n",
      "Iteration 326, loss = 3.05733528\n",
      "Iteration 327, loss = 3.05939901\n",
      "Iteration 328, loss = 3.05817848\n",
      "Iteration 329, loss = 3.05616097\n",
      "Iteration 330, loss = 3.05619388\n",
      "Iteration 331, loss = 3.05483690\n",
      "Iteration 332, loss = 3.05500499\n",
      "Iteration 333, loss = 3.05420899\n",
      "Iteration 334, loss = 3.05277643\n",
      "Iteration 335, loss = 3.05353265\n",
      "Iteration 336, loss = 3.05292528\n",
      "Iteration 337, loss = 3.05162064\n",
      "Iteration 338, loss = 3.05160224\n",
      "Iteration 339, loss = 3.05083674\n",
      "Iteration 340, loss = 3.05023700\n",
      "Iteration 341, loss = 3.04922258\n",
      "Iteration 342, loss = 3.04908912\n",
      "Iteration 343, loss = 3.04887057\n",
      "Iteration 344, loss = 3.04813901\n",
      "Iteration 345, loss = 3.04730036\n",
      "Iteration 346, loss = 3.04719314\n",
      "Iteration 347, loss = 3.04668497\n",
      "Iteration 348, loss = 3.04564812\n",
      "Iteration 349, loss = 3.04533691\n",
      "Iteration 350, loss = 3.04522107\n",
      "Iteration 351, loss = 3.04401752\n",
      "Iteration 352, loss = 3.04329977\n",
      "Iteration 353, loss = 3.04496592\n",
      "Iteration 354, loss = 3.04312904\n",
      "Iteration 355, loss = 3.04205736\n",
      "Iteration 356, loss = 3.04168647\n",
      "Iteration 357, loss = 3.04115801\n",
      "Iteration 358, loss = 3.04133473\n",
      "Iteration 359, loss = 3.04089954\n",
      "Iteration 360, loss = 3.04010514\n",
      "Iteration 361, loss = 3.03921774\n",
      "Iteration 362, loss = 3.03891415\n",
      "Iteration 363, loss = 3.03909580\n",
      "Iteration 364, loss = 3.03738735\n",
      "Iteration 365, loss = 3.03693093\n",
      "Iteration 366, loss = 3.03671150\n",
      "Iteration 367, loss = 3.03703175\n",
      "Iteration 368, loss = 3.03594501\n",
      "Iteration 369, loss = 3.03523226\n",
      "Iteration 370, loss = 3.03452192\n",
      "Iteration 371, loss = 3.03389340\n",
      "Iteration 372, loss = 3.03274420\n",
      "Iteration 373, loss = 3.03309044\n",
      "Iteration 374, loss = 3.03184816\n",
      "Iteration 375, loss = 3.03170079\n",
      "Iteration 376, loss = 3.03106295\n",
      "Iteration 377, loss = 3.03051656\n",
      "Iteration 378, loss = 3.02998779\n",
      "Iteration 379, loss = 3.03010453\n",
      "Iteration 380, loss = 3.02909219\n",
      "Iteration 381, loss = 3.02794268\n",
      "Iteration 382, loss = 3.02797469\n",
      "Iteration 383, loss = 3.02696143\n",
      "Iteration 384, loss = 3.02649881\n",
      "Iteration 385, loss = 3.02624198\n",
      "Iteration 386, loss = 3.02586556\n",
      "Iteration 387, loss = 3.02643384\n",
      "Iteration 388, loss = 3.02568537\n",
      "Iteration 389, loss = 3.02470268\n",
      "Iteration 390, loss = 3.02369546\n",
      "Iteration 391, loss = 3.02322366\n",
      "Iteration 392, loss = 3.02352546\n",
      "Iteration 393, loss = 3.02331731\n",
      "Iteration 394, loss = 3.02416343\n",
      "Iteration 395, loss = 3.02185014\n",
      "Iteration 396, loss = 3.02115020\n",
      "Iteration 397, loss = 3.02070638\n",
      "Iteration 398, loss = 3.01998189\n",
      "Iteration 399, loss = 3.01974898\n",
      "Iteration 400, loss = 3.01930073\n",
      "Iteration 401, loss = 3.01969407\n",
      "Iteration 402, loss = 3.01874129\n",
      "Iteration 403, loss = 3.01831235\n",
      "Iteration 404, loss = 3.01787536\n",
      "Iteration 405, loss = 3.01669894\n",
      "Iteration 406, loss = 3.01739496\n",
      "Iteration 407, loss = 3.01573280\n",
      "Iteration 408, loss = 3.01528942\n",
      "Iteration 409, loss = 3.01489391\n",
      "Iteration 410, loss = 3.01705138\n",
      "Iteration 411, loss = 3.01507356\n",
      "Iteration 412, loss = 3.01305009\n",
      "Iteration 413, loss = 3.01277697\n",
      "Iteration 414, loss = 3.01274667\n",
      "Iteration 415, loss = 3.01257196\n",
      "Iteration 416, loss = 3.01165072\n",
      "Iteration 417, loss = 3.01278486\n",
      "Iteration 418, loss = 3.01203747\n",
      "Iteration 419, loss = 3.01128152\n",
      "Iteration 420, loss = 3.01031434\n",
      "Iteration 421, loss = 3.00955800\n",
      "Iteration 422, loss = 3.00864642\n",
      "Iteration 423, loss = 3.00786243\n",
      "Iteration 424, loss = 3.00733393\n",
      "Iteration 425, loss = 3.00760807\n",
      "Iteration 426, loss = 3.00665859\n",
      "Iteration 427, loss = 3.00571325\n",
      "Iteration 428, loss = 3.00596590\n",
      "Iteration 429, loss = 3.00497387\n",
      "Iteration 430, loss = 3.00549358\n",
      "Iteration 431, loss = 3.00428018\n",
      "Iteration 432, loss = 3.00425242\n",
      "Iteration 433, loss = 3.00527271\n",
      "Iteration 434, loss = 3.00313981\n",
      "Iteration 435, loss = 3.00207588\n",
      "Iteration 436, loss = 3.00179165\n",
      "Iteration 437, loss = 3.00197292\n",
      "Iteration 438, loss = 3.00082581\n",
      "Iteration 439, loss = 3.00170849\n",
      "Iteration 440, loss = 3.00074938\n",
      "Iteration 441, loss = 2.99965314\n",
      "Iteration 442, loss = 3.00008346\n",
      "Iteration 443, loss = 2.99975378\n",
      "Iteration 444, loss = 2.99863199\n",
      "Iteration 445, loss = 2.99877980\n",
      "Iteration 446, loss = 2.99772230\n",
      "Iteration 447, loss = 2.99716091\n",
      "Iteration 448, loss = 2.99625547\n",
      "Iteration 449, loss = 2.99620775\n",
      "Iteration 450, loss = 2.99692225\n",
      "Iteration 451, loss = 2.99593693\n",
      "Iteration 452, loss = 2.99501791\n",
      "Iteration 453, loss = 2.99397207\n",
      "Iteration 454, loss = 2.99423752\n",
      "Iteration 455, loss = 2.99444219\n",
      "Iteration 456, loss = 2.99335622\n",
      "Iteration 457, loss = 2.99380375\n",
      "Iteration 458, loss = 2.99486965\n",
      "Iteration 459, loss = 2.99260833\n",
      "Iteration 460, loss = 2.99134012\n",
      "Iteration 461, loss = 2.99089855\n",
      "Iteration 462, loss = 2.99002353\n",
      "Iteration 463, loss = 2.99070326\n",
      "Iteration 464, loss = 2.98953971\n",
      "Iteration 465, loss = 2.98962659\n",
      "Iteration 466, loss = 2.98928475\n",
      "Iteration 467, loss = 2.98820871\n",
      "Iteration 468, loss = 2.98787278\n",
      "Iteration 469, loss = 2.98733296\n",
      "Iteration 470, loss = 2.98850580\n",
      "Iteration 471, loss = 2.98698072\n",
      "Iteration 472, loss = 2.98672438\n",
      "Iteration 473, loss = 2.98559798\n",
      "Iteration 474, loss = 2.98529087\n",
      "Iteration 475, loss = 2.98458375\n",
      "Iteration 476, loss = 2.98399750\n",
      "Iteration 477, loss = 2.98390748\n",
      "Iteration 478, loss = 2.98325776\n",
      "Iteration 479, loss = 2.98265651\n",
      "Iteration 480, loss = 2.98221695\n",
      "Iteration 481, loss = 2.98141341\n",
      "Iteration 482, loss = 2.98241634\n",
      "Iteration 483, loss = 2.98249301\n",
      "Iteration 484, loss = 2.98176569\n",
      "Iteration 485, loss = 2.98114462\n",
      "Iteration 486, loss = 2.98071415\n",
      "Iteration 487, loss = 2.98037901\n",
      "Iteration 488, loss = 2.97972778\n",
      "Iteration 489, loss = 2.97959534\n",
      "Iteration 490, loss = 2.97879470\n",
      "Iteration 491, loss = 2.97836932\n",
      "Iteration 492, loss = 2.97760891\n",
      "Iteration 493, loss = 2.97764494\n",
      "Iteration 494, loss = 2.97687713\n",
      "Iteration 495, loss = 2.97658082\n",
      "Iteration 496, loss = 2.97649592\n",
      "Iteration 497, loss = 2.97645172\n",
      "Iteration 498, loss = 2.97679310\n",
      "Iteration 499, loss = 2.97590849\n",
      "Iteration 500, loss = 2.97505437\n",
      "Iteration 501, loss = 2.97487269\n",
      "Iteration 502, loss = 2.97338000\n",
      "Iteration 503, loss = 2.97408439\n",
      "Iteration 504, loss = 2.97385126\n",
      "Iteration 505, loss = 2.97239608\n",
      "Iteration 506, loss = 2.97197733\n",
      "Iteration 507, loss = 2.97161268\n",
      "Iteration 508, loss = 2.97176183\n",
      "Iteration 509, loss = 2.97134659\n",
      "Iteration 510, loss = 2.97031238\n",
      "Iteration 511, loss = 2.97112397\n",
      "Iteration 512, loss = 2.96996309\n",
      "Iteration 513, loss = 2.97153501\n",
      "Iteration 514, loss = 2.96874770\n",
      "Iteration 515, loss = 2.96882068\n",
      "Iteration 516, loss = 2.96884964\n",
      "Iteration 517, loss = 2.96764848\n",
      "Iteration 518, loss = 2.96700529\n",
      "Iteration 519, loss = 2.96797234\n",
      "Iteration 520, loss = 2.96640261\n",
      "Iteration 521, loss = 2.96635649\n",
      "Iteration 522, loss = 2.96622337\n",
      "Iteration 523, loss = 2.96535558\n",
      "Iteration 524, loss = 2.96488611\n",
      "Iteration 525, loss = 2.96477300\n",
      "Iteration 526, loss = 2.96517768\n",
      "Iteration 527, loss = 2.96395807\n",
      "Iteration 528, loss = 2.96474295\n",
      "Iteration 529, loss = 2.96477739\n",
      "Iteration 530, loss = 2.96433215\n",
      "Iteration 531, loss = 2.96307535\n",
      "Iteration 532, loss = 2.96289082\n",
      "Iteration 533, loss = 2.96260565\n",
      "Iteration 534, loss = 2.96215165\n",
      "Iteration 535, loss = 2.96119636\n",
      "Iteration 536, loss = 2.96069563\n",
      "Iteration 537, loss = 2.96020332\n",
      "Iteration 538, loss = 2.95946183\n",
      "Iteration 539, loss = 2.95947450\n",
      "Iteration 540, loss = 2.95994474\n",
      "Iteration 541, loss = 2.95940784\n",
      "Iteration 542, loss = 2.95812479\n",
      "Iteration 543, loss = 2.95839146\n",
      "Iteration 544, loss = 2.95732850\n",
      "Iteration 545, loss = 2.95771465\n",
      "Iteration 546, loss = 2.95771620\n",
      "Iteration 547, loss = 2.95508771\n",
      "Iteration 548, loss = 2.95667536\n",
      "Iteration 549, loss = 2.95585283\n",
      "Iteration 550, loss = 2.95671184\n",
      "Iteration 551, loss = 2.95446969\n",
      "Iteration 552, loss = 2.95415093\n",
      "Iteration 553, loss = 2.95393272\n",
      "Iteration 554, loss = 2.95441505\n",
      "Iteration 555, loss = 2.95378127\n",
      "Iteration 556, loss = 2.95306728\n",
      "Iteration 557, loss = 2.95262625\n",
      "Iteration 558, loss = 2.95203905\n",
      "Iteration 559, loss = 2.95193215\n",
      "Iteration 560, loss = 2.95113032\n",
      "Iteration 561, loss = 2.95076529\n",
      "Iteration 562, loss = 2.95085748\n",
      "Iteration 563, loss = 2.95027858\n",
      "Iteration 564, loss = 2.95012522\n",
      "Iteration 565, loss = 2.94967525\n",
      "Iteration 566, loss = 2.94934198\n",
      "Iteration 567, loss = 2.94834449\n",
      "Iteration 568, loss = 2.94860179\n",
      "Iteration 569, loss = 2.94783884\n",
      "Iteration 570, loss = 2.94776882\n",
      "Iteration 571, loss = 2.94735716\n",
      "Iteration 572, loss = 2.94725840\n",
      "Iteration 573, loss = 2.94675925\n",
      "Iteration 574, loss = 2.94639199\n",
      "Iteration 575, loss = 2.94588892\n",
      "Iteration 576, loss = 2.94609297\n",
      "Iteration 577, loss = 2.94547874\n",
      "Iteration 578, loss = 2.94482042\n",
      "Iteration 579, loss = 2.94487161\n",
      "Iteration 580, loss = 2.94620846\n",
      "Iteration 581, loss = 2.94431702\n",
      "Iteration 582, loss = 2.94366902\n",
      "Iteration 583, loss = 2.94294158\n",
      "Iteration 584, loss = 2.94258442\n",
      "Iteration 585, loss = 2.94273494\n",
      "Iteration 586, loss = 2.94341662\n",
      "Iteration 587, loss = 2.94202073\n",
      "Iteration 588, loss = 2.94097216\n",
      "Iteration 589, loss = 2.94105338\n",
      "Iteration 590, loss = 2.94060086\n",
      "Iteration 591, loss = 2.93984496\n",
      "Iteration 592, loss = 2.93997547\n",
      "Iteration 593, loss = 2.94040333\n",
      "Iteration 594, loss = 2.94023488\n",
      "Iteration 595, loss = 2.93867314\n",
      "Iteration 596, loss = 2.93901301\n",
      "Iteration 597, loss = 2.93925724\n",
      "Iteration 598, loss = 2.93802859\n",
      "Iteration 599, loss = 2.93795911\n",
      "Iteration 600, loss = 2.93774747\n",
      "Iteration 601, loss = 2.93682672\n",
      "Iteration 602, loss = 2.93651048\n",
      "Iteration 603, loss = 2.93637019\n",
      "Iteration 604, loss = 2.93598523\n",
      "Iteration 605, loss = 2.93555659\n",
      "Iteration 606, loss = 2.93488533\n",
      "Iteration 607, loss = 2.93451048\n",
      "Iteration 608, loss = 2.93514024\n",
      "Iteration 609, loss = 2.93486270\n",
      "Iteration 610, loss = 2.93413100\n",
      "Iteration 611, loss = 2.93336521\n",
      "Iteration 612, loss = 2.93265676\n",
      "Iteration 613, loss = 2.93209011\n",
      "Iteration 614, loss = 2.93149202\n",
      "Iteration 615, loss = 2.93283072\n",
      "Iteration 616, loss = 2.93218947\n",
      "Iteration 617, loss = 2.93142001\n",
      "Iteration 618, loss = 2.93098062\n",
      "Iteration 619, loss = 2.93037321\n",
      "Iteration 620, loss = 2.93020943\n",
      "Iteration 621, loss = 2.92949431\n",
      "Iteration 622, loss = 2.92884409\n",
      "Iteration 623, loss = 2.92908105\n",
      "Iteration 624, loss = 2.93008674\n",
      "Iteration 625, loss = 2.92975721\n",
      "Iteration 626, loss = 2.92861378\n",
      "Iteration 627, loss = 2.92753169\n",
      "Iteration 628, loss = 2.93093485\n",
      "Iteration 629, loss = 2.93028400\n",
      "Iteration 630, loss = 2.92653073\n",
      "Iteration 631, loss = 2.92805931\n",
      "Iteration 632, loss = 2.92769825\n",
      "Iteration 633, loss = 2.92580644\n",
      "Iteration 634, loss = 2.92654792\n",
      "Iteration 635, loss = 2.92546155\n",
      "Iteration 636, loss = 2.92472502\n",
      "Iteration 637, loss = 2.92386272\n",
      "Iteration 638, loss = 2.92333318\n",
      "Iteration 639, loss = 2.92402507\n",
      "Iteration 640, loss = 2.92378705\n",
      "Iteration 641, loss = 2.92385980\n",
      "Iteration 642, loss = 2.92212683\n",
      "Iteration 643, loss = 2.92243659\n",
      "Iteration 644, loss = 2.92304163\n",
      "Iteration 645, loss = 2.92220499\n",
      "Iteration 646, loss = 2.92101190\n",
      "Iteration 647, loss = 2.92186432\n",
      "Iteration 648, loss = 2.92116681\n",
      "Iteration 649, loss = 2.92019566\n",
      "Iteration 650, loss = 2.91942624\n",
      "Iteration 651, loss = 2.91973131\n",
      "Iteration 652, loss = 2.91815645\n",
      "Iteration 653, loss = 2.91848186\n",
      "Iteration 654, loss = 2.91853998\n",
      "Iteration 655, loss = 2.91758007\n",
      "Iteration 656, loss = 2.91731014\n",
      "Iteration 657, loss = 2.91755777\n",
      "Iteration 658, loss = 2.91677433\n",
      "Iteration 659, loss = 2.91638151\n",
      "Iteration 660, loss = 2.91601428\n",
      "Iteration 661, loss = 2.91620229\n",
      "Iteration 662, loss = 2.91612729\n",
      "Iteration 663, loss = 2.91536512\n",
      "Iteration 664, loss = 2.91507866\n",
      "Iteration 665, loss = 2.91482918\n",
      "Iteration 666, loss = 2.91502169\n",
      "Iteration 667, loss = 2.91493120\n",
      "Iteration 668, loss = 2.91354178\n",
      "Iteration 669, loss = 2.91304911\n",
      "Iteration 670, loss = 2.91320026\n",
      "Iteration 671, loss = 2.91283024\n",
      "Iteration 672, loss = 2.91324193\n",
      "Iteration 673, loss = 2.91233269\n",
      "Iteration 674, loss = 2.91178472\n",
      "Iteration 675, loss = 2.91166730\n",
      "Iteration 676, loss = 2.91189844\n",
      "Iteration 677, loss = 2.91140955\n",
      "Iteration 678, loss = 2.91100016\n",
      "Iteration 679, loss = 2.91065320\n",
      "Iteration 680, loss = 2.91171605\n",
      "Iteration 681, loss = 2.91224369\n",
      "Iteration 682, loss = 2.90987766\n",
      "Iteration 683, loss = 2.90923275\n",
      "Iteration 684, loss = 2.90934026\n",
      "Iteration 685, loss = 2.90854489\n",
      "Iteration 686, loss = 2.90819645\n",
      "Iteration 687, loss = 2.90852912\n",
      "Iteration 688, loss = 2.90908736\n",
      "Iteration 689, loss = 2.90762933\n",
      "Iteration 690, loss = 2.90698420\n",
      "Iteration 691, loss = 2.90629488\n",
      "Iteration 692, loss = 2.90697898\n",
      "Iteration 693, loss = 2.90727662\n",
      "Iteration 694, loss = 2.90601406\n",
      "Iteration 695, loss = 2.90619960\n",
      "Iteration 696, loss = 2.90562264\n",
      "Iteration 697, loss = 2.90548306\n",
      "Iteration 698, loss = 2.90492839\n",
      "Iteration 699, loss = 2.90426994\n",
      "Iteration 700, loss = 2.90488986\n",
      "Iteration 701, loss = 2.90449393\n",
      "Iteration 702, loss = 2.90372895\n",
      "Iteration 703, loss = 2.90394853\n",
      "Iteration 704, loss = 2.90270884\n",
      "Iteration 705, loss = 2.90278552\n",
      "Iteration 706, loss = 2.90257633\n",
      "Iteration 707, loss = 2.90245122\n",
      "Iteration 708, loss = 2.90150666\n",
      "Iteration 709, loss = 2.90157560\n",
      "Iteration 710, loss = 2.90222041\n",
      "Iteration 711, loss = 2.90083801\n",
      "Iteration 712, loss = 2.90134337\n",
      "Iteration 713, loss = 2.90089033\n",
      "Iteration 714, loss = 2.89957652\n",
      "Iteration 715, loss = 2.89944774\n",
      "Iteration 716, loss = 2.89922430\n",
      "Iteration 717, loss = 2.89823536\n",
      "Iteration 718, loss = 2.89908537\n",
      "Iteration 719, loss = 2.89809148\n",
      "Iteration 720, loss = 2.89732517\n",
      "Iteration 721, loss = 2.89763360\n",
      "Iteration 722, loss = 2.89643656\n",
      "Iteration 723, loss = 2.89593367\n",
      "Iteration 724, loss = 2.89637595\n",
      "Iteration 725, loss = 2.89613069\n",
      "Iteration 726, loss = 2.89692949\n",
      "Iteration 727, loss = 2.89631197\n",
      "Iteration 728, loss = 2.89530173\n",
      "Iteration 729, loss = 2.89506975\n",
      "Iteration 730, loss = 2.89514916\n",
      "Iteration 731, loss = 2.89477881\n",
      "Iteration 732, loss = 2.89423304\n",
      "Iteration 733, loss = 2.89387966\n",
      "Iteration 734, loss = 2.89381892\n",
      "Iteration 735, loss = 2.89404035\n",
      "Iteration 736, loss = 2.89308123\n",
      "Iteration 737, loss = 2.89321676\n",
      "Iteration 738, loss = 2.89278982\n",
      "Iteration 739, loss = 2.89290651\n",
      "Iteration 740, loss = 2.89193928\n",
      "Iteration 741, loss = 2.89108957\n",
      "Iteration 742, loss = 2.89075228\n",
      "Iteration 743, loss = 2.89018721\n",
      "Iteration 744, loss = 2.89033301\n",
      "Iteration 745, loss = 2.88942268\n",
      "Iteration 746, loss = 2.88951655\n",
      "Iteration 747, loss = 2.88991608\n",
      "Iteration 748, loss = 2.89009142\n",
      "Iteration 749, loss = 2.88978857\n",
      "Iteration 750, loss = 2.88913312\n",
      "Iteration 751, loss = 2.88799341\n",
      "Iteration 752, loss = 2.88805281\n",
      "Iteration 753, loss = 2.88800613\n",
      "Iteration 754, loss = 2.88733192\n",
      "Iteration 755, loss = 2.88710794\n",
      "Iteration 756, loss = 2.88678091\n",
      "Iteration 757, loss = 2.88688911\n",
      "Iteration 758, loss = 2.88677963\n",
      "Iteration 759, loss = 2.88564789\n",
      "Iteration 760, loss = 2.88533079\n",
      "Iteration 761, loss = 2.88570369\n",
      "Iteration 762, loss = 2.88721255\n",
      "Iteration 763, loss = 2.88572663\n",
      "Iteration 764, loss = 2.88517913\n",
      "Iteration 765, loss = 2.88419285\n",
      "Iteration 766, loss = 2.88483223\n",
      "Iteration 767, loss = 2.88416948\n",
      "Iteration 768, loss = 2.88379450\n",
      "Iteration 769, loss = 2.88387909\n",
      "Iteration 770, loss = 2.88328622\n",
      "Iteration 771, loss = 2.88280980\n",
      "Iteration 772, loss = 2.88218449\n",
      "Iteration 773, loss = 2.88238901\n",
      "Iteration 774, loss = 2.88187795\n",
      "Iteration 775, loss = 2.88139985\n",
      "Iteration 776, loss = 2.88139494\n",
      "Iteration 777, loss = 2.88121371\n",
      "Iteration 778, loss = 2.88160773\n",
      "Iteration 779, loss = 2.88043109\n",
      "Iteration 780, loss = 2.87973576\n",
      "Iteration 781, loss = 2.87964865\n",
      "Iteration 782, loss = 2.87883759\n",
      "Iteration 783, loss = 2.87834074\n",
      "Iteration 784, loss = 2.87804119\n",
      "Iteration 785, loss = 2.87792764\n",
      "Iteration 786, loss = 2.87767505\n",
      "Iteration 787, loss = 2.87764987\n",
      "Iteration 788, loss = 2.87807094\n",
      "Iteration 789, loss = 2.87762136\n",
      "Iteration 790, loss = 2.87688704\n",
      "Iteration 791, loss = 2.87738398\n",
      "Iteration 792, loss = 2.87608120\n",
      "Iteration 793, loss = 2.87696752\n",
      "Iteration 794, loss = 2.87608018\n",
      "Iteration 795, loss = 2.87601552\n",
      "Iteration 796, loss = 2.87559581\n",
      "Iteration 797, loss = 2.87461152\n",
      "Iteration 798, loss = 2.87457175\n",
      "Iteration 799, loss = 2.87491073\n",
      "Iteration 800, loss = 2.87451494\n",
      "Iteration 801, loss = 2.87609130\n",
      "Iteration 802, loss = 2.87576492\n",
      "Iteration 803, loss = 2.87388893\n",
      "Iteration 804, loss = 2.87423142\n",
      "Iteration 805, loss = 2.87348462\n",
      "Iteration 806, loss = 2.87271199\n",
      "Iteration 807, loss = 2.87262298\n",
      "Iteration 808, loss = 2.87204693\n",
      "Iteration 809, loss = 2.87149677\n",
      "Iteration 810, loss = 2.87415085\n",
      "Iteration 811, loss = 2.87163213\n",
      "Iteration 812, loss = 2.87058464\n",
      "Iteration 813, loss = 2.87090005\n",
      "Iteration 814, loss = 2.87091660\n",
      "Iteration 815, loss = 2.87100636\n",
      "Iteration 816, loss = 2.87112256\n",
      "Iteration 817, loss = 2.86913681\n",
      "Iteration 818, loss = 2.86975890\n",
      "Iteration 819, loss = 2.86899358\n",
      "Iteration 820, loss = 2.86969045\n",
      "Iteration 821, loss = 2.86936109\n",
      "Iteration 822, loss = 2.86802546\n",
      "Iteration 823, loss = 2.86744703\n",
      "Iteration 824, loss = 2.86815759\n",
      "Iteration 825, loss = 2.86721690\n",
      "Iteration 826, loss = 2.86734720\n",
      "Iteration 827, loss = 2.86667104\n",
      "Iteration 828, loss = 2.86800183\n",
      "Iteration 829, loss = 2.86597546\n",
      "Iteration 830, loss = 2.86591000\n",
      "Iteration 831, loss = 2.86587459\n",
      "Iteration 832, loss = 2.86553751\n",
      "Iteration 833, loss = 2.86525050\n",
      "Iteration 834, loss = 2.86520679\n",
      "Iteration 835, loss = 2.86548351\n",
      "Iteration 836, loss = 2.86495784\n",
      "Iteration 837, loss = 2.86387971\n",
      "Iteration 838, loss = 2.86385337\n",
      "Iteration 839, loss = 2.86430492\n",
      "Iteration 840, loss = 2.86404784\n",
      "Iteration 841, loss = 2.86331292\n",
      "Iteration 842, loss = 2.86266253\n",
      "Iteration 843, loss = 2.86259844\n",
      "Iteration 844, loss = 2.86108494\n",
      "Iteration 845, loss = 2.86182674\n",
      "Iteration 846, loss = 2.86241414\n",
      "Iteration 847, loss = 2.86121805\n",
      "Iteration 848, loss = 2.86057141\n",
      "Iteration 849, loss = 2.86043326\n",
      "Iteration 850, loss = 2.86057381\n",
      "Iteration 851, loss = 2.86019004\n",
      "Iteration 852, loss = 2.85940455\n",
      "Iteration 853, loss = 2.85910333\n",
      "Iteration 854, loss = 2.85953808\n",
      "Iteration 855, loss = 2.85884885\n",
      "Iteration 856, loss = 2.85921436\n",
      "Iteration 857, loss = 2.86031383\n",
      "Iteration 858, loss = 2.85872625\n",
      "Iteration 859, loss = 2.85757343\n",
      "Iteration 860, loss = 2.85754296\n",
      "Iteration 861, loss = 2.85758579\n",
      "Iteration 862, loss = 2.85762183\n",
      "Iteration 863, loss = 2.85811345\n",
      "Iteration 864, loss = 2.85752630\n",
      "Iteration 865, loss = 2.85673340\n",
      "Iteration 866, loss = 2.85712210\n",
      "Iteration 867, loss = 2.85705015\n",
      "Iteration 868, loss = 2.85649209\n",
      "Iteration 869, loss = 2.85523389\n",
      "Iteration 870, loss = 2.85499740\n",
      "Iteration 871, loss = 2.85450844\n",
      "Iteration 872, loss = 2.85519506\n",
      "Iteration 873, loss = 2.85463387\n",
      "Iteration 874, loss = 2.85468281\n",
      "Iteration 875, loss = 2.85443117\n",
      "Iteration 876, loss = 2.85370520\n",
      "Iteration 877, loss = 2.85416261\n",
      "Iteration 878, loss = 2.85326058\n",
      "Iteration 879, loss = 2.85269261\n",
      "Iteration 880, loss = 2.85231210\n",
      "Iteration 881, loss = 2.85215524\n",
      "Iteration 882, loss = 2.85167911\n",
      "Iteration 883, loss = 2.85121161\n",
      "Iteration 884, loss = 2.85092105\n",
      "Iteration 885, loss = 2.85088655\n",
      "Iteration 886, loss = 2.85165281\n",
      "Iteration 887, loss = 2.85138529\n",
      "Iteration 888, loss = 2.85128862\n",
      "Iteration 889, loss = 2.84933402\n",
      "Iteration 890, loss = 2.84967052\n",
      "Iteration 891, loss = 2.85154928\n",
      "Iteration 892, loss = 2.84934409\n",
      "Iteration 893, loss = 2.84958416\n",
      "Iteration 894, loss = 2.85008959\n",
      "Iteration 895, loss = 2.84899462\n",
      "Iteration 896, loss = 2.84911693\n",
      "Iteration 897, loss = 2.84811768\n",
      "Iteration 898, loss = 2.84789603\n",
      "Iteration 899, loss = 2.84753276\n",
      "Iteration 900, loss = 2.84872654\n",
      "Iteration 901, loss = 2.84905716\n",
      "Iteration 902, loss = 2.84802807\n",
      "Iteration 903, loss = 2.84664704\n",
      "Iteration 904, loss = 2.84627566\n",
      "Iteration 905, loss = 2.84569035\n",
      "Iteration 906, loss = 2.84716759\n",
      "Iteration 907, loss = 2.84604292\n",
      "Iteration 908, loss = 2.84479260\n",
      "Iteration 909, loss = 2.84479729\n",
      "Iteration 910, loss = 2.84455766\n",
      "Iteration 911, loss = 2.84418917\n",
      "Iteration 912, loss = 2.84507214\n",
      "Iteration 913, loss = 2.84627869\n",
      "Iteration 914, loss = 2.84421462\n",
      "Iteration 915, loss = 2.84403289\n",
      "Iteration 916, loss = 2.84344493\n",
      "Iteration 917, loss = 2.84297588\n",
      "Iteration 918, loss = 2.84386651\n",
      "Iteration 919, loss = 2.84250961\n",
      "Iteration 920, loss = 2.84133096\n",
      "Iteration 921, loss = 2.84148344\n",
      "Iteration 922, loss = 2.84189940\n",
      "Iteration 923, loss = 2.84146324\n",
      "Iteration 924, loss = 2.84042082\n",
      "Iteration 925, loss = 2.84083194\n",
      "Iteration 926, loss = 2.84074491\n",
      "Iteration 927, loss = 2.84048962\n",
      "Iteration 928, loss = 2.84010251\n",
      "Iteration 929, loss = 2.84013819\n",
      "Iteration 930, loss = 2.83977399\n",
      "Iteration 931, loss = 2.83910272\n",
      "Iteration 932, loss = 2.83909428\n",
      "Iteration 933, loss = 2.83840610\n",
      "Iteration 934, loss = 2.83855626\n",
      "Iteration 935, loss = 2.83778965\n",
      "Iteration 936, loss = 2.83745880\n",
      "Iteration 937, loss = 2.83726853\n",
      "Iteration 938, loss = 2.83765217\n",
      "Iteration 939, loss = 2.83744025\n",
      "Iteration 940, loss = 2.83714495\n",
      "Iteration 941, loss = 2.83691962\n",
      "Iteration 942, loss = 2.83649065\n",
      "Iteration 943, loss = 2.83567307\n",
      "Iteration 944, loss = 2.83574048\n",
      "Iteration 945, loss = 2.83561997\n",
      "Iteration 946, loss = 2.83570876\n",
      "Iteration 947, loss = 2.83440417\n",
      "Iteration 948, loss = 2.83532817\n",
      "Iteration 949, loss = 2.83493611\n",
      "Iteration 950, loss = 2.83412393\n",
      "Iteration 951, loss = 2.83461854\n",
      "Iteration 952, loss = 2.83419541\n",
      "Iteration 953, loss = 2.83401972\n",
      "Iteration 954, loss = 2.83395393\n",
      "Iteration 955, loss = 2.83308911\n",
      "Iteration 956, loss = 2.83311707\n",
      "Iteration 957, loss = 2.83280955\n",
      "Iteration 958, loss = 2.83223449\n",
      "Iteration 959, loss = 2.83194674\n",
      "Iteration 960, loss = 2.83158124\n",
      "Iteration 961, loss = 2.83205776\n",
      "Iteration 962, loss = 2.83150747\n",
      "Iteration 963, loss = 2.83192827\n",
      "Iteration 964, loss = 2.83153040\n",
      "Iteration 965, loss = 2.83100500\n",
      "Iteration 966, loss = 2.83162654\n",
      "Iteration 967, loss = 2.83119976\n",
      "Iteration 968, loss = 2.83078828\n",
      "Iteration 969, loss = 2.83023326\n",
      "Iteration 970, loss = 2.83026498\n",
      "Iteration 971, loss = 2.83056012\n",
      "Iteration 972, loss = 2.82910888\n",
      "Iteration 973, loss = 2.82889642\n",
      "Iteration 974, loss = 2.82875034\n",
      "Iteration 975, loss = 2.82860198\n",
      "Iteration 976, loss = 2.82864382\n",
      "Iteration 977, loss = 2.82787053\n",
      "Iteration 978, loss = 2.82838096\n",
      "Iteration 979, loss = 2.82790282\n",
      "Iteration 980, loss = 2.82856579\n",
      "Iteration 981, loss = 2.82759792\n",
      "Iteration 982, loss = 2.82718423\n",
      "Iteration 983, loss = 2.82735479\n",
      "Iteration 984, loss = 2.82740917\n",
      "Iteration 985, loss = 2.82748790\n",
      "Iteration 986, loss = 2.82827169\n",
      "Iteration 987, loss = 2.82615641\n",
      "Iteration 988, loss = 2.82700314\n",
      "Iteration 989, loss = 2.82541020\n",
      "Iteration 990, loss = 2.82436538\n",
      "Iteration 991, loss = 2.82435797\n",
      "Iteration 992, loss = 2.82401778\n",
      "Iteration 993, loss = 2.82564884\n",
      "Iteration 994, loss = 2.82435208\n",
      "Iteration 995, loss = 2.82402619\n",
      "Iteration 996, loss = 2.82320958\n",
      "Iteration 997, loss = 2.82285857\n",
      "Iteration 998, loss = 2.82302336\n",
      "Iteration 999, loss = 2.82378448\n",
      "Iteration 1000, loss = 2.82335648\n",
      "Iteration 1001, loss = 2.82220744\n",
      "Iteration 1002, loss = 2.82189312\n",
      "Iteration 1003, loss = 2.82275480\n",
      "Iteration 1004, loss = 2.82193231\n",
      "Iteration 1005, loss = 2.82202173\n",
      "Iteration 1006, loss = 2.82173107\n",
      "Iteration 1007, loss = 2.82139430\n",
      "Iteration 1008, loss = 2.82106831\n",
      "Iteration 1009, loss = 2.82061413\n",
      "Iteration 1010, loss = 2.82068108\n",
      "Iteration 1011, loss = 2.82030642\n",
      "Iteration 1012, loss = 2.81960843\n",
      "Iteration 1013, loss = 2.81967868\n",
      "Iteration 1014, loss = 2.81914138\n",
      "Iteration 1015, loss = 2.81875659\n",
      "Iteration 1016, loss = 2.81852615\n",
      "Iteration 1017, loss = 2.81811561\n",
      "Iteration 1018, loss = 2.81845003\n",
      "Iteration 1019, loss = 2.81917793\n",
      "Iteration 1020, loss = 2.81883345\n",
      "Iteration 1021, loss = 2.81894342\n",
      "Iteration 1022, loss = 2.81895560\n",
      "Iteration 1023, loss = 2.81770407\n",
      "Iteration 1024, loss = 2.81775301\n",
      "Iteration 1025, loss = 2.81759603\n",
      "Iteration 1026, loss = 2.81699244\n",
      "Iteration 1027, loss = 2.81741769\n",
      "Iteration 1028, loss = 2.81692702\n",
      "Iteration 1029, loss = 2.81577342\n",
      "Iteration 1030, loss = 2.81778088\n",
      "Iteration 1031, loss = 2.81589497\n",
      "Iteration 1032, loss = 2.81430344\n",
      "Iteration 1033, loss = 2.81818984\n",
      "Iteration 1034, loss = 2.81644408\n",
      "Iteration 1035, loss = 2.81433299\n",
      "Iteration 1036, loss = 2.81571800\n",
      "Iteration 1037, loss = 2.81453620\n",
      "Iteration 1038, loss = 2.81406517\n",
      "Iteration 1039, loss = 2.81523301\n",
      "Iteration 1040, loss = 2.81402107\n",
      "Iteration 1041, loss = 2.81526257\n",
      "Iteration 1042, loss = 2.81365382\n",
      "Iteration 1043, loss = 2.81386768\n",
      "Iteration 1044, loss = 2.81416351\n",
      "Iteration 1045, loss = 2.81271104\n",
      "Iteration 1046, loss = 2.81338201\n",
      "Iteration 1047, loss = 2.81314938\n",
      "Iteration 1048, loss = 2.81228178\n",
      "Iteration 1049, loss = 2.81113355\n",
      "Iteration 1050, loss = 2.81114671\n",
      "Iteration 1051, loss = 2.81137256\n",
      "Iteration 1052, loss = 2.81052481\n",
      "Iteration 1053, loss = 2.81011545\n",
      "Iteration 1054, loss = 2.81059096\n",
      "Iteration 1055, loss = 2.81188118\n",
      "Iteration 1056, loss = 2.81033968\n",
      "Iteration 1057, loss = 2.81016145\n",
      "Iteration 1058, loss = 2.80988212\n",
      "Iteration 1059, loss = 2.80879678\n",
      "Iteration 1060, loss = 2.80871777\n",
      "Iteration 1061, loss = 2.80847164\n",
      "Iteration 1062, loss = 2.80868563\n",
      "Iteration 1063, loss = 2.80833365\n",
      "Iteration 1064, loss = 2.80835948\n",
      "Iteration 1065, loss = 2.80746698\n",
      "Iteration 1066, loss = 2.80863976\n",
      "Iteration 1067, loss = 2.80818523\n",
      "Iteration 1068, loss = 2.80767885\n",
      "Iteration 1069, loss = 2.80810492\n",
      "Iteration 1070, loss = 2.80764176\n",
      "Iteration 1071, loss = 2.80650526\n",
      "Iteration 1072, loss = 2.80715800\n",
      "Iteration 1073, loss = 2.80704872\n",
      "Iteration 1074, loss = 2.80642424\n",
      "Iteration 1075, loss = 2.80648819\n",
      "Iteration 1076, loss = 2.80611215\n",
      "Iteration 1077, loss = 2.80499090\n",
      "Iteration 1078, loss = 2.80631417\n",
      "Iteration 1079, loss = 2.80477201\n",
      "Iteration 1080, loss = 2.80551260\n",
      "Iteration 1081, loss = 2.80521074\n",
      "Iteration 1082, loss = 2.80557339\n",
      "Iteration 1083, loss = 2.80521299\n",
      "Iteration 1084, loss = 2.80446607\n",
      "Iteration 1085, loss = 2.80301049\n",
      "Iteration 1086, loss = 2.80280464\n",
      "Iteration 1087, loss = 2.80341960\n",
      "Iteration 1088, loss = 2.80287819\n",
      "Iteration 1089, loss = 2.80238154\n",
      "Iteration 1090, loss = 2.80258065\n",
      "Iteration 1091, loss = 2.80219780\n",
      "Iteration 1092, loss = 2.80260969\n",
      "Iteration 1093, loss = 2.80204275\n",
      "Iteration 1094, loss = 2.80299240\n",
      "Iteration 1095, loss = 2.80213075\n",
      "Iteration 1096, loss = 2.80152962\n",
      "Iteration 1097, loss = 2.80113984\n",
      "Iteration 1098, loss = 2.80144940\n",
      "Iteration 1099, loss = 2.80134575\n",
      "Iteration 1100, loss = 2.80118705\n",
      "Iteration 1101, loss = 2.80086549\n",
      "Iteration 1102, loss = 2.80021620\n",
      "Iteration 1103, loss = 2.80032867\n",
      "Iteration 1104, loss = 2.80059170\n",
      "Iteration 1105, loss = 2.79988742\n",
      "Iteration 1106, loss = 2.79923559\n",
      "Iteration 1107, loss = 2.79879773\n",
      "Iteration 1108, loss = 2.79852820\n",
      "Iteration 1109, loss = 2.79875680\n",
      "Iteration 1110, loss = 2.79915907\n",
      "Iteration 1111, loss = 2.79883825\n",
      "Iteration 1112, loss = 2.79870459\n",
      "Iteration 1113, loss = 2.79800382\n",
      "Iteration 1114, loss = 2.79758195\n",
      "Iteration 1115, loss = 2.79851474\n",
      "Iteration 1116, loss = 2.79854630\n",
      "Iteration 1117, loss = 2.79811908\n",
      "Iteration 1118, loss = 2.79711538\n",
      "Iteration 1119, loss = 2.79720587\n",
      "Iteration 1120, loss = 2.79711752\n",
      "Iteration 1121, loss = 2.79729890\n",
      "Iteration 1122, loss = 2.79647016\n",
      "Iteration 1123, loss = 2.79539919\n",
      "Iteration 1124, loss = 2.79562229\n",
      "Iteration 1125, loss = 2.79617492\n",
      "Iteration 1126, loss = 2.79700561\n",
      "Iteration 1127, loss = 2.79605610\n",
      "Iteration 1128, loss = 2.79551163\n",
      "Iteration 1129, loss = 2.79531350\n",
      "Iteration 1130, loss = 2.79594871\n",
      "Iteration 1131, loss = 2.79406680\n",
      "Iteration 1132, loss = 2.79403267\n",
      "Iteration 1133, loss = 2.79529358\n",
      "Iteration 1134, loss = 2.79497780\n",
      "Iteration 1135, loss = 2.79508930\n",
      "Iteration 1136, loss = 2.79501567\n",
      "Iteration 1137, loss = 2.79355681\n",
      "Iteration 1138, loss = 2.79319508\n",
      "Iteration 1139, loss = 2.79323595\n",
      "Iteration 1140, loss = 2.79238021\n",
      "Iteration 1141, loss = 2.79247829\n",
      "Iteration 1142, loss = 2.79162847\n",
      "Iteration 1143, loss = 2.79244341\n",
      "Iteration 1144, loss = 2.79206697\n",
      "Iteration 1145, loss = 2.79171417\n",
      "Iteration 1146, loss = 2.79181966\n",
      "Iteration 1147, loss = 2.79110848\n",
      "Iteration 1148, loss = 2.79109452\n",
      "Iteration 1149, loss = 2.79034520\n",
      "Iteration 1150, loss = 2.79020929\n",
      "Iteration 1151, loss = 2.79042437\n",
      "Iteration 1152, loss = 2.78969890\n",
      "Iteration 1153, loss = 2.78898274\n",
      "Iteration 1154, loss = 2.79019254\n",
      "Iteration 1155, loss = 2.79110348\n",
      "Iteration 1156, loss = 2.78986254\n",
      "Iteration 1157, loss = 2.78873145\n",
      "Iteration 1158, loss = 2.78871450\n",
      "Iteration 1159, loss = 2.78867793\n",
      "Iteration 1160, loss = 2.78750679\n",
      "Iteration 1161, loss = 2.78882261\n",
      "Iteration 1162, loss = 2.78806833\n",
      "Iteration 1163, loss = 2.78933794\n",
      "Iteration 1164, loss = 2.78737327\n",
      "Iteration 1165, loss = 2.78782229\n",
      "Iteration 1166, loss = 2.78738315\n",
      "Iteration 1167, loss = 2.78639858\n",
      "Iteration 1168, loss = 2.78687448\n",
      "Iteration 1169, loss = 2.78657975\n",
      "Iteration 1170, loss = 2.78696226\n",
      "Iteration 1171, loss = 2.78556104\n",
      "Iteration 1172, loss = 2.78555183\n",
      "Iteration 1173, loss = 2.78686233\n",
      "Iteration 1174, loss = 2.78668692\n",
      "Iteration 1175, loss = 2.78569608\n",
      "Iteration 1176, loss = 2.78652346\n",
      "Iteration 1177, loss = 2.78631598\n",
      "Iteration 1178, loss = 2.78536232\n",
      "Iteration 1179, loss = 2.78588809\n",
      "Iteration 1180, loss = 2.78586218\n",
      "Iteration 1181, loss = 2.78513345\n",
      "Iteration 1182, loss = 2.78499275\n",
      "Iteration 1183, loss = 2.78477689\n",
      "Iteration 1184, loss = 2.78354978\n",
      "Iteration 1185, loss = 2.78411688\n",
      "Iteration 1186, loss = 2.78316303\n",
      "Iteration 1187, loss = 2.78320034\n",
      "Iteration 1188, loss = 2.78291010\n",
      "Iteration 1189, loss = 2.78300229\n",
      "Iteration 1190, loss = 2.78248053\n",
      "Iteration 1191, loss = 2.78276743\n",
      "Iteration 1192, loss = 2.78277195\n",
      "Iteration 1193, loss = 2.78299640\n",
      "Iteration 1194, loss = 2.78271067\n",
      "Iteration 1195, loss = 2.78211235\n",
      "Iteration 1196, loss = 2.78052734\n",
      "Iteration 1197, loss = 2.78049700\n",
      "Iteration 1198, loss = 2.78098773\n",
      "Iteration 1199, loss = 2.78045545\n",
      "Iteration 1200, loss = 2.78053301\n",
      "Iteration 1201, loss = 2.78015917\n",
      "Iteration 1202, loss = 2.77965825\n",
      "Iteration 1203, loss = 2.78021732\n",
      "Iteration 1204, loss = 2.77997961\n",
      "Iteration 1205, loss = 2.78006887\n",
      "Iteration 1206, loss = 2.77920015\n",
      "Iteration 1207, loss = 2.77886244\n",
      "Iteration 1208, loss = 2.77884149\n",
      "Iteration 1209, loss = 2.77979253\n",
      "Iteration 1210, loss = 2.78011975\n",
      "Iteration 1211, loss = 2.77954557\n",
      "Iteration 1212, loss = 2.77895442\n",
      "Iteration 1213, loss = 2.77856251\n",
      "Iteration 1214, loss = 2.77785306\n",
      "Iteration 1215, loss = 2.77799982\n",
      "Iteration 1216, loss = 2.77824485\n",
      "Iteration 1217, loss = 2.77775210\n",
      "Iteration 1218, loss = 2.77742803\n",
      "Iteration 1219, loss = 2.77785165\n",
      "Iteration 1220, loss = 2.77741830\n",
      "Iteration 1221, loss = 2.77664036\n",
      "Iteration 1222, loss = 2.77634941\n",
      "Iteration 1223, loss = 2.77595408\n",
      "Iteration 1224, loss = 2.77627766\n",
      "Iteration 1225, loss = 2.77653962\n",
      "Iteration 1226, loss = 2.77601216\n",
      "Iteration 1227, loss = 2.77499133\n",
      "Iteration 1228, loss = 2.77635094\n",
      "Iteration 1229, loss = 2.77462157\n",
      "Iteration 1230, loss = 2.77551890\n",
      "Iteration 1231, loss = 2.77522105\n",
      "Iteration 1232, loss = 2.77418714\n",
      "Iteration 1233, loss = 2.77381993\n",
      "Iteration 1234, loss = 2.77390466\n",
      "Iteration 1235, loss = 2.77435397\n",
      "Iteration 1236, loss = 2.77359431\n",
      "Iteration 1237, loss = 2.77405963\n",
      "Iteration 1238, loss = 2.77309982\n",
      "Iteration 1239, loss = 2.77323231\n",
      "Iteration 1240, loss = 2.77298056\n",
      "Iteration 1241, loss = 2.77242814\n",
      "Iteration 1242, loss = 2.77230545\n",
      "Iteration 1243, loss = 2.77461825\n",
      "Iteration 1244, loss = 2.77183928\n",
      "Iteration 1245, loss = 2.77228174\n",
      "Iteration 1246, loss = 2.77262622\n",
      "Iteration 1247, loss = 2.77142603\n",
      "Iteration 1248, loss = 2.77152985\n",
      "Iteration 1249, loss = 2.77121224\n",
      "Iteration 1250, loss = 2.77088379\n",
      "Iteration 1251, loss = 2.77030127\n",
      "Iteration 1252, loss = 2.77030793\n",
      "Iteration 1253, loss = 2.77041488\n",
      "Iteration 1254, loss = 2.77105419\n",
      "Iteration 1255, loss = 2.76959352\n",
      "Iteration 1256, loss = 2.77073470\n",
      "Iteration 1257, loss = 2.76982878\n",
      "Iteration 1258, loss = 2.76914962\n",
      "Iteration 1259, loss = 2.76960844\n",
      "Iteration 1260, loss = 2.76887611\n",
      "Iteration 1261, loss = 2.76854987\n",
      "Iteration 1262, loss = 2.76906409\n",
      "Iteration 1263, loss = 2.76915017\n",
      "Iteration 1264, loss = 2.76828808\n",
      "Iteration 1265, loss = 2.76993008\n",
      "Iteration 1266, loss = 2.76877470\n",
      "Iteration 1267, loss = 2.76779407\n",
      "Iteration 1268, loss = 2.76914635\n",
      "Iteration 1269, loss = 2.76812693\n",
      "Iteration 1270, loss = 2.76761500\n",
      "Iteration 1271, loss = 2.76710513\n",
      "Iteration 1272, loss = 2.76678124\n",
      "Iteration 1273, loss = 2.76707404\n",
      "Iteration 1274, loss = 2.76852916\n",
      "Iteration 1275, loss = 2.76702217\n",
      "Iteration 1276, loss = 2.76748623\n",
      "Iteration 1277, loss = 2.76650096\n",
      "Iteration 1278, loss = 2.76689017\n",
      "Iteration 1279, loss = 2.76619652\n",
      "Iteration 1280, loss = 2.76556891\n",
      "Iteration 1281, loss = 2.76580384\n",
      "Iteration 1282, loss = 2.76594709\n",
      "Iteration 1283, loss = 2.76507445\n",
      "Iteration 1284, loss = 2.76579004\n",
      "Iteration 1285, loss = 2.76513859\n",
      "Iteration 1286, loss = 2.76423126\n",
      "Iteration 1287, loss = 2.76364185\n",
      "Iteration 1288, loss = 2.76511074\n",
      "Iteration 1289, loss = 2.76481451\n",
      "Iteration 1290, loss = 2.76580045\n",
      "Iteration 1291, loss = 2.76390525\n",
      "Iteration 1292, loss = 2.76460411\n",
      "Iteration 1293, loss = 2.76635350\n",
      "Iteration 1294, loss = 2.76447953\n",
      "Iteration 1295, loss = 2.76397837\n",
      "Iteration 1296, loss = 2.76339803\n",
      "Iteration 1297, loss = 2.76240478\n",
      "Iteration 1298, loss = 2.76253909\n",
      "Iteration 1299, loss = 2.76200815\n",
      "Iteration 1300, loss = 2.76227571\n",
      "Iteration 1301, loss = 2.76230605\n",
      "Iteration 1302, loss = 2.76146592\n",
      "Iteration 1303, loss = 2.76106891\n",
      "Iteration 1304, loss = 2.76096816\n",
      "Iteration 1305, loss = 2.76124397\n",
      "Iteration 1306, loss = 2.76149208\n",
      "Iteration 1307, loss = 2.76245728\n",
      "Iteration 1308, loss = 2.76075104\n",
      "Iteration 1309, loss = 2.76102229\n",
      "Iteration 1310, loss = 2.76090054\n",
      "Iteration 1311, loss = 2.76127775\n",
      "Iteration 1312, loss = 2.75983887\n",
      "Iteration 1313, loss = 2.75982604\n",
      "Iteration 1314, loss = 2.76102564\n",
      "Iteration 1315, loss = 2.76042048\n",
      "Iteration 1316, loss = 2.75940448\n",
      "Iteration 1317, loss = 2.76002214\n",
      "Iteration 1318, loss = 2.76031503\n",
      "Iteration 1319, loss = 2.75872367\n",
      "Iteration 1320, loss = 2.75925545\n",
      "Iteration 1321, loss = 2.75888977\n",
      "Iteration 1322, loss = 2.75929054\n",
      "Iteration 1323, loss = 2.75828233\n",
      "Iteration 1324, loss = 2.75768832\n",
      "Iteration 1325, loss = 2.75753726\n",
      "Iteration 1326, loss = 2.75876281\n",
      "Iteration 1327, loss = 2.75819361\n",
      "Iteration 1328, loss = 2.75890719\n",
      "Iteration 1329, loss = 2.75730153\n",
      "Iteration 1330, loss = 2.75727674\n",
      "Iteration 1331, loss = 2.75645521\n",
      "Iteration 1332, loss = 2.75686203\n",
      "Iteration 1333, loss = 2.75657019\n",
      "Iteration 1334, loss = 2.75606086\n",
      "Iteration 1335, loss = 2.75626966\n",
      "Iteration 1336, loss = 2.75650005\n",
      "Iteration 1337, loss = 2.75747185\n",
      "Iteration 1338, loss = 2.75707653\n",
      "Iteration 1339, loss = 2.75660321\n",
      "Iteration 1340, loss = 2.75480093\n",
      "Iteration 1341, loss = 2.75470924\n",
      "Iteration 1342, loss = 2.75503975\n",
      "Iteration 1343, loss = 2.75493956\n",
      "Iteration 1344, loss = 2.75450452\n",
      "Iteration 1345, loss = 2.75476098\n",
      "Iteration 1346, loss = 2.75657483\n",
      "Iteration 1347, loss = 2.75512913\n",
      "Iteration 1348, loss = 2.75347363\n",
      "Iteration 1349, loss = 2.75406733\n",
      "Iteration 1350, loss = 2.75386054\n",
      "Iteration 1351, loss = 2.75368031\n",
      "Iteration 1352, loss = 2.75318064\n",
      "Iteration 1353, loss = 2.75289719\n",
      "Iteration 1354, loss = 2.75310156\n",
      "Iteration 1355, loss = 2.75436186\n",
      "Iteration 1356, loss = 2.75248695\n",
      "Iteration 1357, loss = 2.75641844\n",
      "Iteration 1358, loss = 2.75425723\n",
      "Iteration 1359, loss = 2.75232937\n",
      "Iteration 1360, loss = 2.75233779\n",
      "Iteration 1361, loss = 2.75265975\n",
      "Iteration 1362, loss = 2.75182168\n",
      "Iteration 1363, loss = 2.75201984\n",
      "Iteration 1364, loss = 2.75225534\n",
      "Iteration 1365, loss = 2.75149537\n",
      "Iteration 1366, loss = 2.75090609\n",
      "Iteration 1367, loss = 2.75151823\n",
      "Iteration 1368, loss = 2.75214769\n",
      "Iteration 1369, loss = 2.75252342\n",
      "Iteration 1370, loss = 2.75101912\n",
      "Iteration 1371, loss = 2.75036091\n",
      "Iteration 1372, loss = 2.75036944\n",
      "Iteration 1373, loss = 2.75008158\n",
      "Iteration 1374, loss = 2.74933814\n",
      "Iteration 1375, loss = 2.74915547\n",
      "Iteration 1376, loss = 2.74860861\n",
      "Iteration 1377, loss = 2.74871018\n",
      "Iteration 1378, loss = 2.74922894\n",
      "Iteration 1379, loss = 2.74832884\n",
      "Iteration 1380, loss = 2.74947701\n",
      "Iteration 1381, loss = 2.75023236\n",
      "Iteration 1382, loss = 2.74866413\n",
      "Iteration 1383, loss = 2.74845305\n",
      "Iteration 1384, loss = 2.74758042\n",
      "Iteration 1385, loss = 2.74697421\n",
      "Iteration 1386, loss = 2.74717283\n",
      "Iteration 1387, loss = 2.74672735\n",
      "Iteration 1388, loss = 2.74955645\n",
      "Iteration 1389, loss = 2.74805575\n",
      "Iteration 1390, loss = 2.74753967\n",
      "Iteration 1391, loss = 2.74721365\n",
      "Iteration 1392, loss = 2.74609454\n",
      "Iteration 1393, loss = 2.74591948\n",
      "Iteration 1394, loss = 2.74779610\n",
      "Iteration 1395, loss = 2.74742765\n",
      "Iteration 1396, loss = 2.74673508\n",
      "Iteration 1397, loss = 2.74503310\n",
      "Iteration 1398, loss = 2.74868350\n",
      "Iteration 1399, loss = 2.74921806\n",
      "Iteration 1400, loss = 2.74646199\n",
      "Iteration 1401, loss = 2.74671177\n",
      "Iteration 1402, loss = 2.74529892\n",
      "Iteration 1403, loss = 2.74490867\n",
      "Iteration 1404, loss = 2.74823003\n",
      "Iteration 1405, loss = 2.74491194\n",
      "Iteration 1406, loss = 2.74482733\n",
      "Iteration 1407, loss = 2.74608872\n",
      "Iteration 1408, loss = 2.74403479\n",
      "Iteration 1409, loss = 2.74525982\n",
      "Iteration 1410, loss = 2.74478135\n",
      "Iteration 1411, loss = 2.74318596\n",
      "Iteration 1412, loss = 2.74377145\n",
      "Iteration 1413, loss = 2.74287345\n",
      "Iteration 1414, loss = 2.74369932\n",
      "Iteration 1415, loss = 2.74413142\n",
      "Iteration 1416, loss = 2.74313687\n",
      "Iteration 1417, loss = 2.74288658\n",
      "Iteration 1418, loss = 2.74358307\n",
      "Iteration 1419, loss = 2.74352263\n",
      "Iteration 1420, loss = 2.74141232\n",
      "Iteration 1421, loss = 2.74242819\n",
      "Iteration 1422, loss = 2.74314854\n",
      "Iteration 1423, loss = 2.74205821\n",
      "Iteration 1424, loss = 2.74114913\n",
      "Iteration 1425, loss = 2.74205411\n",
      "Iteration 1426, loss = 2.74244371\n",
      "Iteration 1427, loss = 2.74224594\n",
      "Iteration 1428, loss = 2.74172150\n",
      "Iteration 1429, loss = 2.74217751\n",
      "Iteration 1430, loss = 2.74184473\n",
      "Iteration 1431, loss = 2.74115174\n",
      "Iteration 1432, loss = 2.73973972\n",
      "Iteration 1433, loss = 2.74023372\n",
      "Iteration 1434, loss = 2.73953535\n",
      "Iteration 1435, loss = 2.74144710\n",
      "Iteration 1436, loss = 2.74196398\n",
      "Iteration 1437, loss = 2.74084605\n",
      "Iteration 1438, loss = 2.74032159\n",
      "Iteration 1439, loss = 2.74099938\n",
      "Iteration 1440, loss = 2.73915753\n",
      "Iteration 1441, loss = 2.74061484\n",
      "Iteration 1442, loss = 2.73980823\n",
      "Iteration 1443, loss = 2.73947596\n",
      "Iteration 1444, loss = 2.73823271\n",
      "Iteration 1445, loss = 2.73849454\n",
      "Iteration 1446, loss = 2.73817458\n",
      "Iteration 1447, loss = 2.73739934\n",
      "Iteration 1448, loss = 2.73770685\n",
      "Iteration 1449, loss = 2.73795107\n",
      "Iteration 1450, loss = 2.73770221\n",
      "Iteration 1451, loss = 2.73688691\n",
      "Iteration 1452, loss = 2.73786991\n",
      "Iteration 1453, loss = 2.73638571\n",
      "Iteration 1454, loss = 2.73628258\n",
      "Iteration 1455, loss = 2.73706753\n",
      "Iteration 1456, loss = 2.73638382\n",
      "Iteration 1457, loss = 2.73560671\n",
      "Iteration 1458, loss = 2.73722736\n",
      "Iteration 1459, loss = 2.73974890\n",
      "Iteration 1460, loss = 2.73733582\n",
      "Iteration 1461, loss = 2.73689793\n",
      "Iteration 1462, loss = 2.73642824\n",
      "Iteration 1463, loss = 2.73473417\n",
      "Iteration 1464, loss = 2.73501406\n",
      "Iteration 1465, loss = 2.73613958\n",
      "Iteration 1466, loss = 2.73609735\n",
      "Iteration 1467, loss = 2.73535171\n",
      "Iteration 1468, loss = 2.73537015\n",
      "Iteration 1469, loss = 2.73427440\n",
      "Iteration 1470, loss = 2.73504602\n",
      "Iteration 1471, loss = 2.73482030\n",
      "Iteration 1472, loss = 2.73405492\n",
      "Iteration 1473, loss = 2.73363522\n",
      "Iteration 1474, loss = 2.73322601\n",
      "Iteration 1475, loss = 2.73463085\n",
      "Iteration 1476, loss = 2.73413908\n",
      "Iteration 1477, loss = 2.73334144\n",
      "Iteration 1478, loss = 2.73291809\n",
      "Iteration 1479, loss = 2.73245635\n",
      "Iteration 1480, loss = 2.73220101\n",
      "Iteration 1481, loss = 2.73236636\n",
      "Iteration 1482, loss = 2.73355093\n",
      "Iteration 1483, loss = 2.73290423\n",
      "Iteration 1484, loss = 2.73193879\n",
      "Iteration 1485, loss = 2.73331348\n",
      "Iteration 1486, loss = 2.73137520\n",
      "Iteration 1487, loss = 2.73141787\n",
      "Iteration 1488, loss = 2.73136225\n",
      "Iteration 1489, loss = 2.73188601\n",
      "Iteration 1490, loss = 2.73166705\n",
      "Iteration 1491, loss = 2.73168897\n",
      "Iteration 1492, loss = 2.73102470\n",
      "Iteration 1493, loss = 2.73150999\n",
      "Iteration 1494, loss = 2.73111363\n",
      "Iteration 1495, loss = 2.73062828\n",
      "Iteration 1496, loss = 2.73139526\n",
      "Iteration 1497, loss = 2.73074982\n",
      "Iteration 1498, loss = 2.73045279\n",
      "Iteration 1499, loss = 2.73073934\n",
      "Iteration 1500, loss = 2.72958388\n",
      "Iteration 1501, loss = 2.72984771\n",
      "Iteration 1502, loss = 2.72981596\n",
      "Iteration 1503, loss = 2.72965750\n",
      "Iteration 1504, loss = 2.72916006\n",
      "Iteration 1505, loss = 2.73050504\n",
      "Iteration 1506, loss = 2.72916058\n",
      "Iteration 1507, loss = 2.72926936\n",
      "Iteration 1508, loss = 2.72866478\n",
      "Iteration 1509, loss = 2.72905983\n",
      "Iteration 1510, loss = 2.72931802\n",
      "Iteration 1511, loss = 2.72863194\n",
      "Iteration 1512, loss = 2.72671547\n",
      "Iteration 1513, loss = 2.72794406\n",
      "Iteration 1514, loss = 2.72767721\n",
      "Iteration 1515, loss = 2.72784124\n",
      "Iteration 1516, loss = 2.72892132\n",
      "Iteration 1517, loss = 2.72831643\n",
      "Iteration 1518, loss = 2.72713325\n",
      "Iteration 1519, loss = 2.72657613\n",
      "Iteration 1520, loss = 2.72579021\n",
      "Iteration 1521, loss = 2.72722996\n",
      "Iteration 1522, loss = 2.72754418\n",
      "Iteration 1523, loss = 2.72763544\n",
      "Iteration 1524, loss = 2.72717795\n",
      "Iteration 1525, loss = 2.72632993\n",
      "Iteration 1526, loss = 2.72590877\n",
      "Iteration 1527, loss = 2.72648139\n",
      "Iteration 1528, loss = 2.72720163\n",
      "Iteration 1529, loss = 2.72698116\n",
      "Iteration 1530, loss = 2.72641736\n",
      "Iteration 1531, loss = 2.72747943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.14674736\n",
      "Iteration 2, loss = 4.01667028\n",
      "Iteration 3, loss = 3.89781221\n",
      "Iteration 4, loss = 3.78381369\n",
      "Iteration 5, loss = 3.67446435\n",
      "Iteration 6, loss = 3.57037206\n",
      "Iteration 7, loss = 3.47715637\n",
      "Iteration 8, loss = 3.39543061\n",
      "Iteration 9, loss = 3.32840349\n",
      "Iteration 10, loss = 3.28076573\n",
      "Iteration 11, loss = 3.25151942\n",
      "Iteration 12, loss = 3.23554038\n",
      "Iteration 13, loss = 3.22997287\n",
      "Iteration 14, loss = 3.22714671\n",
      "Iteration 15, loss = 3.22578872\n",
      "Iteration 16, loss = 3.22396178\n",
      "Iteration 17, loss = 3.22244869\n",
      "Iteration 18, loss = 3.22069178\n",
      "Iteration 19, loss = 3.22025867\n",
      "Iteration 20, loss = 3.21965673\n",
      "Iteration 21, loss = 3.21948241\n",
      "Iteration 22, loss = 3.21866175\n",
      "Iteration 23, loss = 3.21829450\n",
      "Iteration 24, loss = 3.21734484\n",
      "Iteration 25, loss = 3.21733391\n",
      "Iteration 26, loss = 3.21619290\n",
      "Iteration 27, loss = 3.21628878\n",
      "Iteration 28, loss = 3.21568966\n",
      "Iteration 29, loss = 3.21623362\n",
      "Iteration 30, loss = 3.21545925\n",
      "Iteration 31, loss = 3.21492690\n",
      "Iteration 32, loss = 3.21489817\n",
      "Iteration 33, loss = 3.21425798\n",
      "Iteration 34, loss = 3.21379091\n",
      "Iteration 35, loss = 3.21351917\n",
      "Iteration 36, loss = 3.21268976\n",
      "Iteration 37, loss = 3.21196070\n",
      "Iteration 38, loss = 3.21132871\n",
      "Iteration 39, loss = 3.21089470\n",
      "Iteration 40, loss = 3.21057234\n",
      "Iteration 41, loss = 3.21041445\n",
      "Iteration 42, loss = 3.20996340\n",
      "Iteration 43, loss = 3.20932361\n",
      "Iteration 44, loss = 3.20913549\n",
      "Iteration 45, loss = 3.20874488\n",
      "Iteration 46, loss = 3.20836631\n",
      "Iteration 47, loss = 3.20795453\n",
      "Iteration 48, loss = 3.20777418\n",
      "Iteration 49, loss = 3.20713400\n",
      "Iteration 50, loss = 3.20705472\n",
      "Iteration 51, loss = 3.20703501\n",
      "Iteration 52, loss = 3.20644415\n",
      "Iteration 53, loss = 3.20561945\n",
      "Iteration 54, loss = 3.20539116\n",
      "Iteration 55, loss = 3.20560490\n",
      "Iteration 56, loss = 3.20494122\n",
      "Iteration 57, loss = 3.20462889\n",
      "Iteration 58, loss = 3.20406475\n",
      "Iteration 59, loss = 3.20386709\n",
      "Iteration 60, loss = 3.20343563\n",
      "Iteration 61, loss = 3.20320086\n",
      "Iteration 62, loss = 3.20223061\n",
      "Iteration 63, loss = 3.20181474\n",
      "Iteration 64, loss = 3.20136469\n",
      "Iteration 65, loss = 3.20139428\n",
      "Iteration 66, loss = 3.20041681\n",
      "Iteration 67, loss = 3.20041275\n",
      "Iteration 68, loss = 3.20071097\n",
      "Iteration 69, loss = 3.20036955\n",
      "Iteration 70, loss = 3.20049464\n",
      "Iteration 71, loss = 3.19963806\n",
      "Iteration 72, loss = 3.19874790\n",
      "Iteration 73, loss = 3.19825585\n",
      "Iteration 74, loss = 3.19758680\n",
      "Iteration 75, loss = 3.19640554\n",
      "Iteration 76, loss = 3.19596836\n",
      "Iteration 77, loss = 3.19543956\n",
      "Iteration 78, loss = 3.19563864\n",
      "Iteration 79, loss = 3.19491563\n",
      "Iteration 80, loss = 3.19440144\n",
      "Iteration 81, loss = 3.19417058\n",
      "Iteration 82, loss = 3.19396137\n",
      "Iteration 83, loss = 3.19369514\n",
      "Iteration 84, loss = 3.19318792\n",
      "Iteration 85, loss = 3.19284664\n",
      "Iteration 86, loss = 3.19308237\n",
      "Iteration 87, loss = 3.19223270\n",
      "Iteration 88, loss = 3.19194581\n",
      "Iteration 89, loss = 3.19199421\n",
      "Iteration 90, loss = 3.19209585\n",
      "Iteration 91, loss = 3.19119302\n",
      "Iteration 92, loss = 3.19041933\n",
      "Iteration 93, loss = 3.18947614\n",
      "Iteration 94, loss = 3.18893210\n",
      "Iteration 95, loss = 3.18818418\n",
      "Iteration 96, loss = 3.18813865\n",
      "Iteration 97, loss = 3.18751780\n",
      "Iteration 98, loss = 3.18721736\n",
      "Iteration 99, loss = 3.18758821\n",
      "Iteration 100, loss = 3.18721828\n",
      "Iteration 101, loss = 3.18639870\n",
      "Iteration 102, loss = 3.18545695\n",
      "Iteration 103, loss = 3.18522267\n",
      "Iteration 104, loss = 3.18513720\n",
      "Iteration 105, loss = 3.18453231\n",
      "Iteration 106, loss = 3.18431514\n",
      "Iteration 107, loss = 3.18329852\n",
      "Iteration 108, loss = 3.18322845\n",
      "Iteration 109, loss = 3.18246635\n",
      "Iteration 110, loss = 3.18179244\n",
      "Iteration 111, loss = 3.18149152\n",
      "Iteration 112, loss = 3.18100626\n",
      "Iteration 113, loss = 3.18085597\n",
      "Iteration 114, loss = 3.18053714\n",
      "Iteration 115, loss = 3.18068774\n",
      "Iteration 116, loss = 3.18009391\n",
      "Iteration 117, loss = 3.17998644\n",
      "Iteration 118, loss = 3.17924435\n",
      "Iteration 119, loss = 3.17847859\n",
      "Iteration 120, loss = 3.17817230\n",
      "Iteration 121, loss = 3.17787438\n",
      "Iteration 122, loss = 3.17832941\n",
      "Iteration 123, loss = 3.17728663\n",
      "Iteration 124, loss = 3.17696358\n",
      "Iteration 125, loss = 3.17563006\n",
      "Iteration 126, loss = 3.17563606\n",
      "Iteration 127, loss = 3.17507814\n",
      "Iteration 128, loss = 3.17401719\n",
      "Iteration 129, loss = 3.17356440\n",
      "Iteration 130, loss = 3.17335180\n",
      "Iteration 131, loss = 3.17313152\n",
      "Iteration 132, loss = 3.17292644\n",
      "Iteration 133, loss = 3.17259113\n",
      "Iteration 134, loss = 3.17302599\n",
      "Iteration 135, loss = 3.17173427\n",
      "Iteration 136, loss = 3.17118754\n",
      "Iteration 137, loss = 3.17069653\n",
      "Iteration 138, loss = 3.16998714\n",
      "Iteration 139, loss = 3.16973924\n",
      "Iteration 140, loss = 3.16883070\n",
      "Iteration 141, loss = 3.16802375\n",
      "Iteration 142, loss = 3.16780326\n",
      "Iteration 143, loss = 3.16743883\n",
      "Iteration 144, loss = 3.16689765\n",
      "Iteration 145, loss = 3.16600549\n",
      "Iteration 146, loss = 3.16598493\n",
      "Iteration 147, loss = 3.16568579\n",
      "Iteration 148, loss = 3.16517864\n",
      "Iteration 149, loss = 3.16401378\n",
      "Iteration 150, loss = 3.16363225\n",
      "Iteration 151, loss = 3.16338216\n",
      "Iteration 152, loss = 3.16278333\n",
      "Iteration 153, loss = 3.16237762\n",
      "Iteration 154, loss = 3.16284821\n",
      "Iteration 155, loss = 3.16189780\n",
      "Iteration 156, loss = 3.16154620\n",
      "Iteration 157, loss = 3.16101220\n",
      "Iteration 158, loss = 3.16044766\n",
      "Iteration 159, loss = 3.15997640\n",
      "Iteration 160, loss = 3.15960462\n",
      "Iteration 161, loss = 3.15888082\n",
      "Iteration 162, loss = 3.15849509\n",
      "Iteration 163, loss = 3.15799154\n",
      "Iteration 164, loss = 3.15728202\n",
      "Iteration 165, loss = 3.15663255\n",
      "Iteration 166, loss = 3.15615633\n",
      "Iteration 167, loss = 3.15604803\n",
      "Iteration 168, loss = 3.15521847\n",
      "Iteration 169, loss = 3.15484356\n",
      "Iteration 170, loss = 3.15452193\n",
      "Iteration 171, loss = 3.15376892\n",
      "Iteration 172, loss = 3.15315208\n",
      "Iteration 173, loss = 3.15297048\n",
      "Iteration 174, loss = 3.15213709\n",
      "Iteration 175, loss = 3.15131408\n",
      "Iteration 176, loss = 3.15167320\n",
      "Iteration 177, loss = 3.15091735\n",
      "Iteration 178, loss = 3.15048188\n",
      "Iteration 179, loss = 3.14963300\n",
      "Iteration 180, loss = 3.14897340\n",
      "Iteration 181, loss = 3.14906629\n",
      "Iteration 182, loss = 3.14828391\n",
      "Iteration 183, loss = 3.14757322\n",
      "Iteration 184, loss = 3.14795530\n",
      "Iteration 185, loss = 3.14801219\n",
      "Iteration 186, loss = 3.14641780\n",
      "Iteration 187, loss = 3.14598005\n",
      "Iteration 188, loss = 3.14760275\n",
      "Iteration 189, loss = 3.14513576\n",
      "Iteration 190, loss = 3.14496766\n",
      "Iteration 191, loss = 3.14486085\n",
      "Iteration 192, loss = 3.14381647\n",
      "Iteration 193, loss = 3.14345507\n",
      "Iteration 194, loss = 3.14235812\n",
      "Iteration 195, loss = 3.14145980\n",
      "Iteration 196, loss = 3.14129636\n",
      "Iteration 197, loss = 3.14053985\n",
      "Iteration 198, loss = 3.13985543\n",
      "Iteration 199, loss = 3.13955762\n",
      "Iteration 200, loss = 3.13864795\n",
      "Iteration 201, loss = 3.13998685\n",
      "Iteration 202, loss = 3.13947516\n",
      "Iteration 203, loss = 3.13856435\n",
      "Iteration 204, loss = 3.13817055\n",
      "Iteration 205, loss = 3.13728100\n",
      "Iteration 206, loss = 3.13582601\n",
      "Iteration 207, loss = 3.13629559\n",
      "Iteration 208, loss = 3.13519013\n",
      "Iteration 209, loss = 3.13449296\n",
      "Iteration 210, loss = 3.13382567\n",
      "Iteration 211, loss = 3.13303936\n",
      "Iteration 212, loss = 3.13207660\n",
      "Iteration 213, loss = 3.13241128\n",
      "Iteration 214, loss = 3.13186901\n",
      "Iteration 215, loss = 3.13160099\n",
      "Iteration 216, loss = 3.13047566\n",
      "Iteration 217, loss = 3.12992567\n",
      "Iteration 218, loss = 3.12936116\n",
      "Iteration 219, loss = 3.12876721\n",
      "Iteration 220, loss = 3.12863890\n",
      "Iteration 221, loss = 3.12787757\n",
      "Iteration 222, loss = 3.12762184\n",
      "Iteration 223, loss = 3.12670251\n",
      "Iteration 224, loss = 3.12661042\n",
      "Iteration 225, loss = 3.12597156\n",
      "Iteration 226, loss = 3.12556729\n",
      "Iteration 227, loss = 3.12546426\n",
      "Iteration 228, loss = 3.12471270\n",
      "Iteration 229, loss = 3.12420686\n",
      "Iteration 230, loss = 3.12329186\n",
      "Iteration 231, loss = 3.12309297\n",
      "Iteration 232, loss = 3.12252867\n",
      "Iteration 233, loss = 3.12174987\n",
      "Iteration 234, loss = 3.12165610\n",
      "Iteration 235, loss = 3.12018051\n",
      "Iteration 236, loss = 3.12002522\n",
      "Iteration 237, loss = 3.11969723\n",
      "Iteration 238, loss = 3.11975086\n",
      "Iteration 239, loss = 3.11918695\n",
      "Iteration 240, loss = 3.11915472\n",
      "Iteration 241, loss = 3.11769918\n",
      "Iteration 242, loss = 3.11648433\n",
      "Iteration 243, loss = 3.11682289\n",
      "Iteration 244, loss = 3.11774907\n",
      "Iteration 245, loss = 3.11678841\n",
      "Iteration 246, loss = 3.11500213\n",
      "Iteration 247, loss = 3.11449052\n",
      "Iteration 248, loss = 3.11397785\n",
      "Iteration 249, loss = 3.11308799\n",
      "Iteration 250, loss = 3.11271298\n",
      "Iteration 251, loss = 3.11182083\n",
      "Iteration 252, loss = 3.11200795\n",
      "Iteration 253, loss = 3.11144449\n",
      "Iteration 254, loss = 3.11044541\n",
      "Iteration 255, loss = 3.10934331\n",
      "Iteration 256, loss = 3.10920328\n",
      "Iteration 257, loss = 3.10864338\n",
      "Iteration 258, loss = 3.10726219\n",
      "Iteration 259, loss = 3.10706330\n",
      "Iteration 260, loss = 3.10727212\n",
      "Iteration 261, loss = 3.10632473\n",
      "Iteration 262, loss = 3.10595279\n",
      "Iteration 263, loss = 3.10535927\n",
      "Iteration 264, loss = 3.10412738\n",
      "Iteration 265, loss = 3.10316851\n",
      "Iteration 266, loss = 3.10330310\n",
      "Iteration 267, loss = 3.10236880\n",
      "Iteration 268, loss = 3.10219131\n",
      "Iteration 269, loss = 3.10147667\n",
      "Iteration 270, loss = 3.10198092\n",
      "Iteration 271, loss = 3.10126075\n",
      "Iteration 272, loss = 3.09982757\n",
      "Iteration 273, loss = 3.09911223\n",
      "Iteration 274, loss = 3.09907744\n",
      "Iteration 275, loss = 3.09809639\n",
      "Iteration 276, loss = 3.09881536\n",
      "Iteration 277, loss = 3.09730224\n",
      "Iteration 278, loss = 3.09628093\n",
      "Iteration 279, loss = 3.09638250\n",
      "Iteration 280, loss = 3.09641262\n",
      "Iteration 281, loss = 3.09541309\n",
      "Iteration 282, loss = 3.09415601\n",
      "Iteration 283, loss = 3.09337564\n",
      "Iteration 284, loss = 3.09269463\n",
      "Iteration 285, loss = 3.09252560\n",
      "Iteration 286, loss = 3.09205273\n",
      "Iteration 287, loss = 3.09154387\n",
      "Iteration 288, loss = 3.09073389\n",
      "Iteration 289, loss = 3.09064971\n",
      "Iteration 290, loss = 3.08974519\n",
      "Iteration 291, loss = 3.08905296\n",
      "Iteration 292, loss = 3.08893068\n",
      "Iteration 293, loss = 3.08833522\n",
      "Iteration 294, loss = 3.08786177\n",
      "Iteration 295, loss = 3.08656244\n",
      "Iteration 296, loss = 3.08661249\n",
      "Iteration 297, loss = 3.08737286\n",
      "Iteration 298, loss = 3.08608895\n",
      "Iteration 299, loss = 3.08636476\n",
      "Iteration 300, loss = 3.08575348\n",
      "Iteration 301, loss = 3.08388946\n",
      "Iteration 302, loss = 3.08532177\n",
      "Iteration 303, loss = 3.08293541\n",
      "Iteration 304, loss = 3.08310650\n",
      "Iteration 305, loss = 3.08462660\n",
      "Iteration 306, loss = 3.08205200\n",
      "Iteration 307, loss = 3.08051178\n",
      "Iteration 308, loss = 3.08131856\n",
      "Iteration 309, loss = 3.08039900\n",
      "Iteration 310, loss = 3.07827377\n",
      "Iteration 311, loss = 3.07841269\n",
      "Iteration 312, loss = 3.07803590\n",
      "Iteration 313, loss = 3.07754738\n",
      "Iteration 314, loss = 3.07694752\n",
      "Iteration 315, loss = 3.07621855\n",
      "Iteration 316, loss = 3.07562615\n",
      "Iteration 317, loss = 3.07486488\n",
      "Iteration 318, loss = 3.07463016\n",
      "Iteration 319, loss = 3.07359482\n",
      "Iteration 320, loss = 3.07324279\n",
      "Iteration 321, loss = 3.07335025\n",
      "Iteration 322, loss = 3.07242804\n",
      "Iteration 323, loss = 3.07255302\n",
      "Iteration 324, loss = 3.07209421\n",
      "Iteration 325, loss = 3.07140839\n",
      "Iteration 326, loss = 3.07071758\n",
      "Iteration 327, loss = 3.07078527\n",
      "Iteration 328, loss = 3.06961536\n",
      "Iteration 329, loss = 3.06916825\n",
      "Iteration 330, loss = 3.06921962\n",
      "Iteration 331, loss = 3.06880554\n",
      "Iteration 332, loss = 3.06841607\n",
      "Iteration 333, loss = 3.06678096\n",
      "Iteration 334, loss = 3.06717763\n",
      "Iteration 335, loss = 3.06586120\n",
      "Iteration 336, loss = 3.06532976\n",
      "Iteration 337, loss = 3.06503093\n",
      "Iteration 338, loss = 3.06428784\n",
      "Iteration 339, loss = 3.06375147\n",
      "Iteration 340, loss = 3.06298223\n",
      "Iteration 341, loss = 3.06287390\n",
      "Iteration 342, loss = 3.06278957\n",
      "Iteration 343, loss = 3.06209542\n",
      "Iteration 344, loss = 3.06110318\n",
      "Iteration 345, loss = 3.06048309\n",
      "Iteration 346, loss = 3.06030743\n",
      "Iteration 347, loss = 3.06023148\n",
      "Iteration 348, loss = 3.05970950\n",
      "Iteration 349, loss = 3.05929188\n",
      "Iteration 350, loss = 3.05885087\n",
      "Iteration 351, loss = 3.05793786\n",
      "Iteration 352, loss = 3.05642826\n",
      "Iteration 353, loss = 3.05625991\n",
      "Iteration 354, loss = 3.05633217\n",
      "Iteration 355, loss = 3.05568015\n",
      "Iteration 356, loss = 3.05483887\n",
      "Iteration 357, loss = 3.05377920\n",
      "Iteration 358, loss = 3.05320381\n",
      "Iteration 359, loss = 3.05270419\n",
      "Iteration 360, loss = 3.05257587\n",
      "Iteration 361, loss = 3.05201664\n",
      "Iteration 362, loss = 3.05160817\n",
      "Iteration 363, loss = 3.05091642\n",
      "Iteration 364, loss = 3.05044830\n",
      "Iteration 365, loss = 3.04938619\n",
      "Iteration 366, loss = 3.04952255\n",
      "Iteration 367, loss = 3.04932263\n",
      "Iteration 368, loss = 3.04840741\n",
      "Iteration 369, loss = 3.04802715\n",
      "Iteration 370, loss = 3.04757315\n",
      "Iteration 371, loss = 3.04745818\n",
      "Iteration 372, loss = 3.04698555\n",
      "Iteration 373, loss = 3.04674729\n",
      "Iteration 374, loss = 3.04565995\n",
      "Iteration 375, loss = 3.04576453\n",
      "Iteration 376, loss = 3.04490849\n",
      "Iteration 377, loss = 3.04402837\n",
      "Iteration 378, loss = 3.04347582\n",
      "Iteration 379, loss = 3.04308726\n",
      "Iteration 380, loss = 3.04314932\n",
      "Iteration 381, loss = 3.04268496\n",
      "Iteration 382, loss = 3.04194233\n",
      "Iteration 383, loss = 3.04176500\n",
      "Iteration 384, loss = 3.04113628\n",
      "Iteration 385, loss = 3.04022525\n",
      "Iteration 386, loss = 3.03961328\n",
      "Iteration 387, loss = 3.03923948\n",
      "Iteration 388, loss = 3.04025731\n",
      "Iteration 389, loss = 3.04085560\n",
      "Iteration 390, loss = 3.03821728\n",
      "Iteration 391, loss = 3.03787587\n",
      "Iteration 392, loss = 3.03770732\n",
      "Iteration 393, loss = 3.03661308\n",
      "Iteration 394, loss = 3.03569438\n",
      "Iteration 395, loss = 3.03551058\n",
      "Iteration 396, loss = 3.03444179\n",
      "Iteration 397, loss = 3.03554430\n",
      "Iteration 398, loss = 3.03564929\n",
      "Iteration 399, loss = 3.03412356\n",
      "Iteration 400, loss = 3.03290334\n",
      "Iteration 401, loss = 3.03264408\n",
      "Iteration 402, loss = 3.03224543\n",
      "Iteration 403, loss = 3.03193463\n",
      "Iteration 404, loss = 3.03083076\n",
      "Iteration 405, loss = 3.03030129\n",
      "Iteration 406, loss = 3.02983602\n",
      "Iteration 407, loss = 3.02930988\n",
      "Iteration 408, loss = 3.02890170\n",
      "Iteration 409, loss = 3.02834756\n",
      "Iteration 410, loss = 3.02861356\n",
      "Iteration 411, loss = 3.02761951\n",
      "Iteration 412, loss = 3.02648960\n",
      "Iteration 413, loss = 3.02623516\n",
      "Iteration 414, loss = 3.02578336\n",
      "Iteration 415, loss = 3.02595087\n",
      "Iteration 416, loss = 3.02604127\n",
      "Iteration 417, loss = 3.02512951\n",
      "Iteration 418, loss = 3.02587413\n",
      "Iteration 419, loss = 3.02556247\n",
      "Iteration 420, loss = 3.02445173\n",
      "Iteration 421, loss = 3.02280025\n",
      "Iteration 422, loss = 3.02211922\n",
      "Iteration 423, loss = 3.02162461\n",
      "Iteration 424, loss = 3.02144559\n",
      "Iteration 425, loss = 3.02023567\n",
      "Iteration 426, loss = 3.01973084\n",
      "Iteration 427, loss = 3.02003923\n",
      "Iteration 428, loss = 3.01928007\n",
      "Iteration 429, loss = 3.01930266\n",
      "Iteration 430, loss = 3.01865443\n",
      "Iteration 431, loss = 3.01795424\n",
      "Iteration 432, loss = 3.01842273\n",
      "Iteration 433, loss = 3.01778714\n",
      "Iteration 434, loss = 3.01642207\n",
      "Iteration 435, loss = 3.01575130\n",
      "Iteration 436, loss = 3.01616295\n",
      "Iteration 437, loss = 3.01598046\n",
      "Iteration 438, loss = 3.01550884\n",
      "Iteration 439, loss = 3.01435008\n",
      "Iteration 440, loss = 3.01355644\n",
      "Iteration 441, loss = 3.01364546\n",
      "Iteration 442, loss = 3.01386146\n",
      "Iteration 443, loss = 3.01342101\n",
      "Iteration 444, loss = 3.01245287\n",
      "Iteration 445, loss = 3.01222291\n",
      "Iteration 446, loss = 3.01149765\n",
      "Iteration 447, loss = 3.01062466\n",
      "Iteration 448, loss = 3.01029325\n",
      "Iteration 449, loss = 3.00980759\n",
      "Iteration 450, loss = 3.00895484\n",
      "Iteration 451, loss = 3.00813119\n",
      "Iteration 452, loss = 3.00814163\n",
      "Iteration 453, loss = 3.00820181\n",
      "Iteration 454, loss = 3.00783824\n",
      "Iteration 455, loss = 3.00749588\n",
      "Iteration 456, loss = 3.00638525\n",
      "Iteration 457, loss = 3.00645991\n",
      "Iteration 458, loss = 3.00586205\n",
      "Iteration 459, loss = 3.00535548\n",
      "Iteration 460, loss = 3.00470865\n",
      "Iteration 461, loss = 3.00448998\n",
      "Iteration 462, loss = 3.00330351\n",
      "Iteration 463, loss = 3.00386535\n",
      "Iteration 464, loss = 3.00247766\n",
      "Iteration 465, loss = 3.00315675\n",
      "Iteration 466, loss = 3.00256267\n",
      "Iteration 467, loss = 3.00170413\n",
      "Iteration 468, loss = 3.00121552\n",
      "Iteration 469, loss = 3.00010453\n",
      "Iteration 470, loss = 3.00240599\n",
      "Iteration 471, loss = 3.00159649\n",
      "Iteration 472, loss = 3.00040137\n",
      "Iteration 473, loss = 2.99964321\n",
      "Iteration 474, loss = 2.99811821\n",
      "Iteration 475, loss = 2.99807471\n",
      "Iteration 476, loss = 2.99819603\n",
      "Iteration 477, loss = 2.99739991\n",
      "Iteration 478, loss = 2.99670670\n",
      "Iteration 479, loss = 2.99639750\n",
      "Iteration 480, loss = 2.99596551\n",
      "Iteration 481, loss = 2.99566291\n",
      "Iteration 482, loss = 2.99476403\n",
      "Iteration 483, loss = 2.99442963\n",
      "Iteration 484, loss = 2.99459458\n",
      "Iteration 485, loss = 2.99416940\n",
      "Iteration 486, loss = 2.99344630\n",
      "Iteration 487, loss = 2.99342093\n",
      "Iteration 488, loss = 2.99351317\n",
      "Iteration 489, loss = 2.99212855\n",
      "Iteration 490, loss = 2.99136530\n",
      "Iteration 491, loss = 2.99069619\n",
      "Iteration 492, loss = 2.99096847\n",
      "Iteration 493, loss = 2.99016416\n",
      "Iteration 494, loss = 2.99048908\n",
      "Iteration 495, loss = 2.99020677\n",
      "Iteration 496, loss = 2.98970665\n",
      "Iteration 497, loss = 2.98842487\n",
      "Iteration 498, loss = 2.98777475\n",
      "Iteration 499, loss = 2.98868288\n",
      "Iteration 500, loss = 2.98827558\n",
      "Iteration 501, loss = 2.98808662\n",
      "Iteration 502, loss = 2.98665742\n",
      "Iteration 503, loss = 2.98587565\n",
      "Iteration 504, loss = 2.98602114\n",
      "Iteration 505, loss = 2.98513134\n",
      "Iteration 506, loss = 2.98449134\n",
      "Iteration 507, loss = 2.98426817\n",
      "Iteration 508, loss = 2.98415006\n",
      "Iteration 509, loss = 2.98424620\n",
      "Iteration 510, loss = 2.98254938\n",
      "Iteration 511, loss = 2.98277128\n",
      "Iteration 512, loss = 2.98277414\n",
      "Iteration 513, loss = 2.98202018\n",
      "Iteration 514, loss = 2.98159131\n",
      "Iteration 515, loss = 2.98106672\n",
      "Iteration 516, loss = 2.98057732\n",
      "Iteration 517, loss = 2.98047922\n",
      "Iteration 518, loss = 2.97989871\n",
      "Iteration 519, loss = 2.97943007\n",
      "Iteration 520, loss = 2.97892711\n",
      "Iteration 521, loss = 2.98045740\n",
      "Iteration 522, loss = 2.97904533\n",
      "Iteration 523, loss = 2.97814829\n",
      "Iteration 524, loss = 2.97701341\n",
      "Iteration 525, loss = 2.97717021\n",
      "Iteration 526, loss = 2.97670603\n",
      "Iteration 527, loss = 2.97672173\n",
      "Iteration 528, loss = 2.97577081\n",
      "Iteration 529, loss = 2.97485981\n",
      "Iteration 530, loss = 2.97503148\n",
      "Iteration 531, loss = 2.97425749\n",
      "Iteration 532, loss = 2.97315934\n",
      "Iteration 533, loss = 2.97332202\n",
      "Iteration 534, loss = 2.97283850\n",
      "Iteration 535, loss = 2.97221121\n",
      "Iteration 536, loss = 2.97315121\n",
      "Iteration 537, loss = 2.97205841\n",
      "Iteration 538, loss = 2.97290766\n",
      "Iteration 539, loss = 2.97161319\n",
      "Iteration 540, loss = 2.97069591\n",
      "Iteration 541, loss = 2.97032158\n",
      "Iteration 542, loss = 2.97007206\n",
      "Iteration 543, loss = 2.96898625\n",
      "Iteration 544, loss = 2.97030161\n",
      "Iteration 545, loss = 2.96951403\n",
      "Iteration 546, loss = 2.96819904\n",
      "Iteration 547, loss = 2.96776052\n",
      "Iteration 548, loss = 2.96713454\n",
      "Iteration 549, loss = 2.96770139\n",
      "Iteration 550, loss = 2.96665072\n",
      "Iteration 551, loss = 2.96623272\n",
      "Iteration 552, loss = 2.96555048\n",
      "Iteration 553, loss = 2.96487311\n",
      "Iteration 554, loss = 2.96496026\n",
      "Iteration 555, loss = 2.96533198\n",
      "Iteration 556, loss = 2.96442582\n",
      "Iteration 557, loss = 2.96414305\n",
      "Iteration 558, loss = 2.96437069\n",
      "Iteration 559, loss = 2.96362808\n",
      "Iteration 560, loss = 2.96229799\n",
      "Iteration 561, loss = 2.96339581\n",
      "Iteration 562, loss = 2.96279860\n",
      "Iteration 563, loss = 2.96158177\n",
      "Iteration 564, loss = 2.96084962\n",
      "Iteration 565, loss = 2.96111060\n",
      "Iteration 566, loss = 2.96076861\n",
      "Iteration 567, loss = 2.96214093\n",
      "Iteration 568, loss = 2.96174236\n",
      "Iteration 569, loss = 2.95960721\n",
      "Iteration 570, loss = 2.95883489\n",
      "Iteration 571, loss = 2.95837244\n",
      "Iteration 572, loss = 2.95813167\n",
      "Iteration 573, loss = 2.95714644\n",
      "Iteration 574, loss = 2.95809725\n",
      "Iteration 575, loss = 2.95864549\n",
      "Iteration 576, loss = 2.95758410\n",
      "Iteration 577, loss = 2.95666185\n",
      "Iteration 578, loss = 2.95631994\n",
      "Iteration 579, loss = 2.95586392\n",
      "Iteration 580, loss = 2.95588335\n",
      "Iteration 581, loss = 2.95584387\n",
      "Iteration 582, loss = 2.95403079\n",
      "Iteration 583, loss = 2.95377802\n",
      "Iteration 584, loss = 2.95362002\n",
      "Iteration 585, loss = 2.95406273\n",
      "Iteration 586, loss = 2.95329298\n",
      "Iteration 587, loss = 2.95268565\n",
      "Iteration 588, loss = 2.95226247\n",
      "Iteration 589, loss = 2.95207281\n",
      "Iteration 590, loss = 2.95099082\n",
      "Iteration 591, loss = 2.95121199\n",
      "Iteration 592, loss = 2.95064932\n",
      "Iteration 593, loss = 2.95030915\n",
      "Iteration 594, loss = 2.95019997\n",
      "Iteration 595, loss = 2.94997851\n",
      "Iteration 596, loss = 2.95032127\n",
      "Iteration 597, loss = 2.94958128\n",
      "Iteration 598, loss = 2.94892740\n",
      "Iteration 599, loss = 2.94852309\n",
      "Iteration 600, loss = 2.94891560\n",
      "Iteration 601, loss = 2.94744546\n",
      "Iteration 602, loss = 2.94682792\n",
      "Iteration 603, loss = 2.94654072\n",
      "Iteration 604, loss = 2.94607734\n",
      "Iteration 605, loss = 2.94561500\n",
      "Iteration 606, loss = 2.94552400\n",
      "Iteration 607, loss = 2.94540851\n",
      "Iteration 608, loss = 2.94557874\n",
      "Iteration 609, loss = 2.94590476\n",
      "Iteration 610, loss = 2.94410038\n",
      "Iteration 611, loss = 2.94332436\n",
      "Iteration 612, loss = 2.94383282\n",
      "Iteration 613, loss = 2.94323101\n",
      "Iteration 614, loss = 2.94266383\n",
      "Iteration 615, loss = 2.94252040\n",
      "Iteration 616, loss = 2.94223226\n",
      "Iteration 617, loss = 2.94182732\n",
      "Iteration 618, loss = 2.94178031\n",
      "Iteration 619, loss = 2.94138329\n",
      "Iteration 620, loss = 2.94053591\n",
      "Iteration 621, loss = 2.94117369\n",
      "Iteration 622, loss = 2.94097597\n",
      "Iteration 623, loss = 2.94062392\n",
      "Iteration 624, loss = 2.93977915\n",
      "Iteration 625, loss = 2.94011861\n",
      "Iteration 626, loss = 2.93947773\n",
      "Iteration 627, loss = 2.93839233\n",
      "Iteration 628, loss = 2.93798436\n",
      "Iteration 629, loss = 2.93763086\n",
      "Iteration 630, loss = 2.93745642\n",
      "Iteration 631, loss = 2.93693422\n",
      "Iteration 632, loss = 2.93624790\n",
      "Iteration 633, loss = 2.93647874\n",
      "Iteration 634, loss = 2.93583651\n",
      "Iteration 635, loss = 2.93491300\n",
      "Iteration 636, loss = 2.93499958\n",
      "Iteration 637, loss = 2.93417676\n",
      "Iteration 638, loss = 2.93405638\n",
      "Iteration 639, loss = 2.93344891\n",
      "Iteration 640, loss = 2.93395256\n",
      "Iteration 641, loss = 2.93384014\n",
      "Iteration 642, loss = 2.93263016\n",
      "Iteration 643, loss = 2.93214341\n",
      "Iteration 644, loss = 2.93216175\n",
      "Iteration 645, loss = 2.93154445\n",
      "Iteration 646, loss = 2.93210876\n",
      "Iteration 647, loss = 2.93141215\n",
      "Iteration 648, loss = 2.93053396\n",
      "Iteration 649, loss = 2.93141710\n",
      "Iteration 650, loss = 2.93026997\n",
      "Iteration 651, loss = 2.93101035\n",
      "Iteration 652, loss = 2.92989097\n",
      "Iteration 653, loss = 2.93017257\n",
      "Iteration 654, loss = 2.93000188\n",
      "Iteration 655, loss = 2.92891606\n",
      "Iteration 656, loss = 2.92794134\n",
      "Iteration 657, loss = 2.92757442\n",
      "Iteration 658, loss = 2.92774672\n",
      "Iteration 659, loss = 2.92808005\n",
      "Iteration 660, loss = 2.92717591\n",
      "Iteration 661, loss = 2.92653750\n",
      "Iteration 662, loss = 2.92705922\n",
      "Iteration 663, loss = 2.92646381\n",
      "Iteration 664, loss = 2.92517055\n",
      "Iteration 665, loss = 2.92547118\n",
      "Iteration 666, loss = 2.92520271\n",
      "Iteration 667, loss = 2.92515115\n",
      "Iteration 668, loss = 2.92538987\n",
      "Iteration 669, loss = 2.92610618\n",
      "Iteration 670, loss = 2.92475216\n",
      "Iteration 671, loss = 2.92401331\n",
      "Iteration 672, loss = 2.92298408\n",
      "Iteration 673, loss = 2.92306419\n",
      "Iteration 674, loss = 2.92219066\n",
      "Iteration 675, loss = 2.92228494\n",
      "Iteration 676, loss = 2.92130083\n",
      "Iteration 677, loss = 2.92213512\n",
      "Iteration 678, loss = 2.92141048\n",
      "Iteration 679, loss = 2.92124434\n",
      "Iteration 680, loss = 2.92071114\n",
      "Iteration 681, loss = 2.92079100\n",
      "Iteration 682, loss = 2.92083618\n",
      "Iteration 683, loss = 2.91954023\n",
      "Iteration 684, loss = 2.92052942\n",
      "Iteration 685, loss = 2.91988208\n",
      "Iteration 686, loss = 2.91875793\n",
      "Iteration 687, loss = 2.91872662\n",
      "Iteration 688, loss = 2.91821911\n",
      "Iteration 689, loss = 2.91724958\n",
      "Iteration 690, loss = 2.91841804\n",
      "Iteration 691, loss = 2.91706470\n",
      "Iteration 692, loss = 2.91658129\n",
      "Iteration 693, loss = 2.91743565\n",
      "Iteration 694, loss = 2.91692976\n",
      "Iteration 695, loss = 2.91735794\n",
      "Iteration 696, loss = 2.91651576\n",
      "Iteration 697, loss = 2.91567892\n",
      "Iteration 698, loss = 2.91499679\n",
      "Iteration 699, loss = 2.91497725\n",
      "Iteration 700, loss = 2.91422384\n",
      "Iteration 701, loss = 2.91434395\n",
      "Iteration 702, loss = 2.91356529\n",
      "Iteration 703, loss = 2.91322058\n",
      "Iteration 704, loss = 2.91329280\n",
      "Iteration 705, loss = 2.91450395\n",
      "Iteration 706, loss = 2.91463455\n",
      "Iteration 707, loss = 2.91308380\n",
      "Iteration 708, loss = 2.91271312\n",
      "Iteration 709, loss = 2.91216281\n",
      "Iteration 710, loss = 2.91134281\n",
      "Iteration 711, loss = 2.91116438\n",
      "Iteration 712, loss = 2.91059487\n",
      "Iteration 713, loss = 2.90979280\n",
      "Iteration 714, loss = 2.90996050\n",
      "Iteration 715, loss = 2.91059237\n",
      "Iteration 716, loss = 2.90926545\n",
      "Iteration 717, loss = 2.90939928\n",
      "Iteration 718, loss = 2.90916038\n",
      "Iteration 719, loss = 2.90875490\n",
      "Iteration 720, loss = 2.90815264\n",
      "Iteration 721, loss = 2.90825692\n",
      "Iteration 722, loss = 2.90830321\n",
      "Iteration 723, loss = 2.90784669\n",
      "Iteration 724, loss = 2.90673944\n",
      "Iteration 725, loss = 2.90630431\n",
      "Iteration 726, loss = 2.90666390\n",
      "Iteration 727, loss = 2.90631999\n",
      "Iteration 728, loss = 2.90599091\n",
      "Iteration 729, loss = 2.90575576\n",
      "Iteration 730, loss = 2.90604659\n",
      "Iteration 731, loss = 2.90522801\n",
      "Iteration 732, loss = 2.90393774\n",
      "Iteration 733, loss = 2.90396192\n",
      "Iteration 734, loss = 2.90391850\n",
      "Iteration 735, loss = 2.90366413\n",
      "Iteration 736, loss = 2.90336854\n",
      "Iteration 737, loss = 2.90306882\n",
      "Iteration 738, loss = 2.90258678\n",
      "Iteration 739, loss = 2.90229238\n",
      "Iteration 740, loss = 2.90275309\n",
      "Iteration 741, loss = 2.90225049\n",
      "Iteration 742, loss = 2.90286810\n",
      "Iteration 743, loss = 2.90104651\n",
      "Iteration 744, loss = 2.90046326\n",
      "Iteration 745, loss = 2.90107309\n",
      "Iteration 746, loss = 2.90077849\n",
      "Iteration 747, loss = 2.90032953\n",
      "Iteration 748, loss = 2.90009226\n",
      "Iteration 749, loss = 2.90007155\n",
      "Iteration 750, loss = 2.90000977\n",
      "Iteration 751, loss = 2.89964398\n",
      "Iteration 752, loss = 2.89841330\n",
      "Iteration 753, loss = 2.89821576\n",
      "Iteration 754, loss = 2.89870310\n",
      "Iteration 755, loss = 2.89846922\n",
      "Iteration 756, loss = 2.89882432\n",
      "Iteration 757, loss = 2.89783905\n",
      "Iteration 758, loss = 2.89680686\n",
      "Iteration 759, loss = 2.89657906\n",
      "Iteration 760, loss = 2.89626193\n",
      "Iteration 761, loss = 2.89642450\n",
      "Iteration 762, loss = 2.89575497\n",
      "Iteration 763, loss = 2.89580261\n",
      "Iteration 764, loss = 2.89519593\n",
      "Iteration 765, loss = 2.89548755\n",
      "Iteration 766, loss = 2.89517146\n",
      "Iteration 767, loss = 2.89486459\n",
      "Iteration 768, loss = 2.89490936\n",
      "Iteration 769, loss = 2.89450045\n",
      "Iteration 770, loss = 2.89361914\n",
      "Iteration 771, loss = 2.89383381\n",
      "Iteration 772, loss = 2.89350396\n",
      "Iteration 773, loss = 2.89323689\n",
      "Iteration 774, loss = 2.89251338\n",
      "Iteration 775, loss = 2.89225115\n",
      "Iteration 776, loss = 2.89241936\n",
      "Iteration 777, loss = 2.89230296\n",
      "Iteration 778, loss = 2.89179685\n",
      "Iteration 779, loss = 2.89142695\n",
      "Iteration 780, loss = 2.89199325\n",
      "Iteration 781, loss = 2.89135814\n",
      "Iteration 782, loss = 2.89045645\n",
      "Iteration 783, loss = 2.88978283\n",
      "Iteration 784, loss = 2.89060863\n",
      "Iteration 785, loss = 2.89016397\n",
      "Iteration 786, loss = 2.88937882\n",
      "Iteration 787, loss = 2.88961726\n",
      "Iteration 788, loss = 2.88899325\n",
      "Iteration 789, loss = 2.88915612\n",
      "Iteration 790, loss = 2.88924929\n",
      "Iteration 791, loss = 2.88809047\n",
      "Iteration 792, loss = 2.88760423\n",
      "Iteration 793, loss = 2.88735647\n",
      "Iteration 794, loss = 2.88682491\n",
      "Iteration 795, loss = 2.88686008\n",
      "Iteration 796, loss = 2.88684732\n",
      "Iteration 797, loss = 2.88651853\n",
      "Iteration 798, loss = 2.88624452\n",
      "Iteration 799, loss = 2.88573353\n",
      "Iteration 800, loss = 2.88558557\n",
      "Iteration 801, loss = 2.88550414\n",
      "Iteration 802, loss = 2.88470789\n",
      "Iteration 803, loss = 2.88430724\n",
      "Iteration 804, loss = 2.88427933\n",
      "Iteration 805, loss = 2.88447546\n",
      "Iteration 806, loss = 2.88417144\n",
      "Iteration 807, loss = 2.88431781\n",
      "Iteration 808, loss = 2.88319885\n",
      "Iteration 809, loss = 2.88321071\n",
      "Iteration 810, loss = 2.88364876\n",
      "Iteration 811, loss = 2.88267948\n",
      "Iteration 812, loss = 2.88292255\n",
      "Iteration 813, loss = 2.88298326\n",
      "Iteration 814, loss = 2.88183044\n",
      "Iteration 815, loss = 2.88248230\n",
      "Iteration 816, loss = 2.88247867\n",
      "Iteration 817, loss = 2.88101452\n",
      "Iteration 818, loss = 2.88025885\n",
      "Iteration 819, loss = 2.87999043\n",
      "Iteration 820, loss = 2.87932540\n",
      "Iteration 821, loss = 2.87954666\n",
      "Iteration 822, loss = 2.87966956\n",
      "Iteration 823, loss = 2.87970686\n",
      "Iteration 824, loss = 2.87853097\n",
      "Iteration 825, loss = 2.87818072\n",
      "Iteration 826, loss = 2.87818896\n",
      "Iteration 827, loss = 2.87800045\n",
      "Iteration 828, loss = 2.87847039\n",
      "Iteration 829, loss = 2.87739728\n",
      "Iteration 830, loss = 2.87731998\n",
      "Iteration 831, loss = 2.87691294\n",
      "Iteration 832, loss = 2.87672692\n",
      "Iteration 833, loss = 2.87615265\n",
      "Iteration 834, loss = 2.87583158\n",
      "Iteration 835, loss = 2.87635710\n",
      "Iteration 836, loss = 2.87663137\n",
      "Iteration 837, loss = 2.87555722\n",
      "Iteration 838, loss = 2.87595771\n",
      "Iteration 839, loss = 2.87560959\n",
      "Iteration 840, loss = 2.87452730\n",
      "Iteration 841, loss = 2.87511617\n",
      "Iteration 842, loss = 2.87547904\n",
      "Iteration 843, loss = 2.87489410\n",
      "Iteration 844, loss = 2.87354505\n",
      "Iteration 845, loss = 2.87389302\n",
      "Iteration 846, loss = 2.87354714\n",
      "Iteration 847, loss = 2.87297131\n",
      "Iteration 848, loss = 2.87343039\n",
      "Iteration 849, loss = 2.87392130\n",
      "Iteration 850, loss = 2.87291013\n",
      "Iteration 851, loss = 2.87237326\n",
      "Iteration 852, loss = 2.87203272\n",
      "Iteration 853, loss = 2.87372901\n",
      "Iteration 854, loss = 2.87255613\n",
      "Iteration 855, loss = 2.87137962\n",
      "Iteration 856, loss = 2.87127055\n",
      "Iteration 857, loss = 2.87096104\n",
      "Iteration 858, loss = 2.87095727\n",
      "Iteration 859, loss = 2.86997468\n",
      "Iteration 860, loss = 2.87054061\n",
      "Iteration 861, loss = 2.86958461\n",
      "Iteration 862, loss = 2.86915097\n",
      "Iteration 863, loss = 2.86787511\n",
      "Iteration 864, loss = 2.86818308\n",
      "Iteration 865, loss = 2.86862249\n",
      "Iteration 866, loss = 2.86906606\n",
      "Iteration 867, loss = 2.86743939\n",
      "Iteration 868, loss = 2.86814264\n",
      "Iteration 869, loss = 2.86807664\n",
      "Iteration 870, loss = 2.86637340\n",
      "Iteration 871, loss = 2.86640526\n",
      "Iteration 872, loss = 2.86689274\n",
      "Iteration 873, loss = 2.86673470\n",
      "Iteration 874, loss = 2.86632037\n",
      "Iteration 875, loss = 2.86579003\n",
      "Iteration 876, loss = 2.86559124\n",
      "Iteration 877, loss = 2.86525567\n",
      "Iteration 878, loss = 2.86514463\n",
      "Iteration 879, loss = 2.86494170\n",
      "Iteration 880, loss = 2.86453509\n",
      "Iteration 881, loss = 2.86467547\n",
      "Iteration 882, loss = 2.86434885\n",
      "Iteration 883, loss = 2.86452866\n",
      "Iteration 884, loss = 2.86478471\n",
      "Iteration 885, loss = 2.86394398\n",
      "Iteration 886, loss = 2.86298733\n",
      "Iteration 887, loss = 2.86269517\n",
      "Iteration 888, loss = 2.86305774\n",
      "Iteration 889, loss = 2.86185478\n",
      "Iteration 890, loss = 2.86156731\n",
      "Iteration 891, loss = 2.86181724\n",
      "Iteration 892, loss = 2.86178429\n",
      "Iteration 893, loss = 2.86089178\n",
      "Iteration 894, loss = 2.86034432\n",
      "Iteration 895, loss = 2.86125626\n",
      "Iteration 896, loss = 2.86094954\n",
      "Iteration 897, loss = 2.86167263\n",
      "Iteration 898, loss = 2.86154958\n",
      "Iteration 899, loss = 2.86024439\n",
      "Iteration 900, loss = 2.85895855\n",
      "Iteration 901, loss = 2.85912918\n",
      "Iteration 902, loss = 2.85879176\n",
      "Iteration 903, loss = 2.85913563\n",
      "Iteration 904, loss = 2.85847852\n",
      "Iteration 905, loss = 2.85837083\n",
      "Iteration 906, loss = 2.85810686\n",
      "Iteration 907, loss = 2.85679381\n",
      "Iteration 908, loss = 2.85734360\n",
      "Iteration 909, loss = 2.85764786\n",
      "Iteration 910, loss = 2.85724991\n",
      "Iteration 911, loss = 2.85672315\n",
      "Iteration 912, loss = 2.85638452\n",
      "Iteration 913, loss = 2.85735360\n",
      "Iteration 914, loss = 2.85803176\n",
      "Iteration 915, loss = 2.85628250\n",
      "Iteration 916, loss = 2.85557547\n",
      "Iteration 917, loss = 2.85551510\n",
      "Iteration 918, loss = 2.85587829\n",
      "Iteration 919, loss = 2.85433871\n",
      "Iteration 920, loss = 2.85454782\n",
      "Iteration 921, loss = 2.85423304\n",
      "Iteration 922, loss = 2.85422671\n",
      "Iteration 923, loss = 2.85411744\n",
      "Iteration 924, loss = 2.85293140\n",
      "Iteration 925, loss = 2.85333669\n",
      "Iteration 926, loss = 2.85269925\n",
      "Iteration 927, loss = 2.85273478\n",
      "Iteration 928, loss = 2.85352181\n",
      "Iteration 929, loss = 2.85355944\n",
      "Iteration 930, loss = 2.85213179\n",
      "Iteration 931, loss = 2.85245058\n",
      "Iteration 932, loss = 2.85305975\n",
      "Iteration 933, loss = 2.85229808\n",
      "Iteration 934, loss = 2.85208229\n",
      "Iteration 935, loss = 2.85102527\n",
      "Iteration 936, loss = 2.85068427\n",
      "Iteration 937, loss = 2.85091556\n",
      "Iteration 938, loss = 2.85118415\n",
      "Iteration 939, loss = 2.85122640\n",
      "Iteration 940, loss = 2.84999309\n",
      "Iteration 941, loss = 2.85125694\n",
      "Iteration 942, loss = 2.85132718\n",
      "Iteration 943, loss = 2.85081078\n",
      "Iteration 944, loss = 2.84946757\n",
      "Iteration 945, loss = 2.84828797\n",
      "Iteration 946, loss = 2.84890763\n",
      "Iteration 947, loss = 2.84813272\n",
      "Iteration 948, loss = 2.84828658\n",
      "Iteration 949, loss = 2.84830568\n",
      "Iteration 950, loss = 2.84798963\n",
      "Iteration 951, loss = 2.84731080\n",
      "Iteration 952, loss = 2.84745270\n",
      "Iteration 953, loss = 2.84717862\n",
      "Iteration 954, loss = 2.84671490\n",
      "Iteration 955, loss = 2.84676117\n",
      "Iteration 956, loss = 2.84642544\n",
      "Iteration 957, loss = 2.84618723\n",
      "Iteration 958, loss = 2.84674571\n",
      "Iteration 959, loss = 2.84742809\n",
      "Iteration 960, loss = 2.84709292\n",
      "Iteration 961, loss = 2.84612588\n",
      "Iteration 962, loss = 2.84558995\n",
      "Iteration 963, loss = 2.84591731\n",
      "Iteration 964, loss = 2.84486549\n",
      "Iteration 965, loss = 2.84497539\n",
      "Iteration 966, loss = 2.84364550\n",
      "Iteration 967, loss = 2.84356404\n",
      "Iteration 968, loss = 2.84310644\n",
      "Iteration 969, loss = 2.84427736\n",
      "Iteration 970, loss = 2.84490253\n",
      "Iteration 971, loss = 2.84338263\n",
      "Iteration 972, loss = 2.84379858\n",
      "Iteration 973, loss = 2.84388491\n",
      "Iteration 974, loss = 2.84304008\n",
      "Iteration 975, loss = 2.84181606\n",
      "Iteration 976, loss = 2.84182299\n",
      "Iteration 977, loss = 2.84195346\n",
      "Iteration 978, loss = 2.84165760\n",
      "Iteration 979, loss = 2.84066910\n",
      "Iteration 980, loss = 2.84089330\n",
      "Iteration 981, loss = 2.84121784\n",
      "Iteration 982, loss = 2.84128586\n",
      "Iteration 983, loss = 2.84010030\n",
      "Iteration 984, loss = 2.84077927\n",
      "Iteration 985, loss = 2.84014648\n",
      "Iteration 986, loss = 2.83887832\n",
      "Iteration 987, loss = 2.83906712\n",
      "Iteration 988, loss = 2.83967643\n",
      "Iteration 989, loss = 2.83909368\n",
      "Iteration 990, loss = 2.83923962\n",
      "Iteration 991, loss = 2.83856385\n",
      "Iteration 992, loss = 2.83805338\n",
      "Iteration 993, loss = 2.83754439\n",
      "Iteration 994, loss = 2.83745912\n",
      "Iteration 995, loss = 2.83825385\n",
      "Iteration 996, loss = 2.83793740\n",
      "Iteration 997, loss = 2.83742526\n",
      "Iteration 998, loss = 2.83643902\n",
      "Iteration 999, loss = 2.83606452\n",
      "Iteration 1000, loss = 2.83557092\n",
      "Iteration 1001, loss = 2.83590185\n",
      "Iteration 1002, loss = 2.83590940\n",
      "Iteration 1003, loss = 2.83504641\n",
      "Iteration 1004, loss = 2.83482806\n",
      "Iteration 1005, loss = 2.83478192\n",
      "Iteration 1006, loss = 2.83463986\n",
      "Iteration 1007, loss = 2.83500357\n",
      "Iteration 1008, loss = 2.83402732\n",
      "Iteration 1009, loss = 2.83403071\n",
      "Iteration 1010, loss = 2.83425698\n",
      "Iteration 1011, loss = 2.83403534\n",
      "Iteration 1012, loss = 2.83372726\n",
      "Iteration 1013, loss = 2.83319678\n",
      "Iteration 1014, loss = 2.83324477\n",
      "Iteration 1015, loss = 2.83351242\n",
      "Iteration 1016, loss = 2.83347721\n",
      "Iteration 1017, loss = 2.83294818\n",
      "Iteration 1018, loss = 2.83362465\n",
      "Iteration 1019, loss = 2.83326278\n",
      "Iteration 1020, loss = 2.83224514\n",
      "Iteration 1021, loss = 2.83182949\n",
      "Iteration 1022, loss = 2.83151145\n",
      "Iteration 1023, loss = 2.83185923\n",
      "Iteration 1024, loss = 2.83151296\n",
      "Iteration 1025, loss = 2.83087743\n",
      "Iteration 1026, loss = 2.83167901\n",
      "Iteration 1027, loss = 2.83094462\n",
      "Iteration 1028, loss = 2.83039937\n",
      "Iteration 1029, loss = 2.83049708\n",
      "Iteration 1030, loss = 2.83004658\n",
      "Iteration 1031, loss = 2.82957639\n",
      "Iteration 1032, loss = 2.82880386\n",
      "Iteration 1033, loss = 2.82866172\n",
      "Iteration 1034, loss = 2.82933907\n",
      "Iteration 1035, loss = 2.83043107\n",
      "Iteration 1036, loss = 2.82957648\n",
      "Iteration 1037, loss = 2.82863413\n",
      "Iteration 1038, loss = 2.82748956\n",
      "Iteration 1039, loss = 2.82749792\n",
      "Iteration 1040, loss = 2.82726853\n",
      "Iteration 1041, loss = 2.82710782\n",
      "Iteration 1042, loss = 2.82770977\n",
      "Iteration 1043, loss = 2.82753866\n",
      "Iteration 1044, loss = 2.82728861\n",
      "Iteration 1045, loss = 2.82695964\n",
      "Iteration 1046, loss = 2.82643758\n",
      "Iteration 1047, loss = 2.82684318\n",
      "Iteration 1048, loss = 2.82659120\n",
      "Iteration 1049, loss = 2.82609470\n",
      "Iteration 1050, loss = 2.82575899\n",
      "Iteration 1051, loss = 2.82502503\n",
      "Iteration 1052, loss = 2.82512030\n",
      "Iteration 1053, loss = 2.82500258\n",
      "Iteration 1054, loss = 2.82529732\n",
      "Iteration 1055, loss = 2.82454262\n",
      "Iteration 1056, loss = 2.82380124\n",
      "Iteration 1057, loss = 2.82373853\n",
      "Iteration 1058, loss = 2.82376992\n",
      "Iteration 1059, loss = 2.82521174\n",
      "Iteration 1060, loss = 2.82502694\n",
      "Iteration 1061, loss = 2.82389982\n",
      "Iteration 1062, loss = 2.82415959\n",
      "Iteration 1063, loss = 2.82333309\n",
      "Iteration 1064, loss = 2.82266412\n",
      "Iteration 1065, loss = 2.82227093\n",
      "Iteration 1066, loss = 2.82280133\n",
      "Iteration 1067, loss = 2.82162690\n",
      "Iteration 1068, loss = 2.82166478\n",
      "Iteration 1069, loss = 2.82154388\n",
      "Iteration 1070, loss = 2.82151032\n",
      "Iteration 1071, loss = 2.82165329\n",
      "Iteration 1072, loss = 2.82133845\n",
      "Iteration 1073, loss = 2.82088460\n",
      "Iteration 1074, loss = 2.82006526\n",
      "Iteration 1075, loss = 2.82108669\n",
      "Iteration 1076, loss = 2.82082421\n",
      "Iteration 1077, loss = 2.82033240\n",
      "Iteration 1078, loss = 2.81958956\n",
      "Iteration 1079, loss = 2.81956421\n",
      "Iteration 1080, loss = 2.81933217\n",
      "Iteration 1081, loss = 2.81900218\n",
      "Iteration 1082, loss = 2.81963513\n",
      "Iteration 1083, loss = 2.81930201\n",
      "Iteration 1084, loss = 2.81903962\n",
      "Iteration 1085, loss = 2.81806725\n",
      "Iteration 1086, loss = 2.81834538\n",
      "Iteration 1087, loss = 2.81730459\n",
      "Iteration 1088, loss = 2.81785252\n",
      "Iteration 1089, loss = 2.81723300\n",
      "Iteration 1090, loss = 2.81685009\n",
      "Iteration 1091, loss = 2.81717573\n",
      "Iteration 1092, loss = 2.81709661\n",
      "Iteration 1093, loss = 2.81656547\n",
      "Iteration 1094, loss = 2.81632198\n",
      "Iteration 1095, loss = 2.81639375\n",
      "Iteration 1096, loss = 2.81548755\n",
      "Iteration 1097, loss = 2.81529661\n",
      "Iteration 1098, loss = 2.81559000\n",
      "Iteration 1099, loss = 2.81646370\n",
      "Iteration 1100, loss = 2.81598792\n",
      "Iteration 1101, loss = 2.81587314\n",
      "Iteration 1102, loss = 2.81516421\n",
      "Iteration 1103, loss = 2.81482902\n",
      "Iteration 1104, loss = 2.81389998\n",
      "Iteration 1105, loss = 2.81415867\n",
      "Iteration 1106, loss = 2.81413707\n",
      "Iteration 1107, loss = 2.81386169\n",
      "Iteration 1108, loss = 2.81400591\n",
      "Iteration 1109, loss = 2.81335818\n",
      "Iteration 1110, loss = 2.81347288\n",
      "Iteration 1111, loss = 2.81332494\n",
      "Iteration 1112, loss = 2.81396231\n",
      "Iteration 1113, loss = 2.81442551\n",
      "Iteration 1114, loss = 2.81417688\n",
      "Iteration 1115, loss = 2.81327888\n",
      "Iteration 1116, loss = 2.81262359\n",
      "Iteration 1117, loss = 2.81153657\n",
      "Iteration 1118, loss = 2.81284724\n",
      "Iteration 1119, loss = 2.81093703\n",
      "Iteration 1120, loss = 2.81131122\n",
      "Iteration 1121, loss = 2.81167501\n",
      "Iteration 1122, loss = 2.81060775\n",
      "Iteration 1123, loss = 2.81075989\n",
      "Iteration 1124, loss = 2.81080963\n",
      "Iteration 1125, loss = 2.81061357\n",
      "Iteration 1126, loss = 2.81014216\n",
      "Iteration 1127, loss = 2.81051830\n",
      "Iteration 1128, loss = 2.81038064\n",
      "Iteration 1129, loss = 2.81007038\n",
      "Iteration 1130, loss = 2.80929766\n",
      "Iteration 1131, loss = 2.80927922\n",
      "Iteration 1132, loss = 2.80883665\n",
      "Iteration 1133, loss = 2.80962392\n",
      "Iteration 1134, loss = 2.80936569\n",
      "Iteration 1135, loss = 2.80919089\n",
      "Iteration 1136, loss = 2.80887412\n",
      "Iteration 1137, loss = 2.80842397\n",
      "Iteration 1138, loss = 2.80852378\n",
      "Iteration 1139, loss = 2.80796658\n",
      "Iteration 1140, loss = 2.80790918\n",
      "Iteration 1141, loss = 2.80840359\n",
      "Iteration 1142, loss = 2.80770328\n",
      "Iteration 1143, loss = 2.80755650\n",
      "Iteration 1144, loss = 2.80780330\n",
      "Iteration 1145, loss = 2.80699115\n",
      "Iteration 1146, loss = 2.80710334\n",
      "Iteration 1147, loss = 2.80587877\n",
      "Iteration 1148, loss = 2.80648802\n",
      "Iteration 1149, loss = 2.80619040\n",
      "Iteration 1150, loss = 2.80676809\n",
      "Iteration 1151, loss = 2.80593158\n",
      "Iteration 1152, loss = 2.80549023\n",
      "Iteration 1153, loss = 2.80536386\n",
      "Iteration 1154, loss = 2.80506267\n",
      "Iteration 1155, loss = 2.80578306\n",
      "Iteration 1156, loss = 2.80476509\n",
      "Iteration 1157, loss = 2.80428577\n",
      "Iteration 1158, loss = 2.80455206\n",
      "Iteration 1159, loss = 2.80533857\n",
      "Iteration 1160, loss = 2.80555571\n",
      "Iteration 1161, loss = 2.80510299\n",
      "Iteration 1162, loss = 2.80318853\n",
      "Iteration 1163, loss = 2.80407470\n",
      "Iteration 1164, loss = 2.80382892\n",
      "Iteration 1165, loss = 2.80394476\n",
      "Iteration 1166, loss = 2.80315069\n",
      "Iteration 1167, loss = 2.80281267\n",
      "Iteration 1168, loss = 2.80200237\n",
      "Iteration 1169, loss = 2.80193446\n",
      "Iteration 1170, loss = 2.80270657\n",
      "Iteration 1171, loss = 2.80195980\n",
      "Iteration 1172, loss = 2.80172012\n",
      "Iteration 1173, loss = 2.80196724\n",
      "Iteration 1174, loss = 2.80123859\n",
      "Iteration 1175, loss = 2.80064334\n",
      "Iteration 1176, loss = 2.80051464\n",
      "Iteration 1177, loss = 2.80112989\n",
      "Iteration 1178, loss = 2.80019020\n",
      "Iteration 1179, loss = 2.79964527\n",
      "Iteration 1180, loss = 2.79984120\n",
      "Iteration 1181, loss = 2.79968096\n",
      "Iteration 1182, loss = 2.80055703\n",
      "Iteration 1183, loss = 2.80102008\n",
      "Iteration 1184, loss = 2.79949564\n",
      "Iteration 1185, loss = 2.79914693\n",
      "Iteration 1186, loss = 2.79862230\n",
      "Iteration 1187, loss = 2.79873312\n",
      "Iteration 1188, loss = 2.79881050\n",
      "Iteration 1189, loss = 2.79877950\n",
      "Iteration 1190, loss = 2.79838435\n",
      "Iteration 1191, loss = 2.79839609\n",
      "Iteration 1192, loss = 2.79796318\n",
      "Iteration 1193, loss = 2.79744667\n",
      "Iteration 1194, loss = 2.79856890\n",
      "Iteration 1195, loss = 2.79797232\n",
      "Iteration 1196, loss = 2.79715283\n",
      "Iteration 1197, loss = 2.79765410\n",
      "Iteration 1198, loss = 2.79699828\n",
      "Iteration 1199, loss = 2.79682372\n",
      "Iteration 1200, loss = 2.79744333\n",
      "Iteration 1201, loss = 2.79722533\n",
      "Iteration 1202, loss = 2.79628692\n",
      "Iteration 1203, loss = 2.79624017\n",
      "Iteration 1204, loss = 2.79559307\n",
      "Iteration 1205, loss = 2.79493896\n",
      "Iteration 1206, loss = 2.79554320\n",
      "Iteration 1207, loss = 2.79473466\n",
      "Iteration 1208, loss = 2.79511414\n",
      "Iteration 1209, loss = 2.79469153\n",
      "Iteration 1210, loss = 2.79485478\n",
      "Iteration 1211, loss = 2.79421365\n",
      "Iteration 1212, loss = 2.79301618\n",
      "Iteration 1213, loss = 2.79358035\n",
      "Iteration 1214, loss = 2.79349363\n",
      "Iteration 1215, loss = 2.79410440\n",
      "Iteration 1216, loss = 2.79348768\n",
      "Iteration 1217, loss = 2.79310968\n",
      "Iteration 1218, loss = 2.79302980\n",
      "Iteration 1219, loss = 2.79186706\n",
      "Iteration 1220, loss = 2.79176360\n",
      "Iteration 1221, loss = 2.79222731\n",
      "Iteration 1222, loss = 2.79289492\n",
      "Iteration 1223, loss = 2.79232792\n",
      "Iteration 1224, loss = 2.79103821\n",
      "Iteration 1225, loss = 2.79176394\n",
      "Iteration 1226, loss = 2.79115172\n",
      "Iteration 1227, loss = 2.79132714\n",
      "Iteration 1228, loss = 2.79143062\n",
      "Iteration 1229, loss = 2.79033741\n",
      "Iteration 1230, loss = 2.79059399\n",
      "Iteration 1231, loss = 2.79123480\n",
      "Iteration 1232, loss = 2.79054282\n",
      "Iteration 1233, loss = 2.78982952\n",
      "Iteration 1234, loss = 2.78999009\n",
      "Iteration 1235, loss = 2.78991764\n",
      "Iteration 1236, loss = 2.78926838\n",
      "Iteration 1237, loss = 2.79015950\n",
      "Iteration 1238, loss = 2.78926778\n",
      "Iteration 1239, loss = 2.78847142\n",
      "Iteration 1240, loss = 2.78876845\n",
      "Iteration 1241, loss = 2.78937786\n",
      "Iteration 1242, loss = 2.78920678\n",
      "Iteration 1243, loss = 2.78794062\n",
      "Iteration 1244, loss = 2.78923189\n",
      "Iteration 1245, loss = 2.78838601\n",
      "Iteration 1246, loss = 2.78848489\n",
      "Iteration 1247, loss = 2.78808911\n",
      "Iteration 1248, loss = 2.78816692\n",
      "Iteration 1249, loss = 2.78744101\n",
      "Iteration 1250, loss = 2.78782480\n",
      "Iteration 1251, loss = 2.78764195\n",
      "Iteration 1252, loss = 2.78719760\n",
      "Iteration 1253, loss = 2.78724653\n",
      "Iteration 1254, loss = 2.78584463\n",
      "Iteration 1255, loss = 2.78756459\n",
      "Iteration 1256, loss = 2.78778952\n",
      "Iteration 1257, loss = 2.78638385\n",
      "Iteration 1258, loss = 2.78698783\n",
      "Iteration 1259, loss = 2.78566659\n",
      "Iteration 1260, loss = 2.78549472\n",
      "Iteration 1261, loss = 2.78672711\n",
      "Iteration 1262, loss = 2.78495310\n",
      "Iteration 1263, loss = 2.78487330\n",
      "Iteration 1264, loss = 2.78484657\n",
      "Iteration 1265, loss = 2.78494282\n",
      "Iteration 1266, loss = 2.78415341\n",
      "Iteration 1267, loss = 2.78390309\n",
      "Iteration 1268, loss = 2.78352857\n",
      "Iteration 1269, loss = 2.78704273\n",
      "Iteration 1270, loss = 2.78468473\n",
      "Iteration 1271, loss = 2.78295602\n",
      "Iteration 1272, loss = 2.78425007\n",
      "Iteration 1273, loss = 2.78300136\n",
      "Iteration 1274, loss = 2.78267027\n",
      "Iteration 1275, loss = 2.78276990\n",
      "Iteration 1276, loss = 2.78226079\n",
      "Iteration 1277, loss = 2.78279164\n",
      "Iteration 1278, loss = 2.78305836\n",
      "Iteration 1279, loss = 2.78216716\n",
      "Iteration 1280, loss = 2.78227401\n",
      "Iteration 1281, loss = 2.78190928\n",
      "Iteration 1282, loss = 2.78183457\n",
      "Iteration 1283, loss = 2.78289272\n",
      "Iteration 1284, loss = 2.78278015\n",
      "Iteration 1285, loss = 2.78164981\n",
      "Iteration 1286, loss = 2.78168667\n",
      "Iteration 1287, loss = 2.78200682\n",
      "Iteration 1288, loss = 2.78183140\n",
      "Iteration 1289, loss = 2.78099574\n",
      "Iteration 1290, loss = 2.77999454\n",
      "Iteration 1291, loss = 2.78046530\n",
      "Iteration 1292, loss = 2.78017979\n",
      "Iteration 1293, loss = 2.78037256\n",
      "Iteration 1294, loss = 2.77980517\n",
      "Iteration 1295, loss = 2.78191058\n",
      "Iteration 1296, loss = 2.77991436\n",
      "Iteration 1297, loss = 2.77989385\n",
      "Iteration 1298, loss = 2.77941016\n",
      "Iteration 1299, loss = 2.77943214\n",
      "Iteration 1300, loss = 2.77979743\n",
      "Iteration 1301, loss = 2.77943056\n",
      "Iteration 1302, loss = 2.77940519\n",
      "Iteration 1303, loss = 2.77755181\n",
      "Iteration 1304, loss = 2.77848465\n",
      "Iteration 1305, loss = 2.77853154\n",
      "Iteration 1306, loss = 2.77962211\n",
      "Iteration 1307, loss = 2.77849334\n",
      "Iteration 1308, loss = 2.77778383\n",
      "Iteration 1309, loss = 2.77813943\n",
      "Iteration 1310, loss = 2.77813565\n",
      "Iteration 1311, loss = 2.77783418\n",
      "Iteration 1312, loss = 2.77762766\n",
      "Iteration 1313, loss = 2.77743572\n",
      "Iteration 1314, loss = 2.77621950\n",
      "Iteration 1315, loss = 2.77670529\n",
      "Iteration 1316, loss = 2.77645316\n",
      "Iteration 1317, loss = 2.77608937\n",
      "Iteration 1318, loss = 2.77629814\n",
      "Iteration 1319, loss = 2.77538506\n",
      "Iteration 1320, loss = 2.77486934\n",
      "Iteration 1321, loss = 2.77524515\n",
      "Iteration 1322, loss = 2.77531834\n",
      "Iteration 1323, loss = 2.77569171\n",
      "Iteration 1324, loss = 2.77458329\n",
      "Iteration 1325, loss = 2.77503825\n",
      "Iteration 1326, loss = 2.77359169\n",
      "Iteration 1327, loss = 2.77425522\n",
      "Iteration 1328, loss = 2.77330579\n",
      "Iteration 1329, loss = 2.77326322\n",
      "Iteration 1330, loss = 2.77327938\n",
      "Iteration 1331, loss = 2.77407769\n",
      "Iteration 1332, loss = 2.77290821\n",
      "Iteration 1333, loss = 2.77356421\n",
      "Iteration 1334, loss = 2.77481670\n",
      "Iteration 1335, loss = 2.77367895\n",
      "Iteration 1336, loss = 2.77207716\n",
      "Iteration 1337, loss = 2.77220710\n",
      "Iteration 1338, loss = 2.77323577\n",
      "Iteration 1339, loss = 2.77277276\n",
      "Iteration 1340, loss = 2.77309601\n",
      "Iteration 1341, loss = 2.77228010\n",
      "Iteration 1342, loss = 2.77171282\n",
      "Iteration 1343, loss = 2.77136701\n",
      "Iteration 1344, loss = 2.77247720\n",
      "Iteration 1345, loss = 2.77249867\n",
      "Iteration 1346, loss = 2.77212624\n",
      "Iteration 1347, loss = 2.77069726\n",
      "Iteration 1348, loss = 2.77136307\n",
      "Iteration 1349, loss = 2.77149669\n",
      "Iteration 1350, loss = 2.77139975\n",
      "Iteration 1351, loss = 2.77056755\n",
      "Iteration 1352, loss = 2.77091987\n",
      "Iteration 1353, loss = 2.77059811\n",
      "Iteration 1354, loss = 2.76970718\n",
      "Iteration 1355, loss = 2.77048552\n",
      "Iteration 1356, loss = 2.77002083\n",
      "Iteration 1357, loss = 2.77043600\n",
      "Iteration 1358, loss = 2.77047575\n",
      "Iteration 1359, loss = 2.76897845\n",
      "Iteration 1360, loss = 2.76931817\n",
      "Iteration 1361, loss = 2.76915121\n",
      "Iteration 1362, loss = 2.76864464\n",
      "Iteration 1363, loss = 2.76936534\n",
      "Iteration 1364, loss = 2.76965105\n",
      "Iteration 1365, loss = 2.76940818\n",
      "Iteration 1366, loss = 2.76776923\n",
      "Iteration 1367, loss = 2.76835821\n",
      "Iteration 1368, loss = 2.76836746\n",
      "Iteration 1369, loss = 2.76735627\n",
      "Iteration 1370, loss = 2.76870692\n",
      "Iteration 1371, loss = 2.76884804\n",
      "Iteration 1372, loss = 2.76772534\n",
      "Iteration 1373, loss = 2.76682028\n",
      "Iteration 1374, loss = 2.76686579\n",
      "Iteration 1375, loss = 2.76750126\n",
      "Iteration 1376, loss = 2.76769159\n",
      "Iteration 1377, loss = 2.76665291\n",
      "Iteration 1378, loss = 2.76559016\n",
      "Iteration 1379, loss = 2.76536592\n",
      "Iteration 1380, loss = 2.76638583\n",
      "Iteration 1381, loss = 2.76668377\n",
      "Iteration 1382, loss = 2.76551851\n",
      "Iteration 1383, loss = 2.76470379\n",
      "Iteration 1384, loss = 2.76454101\n",
      "Iteration 1385, loss = 2.76445111\n",
      "Iteration 1386, loss = 2.76502968\n",
      "Iteration 1387, loss = 2.76406375\n",
      "Iteration 1388, loss = 2.76475905\n",
      "Iteration 1389, loss = 2.76456243\n",
      "Iteration 1390, loss = 2.76403550\n",
      "Iteration 1391, loss = 2.76420715\n",
      "Iteration 1392, loss = 2.76375195\n",
      "Iteration 1393, loss = 2.76379730\n",
      "Iteration 1394, loss = 2.76432372\n",
      "Iteration 1395, loss = 2.76289594\n",
      "Iteration 1396, loss = 2.76502125\n",
      "Iteration 1397, loss = 2.76477575\n",
      "Iteration 1398, loss = 2.76269246\n",
      "Iteration 1399, loss = 2.76293306\n",
      "Iteration 1400, loss = 2.76259007\n",
      "Iteration 1401, loss = 2.76275407\n",
      "Iteration 1402, loss = 2.76242822\n",
      "Iteration 1403, loss = 2.76416235\n",
      "Iteration 1404, loss = 2.76218284\n",
      "Iteration 1405, loss = 2.76245840\n",
      "Iteration 1406, loss = 2.76273100\n",
      "Iteration 1407, loss = 2.76182627\n",
      "Iteration 1408, loss = 2.76141961\n",
      "Iteration 1409, loss = 2.76078498\n",
      "Iteration 1410, loss = 2.76073999\n",
      "Iteration 1411, loss = 2.76015850\n",
      "Iteration 1412, loss = 2.76030301\n",
      "Iteration 1413, loss = 2.76069213\n",
      "Iteration 1414, loss = 2.76201728\n",
      "Iteration 1415, loss = 2.76060286\n",
      "Iteration 1416, loss = 2.76023467\n",
      "Iteration 1417, loss = 2.76024288\n",
      "Iteration 1418, loss = 2.75992467\n",
      "Iteration 1419, loss = 2.76206000\n",
      "Iteration 1420, loss = 2.76209960\n",
      "Iteration 1421, loss = 2.75982252\n",
      "Iteration 1422, loss = 2.75916342\n",
      "Iteration 1423, loss = 2.75909488\n",
      "Iteration 1424, loss = 2.75880778\n",
      "Iteration 1425, loss = 2.75814356\n",
      "Iteration 1426, loss = 2.75903471\n",
      "Iteration 1427, loss = 2.75977397\n",
      "Iteration 1428, loss = 2.75944287\n",
      "Iteration 1429, loss = 2.75780685\n",
      "Iteration 1430, loss = 2.75813907\n",
      "Iteration 1431, loss = 2.75794975\n",
      "Iteration 1432, loss = 2.75824568\n",
      "Iteration 1433, loss = 2.75745062\n",
      "Iteration 1434, loss = 2.75761333\n",
      "Iteration 1435, loss = 2.75727730\n",
      "Iteration 1436, loss = 2.75710377\n",
      "Iteration 1437, loss = 2.75743703\n",
      "Iteration 1438, loss = 2.75760345\n",
      "Iteration 1439, loss = 2.75632062\n",
      "Iteration 1440, loss = 2.75716239\n",
      "Iteration 1441, loss = 2.75723460\n",
      "Iteration 1442, loss = 2.75670713\n",
      "Iteration 1443, loss = 2.75621774\n",
      "Iteration 1444, loss = 2.75574116\n",
      "Iteration 1445, loss = 2.75582021\n",
      "Iteration 1446, loss = 2.75630016\n",
      "Iteration 1447, loss = 2.75645771\n",
      "Iteration 1448, loss = 2.75719160\n",
      "Iteration 1449, loss = 2.75619310\n",
      "Iteration 1450, loss = 2.75556846\n",
      "Iteration 1451, loss = 2.75514870\n",
      "Iteration 1452, loss = 2.75496146\n",
      "Iteration 1453, loss = 2.75588813\n",
      "Iteration 1454, loss = 2.75538276\n",
      "Iteration 1455, loss = 2.75640479\n",
      "Iteration 1456, loss = 2.75544075\n",
      "Iteration 1457, loss = 2.75370073\n",
      "Iteration 1458, loss = 2.75482577\n",
      "Iteration 1459, loss = 2.75411497\n",
      "Iteration 1460, loss = 2.75377940\n",
      "Iteration 1461, loss = 2.75326291\n",
      "Iteration 1462, loss = 2.75349762\n",
      "Iteration 1463, loss = 2.75309366\n",
      "Iteration 1464, loss = 2.75346495\n",
      "Iteration 1465, loss = 2.75318156\n",
      "Iteration 1466, loss = 2.75312524\n",
      "Iteration 1467, loss = 2.75331331\n",
      "Iteration 1468, loss = 2.75320127\n",
      "Iteration 1469, loss = 2.75179913\n",
      "Iteration 1470, loss = 2.75266541\n",
      "Iteration 1471, loss = 2.75219796\n",
      "Iteration 1472, loss = 2.75124682\n",
      "Iteration 1473, loss = 2.75158283\n",
      "Iteration 1474, loss = 2.75163015\n",
      "Iteration 1475, loss = 2.75203378\n",
      "Iteration 1476, loss = 2.75157136\n",
      "Iteration 1477, loss = 2.75104451\n",
      "Iteration 1478, loss = 2.75228734\n",
      "Iteration 1479, loss = 2.75185760\n",
      "Iteration 1480, loss = 2.75144862\n",
      "Iteration 1481, loss = 2.75117052\n",
      "Iteration 1482, loss = 2.75045513\n",
      "Iteration 1483, loss = 2.75188779\n",
      "Iteration 1484, loss = 2.75108987\n",
      "Iteration 1485, loss = 2.75156281\n",
      "Iteration 1486, loss = 2.75040261\n",
      "Iteration 1487, loss = 2.75039174\n",
      "Iteration 1488, loss = 2.74981463\n",
      "Iteration 1489, loss = 2.75002880\n",
      "Iteration 1490, loss = 2.75048384\n",
      "Iteration 1491, loss = 2.75024230\n",
      "Iteration 1492, loss = 2.74994345\n",
      "Iteration 1493, loss = 2.74904953\n",
      "Iteration 1494, loss = 2.74877305\n",
      "Iteration 1495, loss = 2.74910620\n",
      "Iteration 1496, loss = 2.74839076\n",
      "Iteration 1497, loss = 2.74970793\n",
      "Iteration 1498, loss = 2.74847615\n",
      "Iteration 1499, loss = 2.74863104\n",
      "Iteration 1500, loss = 2.74840112\n",
      "Iteration 1501, loss = 2.74796715\n",
      "Iteration 1502, loss = 2.74724506\n",
      "Iteration 1503, loss = 2.74710940\n",
      "Iteration 1504, loss = 2.74806681\n",
      "Iteration 1505, loss = 2.74700965\n",
      "Iteration 1506, loss = 2.74762220\n",
      "Iteration 1507, loss = 2.74708053\n",
      "Iteration 1508, loss = 2.74672510\n",
      "Iteration 1509, loss = 2.74669040\n",
      "Iteration 1510, loss = 2.74643338\n",
      "Iteration 1511, loss = 2.74675628\n",
      "Iteration 1512, loss = 2.74671570\n",
      "Iteration 1513, loss = 2.74688118\n",
      "Iteration 1514, loss = 2.74617830\n",
      "Iteration 1515, loss = 2.74619997\n",
      "Iteration 1516, loss = 2.74552257\n",
      "Iteration 1517, loss = 2.74572722\n",
      "Iteration 1518, loss = 2.74549048\n",
      "Iteration 1519, loss = 2.74557343\n",
      "Iteration 1520, loss = 2.74508878\n",
      "Iteration 1521, loss = 2.74523763\n",
      "Iteration 1522, loss = 2.74556385\n",
      "Iteration 1523, loss = 2.74594409\n",
      "Iteration 1524, loss = 2.74647922\n",
      "Iteration 1525, loss = 2.74519481\n",
      "Iteration 1526, loss = 2.74479485\n",
      "Iteration 1527, loss = 2.74456656\n",
      "Iteration 1528, loss = 2.74429207\n",
      "Iteration 1529, loss = 2.74417701\n",
      "Iteration 1530, loss = 2.74560255\n",
      "Iteration 1531, loss = 2.74464395\n",
      "Iteration 1532, loss = 2.74481731\n",
      "Iteration 1533, loss = 2.74400913\n",
      "Iteration 1534, loss = 2.74375367\n",
      "Iteration 1535, loss = 2.74430011\n",
      "Iteration 1536, loss = 2.74314739\n",
      "Iteration 1537, loss = 2.74285408\n",
      "Iteration 1538, loss = 2.74325539\n",
      "Iteration 1539, loss = 2.74232553\n",
      "Iteration 1540, loss = 2.74334141\n",
      "Iteration 1541, loss = 2.74262917\n",
      "Iteration 1542, loss = 2.74264822\n",
      "Iteration 1543, loss = 2.74278782\n",
      "Iteration 1544, loss = 2.74188280\n",
      "Iteration 1545, loss = 2.74136050\n",
      "Iteration 1546, loss = 2.74167170\n",
      "Iteration 1547, loss = 2.74124172\n",
      "Iteration 1548, loss = 2.74330038\n",
      "Iteration 1549, loss = 2.74355156\n",
      "Iteration 1550, loss = 2.74401620\n",
      "Iteration 1551, loss = 2.74283823\n",
      "Iteration 1552, loss = 2.74095180\n",
      "Iteration 1553, loss = 2.74096521\n",
      "Iteration 1554, loss = 2.74088381\n",
      "Iteration 1555, loss = 2.74161333\n",
      "Iteration 1556, loss = 2.74136135\n",
      "Iteration 1557, loss = 2.74080842\n",
      "Iteration 1558, loss = 2.74080421\n",
      "Iteration 1559, loss = 2.74113272\n",
      "Iteration 1560, loss = 2.74006883\n",
      "Iteration 1561, loss = 2.73907353\n",
      "Iteration 1562, loss = 2.73991553\n",
      "Iteration 1563, loss = 2.73931003\n",
      "Iteration 1564, loss = 2.73944047\n",
      "Iteration 1565, loss = 2.74019899\n",
      "Iteration 1566, loss = 2.74010675\n",
      "Iteration 1567, loss = 2.74035562\n",
      "Iteration 1568, loss = 2.73826055\n",
      "Iteration 1569, loss = 2.73900889\n",
      "Iteration 1570, loss = 2.73852106\n",
      "Iteration 1571, loss = 2.73739334\n",
      "Iteration 1572, loss = 2.73780563\n",
      "Iteration 1573, loss = 2.73771967\n",
      "Iteration 1574, loss = 2.73712529\n",
      "Iteration 1575, loss = 2.73791781\n",
      "Iteration 1576, loss = 2.73759224\n",
      "Iteration 1577, loss = 2.73879065\n",
      "Iteration 1578, loss = 2.73839697\n",
      "Iteration 1579, loss = 2.73827552\n",
      "Iteration 1580, loss = 2.73720744\n",
      "Iteration 1581, loss = 2.73745646\n",
      "Iteration 1582, loss = 2.73792999\n",
      "Iteration 1583, loss = 2.73691757\n",
      "Iteration 1584, loss = 2.73676747\n",
      "Iteration 1585, loss = 2.73649998\n",
      "Iteration 1586, loss = 2.73677275\n",
      "Iteration 1587, loss = 2.73690705\n",
      "Iteration 1588, loss = 2.73619853\n",
      "Iteration 1589, loss = 2.73646958\n",
      "Iteration 1590, loss = 2.73604832\n",
      "Iteration 1591, loss = 2.73553082\n",
      "Iteration 1592, loss = 2.73583210\n",
      "Iteration 1593, loss = 2.73664560\n",
      "Iteration 1594, loss = 2.73743156\n",
      "Iteration 1595, loss = 2.73619753\n",
      "Iteration 1596, loss = 2.73519800\n",
      "Iteration 1597, loss = 2.73602384\n",
      "Iteration 1598, loss = 2.73677256\n",
      "Iteration 1599, loss = 2.73640315\n",
      "Iteration 1600, loss = 2.73525181\n",
      "Iteration 1601, loss = 2.73492731\n",
      "Iteration 1602, loss = 2.73424255\n",
      "Iteration 1603, loss = 2.73411211\n",
      "Iteration 1604, loss = 2.73457383\n",
      "Iteration 1605, loss = 2.73313514\n",
      "Iteration 1606, loss = 2.73338492\n",
      "Iteration 1607, loss = 2.73233845\n",
      "Iteration 1608, loss = 2.73235447\n",
      "Iteration 1609, loss = 2.73330794\n",
      "Iteration 1610, loss = 2.73350403\n",
      "Iteration 1611, loss = 2.73281659\n",
      "Iteration 1612, loss = 2.73294935\n",
      "Iteration 1613, loss = 2.73233212\n",
      "Iteration 1614, loss = 2.73264029\n",
      "Iteration 1615, loss = 2.73236653\n",
      "Iteration 1616, loss = 2.73183968\n",
      "Iteration 1617, loss = 2.73238683\n",
      "Iteration 1618, loss = 2.73221024\n",
      "Iteration 1619, loss = 2.73158289\n",
      "Iteration 1620, loss = 2.73205724\n",
      "Iteration 1621, loss = 2.73210080\n",
      "Iteration 1622, loss = 2.73220613\n",
      "Iteration 1623, loss = 2.73144104\n",
      "Iteration 1624, loss = 2.73157926\n",
      "Iteration 1625, loss = 2.73114263\n",
      "Iteration 1626, loss = 2.73129824\n",
      "Iteration 1627, loss = 2.73269957\n",
      "Iteration 1628, loss = 2.73133038\n",
      "Iteration 1629, loss = 2.72986441\n",
      "Iteration 1630, loss = 2.73020041\n",
      "Iteration 1631, loss = 2.73041972\n",
      "Iteration 1632, loss = 2.72956010\n",
      "Iteration 1633, loss = 2.72954375\n",
      "Iteration 1634, loss = 2.72977361\n",
      "Iteration 1635, loss = 2.72953871\n",
      "Iteration 1636, loss = 2.72982264\n",
      "Iteration 1637, loss = 2.72940061\n",
      "Iteration 1638, loss = 2.72877407\n",
      "Iteration 1639, loss = 2.72992952\n",
      "Iteration 1640, loss = 2.72872296\n",
      "Iteration 1641, loss = 2.72864494\n",
      "Iteration 1642, loss = 2.72772604\n",
      "Iteration 1643, loss = 2.72814923\n",
      "Iteration 1644, loss = 2.72809218\n",
      "Iteration 1645, loss = 2.72783234\n",
      "Iteration 1646, loss = 2.72835150\n",
      "Iteration 1647, loss = 2.72825209\n",
      "Iteration 1648, loss = 2.72696403\n",
      "Iteration 1649, loss = 2.73179525\n",
      "Iteration 1650, loss = 2.72887267\n",
      "Iteration 1651, loss = 2.72767199\n",
      "Iteration 1652, loss = 2.72853730\n",
      "Iteration 1653, loss = 2.72896608\n",
      "Iteration 1654, loss = 2.73039952\n",
      "Iteration 1655, loss = 2.72880310\n",
      "Iteration 1656, loss = 2.72691629\n",
      "Iteration 1657, loss = 2.72798162\n",
      "Iteration 1658, loss = 2.72700544\n",
      "Iteration 1659, loss = 2.72621450\n",
      "Iteration 1660, loss = 2.72551633\n",
      "Iteration 1661, loss = 2.72560852\n",
      "Iteration 1662, loss = 2.72532600\n",
      "Iteration 1663, loss = 2.72606928\n",
      "Iteration 1664, loss = 2.72592384\n",
      "Iteration 1665, loss = 2.72568791\n",
      "Iteration 1666, loss = 2.72561146\n",
      "Iteration 1667, loss = 2.72570716\n",
      "Iteration 1668, loss = 2.72695471\n",
      "Iteration 1669, loss = 2.72709756\n",
      "Iteration 1670, loss = 2.72561844\n",
      "Iteration 1671, loss = 2.72457370\n",
      "Iteration 1672, loss = 2.72455132\n",
      "Iteration 1673, loss = 2.72503957\n",
      "Iteration 1674, loss = 2.72568122\n",
      "Iteration 1675, loss = 2.72561657\n",
      "Iteration 1676, loss = 2.72677894\n",
      "Iteration 1677, loss = 2.72394794\n",
      "Iteration 1678, loss = 2.72306829\n",
      "Iteration 1679, loss = 2.72369335\n",
      "Iteration 1680, loss = 2.72341902\n",
      "Iteration 1681, loss = 2.72553444\n",
      "Iteration 1682, loss = 2.72507711\n",
      "Iteration 1683, loss = 2.72360106\n",
      "Iteration 1684, loss = 2.72276785\n",
      "Iteration 1, loss = 4.14650867\n",
      "Iteration 2, loss = 4.01911454\n",
      "Iteration 3, loss = 3.89786748\n",
      "Iteration 4, loss = 3.78476629\n",
      "Iteration 5, loss = 3.67726470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1684) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 3.57546967\n",
      "Iteration 7, loss = 3.48066247\n",
      "Iteration 8, loss = 3.39833067\n",
      "Iteration 9, loss = 3.33227816\n",
      "Iteration 10, loss = 3.28601356\n",
      "Iteration 11, loss = 3.25544112\n",
      "Iteration 12, loss = 3.24126686\n",
      "Iteration 13, loss = 3.23504738\n",
      "Iteration 14, loss = 3.23357668\n",
      "Iteration 15, loss = 3.23218575\n",
      "Iteration 16, loss = 3.23023197\n",
      "Iteration 17, loss = 3.22932368\n",
      "Iteration 18, loss = 3.22792999\n",
      "Iteration 19, loss = 3.22722761\n",
      "Iteration 20, loss = 3.22642890\n",
      "Iteration 21, loss = 3.22576619\n",
      "Iteration 22, loss = 3.22509901\n",
      "Iteration 23, loss = 3.22460669\n",
      "Iteration 24, loss = 3.22409610\n",
      "Iteration 25, loss = 3.22337304\n",
      "Iteration 26, loss = 3.22269678\n",
      "Iteration 27, loss = 3.22203310\n",
      "Iteration 28, loss = 3.22136555\n",
      "Iteration 29, loss = 3.22098914\n",
      "Iteration 30, loss = 3.22015690\n",
      "Iteration 31, loss = 3.22026283\n",
      "Iteration 32, loss = 3.21987504\n",
      "Iteration 33, loss = 3.21925390\n",
      "Iteration 34, loss = 3.21897894\n",
      "Iteration 35, loss = 3.21882518\n",
      "Iteration 36, loss = 3.21866630\n",
      "Iteration 37, loss = 3.21798677\n",
      "Iteration 38, loss = 3.21734534\n",
      "Iteration 39, loss = 3.21730700\n",
      "Iteration 40, loss = 3.21744711\n",
      "Iteration 41, loss = 3.21768584\n",
      "Iteration 42, loss = 3.21652632\n",
      "Iteration 43, loss = 3.21601943\n",
      "Iteration 44, loss = 3.21620241\n",
      "Iteration 45, loss = 3.21552299\n",
      "Iteration 46, loss = 3.21448111\n",
      "Iteration 47, loss = 3.21442263\n",
      "Iteration 48, loss = 3.21342856\n",
      "Iteration 49, loss = 3.21391243\n",
      "Iteration 50, loss = 3.21303233\n",
      "Iteration 51, loss = 3.21297478\n",
      "Iteration 52, loss = 3.21281867\n",
      "Iteration 53, loss = 3.21260159\n",
      "Iteration 54, loss = 3.21253935\n",
      "Iteration 55, loss = 3.21224468\n",
      "Iteration 56, loss = 3.21162395\n",
      "Iteration 57, loss = 3.21066119\n",
      "Iteration 58, loss = 3.20968802\n",
      "Iteration 59, loss = 3.20966917\n",
      "Iteration 60, loss = 3.20938280\n",
      "Iteration 61, loss = 3.20874748\n",
      "Iteration 62, loss = 3.20848654\n",
      "Iteration 63, loss = 3.20838620\n",
      "Iteration 64, loss = 3.20733860\n",
      "Iteration 65, loss = 3.20823733\n",
      "Iteration 66, loss = 3.20821733\n",
      "Iteration 67, loss = 3.20757646\n",
      "Iteration 68, loss = 3.20702801\n",
      "Iteration 69, loss = 3.20540797\n",
      "Iteration 70, loss = 3.20514611\n",
      "Iteration 71, loss = 3.20567240\n",
      "Iteration 72, loss = 3.20547808\n",
      "Iteration 73, loss = 3.20400397\n",
      "Iteration 74, loss = 3.20463044\n",
      "Iteration 75, loss = 3.20467613\n",
      "Iteration 76, loss = 3.20386812\n",
      "Iteration 77, loss = 3.20386382\n",
      "Iteration 78, loss = 3.20347363\n",
      "Iteration 79, loss = 3.20310129\n",
      "Iteration 80, loss = 3.20228051\n",
      "Iteration 81, loss = 3.20093123\n",
      "Iteration 82, loss = 3.20095859\n",
      "Iteration 83, loss = 3.20051338\n",
      "Iteration 84, loss = 3.20006012\n",
      "Iteration 85, loss = 3.19992444\n",
      "Iteration 86, loss = 3.19945222\n",
      "Iteration 87, loss = 3.19917660\n",
      "Iteration 88, loss = 3.19896768\n",
      "Iteration 89, loss = 3.19922798\n",
      "Iteration 90, loss = 3.19860507\n",
      "Iteration 91, loss = 3.19800880\n",
      "Iteration 92, loss = 3.19777197\n",
      "Iteration 93, loss = 3.19733743\n",
      "Iteration 94, loss = 3.19684311\n",
      "Iteration 95, loss = 3.19624568\n",
      "Iteration 96, loss = 3.19579129\n",
      "Iteration 97, loss = 3.19566171\n",
      "Iteration 98, loss = 3.19521703\n",
      "Iteration 99, loss = 3.19456401\n",
      "Iteration 100, loss = 3.19391537\n",
      "Iteration 101, loss = 3.19352337\n",
      "Iteration 102, loss = 3.19285020\n",
      "Iteration 103, loss = 3.19246380\n",
      "Iteration 104, loss = 3.19214204\n",
      "Iteration 105, loss = 3.19172264\n",
      "Iteration 106, loss = 3.19127053\n",
      "Iteration 107, loss = 3.19159818\n",
      "Iteration 108, loss = 3.19164030\n",
      "Iteration 109, loss = 3.19079552\n",
      "Iteration 110, loss = 3.19035683\n",
      "Iteration 111, loss = 3.19029711\n",
      "Iteration 112, loss = 3.19019638\n",
      "Iteration 113, loss = 3.18977238\n",
      "Iteration 114, loss = 3.18858774\n",
      "Iteration 115, loss = 3.18811824\n",
      "Iteration 116, loss = 3.18765320\n",
      "Iteration 117, loss = 3.18710927\n",
      "Iteration 118, loss = 3.18643453\n",
      "Iteration 119, loss = 3.18584779\n",
      "Iteration 120, loss = 3.18489798\n",
      "Iteration 121, loss = 3.18526992\n",
      "Iteration 122, loss = 3.18438434\n",
      "Iteration 123, loss = 3.18427814\n",
      "Iteration 124, loss = 3.18335927\n",
      "Iteration 125, loss = 3.18280585\n",
      "Iteration 126, loss = 3.18240010\n",
      "Iteration 127, loss = 3.18248935\n",
      "Iteration 128, loss = 3.18244554\n",
      "Iteration 129, loss = 3.18118196\n",
      "Iteration 130, loss = 3.18062562\n",
      "Iteration 131, loss = 3.18035992\n",
      "Iteration 132, loss = 3.17940418\n",
      "Iteration 133, loss = 3.17927182\n",
      "Iteration 134, loss = 3.17870579\n",
      "Iteration 135, loss = 3.17823354\n",
      "Iteration 136, loss = 3.17750896\n",
      "Iteration 137, loss = 3.17704750\n",
      "Iteration 138, loss = 3.17643705\n",
      "Iteration 139, loss = 3.17578830\n",
      "Iteration 140, loss = 3.17580604\n",
      "Iteration 141, loss = 3.17511759\n",
      "Iteration 142, loss = 3.17471532\n",
      "Iteration 143, loss = 3.17401897\n",
      "Iteration 144, loss = 3.17321135\n",
      "Iteration 145, loss = 3.17262047\n",
      "Iteration 146, loss = 3.17273628\n",
      "Iteration 147, loss = 3.17121434\n",
      "Iteration 148, loss = 3.17096727\n",
      "Iteration 149, loss = 3.17037322\n",
      "Iteration 150, loss = 3.16981378\n",
      "Iteration 151, loss = 3.16912790\n",
      "Iteration 152, loss = 3.16895358\n",
      "Iteration 153, loss = 3.16864621\n",
      "Iteration 154, loss = 3.16824652\n",
      "Iteration 155, loss = 3.16718957\n",
      "Iteration 156, loss = 3.16687411\n",
      "Iteration 157, loss = 3.16620612\n",
      "Iteration 158, loss = 3.16558112\n",
      "Iteration 159, loss = 3.16513423\n",
      "Iteration 160, loss = 3.16523474\n",
      "Iteration 161, loss = 3.16427657\n",
      "Iteration 162, loss = 3.16414347\n",
      "Iteration 163, loss = 3.16329354\n",
      "Iteration 164, loss = 3.16321618\n",
      "Iteration 165, loss = 3.16226152\n",
      "Iteration 166, loss = 3.16162314\n",
      "Iteration 167, loss = 3.16124335\n",
      "Iteration 168, loss = 3.16058539\n",
      "Iteration 169, loss = 3.15954580\n",
      "Iteration 170, loss = 3.15963736\n",
      "Iteration 171, loss = 3.15853245\n",
      "Iteration 172, loss = 3.15807146\n",
      "Iteration 173, loss = 3.15728526\n",
      "Iteration 174, loss = 3.15685327\n",
      "Iteration 175, loss = 3.15617035\n",
      "Iteration 176, loss = 3.15554803\n",
      "Iteration 177, loss = 3.15458532\n",
      "Iteration 178, loss = 3.15406428\n",
      "Iteration 179, loss = 3.15400034\n",
      "Iteration 180, loss = 3.15387589\n",
      "Iteration 181, loss = 3.15320491\n",
      "Iteration 182, loss = 3.15244828\n",
      "Iteration 183, loss = 3.15181130\n",
      "Iteration 184, loss = 3.15116351\n",
      "Iteration 185, loss = 3.15068645\n",
      "Iteration 186, loss = 3.15044015\n",
      "Iteration 187, loss = 3.15005453\n",
      "Iteration 188, loss = 3.14930120\n",
      "Iteration 189, loss = 3.14817061\n",
      "Iteration 190, loss = 3.14750241\n",
      "Iteration 191, loss = 3.14702699\n",
      "Iteration 192, loss = 3.14633722\n",
      "Iteration 193, loss = 3.14572065\n",
      "Iteration 194, loss = 3.14573212\n",
      "Iteration 195, loss = 3.14453388\n",
      "Iteration 196, loss = 3.14452634\n",
      "Iteration 197, loss = 3.14302666\n",
      "Iteration 198, loss = 3.14289572\n",
      "Iteration 199, loss = 3.14200438\n",
      "Iteration 200, loss = 3.14091402\n",
      "Iteration 201, loss = 3.14026791\n",
      "Iteration 202, loss = 3.13985022\n",
      "Iteration 203, loss = 3.13928192\n",
      "Iteration 204, loss = 3.13910289\n",
      "Iteration 205, loss = 3.13833675\n",
      "Iteration 206, loss = 3.13779245\n",
      "Iteration 207, loss = 3.13688917\n",
      "Iteration 208, loss = 3.13628600\n",
      "Iteration 209, loss = 3.13577772\n",
      "Iteration 210, loss = 3.13491114\n",
      "Iteration 211, loss = 3.13408314\n",
      "Iteration 212, loss = 3.13318311\n",
      "Iteration 213, loss = 3.13308622\n",
      "Iteration 214, loss = 3.13306136\n",
      "Iteration 215, loss = 3.13170836\n",
      "Iteration 216, loss = 3.13109083\n",
      "Iteration 217, loss = 3.13010576\n",
      "Iteration 218, loss = 3.12963009\n",
      "Iteration 219, loss = 3.12872321\n",
      "Iteration 220, loss = 3.12804576\n",
      "Iteration 221, loss = 3.12794950\n",
      "Iteration 222, loss = 3.12821359\n",
      "Iteration 223, loss = 3.12669703\n",
      "Iteration 224, loss = 3.12595674\n",
      "Iteration 225, loss = 3.12502568\n",
      "Iteration 226, loss = 3.12441003\n",
      "Iteration 227, loss = 3.12375138\n",
      "Iteration 228, loss = 3.12258583\n",
      "Iteration 229, loss = 3.12237918\n",
      "Iteration 230, loss = 3.12156785\n",
      "Iteration 231, loss = 3.12128014\n",
      "Iteration 232, loss = 3.12059731\n",
      "Iteration 233, loss = 3.11993892\n",
      "Iteration 234, loss = 3.11926044\n",
      "Iteration 235, loss = 3.11811508\n",
      "Iteration 236, loss = 3.11849666\n",
      "Iteration 237, loss = 3.11793017\n",
      "Iteration 238, loss = 3.11667275\n",
      "Iteration 239, loss = 3.11586144\n",
      "Iteration 240, loss = 3.11521722\n",
      "Iteration 241, loss = 3.11430736\n",
      "Iteration 242, loss = 3.11385687\n",
      "Iteration 243, loss = 3.11334640\n",
      "Iteration 244, loss = 3.11281415\n",
      "Iteration 245, loss = 3.11211790\n",
      "Iteration 246, loss = 3.11155211\n",
      "Iteration 247, loss = 3.11063496\n",
      "Iteration 248, loss = 3.10994334\n",
      "Iteration 249, loss = 3.10906465\n",
      "Iteration 250, loss = 3.10893376\n",
      "Iteration 251, loss = 3.10821769\n",
      "Iteration 252, loss = 3.10777988\n",
      "Iteration 253, loss = 3.10609497\n",
      "Iteration 254, loss = 3.10601598\n",
      "Iteration 255, loss = 3.10543256\n",
      "Iteration 256, loss = 3.10458524\n",
      "Iteration 257, loss = 3.10410719\n",
      "Iteration 258, loss = 3.10345044\n",
      "Iteration 259, loss = 3.10241712\n",
      "Iteration 260, loss = 3.10173213\n",
      "Iteration 261, loss = 3.10083873\n",
      "Iteration 262, loss = 3.10013006\n",
      "Iteration 263, loss = 3.10027367\n",
      "Iteration 264, loss = 3.10018516\n",
      "Iteration 265, loss = 3.09985157\n",
      "Iteration 266, loss = 3.09792876\n",
      "Iteration 267, loss = 3.09762752\n",
      "Iteration 268, loss = 3.09691733\n",
      "Iteration 269, loss = 3.09587852\n",
      "Iteration 270, loss = 3.09504767\n",
      "Iteration 271, loss = 3.09483357\n",
      "Iteration 272, loss = 3.09378545\n",
      "Iteration 273, loss = 3.09346615\n",
      "Iteration 274, loss = 3.09275748\n",
      "Iteration 275, loss = 3.09259702\n",
      "Iteration 276, loss = 3.09156553\n",
      "Iteration 277, loss = 3.09127169\n",
      "Iteration 278, loss = 3.09052938\n",
      "Iteration 279, loss = 3.08946690\n",
      "Iteration 280, loss = 3.08877199\n",
      "Iteration 281, loss = 3.08802193\n",
      "Iteration 282, loss = 3.08722455\n",
      "Iteration 283, loss = 3.08671170\n",
      "Iteration 284, loss = 3.08550770\n",
      "Iteration 285, loss = 3.08507831\n",
      "Iteration 286, loss = 3.08391965\n",
      "Iteration 287, loss = 3.08290314\n",
      "Iteration 288, loss = 3.08220166\n",
      "Iteration 289, loss = 3.08227771\n",
      "Iteration 290, loss = 3.08238306\n",
      "Iteration 291, loss = 3.08157040\n",
      "Iteration 292, loss = 3.08007056\n",
      "Iteration 293, loss = 3.07966984\n",
      "Iteration 294, loss = 3.07909541\n",
      "Iteration 295, loss = 3.07858766\n",
      "Iteration 296, loss = 3.07717635\n",
      "Iteration 297, loss = 3.07646794\n",
      "Iteration 298, loss = 3.07598730\n",
      "Iteration 299, loss = 3.07490385\n",
      "Iteration 300, loss = 3.07429792\n",
      "Iteration 301, loss = 3.07379960\n",
      "Iteration 302, loss = 3.07301633\n",
      "Iteration 303, loss = 3.07266233\n",
      "Iteration 304, loss = 3.07148025\n",
      "Iteration 305, loss = 3.07136573\n",
      "Iteration 306, loss = 3.07055200\n",
      "Iteration 307, loss = 3.07011558\n",
      "Iteration 308, loss = 3.06949260\n",
      "Iteration 309, loss = 3.06895058\n",
      "Iteration 310, loss = 3.06839148\n",
      "Iteration 311, loss = 3.06841086\n",
      "Iteration 312, loss = 3.06684976\n",
      "Iteration 313, loss = 3.06632560\n",
      "Iteration 314, loss = 3.06547387\n",
      "Iteration 315, loss = 3.06574433\n",
      "Iteration 316, loss = 3.06476945\n",
      "Iteration 317, loss = 3.06397601\n",
      "Iteration 318, loss = 3.06413019\n",
      "Iteration 319, loss = 3.06339063\n",
      "Iteration 320, loss = 3.06190701\n",
      "Iteration 321, loss = 3.06112052\n",
      "Iteration 322, loss = 3.06066882\n",
      "Iteration 323, loss = 3.06023000\n",
      "Iteration 324, loss = 3.05993992\n",
      "Iteration 325, loss = 3.05956653\n",
      "Iteration 326, loss = 3.05978080\n",
      "Iteration 327, loss = 3.05869442\n",
      "Iteration 328, loss = 3.05723843\n",
      "Iteration 329, loss = 3.05759699\n",
      "Iteration 330, loss = 3.05629103\n",
      "Iteration 331, loss = 3.05578722\n",
      "Iteration 332, loss = 3.05471795\n",
      "Iteration 333, loss = 3.05432854\n",
      "Iteration 334, loss = 3.05345127\n",
      "Iteration 335, loss = 3.05355354\n",
      "Iteration 336, loss = 3.05264838\n",
      "Iteration 337, loss = 3.05208892\n",
      "Iteration 338, loss = 3.05141731\n",
      "Iteration 339, loss = 3.05110904\n",
      "Iteration 340, loss = 3.04983557\n",
      "Iteration 341, loss = 3.04958686\n",
      "Iteration 342, loss = 3.04831438\n",
      "Iteration 343, loss = 3.04778997\n",
      "Iteration 344, loss = 3.04698811\n",
      "Iteration 345, loss = 3.04630313\n",
      "Iteration 346, loss = 3.04617740\n",
      "Iteration 347, loss = 3.04522555\n",
      "Iteration 348, loss = 3.04468667\n",
      "Iteration 349, loss = 3.04435994\n",
      "Iteration 350, loss = 3.04363184\n",
      "Iteration 351, loss = 3.04269497\n",
      "Iteration 352, loss = 3.04240578\n",
      "Iteration 353, loss = 3.04231585\n",
      "Iteration 354, loss = 3.04170020\n",
      "Iteration 355, loss = 3.04101528\n",
      "Iteration 356, loss = 3.04021990\n",
      "Iteration 357, loss = 3.03917173\n",
      "Iteration 358, loss = 3.03836985\n",
      "Iteration 359, loss = 3.03821497\n",
      "Iteration 360, loss = 3.03830802\n",
      "Iteration 361, loss = 3.03833343\n",
      "Iteration 362, loss = 3.03638647\n",
      "Iteration 363, loss = 3.03570596\n",
      "Iteration 364, loss = 3.03498243\n",
      "Iteration 365, loss = 3.03396616\n",
      "Iteration 366, loss = 3.03421131\n",
      "Iteration 367, loss = 3.03319573\n",
      "Iteration 368, loss = 3.03266771\n",
      "Iteration 369, loss = 3.03192464\n",
      "Iteration 370, loss = 3.03154402\n",
      "Iteration 371, loss = 3.03071930\n",
      "Iteration 372, loss = 3.03041884\n",
      "Iteration 373, loss = 3.02976711\n",
      "Iteration 374, loss = 3.02928919\n",
      "Iteration 375, loss = 3.02856876\n",
      "Iteration 376, loss = 3.02830828\n",
      "Iteration 377, loss = 3.02809637\n",
      "Iteration 378, loss = 3.02677033\n",
      "Iteration 379, loss = 3.02623419\n",
      "Iteration 380, loss = 3.02564828\n",
      "Iteration 381, loss = 3.02492160\n",
      "Iteration 382, loss = 3.02557355\n",
      "Iteration 383, loss = 3.02497713\n",
      "Iteration 384, loss = 3.02450214\n",
      "Iteration 385, loss = 3.02275717\n",
      "Iteration 386, loss = 3.02230763\n",
      "Iteration 387, loss = 3.02168471\n",
      "Iteration 388, loss = 3.02110526\n",
      "Iteration 389, loss = 3.02020657\n",
      "Iteration 390, loss = 3.01984216\n",
      "Iteration 391, loss = 3.01976637\n",
      "Iteration 392, loss = 3.01837516\n",
      "Iteration 393, loss = 3.01826578\n",
      "Iteration 394, loss = 3.01791234\n",
      "Iteration 395, loss = 3.01685634\n",
      "Iteration 396, loss = 3.01700988\n",
      "Iteration 397, loss = 3.01654339\n",
      "Iteration 398, loss = 3.01578224\n",
      "Iteration 399, loss = 3.01480825\n",
      "Iteration 400, loss = 3.01510948\n",
      "Iteration 401, loss = 3.01431144\n",
      "Iteration 402, loss = 3.01416722\n",
      "Iteration 403, loss = 3.01311440\n",
      "Iteration 404, loss = 3.01288800\n",
      "Iteration 405, loss = 3.01185990\n",
      "Iteration 406, loss = 3.01092477\n",
      "Iteration 407, loss = 3.01082054\n",
      "Iteration 408, loss = 3.01003589\n",
      "Iteration 409, loss = 3.00948956\n",
      "Iteration 410, loss = 3.01014741\n",
      "Iteration 411, loss = 3.00923477\n",
      "Iteration 412, loss = 3.00750269\n",
      "Iteration 413, loss = 3.00726295\n",
      "Iteration 414, loss = 3.00760594\n",
      "Iteration 415, loss = 3.00713911\n",
      "Iteration 416, loss = 3.00594416\n",
      "Iteration 417, loss = 3.00557817\n",
      "Iteration 418, loss = 3.00489665\n",
      "Iteration 419, loss = 3.00435586\n",
      "Iteration 420, loss = 3.00378479\n",
      "Iteration 421, loss = 3.00313696\n",
      "Iteration 422, loss = 3.00347950\n",
      "Iteration 423, loss = 3.00242361\n",
      "Iteration 424, loss = 3.00330359\n",
      "Iteration 425, loss = 3.00221311\n",
      "Iteration 426, loss = 3.00172497\n",
      "Iteration 427, loss = 3.00043559\n",
      "Iteration 428, loss = 3.00052949\n",
      "Iteration 429, loss = 2.99972379\n",
      "Iteration 430, loss = 2.99967841\n",
      "Iteration 431, loss = 2.99897494\n",
      "Iteration 432, loss = 2.99866815\n",
      "Iteration 433, loss = 2.99755368\n",
      "Iteration 434, loss = 2.99625694\n",
      "Iteration 435, loss = 2.99615036\n",
      "Iteration 436, loss = 2.99564848\n",
      "Iteration 437, loss = 2.99535632\n",
      "Iteration 438, loss = 2.99501191\n",
      "Iteration 439, loss = 2.99460990\n",
      "Iteration 440, loss = 2.99367968\n",
      "Iteration 441, loss = 2.99384928\n",
      "Iteration 442, loss = 2.99339986\n",
      "Iteration 443, loss = 2.99239510\n",
      "Iteration 444, loss = 2.99244218\n",
      "Iteration 445, loss = 2.99171359\n",
      "Iteration 446, loss = 2.99048864\n",
      "Iteration 447, loss = 2.99003456\n",
      "Iteration 448, loss = 2.98975867\n",
      "Iteration 449, loss = 2.98989887\n",
      "Iteration 450, loss = 2.98885748\n",
      "Iteration 451, loss = 2.98880606\n",
      "Iteration 452, loss = 2.98835700\n",
      "Iteration 453, loss = 2.98726578\n",
      "Iteration 454, loss = 2.98664027\n",
      "Iteration 455, loss = 2.98653059\n",
      "Iteration 456, loss = 2.98629848\n",
      "Iteration 457, loss = 2.98574867\n",
      "Iteration 458, loss = 2.98546992\n",
      "Iteration 459, loss = 2.98462003\n",
      "Iteration 460, loss = 2.98393860\n",
      "Iteration 461, loss = 2.98367571\n",
      "Iteration 462, loss = 2.98283247\n",
      "Iteration 463, loss = 2.98288720\n",
      "Iteration 464, loss = 2.98219276\n",
      "Iteration 465, loss = 2.98187038\n",
      "Iteration 466, loss = 2.98251814\n",
      "Iteration 467, loss = 2.98208245\n",
      "Iteration 468, loss = 2.98052417\n",
      "Iteration 469, loss = 2.97998739\n",
      "Iteration 470, loss = 2.97871439\n",
      "Iteration 471, loss = 2.97817580\n",
      "Iteration 472, loss = 2.97880990\n",
      "Iteration 473, loss = 2.97810340\n",
      "Iteration 474, loss = 2.97716266\n",
      "Iteration 475, loss = 2.97626623\n",
      "Iteration 476, loss = 2.97625311\n",
      "Iteration 477, loss = 2.97622688\n",
      "Iteration 478, loss = 2.97586488\n",
      "Iteration 479, loss = 2.97564634\n",
      "Iteration 480, loss = 2.97461404\n",
      "Iteration 481, loss = 2.97353341\n",
      "Iteration 482, loss = 2.97307949\n",
      "Iteration 483, loss = 2.97374582\n",
      "Iteration 484, loss = 2.97318997\n",
      "Iteration 485, loss = 2.97270895\n",
      "Iteration 486, loss = 2.97196581\n",
      "Iteration 487, loss = 2.97164187\n",
      "Iteration 488, loss = 2.97110692\n",
      "Iteration 489, loss = 2.97061484\n",
      "Iteration 490, loss = 2.97001206\n",
      "Iteration 491, loss = 2.97008269\n",
      "Iteration 492, loss = 2.96940254\n",
      "Iteration 493, loss = 2.96928747\n",
      "Iteration 494, loss = 2.96927797\n",
      "Iteration 495, loss = 2.96797982\n",
      "Iteration 496, loss = 2.96717565\n",
      "Iteration 497, loss = 2.96684624\n",
      "Iteration 498, loss = 2.96576559\n",
      "Iteration 499, loss = 2.96550829\n",
      "Iteration 500, loss = 2.96605003\n",
      "Iteration 501, loss = 2.96525919\n",
      "Iteration 502, loss = 2.96585973\n",
      "Iteration 503, loss = 2.96599095\n",
      "Iteration 504, loss = 2.96377047\n",
      "Iteration 505, loss = 2.96465333\n",
      "Iteration 506, loss = 2.96452061\n",
      "Iteration 507, loss = 2.96295472\n",
      "Iteration 508, loss = 2.96207256\n",
      "Iteration 509, loss = 2.96164775\n",
      "Iteration 510, loss = 2.96054200\n",
      "Iteration 511, loss = 2.96033988\n",
      "Iteration 512, loss = 2.96098397\n",
      "Iteration 513, loss = 2.96095458\n",
      "Iteration 514, loss = 2.95991935\n",
      "Iteration 515, loss = 2.95883821\n",
      "Iteration 516, loss = 2.95850447\n",
      "Iteration 517, loss = 2.95807274\n",
      "Iteration 518, loss = 2.95845800\n",
      "Iteration 519, loss = 2.95723176\n",
      "Iteration 520, loss = 2.95743542\n",
      "Iteration 521, loss = 2.95695143\n",
      "Iteration 522, loss = 2.95571401\n",
      "Iteration 523, loss = 2.95598379\n",
      "Iteration 524, loss = 2.95504522\n",
      "Iteration 525, loss = 2.95444306\n",
      "Iteration 526, loss = 2.95404179\n",
      "Iteration 527, loss = 2.95434375\n",
      "Iteration 528, loss = 2.95388025\n",
      "Iteration 529, loss = 2.95366010\n",
      "Iteration 530, loss = 2.95264180\n",
      "Iteration 531, loss = 2.95255942\n",
      "Iteration 532, loss = 2.95174938\n",
      "Iteration 533, loss = 2.95090820\n",
      "Iteration 534, loss = 2.95098684\n",
      "Iteration 535, loss = 2.95054457\n",
      "Iteration 536, loss = 2.94954362\n",
      "Iteration 537, loss = 2.95055802\n",
      "Iteration 538, loss = 2.95023953\n",
      "Iteration 539, loss = 2.94938181\n",
      "Iteration 540, loss = 2.94862690\n",
      "Iteration 541, loss = 2.94788243\n",
      "Iteration 542, loss = 2.94773846\n",
      "Iteration 543, loss = 2.94740570\n",
      "Iteration 544, loss = 2.94678768\n",
      "Iteration 545, loss = 2.94722213\n",
      "Iteration 546, loss = 2.94614877\n",
      "Iteration 547, loss = 2.94664297\n",
      "Iteration 548, loss = 2.94537575\n",
      "Iteration 549, loss = 2.94412365\n",
      "Iteration 550, loss = 2.94419466\n",
      "Iteration 551, loss = 2.94393290\n",
      "Iteration 552, loss = 2.94370424\n",
      "Iteration 553, loss = 2.94363607\n",
      "Iteration 554, loss = 2.94308824\n",
      "Iteration 555, loss = 2.94245185\n",
      "Iteration 556, loss = 2.94234197\n",
      "Iteration 557, loss = 2.94160705\n",
      "Iteration 558, loss = 2.94158592\n",
      "Iteration 559, loss = 2.94150790\n",
      "Iteration 560, loss = 2.94074507\n",
      "Iteration 561, loss = 2.94086680\n",
      "Iteration 562, loss = 2.93957117\n",
      "Iteration 563, loss = 2.93980109\n",
      "Iteration 564, loss = 2.93959237\n",
      "Iteration 565, loss = 2.93866065\n",
      "Iteration 566, loss = 2.93825181\n",
      "Iteration 567, loss = 2.93752837\n",
      "Iteration 568, loss = 2.93683076\n",
      "Iteration 569, loss = 2.93638810\n",
      "Iteration 570, loss = 2.93581156\n",
      "Iteration 571, loss = 2.93528140\n",
      "Iteration 572, loss = 2.93491900\n",
      "Iteration 573, loss = 2.93479927\n",
      "Iteration 574, loss = 2.93450242\n",
      "Iteration 575, loss = 2.93348869\n",
      "Iteration 576, loss = 2.93350228\n",
      "Iteration 577, loss = 2.93357801\n",
      "Iteration 578, loss = 2.93316455\n",
      "Iteration 579, loss = 2.93337821\n",
      "Iteration 580, loss = 2.93375783\n",
      "Iteration 581, loss = 2.93235384\n",
      "Iteration 582, loss = 2.93203822\n",
      "Iteration 583, loss = 2.93136981\n",
      "Iteration 584, loss = 2.93147668\n",
      "Iteration 585, loss = 2.93095462\n",
      "Iteration 586, loss = 2.93059640\n",
      "Iteration 587, loss = 2.92997541\n",
      "Iteration 588, loss = 2.92907474\n",
      "Iteration 589, loss = 2.92843954\n",
      "Iteration 590, loss = 2.92819067\n",
      "Iteration 591, loss = 2.92815536\n",
      "Iteration 592, loss = 2.92723464\n",
      "Iteration 593, loss = 2.92685364\n",
      "Iteration 594, loss = 2.92733579\n",
      "Iteration 595, loss = 2.92713021\n",
      "Iteration 596, loss = 2.92607254\n",
      "Iteration 597, loss = 2.92595516\n",
      "Iteration 598, loss = 2.92780060\n",
      "Iteration 599, loss = 2.92582412\n",
      "Iteration 600, loss = 2.92347906\n",
      "Iteration 601, loss = 2.92390773\n",
      "Iteration 602, loss = 2.92333874\n",
      "Iteration 603, loss = 2.92345732\n",
      "Iteration 604, loss = 2.92411971\n",
      "Iteration 605, loss = 2.92356655\n",
      "Iteration 606, loss = 2.92348129\n",
      "Iteration 607, loss = 2.92135952\n",
      "Iteration 608, loss = 2.92137143\n",
      "Iteration 609, loss = 2.92042730\n",
      "Iteration 610, loss = 2.92022343\n",
      "Iteration 611, loss = 2.92011195\n",
      "Iteration 612, loss = 2.92008002\n",
      "Iteration 613, loss = 2.91984077\n",
      "Iteration 614, loss = 2.91859523\n",
      "Iteration 615, loss = 2.91926955\n",
      "Iteration 616, loss = 2.91922153\n",
      "Iteration 617, loss = 2.91813969\n",
      "Iteration 618, loss = 2.91783517\n",
      "Iteration 619, loss = 2.91724525\n",
      "Iteration 620, loss = 2.91621959\n",
      "Iteration 621, loss = 2.91855277\n",
      "Iteration 622, loss = 2.92042371\n",
      "Iteration 623, loss = 2.91755373\n",
      "Iteration 624, loss = 2.91746003\n",
      "Iteration 625, loss = 2.91628023\n",
      "Iteration 626, loss = 2.91505170\n",
      "Iteration 627, loss = 2.91509309\n",
      "Iteration 628, loss = 2.91408223\n",
      "Iteration 629, loss = 2.91369515\n",
      "Iteration 630, loss = 2.91361055\n",
      "Iteration 631, loss = 2.91365186\n",
      "Iteration 632, loss = 2.91301383\n",
      "Iteration 633, loss = 2.91329707\n",
      "Iteration 634, loss = 2.91295278\n",
      "Iteration 635, loss = 2.91175726\n",
      "Iteration 636, loss = 2.91093272\n",
      "Iteration 637, loss = 2.91109493\n",
      "Iteration 638, loss = 2.91215712\n",
      "Iteration 639, loss = 2.91227527\n",
      "Iteration 640, loss = 2.91033954\n",
      "Iteration 641, loss = 2.90932283\n",
      "Iteration 642, loss = 2.91004168\n",
      "Iteration 643, loss = 2.90888208\n",
      "Iteration 644, loss = 2.90812508\n",
      "Iteration 645, loss = 2.90900162\n",
      "Iteration 646, loss = 2.90782780\n",
      "Iteration 647, loss = 2.90765865\n",
      "Iteration 648, loss = 2.90740507\n",
      "Iteration 649, loss = 2.90674310\n",
      "Iteration 650, loss = 2.90683098\n",
      "Iteration 651, loss = 2.90638252\n",
      "Iteration 652, loss = 2.90637383\n",
      "Iteration 653, loss = 2.90446798\n",
      "Iteration 654, loss = 2.90628805\n",
      "Iteration 655, loss = 2.90601854\n",
      "Iteration 656, loss = 2.90472564\n",
      "Iteration 657, loss = 2.90426929\n",
      "Iteration 658, loss = 2.90560685\n",
      "Iteration 659, loss = 2.90474490\n",
      "Iteration 660, loss = 2.90367127\n",
      "Iteration 661, loss = 2.90223595\n",
      "Iteration 662, loss = 2.90196925\n",
      "Iteration 663, loss = 2.90198083\n",
      "Iteration 664, loss = 2.90074835\n",
      "Iteration 665, loss = 2.90092571\n",
      "Iteration 666, loss = 2.90120145\n",
      "Iteration 667, loss = 2.90063414\n",
      "Iteration 668, loss = 2.90041860\n",
      "Iteration 669, loss = 2.89961149\n",
      "Iteration 670, loss = 2.89906548\n",
      "Iteration 671, loss = 2.89907104\n",
      "Iteration 672, loss = 2.89931392\n",
      "Iteration 673, loss = 2.89882178\n",
      "Iteration 674, loss = 2.89837181\n",
      "Iteration 675, loss = 2.89839088\n",
      "Iteration 676, loss = 2.89793536\n",
      "Iteration 677, loss = 2.89721271\n",
      "Iteration 678, loss = 2.89693177\n",
      "Iteration 679, loss = 2.89689791\n",
      "Iteration 680, loss = 2.89684699\n",
      "Iteration 681, loss = 2.89704068\n",
      "Iteration 682, loss = 2.89631393\n",
      "Iteration 683, loss = 2.89704088\n",
      "Iteration 684, loss = 2.89514415\n",
      "Iteration 685, loss = 2.89416893\n",
      "Iteration 686, loss = 2.89384762\n",
      "Iteration 687, loss = 2.89292118\n",
      "Iteration 688, loss = 2.89285376\n",
      "Iteration 689, loss = 2.89293809\n",
      "Iteration 690, loss = 2.89264175\n",
      "Iteration 691, loss = 2.89175952\n",
      "Iteration 692, loss = 2.89180159\n",
      "Iteration 693, loss = 2.89172406\n",
      "Iteration 694, loss = 2.89127684\n",
      "Iteration 695, loss = 2.89090219\n",
      "Iteration 696, loss = 2.89032770\n",
      "Iteration 697, loss = 2.89021898\n",
      "Iteration 698, loss = 2.88962414\n",
      "Iteration 699, loss = 2.88960167\n",
      "Iteration 700, loss = 2.88904022\n",
      "Iteration 701, loss = 2.88838822\n",
      "Iteration 702, loss = 2.88838156\n",
      "Iteration 703, loss = 2.88831901\n",
      "Iteration 704, loss = 2.88804968\n",
      "Iteration 705, loss = 2.88742588\n",
      "Iteration 706, loss = 2.88699430\n",
      "Iteration 707, loss = 2.88682546\n",
      "Iteration 708, loss = 2.88668569\n",
      "Iteration 709, loss = 2.88662448\n",
      "Iteration 710, loss = 2.88555970\n",
      "Iteration 711, loss = 2.88598758\n",
      "Iteration 712, loss = 2.88780604\n",
      "Iteration 713, loss = 2.88778407\n",
      "Iteration 714, loss = 2.88602375\n",
      "Iteration 715, loss = 2.88486495\n",
      "Iteration 716, loss = 2.88479844\n",
      "Iteration 717, loss = 2.88510964\n",
      "Iteration 718, loss = 2.88464564\n",
      "Iteration 719, loss = 2.88364920\n",
      "Iteration 720, loss = 2.88276122\n",
      "Iteration 721, loss = 2.88277542\n",
      "Iteration 722, loss = 2.88287501\n",
      "Iteration 723, loss = 2.88135028\n",
      "Iteration 724, loss = 2.88161761\n",
      "Iteration 725, loss = 2.88363749\n",
      "Iteration 726, loss = 2.88139560\n",
      "Iteration 727, loss = 2.88085351\n",
      "Iteration 728, loss = 2.88068083\n",
      "Iteration 729, loss = 2.88079958\n",
      "Iteration 730, loss = 2.87998682\n",
      "Iteration 731, loss = 2.87894035\n",
      "Iteration 732, loss = 2.87899648\n",
      "Iteration 733, loss = 2.87927489\n",
      "Iteration 734, loss = 2.87900454\n",
      "Iteration 735, loss = 2.87948998\n",
      "Iteration 736, loss = 2.87789603\n",
      "Iteration 737, loss = 2.87759136\n",
      "Iteration 738, loss = 2.87731349\n",
      "Iteration 739, loss = 2.87700581\n",
      "Iteration 740, loss = 2.87691180\n",
      "Iteration 741, loss = 2.87600711\n",
      "Iteration 742, loss = 2.87604908\n",
      "Iteration 743, loss = 2.87550519\n",
      "Iteration 744, loss = 2.87561848\n",
      "Iteration 745, loss = 2.87524849\n",
      "Iteration 746, loss = 2.87530799\n",
      "Iteration 747, loss = 2.87451226\n",
      "Iteration 748, loss = 2.87442455\n",
      "Iteration 749, loss = 2.87386119\n",
      "Iteration 750, loss = 2.87380338\n",
      "Iteration 751, loss = 2.87345569\n",
      "Iteration 752, loss = 2.87270447\n",
      "Iteration 753, loss = 2.87372005\n",
      "Iteration 754, loss = 2.87336972\n",
      "Iteration 755, loss = 2.87342288\n",
      "Iteration 756, loss = 2.87252184\n",
      "Iteration 757, loss = 2.87196368\n",
      "Iteration 758, loss = 2.87113371\n",
      "Iteration 759, loss = 2.87157693\n",
      "Iteration 760, loss = 2.87140554\n",
      "Iteration 761, loss = 2.86981210\n",
      "Iteration 762, loss = 2.87027572\n",
      "Iteration 763, loss = 2.86961260\n",
      "Iteration 764, loss = 2.86906297\n",
      "Iteration 765, loss = 2.86875169\n",
      "Iteration 766, loss = 2.86946806\n",
      "Iteration 767, loss = 2.86902124\n",
      "Iteration 768, loss = 2.86817222\n",
      "Iteration 769, loss = 2.86842936\n",
      "Iteration 770, loss = 2.86825043\n",
      "Iteration 771, loss = 2.86785649\n",
      "Iteration 772, loss = 2.86792648\n",
      "Iteration 773, loss = 2.86684067\n",
      "Iteration 774, loss = 2.86590123\n",
      "Iteration 775, loss = 2.86691727\n",
      "Iteration 776, loss = 2.86579677\n",
      "Iteration 777, loss = 2.86513475\n",
      "Iteration 778, loss = 2.86479410\n",
      "Iteration 779, loss = 2.86534239\n",
      "Iteration 780, loss = 2.86529598\n",
      "Iteration 781, loss = 2.86564962\n",
      "Iteration 782, loss = 2.86468621\n",
      "Iteration 783, loss = 2.86404577\n",
      "Iteration 784, loss = 2.86377519\n",
      "Iteration 785, loss = 2.86354533\n",
      "Iteration 786, loss = 2.86392174\n",
      "Iteration 787, loss = 2.86357108\n",
      "Iteration 788, loss = 2.86276613\n",
      "Iteration 789, loss = 2.86204910\n",
      "Iteration 790, loss = 2.86165452\n",
      "Iteration 791, loss = 2.86162226\n",
      "Iteration 792, loss = 2.86155686\n",
      "Iteration 793, loss = 2.86084873\n",
      "Iteration 794, loss = 2.86082965\n",
      "Iteration 795, loss = 2.85997397\n",
      "Iteration 796, loss = 2.85976428\n",
      "Iteration 797, loss = 2.86096162\n",
      "Iteration 798, loss = 2.86081264\n",
      "Iteration 799, loss = 2.85938316\n",
      "Iteration 800, loss = 2.85853507\n",
      "Iteration 801, loss = 2.85839967\n",
      "Iteration 802, loss = 2.85774350\n",
      "Iteration 803, loss = 2.85744339\n",
      "Iteration 804, loss = 2.85688572\n",
      "Iteration 805, loss = 2.85693646\n",
      "Iteration 806, loss = 2.85652369\n",
      "Iteration 807, loss = 2.85726273\n",
      "Iteration 808, loss = 2.85666740\n",
      "Iteration 809, loss = 2.85632183\n",
      "Iteration 810, loss = 2.85676184\n",
      "Iteration 811, loss = 2.85640748\n",
      "Iteration 812, loss = 2.85530830\n",
      "Iteration 813, loss = 2.85519270\n",
      "Iteration 814, loss = 2.85442035\n",
      "Iteration 815, loss = 2.85433190\n",
      "Iteration 816, loss = 2.85422507\n",
      "Iteration 817, loss = 2.85400381\n",
      "Iteration 818, loss = 2.85293790\n",
      "Iteration 819, loss = 2.85297025\n",
      "Iteration 820, loss = 2.85263860\n",
      "Iteration 821, loss = 2.85427598\n",
      "Iteration 822, loss = 2.85474841\n",
      "Iteration 823, loss = 2.85227450\n",
      "Iteration 824, loss = 2.85239627\n",
      "Iteration 825, loss = 2.85217392\n",
      "Iteration 826, loss = 2.85221205\n",
      "Iteration 827, loss = 2.85149442\n",
      "Iteration 828, loss = 2.85042260\n",
      "Iteration 829, loss = 2.85003178\n",
      "Iteration 830, loss = 2.84999674\n",
      "Iteration 831, loss = 2.85086955\n",
      "Iteration 832, loss = 2.85049203\n",
      "Iteration 833, loss = 2.85004703\n",
      "Iteration 834, loss = 2.84929075\n",
      "Iteration 835, loss = 2.84918853\n",
      "Iteration 836, loss = 2.84907556\n",
      "Iteration 837, loss = 2.84831872\n",
      "Iteration 838, loss = 2.84789469\n",
      "Iteration 839, loss = 2.84761042\n",
      "Iteration 840, loss = 2.84754766\n",
      "Iteration 841, loss = 2.84731376\n",
      "Iteration 842, loss = 2.84670036\n",
      "Iteration 843, loss = 2.84744880\n",
      "Iteration 844, loss = 2.84623872\n",
      "Iteration 845, loss = 2.84683963\n",
      "Iteration 846, loss = 2.84671371\n",
      "Iteration 847, loss = 2.84626232\n",
      "Iteration 848, loss = 2.84559617\n",
      "Iteration 849, loss = 2.84548736\n",
      "Iteration 850, loss = 2.84469392\n",
      "Iteration 851, loss = 2.84571561\n",
      "Iteration 852, loss = 2.84523304\n",
      "Iteration 853, loss = 2.84485624\n",
      "Iteration 854, loss = 2.84376255\n",
      "Iteration 855, loss = 2.84341424\n",
      "Iteration 856, loss = 2.84285203\n",
      "Iteration 857, loss = 2.84268108\n",
      "Iteration 858, loss = 2.84227503\n",
      "Iteration 859, loss = 2.84231491\n",
      "Iteration 860, loss = 2.84246643\n",
      "Iteration 861, loss = 2.84156752\n",
      "Iteration 862, loss = 2.84115167\n",
      "Iteration 863, loss = 2.84140640\n",
      "Iteration 864, loss = 2.84132778\n",
      "Iteration 865, loss = 2.84088568\n",
      "Iteration 866, loss = 2.84075846\n",
      "Iteration 867, loss = 2.83981897\n",
      "Iteration 868, loss = 2.83979542\n",
      "Iteration 869, loss = 2.83996946\n",
      "Iteration 870, loss = 2.84023274\n",
      "Iteration 871, loss = 2.84072049\n",
      "Iteration 872, loss = 2.83936276\n",
      "Iteration 873, loss = 2.83958167\n",
      "Iteration 874, loss = 2.83929850\n",
      "Iteration 875, loss = 2.83862093\n",
      "Iteration 876, loss = 2.83841674\n",
      "Iteration 877, loss = 2.83866144\n",
      "Iteration 878, loss = 2.83865294\n",
      "Iteration 879, loss = 2.83760895\n",
      "Iteration 880, loss = 2.83691329\n",
      "Iteration 881, loss = 2.83653588\n",
      "Iteration 882, loss = 2.83617792\n",
      "Iteration 883, loss = 2.83586772\n",
      "Iteration 884, loss = 2.83571285\n",
      "Iteration 885, loss = 2.83641592\n",
      "Iteration 886, loss = 2.83587804\n",
      "Iteration 887, loss = 2.83527274\n",
      "Iteration 888, loss = 2.83596192\n",
      "Iteration 889, loss = 2.83541786\n",
      "Iteration 890, loss = 2.83453258\n",
      "Iteration 891, loss = 2.83446126\n",
      "Iteration 892, loss = 2.83516584\n",
      "Iteration 893, loss = 2.83376195\n",
      "Iteration 894, loss = 2.83386499\n",
      "Iteration 895, loss = 2.83352904\n",
      "Iteration 896, loss = 2.83390670\n",
      "Iteration 897, loss = 2.83348718\n",
      "Iteration 898, loss = 2.83240033\n",
      "Iteration 899, loss = 2.83314590\n",
      "Iteration 900, loss = 2.83199728\n",
      "Iteration 901, loss = 2.83176074\n",
      "Iteration 902, loss = 2.83178107\n",
      "Iteration 903, loss = 2.83143468\n",
      "Iteration 904, loss = 2.83014452\n",
      "Iteration 905, loss = 2.83184638\n",
      "Iteration 906, loss = 2.83201384\n",
      "Iteration 907, loss = 2.82974768\n",
      "Iteration 908, loss = 2.83047478\n",
      "Iteration 909, loss = 2.83013737\n",
      "Iteration 910, loss = 2.82932806\n",
      "Iteration 911, loss = 2.82985941\n",
      "Iteration 912, loss = 2.82971476\n",
      "Iteration 913, loss = 2.82891740\n",
      "Iteration 914, loss = 2.82875186\n",
      "Iteration 915, loss = 2.82794333\n",
      "Iteration 916, loss = 2.82838880\n",
      "Iteration 917, loss = 2.82749436\n",
      "Iteration 918, loss = 2.82679640\n",
      "Iteration 919, loss = 2.82729059\n",
      "Iteration 920, loss = 2.82774353\n",
      "Iteration 921, loss = 2.82770987\n",
      "Iteration 922, loss = 2.82617226\n",
      "Iteration 923, loss = 2.82601422\n",
      "Iteration 924, loss = 2.82550185\n",
      "Iteration 925, loss = 2.82612471\n",
      "Iteration 926, loss = 2.82626759\n",
      "Iteration 927, loss = 2.82534292\n",
      "Iteration 928, loss = 2.82516155\n",
      "Iteration 929, loss = 2.82495948\n",
      "Iteration 930, loss = 2.82466357\n",
      "Iteration 931, loss = 2.82472516\n",
      "Iteration 932, loss = 2.82406831\n",
      "Iteration 933, loss = 2.82505669\n",
      "Iteration 934, loss = 2.82410424\n",
      "Iteration 935, loss = 2.82342771\n",
      "Iteration 936, loss = 2.82298284\n",
      "Iteration 937, loss = 2.82310337\n",
      "Iteration 938, loss = 2.82194902\n",
      "Iteration 939, loss = 2.82245423\n",
      "Iteration 940, loss = 2.82222407\n",
      "Iteration 941, loss = 2.82164083\n",
      "Iteration 942, loss = 2.82122696\n",
      "Iteration 943, loss = 2.82128228\n",
      "Iteration 944, loss = 2.82145403\n",
      "Iteration 945, loss = 2.82040275\n",
      "Iteration 946, loss = 2.82043706\n",
      "Iteration 947, loss = 2.82096619\n",
      "Iteration 948, loss = 2.82062649\n",
      "Iteration 949, loss = 2.82102597\n",
      "Iteration 950, loss = 2.82194245\n",
      "Iteration 951, loss = 2.82162394\n",
      "Iteration 952, loss = 2.82090832\n",
      "Iteration 953, loss = 2.81925758\n",
      "Iteration 954, loss = 2.81983903\n",
      "Iteration 955, loss = 2.81925737\n",
      "Iteration 956, loss = 2.82000194\n",
      "Iteration 957, loss = 2.82104509\n",
      "Iteration 958, loss = 2.81962488\n",
      "Iteration 959, loss = 2.81833106\n",
      "Iteration 960, loss = 2.81776303\n",
      "Iteration 961, loss = 2.81794668\n",
      "Iteration 962, loss = 2.81775130\n",
      "Iteration 963, loss = 2.81726209\n",
      "Iteration 964, loss = 2.81665002\n",
      "Iteration 965, loss = 2.81695546\n",
      "Iteration 966, loss = 2.81605743\n",
      "Iteration 967, loss = 2.81551887\n",
      "Iteration 968, loss = 2.81530312\n",
      "Iteration 969, loss = 2.81509866\n",
      "Iteration 970, loss = 2.81581189\n",
      "Iteration 971, loss = 2.81495647\n",
      "Iteration 972, loss = 2.81519737\n",
      "Iteration 973, loss = 2.81564236\n",
      "Iteration 974, loss = 2.81382675\n",
      "Iteration 975, loss = 2.81408595\n",
      "Iteration 976, loss = 2.81291451\n",
      "Iteration 977, loss = 2.81379612\n",
      "Iteration 978, loss = 2.81309630\n",
      "Iteration 979, loss = 2.81329024\n",
      "Iteration 980, loss = 2.81493700\n",
      "Iteration 981, loss = 2.81397737\n",
      "Iteration 982, loss = 2.81172683\n",
      "Iteration 983, loss = 2.81179473\n",
      "Iteration 984, loss = 2.81159906\n",
      "Iteration 985, loss = 2.81221659\n",
      "Iteration 986, loss = 2.81221570\n",
      "Iteration 987, loss = 2.81178967\n",
      "Iteration 988, loss = 2.81027397\n",
      "Iteration 989, loss = 2.81032026\n",
      "Iteration 990, loss = 2.81087109\n",
      "Iteration 991, loss = 2.81116566\n",
      "Iteration 992, loss = 2.81069855\n",
      "Iteration 993, loss = 2.81084232\n",
      "Iteration 994, loss = 2.81023008\n",
      "Iteration 995, loss = 2.80976886\n",
      "Iteration 996, loss = 2.80947442\n",
      "Iteration 997, loss = 2.80948006\n",
      "Iteration 998, loss = 2.81035573\n",
      "Iteration 999, loss = 2.80897614\n",
      "Iteration 1000, loss = 2.80897440\n",
      "Iteration 1001, loss = 2.80851331\n",
      "Iteration 1002, loss = 2.80825212\n",
      "Iteration 1003, loss = 2.80720761\n",
      "Iteration 1004, loss = 2.80693386\n",
      "Iteration 1005, loss = 2.80706396\n",
      "Iteration 1006, loss = 2.80732831\n",
      "Iteration 1007, loss = 2.80676049\n",
      "Iteration 1008, loss = 2.80692107\n",
      "Iteration 1009, loss = 2.80789847\n",
      "Iteration 1010, loss = 2.80598162\n",
      "Iteration 1011, loss = 2.80584421\n",
      "Iteration 1012, loss = 2.80628625\n",
      "Iteration 1013, loss = 2.80663020\n",
      "Iteration 1014, loss = 2.80478278\n",
      "Iteration 1015, loss = 2.80451466\n",
      "Iteration 1016, loss = 2.80542414\n",
      "Iteration 1017, loss = 2.80499786\n",
      "Iteration 1018, loss = 2.80520402\n",
      "Iteration 1019, loss = 2.80485195\n",
      "Iteration 1020, loss = 2.80314178\n",
      "Iteration 1021, loss = 2.80247306\n",
      "Iteration 1022, loss = 2.80387057\n",
      "Iteration 1023, loss = 2.80338510\n",
      "Iteration 1024, loss = 2.80294667\n",
      "Iteration 1025, loss = 2.80272865\n",
      "Iteration 1026, loss = 2.80265406\n",
      "Iteration 1027, loss = 2.80378550\n",
      "Iteration 1028, loss = 2.80137823\n",
      "Iteration 1029, loss = 2.80256787\n",
      "Iteration 1030, loss = 2.80155220\n",
      "Iteration 1031, loss = 2.80281902\n",
      "Iteration 1032, loss = 2.80215480\n",
      "Iteration 1033, loss = 2.80041347\n",
      "Iteration 1034, loss = 2.80112021\n",
      "Iteration 1035, loss = 2.80057360\n",
      "Iteration 1036, loss = 2.80062603\n",
      "Iteration 1037, loss = 2.79975514\n",
      "Iteration 1038, loss = 2.80049404\n",
      "Iteration 1039, loss = 2.80000047\n",
      "Iteration 1040, loss = 2.79984088\n",
      "Iteration 1041, loss = 2.79985763\n",
      "Iteration 1042, loss = 2.79896170\n",
      "Iteration 1043, loss = 2.79843540\n",
      "Iteration 1044, loss = 2.79903473\n",
      "Iteration 1045, loss = 2.80056222\n",
      "Iteration 1046, loss = 2.79973127\n",
      "Iteration 1047, loss = 2.79886069\n",
      "Iteration 1048, loss = 2.79775083\n",
      "Iteration 1049, loss = 2.79788733\n",
      "Iteration 1050, loss = 2.79673154\n",
      "Iteration 1051, loss = 2.79698894\n",
      "Iteration 1052, loss = 2.79704734\n",
      "Iteration 1053, loss = 2.79669066\n",
      "Iteration 1054, loss = 2.79629336\n",
      "Iteration 1055, loss = 2.79640619\n",
      "Iteration 1056, loss = 2.79578157\n",
      "Iteration 1057, loss = 2.79610478\n",
      "Iteration 1058, loss = 2.79566199\n",
      "Iteration 1059, loss = 2.79560489\n",
      "Iteration 1060, loss = 2.79525041\n",
      "Iteration 1061, loss = 2.79508268\n",
      "Iteration 1062, loss = 2.79499736\n",
      "Iteration 1063, loss = 2.79470398\n",
      "Iteration 1064, loss = 2.79407646\n",
      "Iteration 1065, loss = 2.79383852\n",
      "Iteration 1066, loss = 2.79305394\n",
      "Iteration 1067, loss = 2.79338483\n",
      "Iteration 1068, loss = 2.79310814\n",
      "Iteration 1069, loss = 2.79293392\n",
      "Iteration 1070, loss = 2.79283012\n",
      "Iteration 1071, loss = 2.79355850\n",
      "Iteration 1072, loss = 2.79307720\n",
      "Iteration 1073, loss = 2.79189280\n",
      "Iteration 1074, loss = 2.79160101\n",
      "Iteration 1075, loss = 2.79258466\n",
      "Iteration 1076, loss = 2.79189802\n",
      "Iteration 1077, loss = 2.79264579\n",
      "Iteration 1078, loss = 2.79129423\n",
      "Iteration 1079, loss = 2.79072171\n",
      "Iteration 1080, loss = 2.79081858\n",
      "Iteration 1081, loss = 2.79032478\n",
      "Iteration 1082, loss = 2.79079433\n",
      "Iteration 1083, loss = 2.79062718\n",
      "Iteration 1084, loss = 2.79149132\n",
      "Iteration 1085, loss = 2.79107281\n",
      "Iteration 1086, loss = 2.79067554\n",
      "Iteration 1087, loss = 2.79019786\n",
      "Iteration 1088, loss = 2.78886375\n",
      "Iteration 1089, loss = 2.78980230\n",
      "Iteration 1090, loss = 2.79001468\n",
      "Iteration 1091, loss = 2.78949920\n",
      "Iteration 1092, loss = 2.79036402\n",
      "Iteration 1093, loss = 2.78917769\n",
      "Iteration 1094, loss = 2.78830691\n",
      "Iteration 1095, loss = 2.78892564\n",
      "Iteration 1096, loss = 2.78898680\n",
      "Iteration 1097, loss = 2.78757300\n",
      "Iteration 1098, loss = 2.78839677\n",
      "Iteration 1099, loss = 2.78682827\n",
      "Iteration 1100, loss = 2.78711462\n",
      "Iteration 1101, loss = 2.78657803\n",
      "Iteration 1102, loss = 2.78669828\n",
      "Iteration 1103, loss = 2.78638447\n",
      "Iteration 1104, loss = 2.78634876\n",
      "Iteration 1105, loss = 2.78815196\n",
      "Iteration 1106, loss = 2.78834915\n",
      "Iteration 1107, loss = 2.78576780\n",
      "Iteration 1108, loss = 2.78533919\n",
      "Iteration 1109, loss = 2.78582277\n",
      "Iteration 1110, loss = 2.78499094\n",
      "Iteration 1111, loss = 2.78425439\n",
      "Iteration 1112, loss = 2.78568417\n",
      "Iteration 1113, loss = 2.78478156\n",
      "Iteration 1114, loss = 2.78446962\n",
      "Iteration 1115, loss = 2.78413066\n",
      "Iteration 1116, loss = 2.78374898\n",
      "Iteration 1117, loss = 2.78308688\n",
      "Iteration 1118, loss = 2.78352657\n",
      "Iteration 1119, loss = 2.78375888\n",
      "Iteration 1120, loss = 2.78261403\n",
      "Iteration 1121, loss = 2.78236357\n",
      "Iteration 1122, loss = 2.78250257\n",
      "Iteration 1123, loss = 2.78203286\n",
      "Iteration 1124, loss = 2.78222386\n",
      "Iteration 1125, loss = 2.78187642\n",
      "Iteration 1126, loss = 2.78161033\n",
      "Iteration 1127, loss = 2.78078147\n",
      "Iteration 1128, loss = 2.78075004\n",
      "Iteration 1129, loss = 2.78078114\n",
      "Iteration 1130, loss = 2.78033449\n",
      "Iteration 1131, loss = 2.78037017\n",
      "Iteration 1132, loss = 2.77997241\n",
      "Iteration 1133, loss = 2.77994112\n",
      "Iteration 1134, loss = 2.77972889\n",
      "Iteration 1135, loss = 2.77926664\n",
      "Iteration 1136, loss = 2.77950997\n",
      "Iteration 1137, loss = 2.77949492\n",
      "Iteration 1138, loss = 2.77912933\n",
      "Iteration 1139, loss = 2.77957907\n",
      "Iteration 1140, loss = 2.77904156\n",
      "Iteration 1141, loss = 2.77951457\n",
      "Iteration 1142, loss = 2.77894475\n",
      "Iteration 1143, loss = 2.77852842\n",
      "Iteration 1144, loss = 2.77864367\n",
      "Iteration 1145, loss = 2.77817970\n",
      "Iteration 1146, loss = 2.77778315\n",
      "Iteration 1147, loss = 2.77729774\n",
      "Iteration 1148, loss = 2.77782086\n",
      "Iteration 1149, loss = 2.77836852\n",
      "Iteration 1150, loss = 2.77832211\n",
      "Iteration 1151, loss = 2.77771562\n",
      "Iteration 1152, loss = 2.77603446\n",
      "Iteration 1153, loss = 2.77712183\n",
      "Iteration 1154, loss = 2.77701135\n",
      "Iteration 1155, loss = 2.77636271\n",
      "Iteration 1156, loss = 2.77580342\n",
      "Iteration 1157, loss = 2.77717162\n",
      "Iteration 1158, loss = 2.77573896\n",
      "Iteration 1159, loss = 2.77470099\n",
      "Iteration 1160, loss = 2.77558424\n",
      "Iteration 1161, loss = 2.77506646\n",
      "Iteration 1162, loss = 2.77446297\n",
      "Iteration 1163, loss = 2.77421432\n",
      "Iteration 1164, loss = 2.77446964\n",
      "Iteration 1165, loss = 2.77408408\n",
      "Iteration 1166, loss = 2.77399006\n",
      "Iteration 1167, loss = 2.77396322\n",
      "Iteration 1168, loss = 2.77311755\n",
      "Iteration 1169, loss = 2.77324683\n",
      "Iteration 1170, loss = 2.77315427\n",
      "Iteration 1171, loss = 2.77402875\n",
      "Iteration 1172, loss = 2.77392965\n",
      "Iteration 1173, loss = 2.77408154\n",
      "Iteration 1174, loss = 2.77372352\n",
      "Iteration 1175, loss = 2.77199516\n",
      "Iteration 1176, loss = 2.77270768\n",
      "Iteration 1177, loss = 2.77237368\n",
      "Iteration 1178, loss = 2.77181301\n",
      "Iteration 1179, loss = 2.77140792\n",
      "Iteration 1180, loss = 2.77107401\n",
      "Iteration 1181, loss = 2.77067657\n",
      "Iteration 1182, loss = 2.77218393\n",
      "Iteration 1183, loss = 2.77069915\n",
      "Iteration 1184, loss = 2.77126852\n",
      "Iteration 1185, loss = 2.77067375\n",
      "Iteration 1186, loss = 2.77058986\n",
      "Iteration 1187, loss = 2.77063488\n",
      "Iteration 1188, loss = 2.77027500\n",
      "Iteration 1189, loss = 2.76951430\n",
      "Iteration 1190, loss = 2.76993838\n",
      "Iteration 1191, loss = 2.76939746\n",
      "Iteration 1192, loss = 2.76862634\n",
      "Iteration 1193, loss = 2.76850671\n",
      "Iteration 1194, loss = 2.76814049\n",
      "Iteration 1195, loss = 2.76870579\n",
      "Iteration 1196, loss = 2.76849334\n",
      "Iteration 1197, loss = 2.76867992\n",
      "Iteration 1198, loss = 2.76801695\n",
      "Iteration 1199, loss = 2.76754222\n",
      "Iteration 1200, loss = 2.76763343\n",
      "Iteration 1201, loss = 2.76743790\n",
      "Iteration 1202, loss = 2.76736982\n",
      "Iteration 1203, loss = 2.76725893\n",
      "Iteration 1204, loss = 2.76714878\n",
      "Iteration 1205, loss = 2.76731420\n",
      "Iteration 1206, loss = 2.76636139\n",
      "Iteration 1207, loss = 2.76652813\n",
      "Iteration 1208, loss = 2.76767423\n",
      "Iteration 1209, loss = 2.76703599\n",
      "Iteration 1210, loss = 2.76622395\n",
      "Iteration 1211, loss = 2.76551421\n",
      "Iteration 1212, loss = 2.76559462\n",
      "Iteration 1213, loss = 2.76828176\n",
      "Iteration 1214, loss = 2.76716097\n",
      "Iteration 1215, loss = 2.76630266\n",
      "Iteration 1216, loss = 2.76510136\n",
      "Iteration 1217, loss = 2.76418753\n",
      "Iteration 1218, loss = 2.76629201\n",
      "Iteration 1219, loss = 2.76469443\n",
      "Iteration 1220, loss = 2.76486104\n",
      "Iteration 1221, loss = 2.76384139\n",
      "Iteration 1222, loss = 2.76372112\n",
      "Iteration 1223, loss = 2.76363556\n",
      "Iteration 1224, loss = 2.76345317\n",
      "Iteration 1225, loss = 2.76260199\n",
      "Iteration 1226, loss = 2.76375262\n",
      "Iteration 1227, loss = 2.76244404\n",
      "Iteration 1228, loss = 2.76457034\n",
      "Iteration 1229, loss = 2.76311136\n",
      "Iteration 1230, loss = 2.76261978\n",
      "Iteration 1231, loss = 2.76329552\n",
      "Iteration 1232, loss = 2.76294066\n",
      "Iteration 1233, loss = 2.76245603\n",
      "Iteration 1234, loss = 2.76278124\n",
      "Iteration 1235, loss = 2.76257183\n",
      "Iteration 1236, loss = 2.76243944\n",
      "Iteration 1237, loss = 2.76161180\n",
      "Iteration 1238, loss = 2.76065036\n",
      "Iteration 1239, loss = 2.76096665\n",
      "Iteration 1240, loss = 2.76043422\n",
      "Iteration 1241, loss = 2.76199555\n",
      "Iteration 1242, loss = 2.76144727\n",
      "Iteration 1243, loss = 2.75992875\n",
      "Iteration 1244, loss = 2.75916188\n",
      "Iteration 1245, loss = 2.75965396\n",
      "Iteration 1246, loss = 2.76009638\n",
      "Iteration 1247, loss = 2.75906658\n",
      "Iteration 1248, loss = 2.75942114\n",
      "Iteration 1249, loss = 2.75909851\n",
      "Iteration 1250, loss = 2.75928434\n",
      "Iteration 1251, loss = 2.75887843\n",
      "Iteration 1252, loss = 2.75964500\n",
      "Iteration 1253, loss = 2.75870623\n",
      "Iteration 1254, loss = 2.76001509\n",
      "Iteration 1255, loss = 2.75913729\n",
      "Iteration 1256, loss = 2.75911140\n",
      "Iteration 1257, loss = 2.75783148\n",
      "Iteration 1258, loss = 2.75702069\n",
      "Iteration 1259, loss = 2.75824420\n",
      "Iteration 1260, loss = 2.75770283\n",
      "Iteration 1261, loss = 2.75731893\n",
      "Iteration 1262, loss = 2.75737794\n",
      "Iteration 1263, loss = 2.75730597\n",
      "Iteration 1264, loss = 2.75700305\n",
      "Iteration 1265, loss = 2.75692508\n",
      "Iteration 1266, loss = 2.75683245\n",
      "Iteration 1267, loss = 2.75575134\n",
      "Iteration 1268, loss = 2.75544357\n",
      "Iteration 1269, loss = 2.75576235\n",
      "Iteration 1270, loss = 2.75723170\n",
      "Iteration 1271, loss = 2.75604690\n",
      "Iteration 1272, loss = 2.75455347\n",
      "Iteration 1273, loss = 2.75509127\n",
      "Iteration 1274, loss = 2.75461353\n",
      "Iteration 1275, loss = 2.75370381\n",
      "Iteration 1276, loss = 2.75462783\n",
      "Iteration 1277, loss = 2.75388032\n",
      "Iteration 1278, loss = 2.75317404\n",
      "Iteration 1279, loss = 2.75328625\n",
      "Iteration 1280, loss = 2.75321202\n",
      "Iteration 1281, loss = 2.75406736\n",
      "Iteration 1282, loss = 2.75376389\n",
      "Iteration 1283, loss = 2.75371603\n",
      "Iteration 1284, loss = 2.75276746\n",
      "Iteration 1285, loss = 2.75294668\n",
      "Iteration 1286, loss = 2.75236572\n",
      "Iteration 1287, loss = 2.75270229\n",
      "Iteration 1288, loss = 2.75313296\n",
      "Iteration 1289, loss = 2.75343639\n",
      "Iteration 1290, loss = 2.75244475\n",
      "Iteration 1291, loss = 2.75304357\n",
      "Iteration 1292, loss = 2.75225429\n",
      "Iteration 1293, loss = 2.75119558\n",
      "Iteration 1294, loss = 2.75173870\n",
      "Iteration 1295, loss = 2.75170823\n",
      "Iteration 1296, loss = 2.75101076\n",
      "Iteration 1297, loss = 2.75121193\n",
      "Iteration 1298, loss = 2.75127650\n",
      "Iteration 1299, loss = 2.75170938\n",
      "Iteration 1300, loss = 2.75112829\n",
      "Iteration 1301, loss = 2.75032868\n",
      "Iteration 1302, loss = 2.75014103\n",
      "Iteration 1303, loss = 2.74960195\n",
      "Iteration 1304, loss = 2.75001893\n",
      "Iteration 1305, loss = 2.74957534\n",
      "Iteration 1306, loss = 2.74981852\n",
      "Iteration 1307, loss = 2.74988508\n",
      "Iteration 1308, loss = 2.74927643\n",
      "Iteration 1309, loss = 2.74925681\n",
      "Iteration 1310, loss = 2.74957253\n",
      "Iteration 1311, loss = 2.74888536\n",
      "Iteration 1312, loss = 2.74815408\n",
      "Iteration 1313, loss = 2.74917986\n",
      "Iteration 1314, loss = 2.74863266\n",
      "Iteration 1315, loss = 2.74885764\n",
      "Iteration 1316, loss = 2.74757815\n",
      "Iteration 1317, loss = 2.74850576\n",
      "Iteration 1318, loss = 2.74804672\n",
      "Iteration 1319, loss = 2.74702448\n",
      "Iteration 1320, loss = 2.74724716\n",
      "Iteration 1321, loss = 2.74709031\n",
      "Iteration 1322, loss = 2.74784531\n",
      "Iteration 1323, loss = 2.74802655\n",
      "Iteration 1324, loss = 2.74625609\n",
      "Iteration 1325, loss = 2.74810020\n",
      "Iteration 1326, loss = 2.74668258\n",
      "Iteration 1327, loss = 2.74828315\n",
      "Iteration 1328, loss = 2.74804547\n",
      "Iteration 1329, loss = 2.74766267\n",
      "Iteration 1330, loss = 2.74662568\n",
      "Iteration 1331, loss = 2.74539578\n",
      "Iteration 1332, loss = 2.74548382\n",
      "Iteration 1333, loss = 2.74613987\n",
      "Iteration 1334, loss = 2.74537452\n",
      "Iteration 1335, loss = 2.74658001\n",
      "Iteration 1336, loss = 2.74524986\n",
      "Iteration 1337, loss = 2.74497181\n",
      "Iteration 1338, loss = 2.74507750\n",
      "Iteration 1339, loss = 2.74423742\n",
      "Iteration 1340, loss = 2.74468029\n",
      "Iteration 1341, loss = 2.74451172\n",
      "Iteration 1342, loss = 2.74459285\n",
      "Iteration 1343, loss = 2.74370133\n",
      "Iteration 1344, loss = 2.74423934\n",
      "Iteration 1345, loss = 2.74381281\n",
      "Iteration 1346, loss = 2.74317042\n",
      "Iteration 1347, loss = 2.74277936\n",
      "Iteration 1348, loss = 2.74386015\n",
      "Iteration 1349, loss = 2.74409027\n",
      "Iteration 1350, loss = 2.74353510\n",
      "Iteration 1351, loss = 2.74301414\n",
      "Iteration 1352, loss = 2.74296324\n",
      "Iteration 1353, loss = 2.74256767\n",
      "Iteration 1354, loss = 2.74169722\n",
      "Iteration 1355, loss = 2.74190772\n",
      "Iteration 1356, loss = 2.74191037\n",
      "Iteration 1357, loss = 2.74152364\n",
      "Iteration 1358, loss = 2.74093825\n",
      "Iteration 1359, loss = 2.74098711\n",
      "Iteration 1360, loss = 2.74098983\n",
      "Iteration 1361, loss = 2.74121342\n",
      "Iteration 1362, loss = 2.74059629\n",
      "Iteration 1363, loss = 2.74167186\n",
      "Iteration 1364, loss = 2.74120035\n",
      "Iteration 1365, loss = 2.74009290\n",
      "Iteration 1366, loss = 2.74048125\n",
      "Iteration 1367, loss = 2.74044874\n",
      "Iteration 1368, loss = 2.73974806\n",
      "Iteration 1369, loss = 2.74055731\n",
      "Iteration 1370, loss = 2.73981173\n",
      "Iteration 1371, loss = 2.73981573\n",
      "Iteration 1372, loss = 2.73983434\n",
      "Iteration 1373, loss = 2.73966063\n",
      "Iteration 1374, loss = 2.73999989\n",
      "Iteration 1375, loss = 2.73967522\n",
      "Iteration 1376, loss = 2.73909760\n",
      "Iteration 1377, loss = 2.73878785\n",
      "Iteration 1378, loss = 2.73839483\n",
      "Iteration 1379, loss = 2.73797886\n",
      "Iteration 1380, loss = 2.73927829\n",
      "Iteration 1381, loss = 2.73774443\n",
      "Iteration 1382, loss = 2.73892820\n",
      "Iteration 1383, loss = 2.73926705\n",
      "Iteration 1384, loss = 2.73745844\n",
      "Iteration 1385, loss = 2.73743959\n",
      "Iteration 1386, loss = 2.73759953\n",
      "Iteration 1387, loss = 2.73748804\n",
      "Iteration 1388, loss = 2.74095780\n",
      "Iteration 1389, loss = 2.73825972\n",
      "Iteration 1390, loss = 2.73622172\n",
      "Iteration 1391, loss = 2.73625754\n",
      "Iteration 1392, loss = 2.73689606\n",
      "Iteration 1393, loss = 2.73680706\n",
      "Iteration 1394, loss = 2.73634852\n",
      "Iteration 1395, loss = 2.73569122\n",
      "Iteration 1396, loss = 2.73583467\n",
      "Iteration 1397, loss = 2.73632125\n",
      "Iteration 1398, loss = 2.73542792\n",
      "Iteration 1399, loss = 2.73707193\n",
      "Iteration 1400, loss = 2.73408366\n",
      "Iteration 1401, loss = 2.73515267\n",
      "Iteration 1402, loss = 2.73504267\n",
      "Iteration 1403, loss = 2.73480121\n",
      "Iteration 1404, loss = 2.73478303\n",
      "Iteration 1405, loss = 2.73496314\n",
      "Iteration 1406, loss = 2.73473694\n",
      "Iteration 1407, loss = 2.73411215\n",
      "Iteration 1408, loss = 2.73433609\n",
      "Iteration 1409, loss = 2.73391046\n",
      "Iteration 1410, loss = 2.73316293\n",
      "Iteration 1411, loss = 2.73333605\n",
      "Iteration 1412, loss = 2.73345258\n",
      "Iteration 1413, loss = 2.73418540\n",
      "Iteration 1414, loss = 2.73371666\n",
      "Iteration 1415, loss = 2.73341375\n",
      "Iteration 1416, loss = 2.73351504\n",
      "Iteration 1417, loss = 2.73329089\n",
      "Iteration 1418, loss = 2.73242214\n",
      "Iteration 1419, loss = 2.73164662\n",
      "Iteration 1420, loss = 2.73291316\n",
      "Iteration 1421, loss = 2.73301267\n",
      "Iteration 1422, loss = 2.73209358\n",
      "Iteration 1423, loss = 2.73225292\n",
      "Iteration 1424, loss = 2.73194345\n",
      "Iteration 1425, loss = 2.73183195\n",
      "Iteration 1426, loss = 2.73075984\n",
      "Iteration 1427, loss = 2.73132539\n",
      "Iteration 1428, loss = 2.73056655\n",
      "Iteration 1429, loss = 2.73083791\n",
      "Iteration 1430, loss = 2.73058777\n",
      "Iteration 1431, loss = 2.73040200\n",
      "Iteration 1432, loss = 2.72985224\n",
      "Iteration 1433, loss = 2.72980816\n",
      "Iteration 1434, loss = 2.73075889\n",
      "Iteration 1435, loss = 2.73096470\n",
      "Iteration 1436, loss = 2.72994759\n",
      "Iteration 1437, loss = 2.72970150\n",
      "Iteration 1438, loss = 2.72919027\n",
      "Iteration 1439, loss = 2.72942943\n",
      "Iteration 1440, loss = 2.73030323\n",
      "Iteration 1441, loss = 2.72972380\n",
      "Iteration 1442, loss = 2.72887012\n",
      "Iteration 1443, loss = 2.72933718\n",
      "Iteration 1444, loss = 2.72941853\n",
      "Iteration 1445, loss = 2.72849278\n",
      "Iteration 1446, loss = 2.72851056\n",
      "Iteration 1447, loss = 2.72806639\n",
      "Iteration 1448, loss = 2.72780206\n",
      "Iteration 1449, loss = 2.72861670\n",
      "Iteration 1450, loss = 2.72833955\n",
      "Iteration 1451, loss = 2.72845015\n",
      "Iteration 1452, loss = 2.72727379\n",
      "Iteration 1453, loss = 2.72865024\n",
      "Iteration 1454, loss = 2.72769360\n",
      "Iteration 1455, loss = 2.72658794\n",
      "Iteration 1456, loss = 2.72783915\n",
      "Iteration 1457, loss = 2.72788974\n",
      "Iteration 1458, loss = 2.72740343\n",
      "Iteration 1459, loss = 2.72713132\n",
      "Iteration 1460, loss = 2.72684756\n",
      "Iteration 1461, loss = 2.72595522\n",
      "Iteration 1462, loss = 2.72578403\n",
      "Iteration 1463, loss = 2.72604555\n",
      "Iteration 1464, loss = 2.72542828\n",
      "Iteration 1465, loss = 2.72575682\n",
      "Iteration 1466, loss = 2.72506572\n",
      "Iteration 1467, loss = 2.72531779\n",
      "Iteration 1468, loss = 2.72570043\n",
      "Iteration 1469, loss = 2.72696219\n",
      "Iteration 1470, loss = 2.72459973\n",
      "Iteration 1471, loss = 2.72552368\n",
      "Iteration 1472, loss = 2.72507216\n",
      "Iteration 1473, loss = 2.72486003\n",
      "Iteration 1474, loss = 2.72456390\n",
      "Iteration 1475, loss = 2.72430919\n",
      "Iteration 1476, loss = 2.72384614\n",
      "Iteration 1477, loss = 2.72380038\n",
      "Iteration 1478, loss = 2.72299253\n",
      "Iteration 1479, loss = 2.72330010\n",
      "Iteration 1480, loss = 2.72424836\n",
      "Iteration 1481, loss = 2.72459256\n",
      "Iteration 1482, loss = 2.72333992\n",
      "Iteration 1483, loss = 2.72326622\n",
      "Iteration 1484, loss = 2.72290567\n",
      "Iteration 1485, loss = 2.72334973\n",
      "Iteration 1486, loss = 2.72213913\n",
      "Iteration 1487, loss = 2.72285373\n",
      "Iteration 1488, loss = 2.72386728\n",
      "Iteration 1489, loss = 2.72226588\n",
      "Iteration 1490, loss = 2.72277866\n",
      "Iteration 1491, loss = 2.72278785\n",
      "Iteration 1492, loss = 2.72371244\n",
      "Iteration 1493, loss = 2.72261236\n",
      "Iteration 1494, loss = 2.72189917\n",
      "Iteration 1495, loss = 2.72221071\n",
      "Iteration 1496, loss = 2.72119359\n",
      "Iteration 1497, loss = 2.72119799\n",
      "Iteration 1498, loss = 2.72101066\n",
      "Iteration 1499, loss = 2.72105883\n",
      "Iteration 1500, loss = 2.72006976\n",
      "Iteration 1501, loss = 2.72079248\n",
      "Iteration 1502, loss = 2.72139944\n",
      "Iteration 1503, loss = 2.72069627\n",
      "Iteration 1504, loss = 2.72082364\n",
      "Iteration 1505, loss = 2.72057785\n",
      "Iteration 1506, loss = 2.72256891\n",
      "Iteration 1507, loss = 2.72081876\n",
      "Iteration 1508, loss = 2.71922252\n",
      "Iteration 1509, loss = 2.71993343\n",
      "Iteration 1510, loss = 2.71977544\n",
      "Iteration 1511, loss = 2.71964699\n",
      "Iteration 1512, loss = 2.72179835\n",
      "Iteration 1513, loss = 2.72052473\n",
      "Iteration 1514, loss = 2.71939893\n",
      "Iteration 1515, loss = 2.71890008\n",
      "Iteration 1516, loss = 2.72005109\n",
      "Iteration 1517, loss = 2.72041364\n",
      "Iteration 1518, loss = 2.71817108\n",
      "Iteration 1519, loss = 2.71876131\n",
      "Iteration 1520, loss = 2.71868771\n",
      "Iteration 1521, loss = 2.71910049\n",
      "Iteration 1522, loss = 2.72040188\n",
      "Iteration 1523, loss = 2.72134275\n",
      "Iteration 1524, loss = 2.72018997\n",
      "Iteration 1525, loss = 2.71853695\n",
      "Iteration 1526, loss = 2.71842776\n",
      "Iteration 1527, loss = 2.71820157\n",
      "Iteration 1528, loss = 2.71761897\n",
      "Iteration 1529, loss = 2.71819361\n",
      "Iteration 1530, loss = 2.71782122\n",
      "Iteration 1531, loss = 2.71760697\n",
      "Iteration 1532, loss = 2.71665305\n",
      "Iteration 1533, loss = 2.71651645\n",
      "Iteration 1534, loss = 2.71625194\n",
      "Iteration 1535, loss = 2.71655158\n",
      "Iteration 1536, loss = 2.71754939\n",
      "Iteration 1537, loss = 2.71628912\n",
      "Iteration 1538, loss = 2.71639136\n",
      "Iteration 1539, loss = 2.71703624\n",
      "Iteration 1540, loss = 2.71654371\n",
      "Iteration 1541, loss = 2.71573302\n",
      "Iteration 1542, loss = 2.71576730\n",
      "Iteration 1543, loss = 2.71526810\n",
      "Iteration 1544, loss = 2.71603289\n",
      "Iteration 1545, loss = 2.71484867\n",
      "Iteration 1546, loss = 2.71468279\n",
      "Iteration 1547, loss = 2.71589191\n",
      "Iteration 1548, loss = 2.71475032\n",
      "Iteration 1549, loss = 2.71571760\n",
      "Iteration 1550, loss = 2.71553190\n",
      "Iteration 1551, loss = 2.71473824\n",
      "Iteration 1552, loss = 2.71595740\n",
      "Iteration 1553, loss = 2.71423804\n",
      "Iteration 1554, loss = 2.71376501\n",
      "Iteration 1555, loss = 2.71427349\n",
      "Iteration 1556, loss = 2.71393116\n",
      "Iteration 1557, loss = 2.71362297\n",
      "Iteration 1558, loss = 2.71349751\n",
      "Iteration 1559, loss = 2.71299333\n",
      "Iteration 1560, loss = 2.71280055\n",
      "Iteration 1561, loss = 2.71212901\n",
      "Iteration 1562, loss = 2.71345124\n",
      "Iteration 1563, loss = 2.71460064\n",
      "Iteration 1564, loss = 2.71414638\n",
      "Iteration 1565, loss = 2.71423614\n",
      "Iteration 1566, loss = 2.71270712\n",
      "Iteration 1567, loss = 2.71295449\n",
      "Iteration 1568, loss = 2.71414834\n",
      "Iteration 1569, loss = 2.71241921\n",
      "Iteration 1570, loss = 2.71232804\n",
      "Iteration 1571, loss = 2.71149257\n",
      "Iteration 1572, loss = 2.71236498\n",
      "Iteration 1573, loss = 2.71241680\n",
      "Iteration 1574, loss = 2.71334946\n",
      "Iteration 1575, loss = 2.71140974\n",
      "Iteration 1576, loss = 2.71419222\n",
      "Iteration 1577, loss = 2.71275241\n",
      "Iteration 1578, loss = 2.71268921\n",
      "Iteration 1579, loss = 2.71094019\n",
      "Iteration 1580, loss = 2.71053727\n",
      "Iteration 1581, loss = 2.71013230\n",
      "Iteration 1582, loss = 2.70999692\n",
      "Iteration 1583, loss = 2.70965915\n",
      "Iteration 1584, loss = 2.70970545\n",
      "Iteration 1585, loss = 2.70969132\n",
      "Iteration 1586, loss = 2.70971604\n",
      "Iteration 1587, loss = 2.70994366\n",
      "Iteration 1588, loss = 2.70964466\n",
      "Iteration 1589, loss = 2.70940393\n",
      "Iteration 1590, loss = 2.70911557\n",
      "Iteration 1591, loss = 2.70991217\n",
      "Iteration 1592, loss = 2.70982337\n",
      "Iteration 1593, loss = 2.70848048\n",
      "Iteration 1594, loss = 2.70910891\n",
      "Iteration 1595, loss = 2.70821731\n",
      "Iteration 1596, loss = 2.70878491\n",
      "Iteration 1597, loss = 2.70835888\n",
      "Iteration 1598, loss = 2.71067834\n",
      "Iteration 1599, loss = 2.70928134\n",
      "Iteration 1600, loss = 2.70763174\n",
      "Iteration 1601, loss = 2.70745299\n",
      "Iteration 1602, loss = 2.70813856\n",
      "Iteration 1603, loss = 2.70792664\n",
      "Iteration 1604, loss = 2.70746022\n",
      "Iteration 1605, loss = 2.70803365\n",
      "Iteration 1606, loss = 2.70799406\n",
      "Iteration 1607, loss = 2.70726395\n",
      "Iteration 1608, loss = 2.70826954\n",
      "Iteration 1609, loss = 2.70851001\n",
      "Iteration 1610, loss = 2.70709305\n",
      "Iteration 1611, loss = 2.70720314\n",
      "Iteration 1612, loss = 2.70608051\n",
      "Iteration 1613, loss = 2.70643915\n",
      "Iteration 1614, loss = 2.70682253\n",
      "Iteration 1615, loss = 2.70696916\n",
      "Iteration 1616, loss = 2.70608560\n",
      "Iteration 1617, loss = 2.70583680\n",
      "Iteration 1618, loss = 2.70590577\n",
      "Iteration 1619, loss = 2.70538387\n",
      "Iteration 1620, loss = 2.70564117\n",
      "Iteration 1621, loss = 2.70546238\n",
      "Iteration 1622, loss = 2.70653947\n",
      "Iteration 1623, loss = 2.70640933\n",
      "Iteration 1624, loss = 2.70462772\n",
      "Iteration 1625, loss = 2.70749940\n",
      "Iteration 1626, loss = 2.70630046\n",
      "Iteration 1627, loss = 2.70551307\n",
      "Iteration 1628, loss = 2.70433032\n",
      "Iteration 1629, loss = 2.70586550\n",
      "Iteration 1630, loss = 2.70518791\n",
      "Iteration 1631, loss = 2.70459170\n",
      "Iteration 1632, loss = 2.70453388\n",
      "Iteration 1633, loss = 2.70573072\n",
      "Iteration 1634, loss = 2.70365938\n",
      "Iteration 1635, loss = 2.70423514\n",
      "Iteration 1636, loss = 2.70418141\n",
      "Iteration 1637, loss = 2.70314082\n",
      "Iteration 1638, loss = 2.70343337\n",
      "Iteration 1639, loss = 2.70337963\n",
      "Iteration 1640, loss = 2.70378087\n",
      "Iteration 1641, loss = 2.70292948\n",
      "Iteration 1642, loss = 2.70345338\n",
      "Iteration 1643, loss = 2.70345904\n",
      "Iteration 1644, loss = 2.70286084\n",
      "Iteration 1645, loss = 2.70272088\n",
      "Iteration 1646, loss = 2.70319033\n",
      "Iteration 1647, loss = 2.70374126\n",
      "Iteration 1648, loss = 2.70245612\n",
      "Iteration 1649, loss = 2.70216035\n",
      "Iteration 1650, loss = 2.70184110\n",
      "Iteration 1651, loss = 2.70275641\n",
      "Iteration 1652, loss = 2.70235987\n",
      "Iteration 1653, loss = 2.70215002\n",
      "Iteration 1654, loss = 2.70288191\n",
      "Iteration 1655, loss = 2.70386336\n",
      "Iteration 1656, loss = 2.70178820\n",
      "Iteration 1657, loss = 2.70166720\n",
      "Iteration 1658, loss = 2.70119851\n",
      "Iteration 1659, loss = 2.70123906\n",
      "Iteration 1660, loss = 2.70251653\n",
      "Iteration 1661, loss = 2.70106755\n",
      "Iteration 1662, loss = 2.70047857\n",
      "Iteration 1663, loss = 2.70067017\n",
      "Iteration 1664, loss = 2.70084360\n",
      "Iteration 1665, loss = 2.70121949\n",
      "Iteration 1666, loss = 2.70085220\n",
      "Iteration 1667, loss = 2.70026997\n",
      "Iteration 1668, loss = 2.69949559\n",
      "Iteration 1669, loss = 2.69974454\n",
      "Iteration 1670, loss = 2.70120122\n",
      "Iteration 1671, loss = 2.70137508\n",
      "Iteration 1672, loss = 2.70085522\n",
      "Iteration 1673, loss = 2.69982135\n",
      "Iteration 1674, loss = 2.69963597\n",
      "Iteration 1675, loss = 2.69955726\n",
      "Iteration 1676, loss = 2.69911015\n",
      "Iteration 1677, loss = 2.70174681\n",
      "Iteration 1678, loss = 2.70058953\n",
      "Iteration 1679, loss = 2.69856772\n",
      "Iteration 1680, loss = 2.69974195\n",
      "Iteration 1681, loss = 2.69862946\n",
      "Iteration 1682, loss = 2.69835590\n",
      "Iteration 1683, loss = 2.69852936\n",
      "Iteration 1684, loss = 2.69805480\n",
      "Iteration 1, loss = 4.13321276\n",
      "Iteration 2, loss = 4.05409499\n",
      "Iteration 3, loss = 3.97921687\n",
      "Iteration 4, loss = 3.90693943\n",
      "Iteration 5, loss = 3.83274106\n",
      "Iteration 6, loss = 3.75921417\n",
      "Iteration 7, loss = 3.68712796\n",
      "Iteration 8, loss = 3.61365097\n",
      "Iteration 9, loss = 3.54446230\n",
      "Iteration 10, loss = 3.47972879\n",
      "Iteration 11, loss = 3.41776081\n",
      "Iteration 12, loss = 3.36697823\n",
      "Iteration 13, loss = 3.32349065\n",
      "Iteration 14, loss = 3.29021231\n",
      "Iteration 15, loss = 3.26453952\n",
      "Iteration 16, loss = 3.24840732\n",
      "Iteration 17, loss = 3.23847087\n",
      "Iteration 18, loss = 3.23434060\n",
      "Iteration 19, loss = 3.23176896\n",
      "Iteration 20, loss = 3.23076464\n",
      "Iteration 21, loss = 3.22931267\n",
      "Iteration 22, loss = 3.22818219\n",
      "Iteration 23, loss = 3.22705914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1684) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 3.22607978\n",
      "Iteration 25, loss = 3.22554004\n",
      "Iteration 26, loss = 3.22491270\n",
      "Iteration 27, loss = 3.22402896\n",
      "Iteration 28, loss = 3.22360210\n",
      "Iteration 29, loss = 3.22329332\n",
      "Iteration 30, loss = 3.22279894\n",
      "Iteration 31, loss = 3.22237195\n",
      "Iteration 32, loss = 3.22169658\n",
      "Iteration 33, loss = 3.22100164\n",
      "Iteration 34, loss = 3.22047563\n",
      "Iteration 35, loss = 3.22042704\n",
      "Iteration 36, loss = 3.21979997\n",
      "Iteration 37, loss = 3.21915433\n",
      "Iteration 38, loss = 3.21888236\n",
      "Iteration 39, loss = 3.21847860\n",
      "Iteration 40, loss = 3.21810849\n",
      "Iteration 41, loss = 3.21798670\n",
      "Iteration 42, loss = 3.21743523\n",
      "Iteration 43, loss = 3.21728721\n",
      "Iteration 44, loss = 3.21676338\n",
      "Iteration 45, loss = 3.21622797\n",
      "Iteration 46, loss = 3.21588585\n",
      "Iteration 47, loss = 3.21555307\n",
      "Iteration 48, loss = 3.21561903\n",
      "Iteration 49, loss = 3.21492848\n",
      "Iteration 50, loss = 3.21489355\n",
      "Iteration 51, loss = 3.21454765\n",
      "Iteration 52, loss = 3.21411627\n",
      "Iteration 53, loss = 3.21376129\n",
      "Iteration 54, loss = 3.21343635\n",
      "Iteration 55, loss = 3.21295747\n",
      "Iteration 56, loss = 3.21281161\n",
      "Iteration 57, loss = 3.21268863\n",
      "Iteration 58, loss = 3.21186310\n",
      "Iteration 59, loss = 3.21148104\n",
      "Iteration 60, loss = 3.21179588\n",
      "Iteration 61, loss = 3.21112698\n",
      "Iteration 62, loss = 3.21092743\n",
      "Iteration 63, loss = 3.21074592\n",
      "Iteration 64, loss = 3.21040345\n",
      "Iteration 65, loss = 3.21000764\n",
      "Iteration 66, loss = 3.20971176\n",
      "Iteration 67, loss = 3.20970844\n",
      "Iteration 68, loss = 3.20949003\n",
      "Iteration 69, loss = 3.20930805\n",
      "Iteration 70, loss = 3.20862392\n",
      "Iteration 71, loss = 3.20859966\n",
      "Iteration 72, loss = 3.20850100\n",
      "Iteration 73, loss = 3.20778708\n",
      "Iteration 74, loss = 3.20797192\n",
      "Iteration 75, loss = 3.20787172\n",
      "Iteration 76, loss = 3.20739053\n",
      "Iteration 77, loss = 3.20711297\n",
      "Iteration 78, loss = 3.20654653\n",
      "Iteration 79, loss = 3.20603506\n",
      "Iteration 80, loss = 3.20624439\n",
      "Iteration 81, loss = 3.20558275\n",
      "Iteration 82, loss = 3.20547178\n",
      "Iteration 83, loss = 3.20553176\n",
      "Iteration 84, loss = 3.20558768\n",
      "Iteration 85, loss = 3.20521168\n",
      "Iteration 86, loss = 3.20507977\n",
      "Iteration 87, loss = 3.20475003\n",
      "Iteration 88, loss = 3.20461200\n",
      "Iteration 89, loss = 3.20448044\n",
      "Iteration 90, loss = 3.20364572\n",
      "Iteration 91, loss = 3.20325539\n",
      "Iteration 92, loss = 3.20302327\n",
      "Iteration 93, loss = 3.20310563\n",
      "Iteration 94, loss = 3.20247975\n",
      "Iteration 95, loss = 3.20221022\n",
      "Iteration 96, loss = 3.20178753\n",
      "Iteration 97, loss = 3.20148011\n",
      "Iteration 98, loss = 3.20175892\n",
      "Iteration 99, loss = 3.20160205\n",
      "Iteration 100, loss = 3.20135720\n",
      "Iteration 101, loss = 3.20143111\n",
      "Iteration 102, loss = 3.20107200\n",
      "Iteration 103, loss = 3.20038863\n",
      "Iteration 104, loss = 3.20037055\n",
      "Iteration 105, loss = 3.20013392\n",
      "Iteration 106, loss = 3.19986177\n",
      "Iteration 107, loss = 3.19994297\n",
      "Iteration 108, loss = 3.19915526\n",
      "Iteration 109, loss = 3.19883250\n",
      "Iteration 110, loss = 3.19843171\n",
      "Iteration 111, loss = 3.19832865\n",
      "Iteration 112, loss = 3.19794398\n",
      "Iteration 113, loss = 3.19749865\n",
      "Iteration 114, loss = 3.19732123\n",
      "Iteration 115, loss = 3.19706083\n",
      "Iteration 116, loss = 3.19684573\n",
      "Iteration 117, loss = 3.19634334\n",
      "Iteration 118, loss = 3.19629059\n",
      "Iteration 119, loss = 3.19614053\n",
      "Iteration 120, loss = 3.19597300\n",
      "Iteration 121, loss = 3.19584388\n",
      "Iteration 122, loss = 3.19590961\n",
      "Iteration 123, loss = 3.19590083\n",
      "Iteration 124, loss = 3.19577793\n",
      "Iteration 125, loss = 3.19538927\n",
      "Iteration 126, loss = 3.19495280\n",
      "Iteration 127, loss = 3.19499831\n",
      "Iteration 128, loss = 3.19428875\n",
      "Iteration 129, loss = 3.19428900\n",
      "Iteration 130, loss = 3.19445438\n",
      "Iteration 131, loss = 3.19447599\n",
      "Iteration 132, loss = 3.19384388\n",
      "Iteration 133, loss = 3.19343838\n",
      "Iteration 134, loss = 3.19266410\n",
      "Iteration 135, loss = 3.19228853\n",
      "Iteration 136, loss = 3.19233776\n",
      "Iteration 137, loss = 3.19214514\n",
      "Iteration 138, loss = 3.19176992\n",
      "Iteration 139, loss = 3.19222097\n",
      "Iteration 140, loss = 3.19223992\n",
      "Iteration 141, loss = 3.19113148\n",
      "Iteration 142, loss = 3.19075096\n",
      "Iteration 143, loss = 3.19116207\n",
      "Iteration 144, loss = 3.19047802\n",
      "Iteration 145, loss = 3.19031749\n",
      "Iteration 146, loss = 3.19065508\n",
      "Iteration 147, loss = 3.18991076\n",
      "Iteration 148, loss = 3.18940117\n",
      "Iteration 149, loss = 3.18953857\n",
      "Iteration 150, loss = 3.18939179\n",
      "Iteration 151, loss = 3.18844113\n",
      "Iteration 152, loss = 3.18796799\n",
      "Iteration 153, loss = 3.18850275\n",
      "Iteration 154, loss = 3.18813883\n",
      "Iteration 155, loss = 3.18821850\n",
      "Iteration 156, loss = 3.18755068\n",
      "Iteration 157, loss = 3.18760583\n",
      "Iteration 158, loss = 3.18736357\n",
      "Iteration 159, loss = 3.18722464\n",
      "Iteration 160, loss = 3.18651048\n",
      "Iteration 161, loss = 3.18631786\n",
      "Iteration 162, loss = 3.18612018\n",
      "Iteration 163, loss = 3.18604890\n",
      "Iteration 164, loss = 3.18590771\n",
      "Iteration 165, loss = 3.18581409\n",
      "Iteration 166, loss = 3.18563051\n",
      "Iteration 167, loss = 3.18515506\n",
      "Iteration 168, loss = 3.18462209\n",
      "Iteration 169, loss = 3.18448281\n",
      "Iteration 170, loss = 3.18409411\n",
      "Iteration 171, loss = 3.18426131\n",
      "Iteration 172, loss = 3.18391801\n",
      "Iteration 173, loss = 3.18393569\n",
      "Iteration 174, loss = 3.18348366\n",
      "Iteration 175, loss = 3.18308173\n",
      "Iteration 176, loss = 3.18279925\n",
      "Iteration 177, loss = 3.18265188\n",
      "Iteration 178, loss = 3.18216081\n",
      "Iteration 179, loss = 3.18199971\n",
      "Iteration 180, loss = 3.18144136\n",
      "Iteration 181, loss = 3.18158071\n",
      "Iteration 182, loss = 3.18174630\n",
      "Iteration 183, loss = 3.18114935\n",
      "Iteration 184, loss = 3.18120323\n",
      "Iteration 185, loss = 3.18108094\n",
      "Iteration 186, loss = 3.18082336\n",
      "Iteration 187, loss = 3.18018363\n",
      "Iteration 188, loss = 3.17996807\n",
      "Iteration 189, loss = 3.17986761\n",
      "Iteration 190, loss = 3.17952102\n",
      "Iteration 191, loss = 3.17895057\n",
      "Iteration 192, loss = 3.17927635\n",
      "Iteration 193, loss = 3.17901947\n",
      "Iteration 194, loss = 3.17866214\n",
      "Iteration 195, loss = 3.17855053\n",
      "Iteration 196, loss = 3.17838878\n",
      "Iteration 197, loss = 3.17786084\n",
      "Iteration 198, loss = 3.17743064\n",
      "Iteration 199, loss = 3.17739892\n",
      "Iteration 200, loss = 3.17717011\n",
      "Iteration 201, loss = 3.17666750\n",
      "Iteration 202, loss = 3.17607487\n",
      "Iteration 203, loss = 3.17610259\n",
      "Iteration 204, loss = 3.17597091\n",
      "Iteration 205, loss = 3.17562576\n",
      "Iteration 206, loss = 3.17562262\n",
      "Iteration 207, loss = 3.17492163\n",
      "Iteration 208, loss = 3.17491575\n",
      "Iteration 209, loss = 3.17492414\n",
      "Iteration 210, loss = 3.17466677\n",
      "Iteration 211, loss = 3.17404918\n",
      "Iteration 212, loss = 3.17368490\n",
      "Iteration 213, loss = 3.17349769\n",
      "Iteration 214, loss = 3.17322457\n",
      "Iteration 215, loss = 3.17285245\n",
      "Iteration 216, loss = 3.17239002\n",
      "Iteration 217, loss = 3.17170366\n",
      "Iteration 218, loss = 3.17172303\n",
      "Iteration 219, loss = 3.17159930\n",
      "Iteration 220, loss = 3.17139645\n",
      "Iteration 221, loss = 3.17131573\n",
      "Iteration 222, loss = 3.17133590\n",
      "Iteration 223, loss = 3.17081834\n",
      "Iteration 224, loss = 3.17104810\n",
      "Iteration 225, loss = 3.17039016\n",
      "Iteration 226, loss = 3.16977989\n",
      "Iteration 227, loss = 3.16906645\n",
      "Iteration 228, loss = 3.16981835\n",
      "Iteration 229, loss = 3.16943993\n",
      "Iteration 230, loss = 3.16867722\n",
      "Iteration 231, loss = 3.16877744\n",
      "Iteration 232, loss = 3.16821096\n",
      "Iteration 233, loss = 3.16704372\n",
      "Iteration 234, loss = 3.16677401\n",
      "Iteration 235, loss = 3.16676252\n",
      "Iteration 236, loss = 3.16665348\n",
      "Iteration 237, loss = 3.16646313\n",
      "Iteration 238, loss = 3.16586128\n",
      "Iteration 239, loss = 3.16533946\n",
      "Iteration 240, loss = 3.16518413\n",
      "Iteration 241, loss = 3.16515660\n",
      "Iteration 242, loss = 3.16468632\n",
      "Iteration 243, loss = 3.16462875\n",
      "Iteration 244, loss = 3.16478260\n",
      "Iteration 245, loss = 3.16403532\n",
      "Iteration 246, loss = 3.16388833\n",
      "Iteration 247, loss = 3.16306310\n",
      "Iteration 248, loss = 3.16226591\n",
      "Iteration 249, loss = 3.16223798\n",
      "Iteration 250, loss = 3.16222139\n",
      "Iteration 251, loss = 3.16205909\n",
      "Iteration 252, loss = 3.16145490\n",
      "Iteration 253, loss = 3.16123621\n",
      "Iteration 254, loss = 3.16105253\n",
      "Iteration 255, loss = 3.16075243\n",
      "Iteration 256, loss = 3.16051623\n",
      "Iteration 257, loss = 3.16042151\n",
      "Iteration 258, loss = 3.15996994\n",
      "Iteration 259, loss = 3.15937716\n",
      "Iteration 260, loss = 3.15841362\n",
      "Iteration 261, loss = 3.15843198\n",
      "Iteration 262, loss = 3.15858225\n",
      "Iteration 263, loss = 3.15783071\n",
      "Iteration 264, loss = 3.15720884\n",
      "Iteration 265, loss = 3.15691199\n",
      "Iteration 266, loss = 3.15643725\n",
      "Iteration 267, loss = 3.15675261\n",
      "Iteration 268, loss = 3.15632105\n",
      "Iteration 269, loss = 3.15579179\n",
      "Iteration 270, loss = 3.15586101\n",
      "Iteration 271, loss = 3.15518019\n",
      "Iteration 272, loss = 3.15493186\n",
      "Iteration 273, loss = 3.15488251\n",
      "Iteration 274, loss = 3.15464109\n",
      "Iteration 275, loss = 3.15435459\n",
      "Iteration 276, loss = 3.15367973\n",
      "Iteration 277, loss = 3.15390398\n",
      "Iteration 278, loss = 3.15338787\n",
      "Iteration 279, loss = 3.15290893\n",
      "Iteration 280, loss = 3.15302613\n",
      "Iteration 281, loss = 3.15228712\n",
      "Iteration 282, loss = 3.15173298\n",
      "Iteration 283, loss = 3.15151835\n",
      "Iteration 284, loss = 3.15123824\n",
      "Iteration 285, loss = 3.15083319\n",
      "Iteration 286, loss = 3.15079689\n",
      "Iteration 287, loss = 3.15058464\n",
      "Iteration 288, loss = 3.14964759\n",
      "Iteration 289, loss = 3.14902251\n",
      "Iteration 290, loss = 3.14875543\n",
      "Iteration 291, loss = 3.14855332\n",
      "Iteration 292, loss = 3.14808703\n",
      "Iteration 293, loss = 3.14830743\n",
      "Iteration 294, loss = 3.14775141\n",
      "Iteration 295, loss = 3.14714731\n",
      "Iteration 296, loss = 3.14678826\n",
      "Iteration 297, loss = 3.14640239\n",
      "Iteration 298, loss = 3.14605499\n",
      "Iteration 299, loss = 3.14538972\n",
      "Iteration 300, loss = 3.14549826\n",
      "Iteration 301, loss = 3.14567025\n",
      "Iteration 302, loss = 3.14471286\n",
      "Iteration 303, loss = 3.14473732\n",
      "Iteration 304, loss = 3.14514890\n",
      "Iteration 305, loss = 3.14425067\n",
      "Iteration 306, loss = 3.14306266\n",
      "Iteration 307, loss = 3.14364143\n",
      "Iteration 308, loss = 3.14336277\n",
      "Iteration 309, loss = 3.14227294\n",
      "Iteration 310, loss = 3.14163389\n",
      "Iteration 311, loss = 3.14178435\n",
      "Iteration 312, loss = 3.14159897\n",
      "Iteration 313, loss = 3.14189893\n",
      "Iteration 314, loss = 3.14085272\n",
      "Iteration 315, loss = 3.14085697\n",
      "Iteration 316, loss = 3.14088505\n",
      "Iteration 317, loss = 3.14067064\n",
      "Iteration 318, loss = 3.13922286\n",
      "Iteration 319, loss = 3.13865846\n",
      "Iteration 320, loss = 3.13864031\n",
      "Iteration 321, loss = 3.13875959\n",
      "Iteration 322, loss = 3.13883004\n",
      "Iteration 323, loss = 3.13714653\n",
      "Iteration 324, loss = 3.13683796\n",
      "Iteration 325, loss = 3.13698295\n",
      "Iteration 326, loss = 3.13635157\n",
      "Iteration 327, loss = 3.13600669\n",
      "Iteration 328, loss = 3.13521551\n",
      "Iteration 329, loss = 3.13489327\n",
      "Iteration 330, loss = 3.13469355\n",
      "Iteration 331, loss = 3.13391638\n",
      "Iteration 332, loss = 3.13358092\n",
      "Iteration 333, loss = 3.13324381\n",
      "Iteration 334, loss = 3.13306449\n",
      "Iteration 335, loss = 3.13272413\n",
      "Iteration 336, loss = 3.13244574\n",
      "Iteration 337, loss = 3.13244167\n",
      "Iteration 338, loss = 3.13169688\n",
      "Iteration 339, loss = 3.13165484\n",
      "Iteration 340, loss = 3.13119861\n",
      "Iteration 341, loss = 3.13072248\n",
      "Iteration 342, loss = 3.13021560\n",
      "Iteration 343, loss = 3.13030203\n",
      "Iteration 344, loss = 3.13009101\n",
      "Iteration 345, loss = 3.12949242\n",
      "Iteration 346, loss = 3.12906292\n",
      "Iteration 347, loss = 3.12818736\n",
      "Iteration 348, loss = 3.12804150\n",
      "Iteration 349, loss = 3.12763380\n",
      "Iteration 350, loss = 3.12706601\n",
      "Iteration 351, loss = 3.12677150\n",
      "Iteration 352, loss = 3.12685737\n",
      "Iteration 353, loss = 3.12636864\n",
      "Iteration 354, loss = 3.12602514\n",
      "Iteration 355, loss = 3.12544987\n",
      "Iteration 356, loss = 3.12482660\n",
      "Iteration 357, loss = 3.12567954\n",
      "Iteration 358, loss = 3.12503453\n",
      "Iteration 359, loss = 3.12357458\n",
      "Iteration 360, loss = 3.12307326\n",
      "Iteration 361, loss = 3.12288405\n",
      "Iteration 362, loss = 3.12254428\n",
      "Iteration 363, loss = 3.12250726\n",
      "Iteration 364, loss = 3.12203312\n",
      "Iteration 365, loss = 3.12132865\n",
      "Iteration 366, loss = 3.12103040\n",
      "Iteration 367, loss = 3.12037897\n",
      "Iteration 368, loss = 3.12017590\n",
      "Iteration 369, loss = 3.11954083\n",
      "Iteration 370, loss = 3.11947163\n",
      "Iteration 371, loss = 3.11899649\n",
      "Iteration 372, loss = 3.11856872\n",
      "Iteration 373, loss = 3.11876444\n",
      "Iteration 374, loss = 3.11893789\n",
      "Iteration 375, loss = 3.11738926\n",
      "Iteration 376, loss = 3.11725152\n",
      "Iteration 377, loss = 3.11702153\n",
      "Iteration 378, loss = 3.11649518\n",
      "Iteration 379, loss = 3.11657615\n",
      "Iteration 380, loss = 3.11668777\n",
      "Iteration 381, loss = 3.11563837\n",
      "Iteration 382, loss = 3.11461737\n",
      "Iteration 383, loss = 3.11435596\n",
      "Iteration 384, loss = 3.11398101\n",
      "Iteration 385, loss = 3.11361849\n",
      "Iteration 386, loss = 3.11361627\n",
      "Iteration 387, loss = 3.11376565\n",
      "Iteration 388, loss = 3.11277285\n",
      "Iteration 389, loss = 3.11214934\n",
      "Iteration 390, loss = 3.11183843\n",
      "Iteration 391, loss = 3.11110224\n",
      "Iteration 392, loss = 3.11094289\n",
      "Iteration 393, loss = 3.11047786\n",
      "Iteration 394, loss = 3.11020412\n",
      "Iteration 395, loss = 3.10981897\n",
      "Iteration 396, loss = 3.10944913\n",
      "Iteration 397, loss = 3.10974158\n",
      "Iteration 398, loss = 3.10910802\n",
      "Iteration 399, loss = 3.10823464\n",
      "Iteration 400, loss = 3.10788063\n",
      "Iteration 401, loss = 3.10756484\n",
      "Iteration 402, loss = 3.10758499\n",
      "Iteration 403, loss = 3.10672012\n",
      "Iteration 404, loss = 3.10669785\n",
      "Iteration 405, loss = 3.10644994\n",
      "Iteration 406, loss = 3.10601741\n",
      "Iteration 407, loss = 3.10556159\n",
      "Iteration 408, loss = 3.10523449\n",
      "Iteration 409, loss = 3.10481576\n",
      "Iteration 410, loss = 3.10429593\n",
      "Iteration 411, loss = 3.10394582\n",
      "Iteration 412, loss = 3.10348554\n",
      "Iteration 413, loss = 3.10319805\n",
      "Iteration 414, loss = 3.10266432\n",
      "Iteration 415, loss = 3.10266028\n",
      "Iteration 416, loss = 3.10223125\n",
      "Iteration 417, loss = 3.10166641\n",
      "Iteration 418, loss = 3.10113898\n",
      "Iteration 419, loss = 3.10051653\n",
      "Iteration 420, loss = 3.10009523\n",
      "Iteration 421, loss = 3.09988799\n",
      "Iteration 422, loss = 3.09894042\n",
      "Iteration 423, loss = 3.09856542\n",
      "Iteration 424, loss = 3.09827093\n",
      "Iteration 425, loss = 3.09799546\n",
      "Iteration 426, loss = 3.09760697\n",
      "Iteration 427, loss = 3.09713702\n",
      "Iteration 428, loss = 3.09678220\n",
      "Iteration 429, loss = 3.09630080\n",
      "Iteration 430, loss = 3.09581829\n",
      "Iteration 431, loss = 3.09512982\n",
      "Iteration 432, loss = 3.09488880\n",
      "Iteration 433, loss = 3.09475018\n",
      "Iteration 434, loss = 3.09463901\n",
      "Iteration 435, loss = 3.09391851\n",
      "Iteration 436, loss = 3.09384036\n",
      "Iteration 437, loss = 3.09327775\n",
      "Iteration 438, loss = 3.09281839\n",
      "Iteration 439, loss = 3.09217664\n",
      "Iteration 440, loss = 3.09181339\n",
      "Iteration 441, loss = 3.09126112\n",
      "Iteration 442, loss = 3.09122780\n",
      "Iteration 443, loss = 3.09084889\n",
      "Iteration 444, loss = 3.09045869\n",
      "Iteration 445, loss = 3.09114979\n",
      "Iteration 446, loss = 3.08980745\n",
      "Iteration 447, loss = 3.08957154\n",
      "Iteration 448, loss = 3.08919852\n",
      "Iteration 449, loss = 3.08866641\n",
      "Iteration 450, loss = 3.08797314\n",
      "Iteration 451, loss = 3.08796344\n",
      "Iteration 452, loss = 3.08752245\n",
      "Iteration 453, loss = 3.08711218\n",
      "Iteration 454, loss = 3.08646067\n",
      "Iteration 455, loss = 3.08668892\n",
      "Iteration 456, loss = 3.08596215\n",
      "Iteration 457, loss = 3.08556974\n",
      "Iteration 458, loss = 3.08505232\n",
      "Iteration 459, loss = 3.08462300\n",
      "Iteration 460, loss = 3.08457235\n",
      "Iteration 461, loss = 3.08437465\n",
      "Iteration 462, loss = 3.08375419\n",
      "Iteration 463, loss = 3.08352350\n",
      "Iteration 464, loss = 3.08323237\n",
      "Iteration 465, loss = 3.08290966\n",
      "Iteration 466, loss = 3.08232387\n",
      "Iteration 467, loss = 3.08180095\n",
      "Iteration 468, loss = 3.08121726\n",
      "Iteration 469, loss = 3.08127050\n",
      "Iteration 470, loss = 3.08148534\n",
      "Iteration 471, loss = 3.08117540\n",
      "Iteration 472, loss = 3.08037604\n",
      "Iteration 473, loss = 3.07976164\n",
      "Iteration 474, loss = 3.07923783\n",
      "Iteration 475, loss = 3.07884428\n",
      "Iteration 476, loss = 3.07864840\n",
      "Iteration 477, loss = 3.07820677\n",
      "Iteration 478, loss = 3.07798406\n",
      "Iteration 479, loss = 3.07719809\n",
      "Iteration 480, loss = 3.07716475\n",
      "Iteration 481, loss = 3.07684726\n",
      "Iteration 482, loss = 3.07604196\n",
      "Iteration 483, loss = 3.07571512\n",
      "Iteration 484, loss = 3.07523093\n",
      "Iteration 485, loss = 3.07526150\n",
      "Iteration 486, loss = 3.07502830\n",
      "Iteration 487, loss = 3.07470018\n",
      "Iteration 488, loss = 3.07499593\n",
      "Iteration 489, loss = 3.07497292\n",
      "Iteration 490, loss = 3.07369370\n",
      "Iteration 491, loss = 3.07322770\n",
      "Iteration 492, loss = 3.07300813\n",
      "Iteration 493, loss = 3.07232474\n",
      "Iteration 494, loss = 3.07192219\n",
      "Iteration 495, loss = 3.07185506\n",
      "Iteration 496, loss = 3.07128124\n",
      "Iteration 497, loss = 3.07131087\n",
      "Iteration 498, loss = 3.07047031\n",
      "Iteration 499, loss = 3.07013455\n",
      "Iteration 500, loss = 3.06976331\n",
      "Iteration 501, loss = 3.06954157\n",
      "Iteration 502, loss = 3.06911507\n",
      "Iteration 503, loss = 3.06857231\n",
      "Iteration 504, loss = 3.06826492\n",
      "Iteration 505, loss = 3.06813570\n",
      "Iteration 506, loss = 3.06769445\n",
      "Iteration 507, loss = 3.06742296\n",
      "Iteration 508, loss = 3.06691349\n",
      "Iteration 509, loss = 3.06673005\n",
      "Iteration 510, loss = 3.06609775\n",
      "Iteration 511, loss = 3.06617757\n",
      "Iteration 512, loss = 3.06584569\n",
      "Iteration 513, loss = 3.06547156\n",
      "Iteration 514, loss = 3.06514405\n",
      "Iteration 515, loss = 3.06457727\n",
      "Iteration 516, loss = 3.06440234\n",
      "Iteration 517, loss = 3.06424506\n",
      "Iteration 518, loss = 3.06370568\n",
      "Iteration 519, loss = 3.06313827\n",
      "Iteration 520, loss = 3.06309702\n",
      "Iteration 521, loss = 3.06301921\n",
      "Iteration 522, loss = 3.06275323\n",
      "Iteration 523, loss = 3.06127535\n",
      "Iteration 524, loss = 3.06095383\n",
      "Iteration 525, loss = 3.06055023\n",
      "Iteration 526, loss = 3.06044565\n",
      "Iteration 527, loss = 3.06004870\n",
      "Iteration 528, loss = 3.05959603\n",
      "Iteration 529, loss = 3.05952355\n",
      "Iteration 530, loss = 3.05922299\n",
      "Iteration 531, loss = 3.05902262\n",
      "Iteration 532, loss = 3.05823534\n",
      "Iteration 533, loss = 3.05756856\n",
      "Iteration 534, loss = 3.05761416\n",
      "Iteration 535, loss = 3.05786385\n",
      "Iteration 536, loss = 3.05731431\n",
      "Iteration 537, loss = 3.05655681\n",
      "Iteration 538, loss = 3.05606965\n",
      "Iteration 539, loss = 3.05559955\n",
      "Iteration 540, loss = 3.05532166\n",
      "Iteration 541, loss = 3.05508645\n",
      "Iteration 542, loss = 3.05469847\n",
      "Iteration 543, loss = 3.05419138\n",
      "Iteration 544, loss = 3.05389628\n",
      "Iteration 545, loss = 3.05408177\n",
      "Iteration 546, loss = 3.05354154\n",
      "Iteration 547, loss = 3.05289496\n",
      "Iteration 548, loss = 3.05233990\n",
      "Iteration 549, loss = 3.05224013\n",
      "Iteration 550, loss = 3.05193255\n",
      "Iteration 551, loss = 3.05168087\n",
      "Iteration 552, loss = 3.05143356\n",
      "Iteration 553, loss = 3.05088084\n",
      "Iteration 554, loss = 3.05056433\n",
      "Iteration 555, loss = 3.05006046\n",
      "Iteration 556, loss = 3.04976170\n",
      "Iteration 557, loss = 3.04940721\n",
      "Iteration 558, loss = 3.04929609\n",
      "Iteration 559, loss = 3.04826587\n",
      "Iteration 560, loss = 3.04803654\n",
      "Iteration 561, loss = 3.04781778\n",
      "Iteration 562, loss = 3.04732142\n",
      "Iteration 563, loss = 3.04688486\n",
      "Iteration 564, loss = 3.04669481\n",
      "Iteration 565, loss = 3.04650648\n",
      "Iteration 566, loss = 3.04626161\n",
      "Iteration 567, loss = 3.04621999\n",
      "Iteration 568, loss = 3.04547616\n",
      "Iteration 569, loss = 3.04505482\n",
      "Iteration 570, loss = 3.04526039\n",
      "Iteration 571, loss = 3.04492802\n",
      "Iteration 572, loss = 3.04443505\n",
      "Iteration 573, loss = 3.04396551\n",
      "Iteration 574, loss = 3.04395330\n",
      "Iteration 575, loss = 3.04353951\n",
      "Iteration 576, loss = 3.04284354\n",
      "Iteration 577, loss = 3.04248423\n",
      "Iteration 578, loss = 3.04223809\n",
      "Iteration 579, loss = 3.04154409\n",
      "Iteration 580, loss = 3.04149999\n",
      "Iteration 581, loss = 3.04131934\n",
      "Iteration 582, loss = 3.04106572\n",
      "Iteration 583, loss = 3.04036031\n",
      "Iteration 584, loss = 3.04027730\n",
      "Iteration 585, loss = 3.04015584\n",
      "Iteration 586, loss = 3.03979233\n",
      "Iteration 587, loss = 3.03897813\n",
      "Iteration 588, loss = 3.03886743\n",
      "Iteration 589, loss = 3.03802419\n",
      "Iteration 590, loss = 3.03829579\n",
      "Iteration 591, loss = 3.03800229\n",
      "Iteration 592, loss = 3.03729533\n",
      "Iteration 593, loss = 3.03664210\n",
      "Iteration 594, loss = 3.03603709\n",
      "Iteration 595, loss = 3.03589842\n",
      "Iteration 596, loss = 3.03570357\n",
      "Iteration 597, loss = 3.03563727\n",
      "Iteration 598, loss = 3.03515365\n",
      "Iteration 599, loss = 3.03509752\n",
      "Iteration 600, loss = 3.03438587\n",
      "Iteration 601, loss = 3.03392475\n",
      "Iteration 602, loss = 3.03407151\n",
      "Iteration 603, loss = 3.03411899\n",
      "Iteration 604, loss = 3.03349847\n",
      "Iteration 605, loss = 3.03310699\n",
      "Iteration 606, loss = 3.03258510\n",
      "Iteration 607, loss = 3.03243595\n",
      "Iteration 608, loss = 3.03256557\n",
      "Iteration 609, loss = 3.03173253\n",
      "Iteration 610, loss = 3.03115870\n",
      "Iteration 611, loss = 3.03106177\n",
      "Iteration 612, loss = 3.03058005\n",
      "Iteration 613, loss = 3.03026142\n",
      "Iteration 614, loss = 3.02977168\n",
      "Iteration 615, loss = 3.02963610\n",
      "Iteration 616, loss = 3.02919549\n",
      "Iteration 617, loss = 3.02896328\n",
      "Iteration 618, loss = 3.02869715\n",
      "Iteration 619, loss = 3.02883369\n",
      "Iteration 620, loss = 3.02785285\n",
      "Iteration 621, loss = 3.02766998\n",
      "Iteration 622, loss = 3.02744881\n",
      "Iteration 623, loss = 3.02713049\n",
      "Iteration 624, loss = 3.02673179\n",
      "Iteration 625, loss = 3.02628129\n",
      "Iteration 626, loss = 3.02598168\n",
      "Iteration 627, loss = 3.02612721\n",
      "Iteration 628, loss = 3.02555322\n",
      "Iteration 629, loss = 3.02516482\n",
      "Iteration 630, loss = 3.02480402\n",
      "Iteration 631, loss = 3.02443328\n",
      "Iteration 632, loss = 3.02369466\n",
      "Iteration 633, loss = 3.02377786\n",
      "Iteration 634, loss = 3.02320998\n",
      "Iteration 635, loss = 3.02305608\n",
      "Iteration 636, loss = 3.02258389\n",
      "Iteration 637, loss = 3.02212965\n",
      "Iteration 638, loss = 3.02248004\n",
      "Iteration 639, loss = 3.02175226\n",
      "Iteration 640, loss = 3.02122265\n",
      "Iteration 641, loss = 3.02080432\n",
      "Iteration 642, loss = 3.02069444\n",
      "Iteration 643, loss = 3.02038303\n",
      "Iteration 644, loss = 3.01994231\n",
      "Iteration 645, loss = 3.01942803\n",
      "Iteration 646, loss = 3.01913958\n",
      "Iteration 647, loss = 3.01906287\n",
      "Iteration 648, loss = 3.01845631\n",
      "Iteration 649, loss = 3.01831266\n",
      "Iteration 650, loss = 3.01837455\n",
      "Iteration 651, loss = 3.01807805\n",
      "Iteration 652, loss = 3.01721163\n",
      "Iteration 653, loss = 3.01711399\n",
      "Iteration 654, loss = 3.01692526\n",
      "Iteration 655, loss = 3.01623660\n",
      "Iteration 656, loss = 3.01611846\n",
      "Iteration 657, loss = 3.01587022\n",
      "Iteration 658, loss = 3.01556731\n",
      "Iteration 659, loss = 3.01500258\n",
      "Iteration 660, loss = 3.01514100\n",
      "Iteration 661, loss = 3.01441089\n",
      "Iteration 662, loss = 3.01426406\n",
      "Iteration 663, loss = 3.01385617\n",
      "Iteration 664, loss = 3.01383102\n",
      "Iteration 665, loss = 3.01346698\n",
      "Iteration 666, loss = 3.01304126\n",
      "Iteration 667, loss = 3.01284300\n",
      "Iteration 668, loss = 3.01236575\n",
      "Iteration 669, loss = 3.01196207\n",
      "Iteration 670, loss = 3.01136531\n",
      "Iteration 671, loss = 3.01133911\n",
      "Iteration 672, loss = 3.01071610\n",
      "Iteration 673, loss = 3.01072190\n",
      "Iteration 674, loss = 3.01027095\n",
      "Iteration 675, loss = 3.01004240\n",
      "Iteration 676, loss = 3.00968352\n",
      "Iteration 677, loss = 3.00906203\n",
      "Iteration 678, loss = 3.00871937\n",
      "Iteration 679, loss = 3.00840176\n",
      "Iteration 680, loss = 3.00808909\n",
      "Iteration 681, loss = 3.00784346\n",
      "Iteration 682, loss = 3.00747734\n",
      "Iteration 683, loss = 3.00727831\n",
      "Iteration 684, loss = 3.00737388\n",
      "Iteration 685, loss = 3.00727594\n",
      "Iteration 686, loss = 3.00645707\n",
      "Iteration 687, loss = 3.00649070\n",
      "Iteration 688, loss = 3.00626621\n",
      "Iteration 689, loss = 3.00576321\n",
      "Iteration 690, loss = 3.00500884\n",
      "Iteration 691, loss = 3.00484813\n",
      "Iteration 692, loss = 3.00476727\n",
      "Iteration 693, loss = 3.00464726\n",
      "Iteration 694, loss = 3.00371787\n",
      "Iteration 695, loss = 3.00356310\n",
      "Iteration 696, loss = 3.00332419\n",
      "Iteration 697, loss = 3.00290841\n",
      "Iteration 698, loss = 3.00243210\n",
      "Iteration 699, loss = 3.00220298\n",
      "Iteration 700, loss = 3.00210500\n",
      "Iteration 701, loss = 3.00219937\n",
      "Iteration 702, loss = 3.00087398\n",
      "Iteration 703, loss = 3.00084791\n",
      "Iteration 704, loss = 3.00071006\n",
      "Iteration 705, loss = 3.00039140\n",
      "Iteration 706, loss = 2.99984883\n",
      "Iteration 707, loss = 3.00002537\n",
      "Iteration 708, loss = 2.99978482\n",
      "Iteration 709, loss = 2.99995091\n",
      "Iteration 710, loss = 2.99936287\n",
      "Iteration 711, loss = 2.99941223\n",
      "Iteration 712, loss = 2.99852366\n",
      "Iteration 713, loss = 2.99770135\n",
      "Iteration 714, loss = 2.99777321\n",
      "Iteration 715, loss = 2.99777769\n",
      "Iteration 716, loss = 2.99681607\n",
      "Iteration 717, loss = 2.99669558\n",
      "Iteration 718, loss = 2.99644493\n",
      "Iteration 719, loss = 2.99632954\n",
      "Iteration 720, loss = 2.99588972\n",
      "Iteration 721, loss = 2.99553569\n",
      "Iteration 722, loss = 2.99551232\n",
      "Iteration 723, loss = 2.99555025\n",
      "Iteration 724, loss = 2.99457118\n",
      "Iteration 725, loss = 2.99466341\n",
      "Iteration 726, loss = 2.99387316\n",
      "Iteration 727, loss = 2.99355213\n",
      "Iteration 728, loss = 2.99318633\n",
      "Iteration 729, loss = 2.99263402\n",
      "Iteration 730, loss = 2.99252638\n",
      "Iteration 731, loss = 2.99214605\n",
      "Iteration 732, loss = 2.99211649\n",
      "Iteration 733, loss = 2.99252916\n",
      "Iteration 734, loss = 2.99232601\n",
      "Iteration 735, loss = 2.99173580\n",
      "Iteration 736, loss = 2.99110324\n",
      "Iteration 737, loss = 2.99158613\n",
      "Iteration 738, loss = 2.99116241\n",
      "Iteration 739, loss = 2.99080704\n",
      "Iteration 740, loss = 2.99066624\n",
      "Iteration 741, loss = 2.99004449\n",
      "Iteration 742, loss = 2.98978195\n",
      "Iteration 743, loss = 2.98940277\n",
      "Iteration 744, loss = 2.98878143\n",
      "Iteration 745, loss = 2.98875114\n",
      "Iteration 746, loss = 2.98808983\n",
      "Iteration 747, loss = 2.98793875\n",
      "Iteration 748, loss = 2.98753178\n",
      "Iteration 749, loss = 2.98699921\n",
      "Iteration 750, loss = 2.98702280\n",
      "Iteration 751, loss = 2.98646758\n",
      "Iteration 752, loss = 2.98624988\n",
      "Iteration 753, loss = 2.98589805\n",
      "Iteration 754, loss = 2.98584725\n",
      "Iteration 755, loss = 2.98533035\n",
      "Iteration 756, loss = 2.98551972\n",
      "Iteration 757, loss = 2.98562244\n",
      "Iteration 758, loss = 2.98495113\n",
      "Iteration 759, loss = 2.98486569\n",
      "Iteration 760, loss = 2.98435661\n",
      "Iteration 761, loss = 2.98408670\n",
      "Iteration 762, loss = 2.98399820\n",
      "Iteration 763, loss = 2.98307359\n",
      "Iteration 764, loss = 2.98273309\n",
      "Iteration 765, loss = 2.98236199\n",
      "Iteration 766, loss = 2.98238301\n",
      "Iteration 767, loss = 2.98214473\n",
      "Iteration 768, loss = 2.98183424\n",
      "Iteration 769, loss = 2.98141319\n",
      "Iteration 770, loss = 2.98091722\n",
      "Iteration 771, loss = 2.98064514\n",
      "Iteration 772, loss = 2.98061480\n",
      "Iteration 773, loss = 2.98065630\n",
      "Iteration 774, loss = 2.97990255\n",
      "Iteration 775, loss = 2.97972274\n",
      "Iteration 776, loss = 2.97978559\n",
      "Iteration 777, loss = 2.97921829\n",
      "Iteration 778, loss = 2.97872423\n",
      "Iteration 779, loss = 2.97875098\n",
      "Iteration 780, loss = 2.97816278\n",
      "Iteration 781, loss = 2.97796164\n",
      "Iteration 782, loss = 2.97777060\n",
      "Iteration 783, loss = 2.97773640\n",
      "Iteration 784, loss = 2.97743422\n",
      "Iteration 785, loss = 2.97702554\n",
      "Iteration 786, loss = 2.97707001\n",
      "Iteration 787, loss = 2.97687533\n",
      "Iteration 788, loss = 2.97678410\n",
      "Iteration 789, loss = 2.97610233\n",
      "Iteration 790, loss = 2.97616236\n",
      "Iteration 791, loss = 2.97661741\n",
      "Iteration 792, loss = 2.97510348\n",
      "Iteration 793, loss = 2.97437564\n",
      "Iteration 794, loss = 2.97405324\n",
      "Iteration 795, loss = 2.97380047\n",
      "Iteration 796, loss = 2.97346879\n",
      "Iteration 797, loss = 2.97324714\n",
      "Iteration 798, loss = 2.97324208\n",
      "Iteration 799, loss = 2.97355749\n",
      "Iteration 800, loss = 2.97257107\n",
      "Iteration 801, loss = 2.97208261\n",
      "Iteration 802, loss = 2.97191197\n",
      "Iteration 803, loss = 2.97170202\n",
      "Iteration 804, loss = 2.97156939\n",
      "Iteration 805, loss = 2.97096284\n",
      "Iteration 806, loss = 2.97133692\n",
      "Iteration 807, loss = 2.97115313\n",
      "Iteration 808, loss = 2.97085146\n",
      "Iteration 809, loss = 2.97009130\n",
      "Iteration 810, loss = 2.96965306\n",
      "Iteration 811, loss = 2.96992986\n",
      "Iteration 812, loss = 2.96959738\n",
      "Iteration 813, loss = 2.96887280\n",
      "Iteration 814, loss = 2.96829496\n",
      "Iteration 815, loss = 2.96836871\n",
      "Iteration 816, loss = 2.96813533\n",
      "Iteration 817, loss = 2.96743584\n",
      "Iteration 818, loss = 2.96723084\n",
      "Iteration 819, loss = 2.96730230\n",
      "Iteration 820, loss = 2.96729043\n",
      "Iteration 821, loss = 2.96688365\n",
      "Iteration 822, loss = 2.96651029\n",
      "Iteration 823, loss = 2.96655851\n",
      "Iteration 824, loss = 2.96663699\n",
      "Iteration 825, loss = 2.96657278\n",
      "Iteration 826, loss = 2.96535493\n",
      "Iteration 827, loss = 2.96532937\n",
      "Iteration 828, loss = 2.96491906\n",
      "Iteration 829, loss = 2.96516700\n",
      "Iteration 830, loss = 2.96517555\n",
      "Iteration 831, loss = 2.96427267\n",
      "Iteration 832, loss = 2.96347563\n",
      "Iteration 833, loss = 2.96311961\n",
      "Iteration 834, loss = 2.96296575\n",
      "Iteration 835, loss = 2.96260572\n",
      "Iteration 836, loss = 2.96241161\n",
      "Iteration 837, loss = 2.96322269\n",
      "Iteration 838, loss = 2.96167087\n",
      "Iteration 839, loss = 2.96215766\n",
      "Iteration 840, loss = 2.96168615\n",
      "Iteration 841, loss = 2.96118386\n",
      "Iteration 842, loss = 2.96132641\n",
      "Iteration 843, loss = 2.96049921\n",
      "Iteration 844, loss = 2.96001229\n",
      "Iteration 845, loss = 2.96196498\n",
      "Iteration 846, loss = 2.96126684\n",
      "Iteration 847, loss = 2.95945471\n",
      "Iteration 848, loss = 2.95906574\n",
      "Iteration 849, loss = 2.95890853\n",
      "Iteration 850, loss = 2.95859318\n",
      "Iteration 851, loss = 2.95852786\n",
      "Iteration 852, loss = 2.95827106\n",
      "Iteration 853, loss = 2.95794105\n",
      "Iteration 854, loss = 2.95751084\n",
      "Iteration 855, loss = 2.95766169\n",
      "Iteration 856, loss = 2.95758618\n",
      "Iteration 857, loss = 2.95668623\n",
      "Iteration 858, loss = 2.95635801\n",
      "Iteration 859, loss = 2.95593441\n",
      "Iteration 860, loss = 2.95631924\n",
      "Iteration 861, loss = 2.95583722\n",
      "Iteration 862, loss = 2.95605188\n",
      "Iteration 863, loss = 2.95610196\n",
      "Iteration 864, loss = 2.95469293\n",
      "Iteration 865, loss = 2.95574133\n",
      "Iteration 866, loss = 2.95491476\n",
      "Iteration 867, loss = 2.95362345\n",
      "Iteration 868, loss = 2.95374777\n",
      "Iteration 869, loss = 2.95347415\n",
      "Iteration 870, loss = 2.95288981\n",
      "Iteration 871, loss = 2.95232503\n",
      "Iteration 872, loss = 2.95263431\n",
      "Iteration 873, loss = 2.95196149\n",
      "Iteration 874, loss = 2.95164170\n",
      "Iteration 875, loss = 2.95144293\n",
      "Iteration 876, loss = 2.95119500\n",
      "Iteration 877, loss = 2.95101818\n",
      "Iteration 878, loss = 2.95093943\n",
      "Iteration 879, loss = 2.95100204\n",
      "Iteration 880, loss = 2.95093480\n",
      "Iteration 881, loss = 2.95011661\n",
      "Iteration 882, loss = 2.94941380\n",
      "Iteration 883, loss = 2.94927898\n",
      "Iteration 884, loss = 2.94920627\n",
      "Iteration 885, loss = 2.94900455\n",
      "Iteration 886, loss = 2.94822107\n",
      "Iteration 887, loss = 2.94830275\n",
      "Iteration 888, loss = 2.94825652\n",
      "Iteration 889, loss = 2.94807952\n",
      "Iteration 890, loss = 2.94752219\n",
      "Iteration 891, loss = 2.94731289\n",
      "Iteration 892, loss = 2.94749907\n",
      "Iteration 893, loss = 2.94729062\n",
      "Iteration 894, loss = 2.94661466\n",
      "Iteration 895, loss = 2.94625082\n",
      "Iteration 896, loss = 2.94637929\n",
      "Iteration 897, loss = 2.94655361\n",
      "Iteration 898, loss = 2.94608411\n",
      "Iteration 899, loss = 2.94551796\n",
      "Iteration 900, loss = 2.94513598\n",
      "Iteration 901, loss = 2.94495229\n",
      "Iteration 902, loss = 2.94471600\n",
      "Iteration 903, loss = 2.94425254\n",
      "Iteration 904, loss = 2.94371762\n",
      "Iteration 905, loss = 2.94330377\n",
      "Iteration 906, loss = 2.94333453\n",
      "Iteration 907, loss = 2.94297348\n",
      "Iteration 908, loss = 2.94282511\n",
      "Iteration 909, loss = 2.94262707\n",
      "Iteration 910, loss = 2.94312588\n",
      "Iteration 911, loss = 2.94190291\n",
      "Iteration 912, loss = 2.94158261\n",
      "Iteration 913, loss = 2.94151002\n",
      "Iteration 914, loss = 2.94120917\n",
      "Iteration 915, loss = 2.94141545\n",
      "Iteration 916, loss = 2.94094786\n",
      "Iteration 917, loss = 2.94135131\n",
      "Iteration 918, loss = 2.94112728\n",
      "Iteration 919, loss = 2.94060957\n",
      "Iteration 920, loss = 2.93985186\n",
      "Iteration 921, loss = 2.93969574\n",
      "Iteration 922, loss = 2.93953633\n",
      "Iteration 923, loss = 2.93917514\n",
      "Iteration 924, loss = 2.93847243\n",
      "Iteration 925, loss = 2.93821070\n",
      "Iteration 926, loss = 2.93805499\n",
      "Iteration 927, loss = 2.93774724\n",
      "Iteration 928, loss = 2.93771770\n",
      "Iteration 929, loss = 2.93741324\n",
      "Iteration 930, loss = 2.93725167\n",
      "Iteration 931, loss = 2.93788231\n",
      "Iteration 932, loss = 2.93751686\n",
      "Iteration 933, loss = 2.93694527\n",
      "Iteration 934, loss = 2.93639309\n",
      "Iteration 935, loss = 2.93587816\n",
      "Iteration 936, loss = 2.93587273\n",
      "Iteration 937, loss = 2.93524014\n",
      "Iteration 938, loss = 2.93507492\n",
      "Iteration 939, loss = 2.93538601\n",
      "Iteration 940, loss = 2.93519159\n",
      "Iteration 941, loss = 2.93467358\n",
      "Iteration 942, loss = 2.93460713\n",
      "Iteration 943, loss = 2.93383360\n",
      "Iteration 944, loss = 2.93335381\n",
      "Iteration 945, loss = 2.93321343\n",
      "Iteration 946, loss = 2.93297961\n",
      "Iteration 947, loss = 2.93281297\n",
      "Iteration 948, loss = 2.93258486\n",
      "Iteration 949, loss = 2.93202408\n",
      "Iteration 950, loss = 2.93200186\n",
      "Iteration 951, loss = 2.93117651\n",
      "Iteration 952, loss = 2.93123725\n",
      "Iteration 953, loss = 2.93150808\n",
      "Iteration 954, loss = 2.93094392\n",
      "Iteration 955, loss = 2.93078772\n",
      "Iteration 956, loss = 2.93097600\n",
      "Iteration 957, loss = 2.93071885\n",
      "Iteration 958, loss = 2.93037792\n",
      "Iteration 959, loss = 2.92978920\n",
      "Iteration 960, loss = 2.92973410\n",
      "Iteration 961, loss = 2.92933222\n",
      "Iteration 962, loss = 2.92881054\n",
      "Iteration 963, loss = 2.92887225\n",
      "Iteration 964, loss = 2.92865485\n",
      "Iteration 965, loss = 2.92834567\n",
      "Iteration 966, loss = 2.92768304\n",
      "Iteration 967, loss = 2.92729552\n",
      "Iteration 968, loss = 2.92661327\n",
      "Iteration 969, loss = 2.92667201\n",
      "Iteration 970, loss = 2.92666721\n",
      "Iteration 971, loss = 2.92668664\n",
      "Iteration 972, loss = 2.92617906\n",
      "Iteration 973, loss = 2.92600011\n",
      "Iteration 974, loss = 2.92572041\n",
      "Iteration 975, loss = 2.92536360\n",
      "Iteration 976, loss = 2.92525443\n",
      "Iteration 977, loss = 2.92528636\n",
      "Iteration 978, loss = 2.92466797\n",
      "Iteration 979, loss = 2.92406200\n",
      "Iteration 980, loss = 2.92393338\n",
      "Iteration 981, loss = 2.92421148\n",
      "Iteration 982, loss = 2.92363320\n",
      "Iteration 983, loss = 2.92311533\n",
      "Iteration 984, loss = 2.92297739\n",
      "Iteration 985, loss = 2.92304005\n",
      "Iteration 986, loss = 2.92248120\n",
      "Iteration 987, loss = 2.92197608\n",
      "Iteration 988, loss = 2.92193488\n",
      "Iteration 989, loss = 2.92125281\n",
      "Iteration 990, loss = 2.92137316\n",
      "Iteration 991, loss = 2.92137020\n",
      "Iteration 992, loss = 2.92131002\n",
      "Iteration 993, loss = 2.92112661\n",
      "Iteration 994, loss = 2.92104619\n",
      "Iteration 995, loss = 2.92052859\n",
      "Iteration 996, loss = 2.92023001\n",
      "Iteration 997, loss = 2.92053236\n",
      "Iteration 998, loss = 2.92006016\n",
      "Iteration 999, loss = 2.91945438\n",
      "Iteration 1000, loss = 2.91972922\n",
      "Iteration 1001, loss = 2.91912423\n",
      "Iteration 1002, loss = 2.91884499\n",
      "Iteration 1003, loss = 2.91850888\n",
      "Iteration 1004, loss = 2.91828223\n",
      "Iteration 1005, loss = 2.91796494\n",
      "Iteration 1006, loss = 2.91753440\n",
      "Iteration 1007, loss = 2.91790281\n",
      "Iteration 1008, loss = 2.91750404\n",
      "Iteration 1009, loss = 2.91677560\n",
      "Iteration 1010, loss = 2.91684921\n",
      "Iteration 1011, loss = 2.91677914\n",
      "Iteration 1012, loss = 2.91615440\n",
      "Iteration 1013, loss = 2.91584455\n",
      "Iteration 1014, loss = 2.91578141\n",
      "Iteration 1015, loss = 2.91642309\n",
      "Iteration 1016, loss = 2.91544926\n",
      "Iteration 1017, loss = 2.91474745\n",
      "Iteration 1018, loss = 2.91485309\n",
      "Iteration 1019, loss = 2.91411131\n",
      "Iteration 1020, loss = 2.91407544\n",
      "Iteration 1021, loss = 2.91380506\n",
      "Iteration 1022, loss = 2.91336784\n",
      "Iteration 1023, loss = 2.91369174\n",
      "Iteration 1024, loss = 2.91471059\n",
      "Iteration 1025, loss = 2.91463052\n",
      "Iteration 1026, loss = 2.91290613\n",
      "Iteration 1027, loss = 2.91266124\n",
      "Iteration 1028, loss = 2.91216809\n",
      "Iteration 1029, loss = 2.91249040\n",
      "Iteration 1030, loss = 2.91188069\n",
      "Iteration 1031, loss = 2.91192390\n",
      "Iteration 1032, loss = 2.91190679\n",
      "Iteration 1033, loss = 2.91166192\n",
      "Iteration 1034, loss = 2.91132180\n",
      "Iteration 1035, loss = 2.91053508\n",
      "Iteration 1036, loss = 2.91085607\n",
      "Iteration 1037, loss = 2.91059418\n",
      "Iteration 1038, loss = 2.91023565\n",
      "Iteration 1039, loss = 2.91050065\n",
      "Iteration 1040, loss = 2.91078989\n",
      "Iteration 1041, loss = 2.90949840\n",
      "Iteration 1042, loss = 2.90910445\n",
      "Iteration 1043, loss = 2.90962878\n",
      "Iteration 1044, loss = 2.90923043\n",
      "Iteration 1045, loss = 2.90831109\n",
      "Iteration 1046, loss = 2.90755119\n",
      "Iteration 1047, loss = 2.90725585\n",
      "Iteration 1048, loss = 2.90730883\n",
      "Iteration 1049, loss = 2.90725779\n",
      "Iteration 1050, loss = 2.90704160\n",
      "Iteration 1051, loss = 2.90673144\n",
      "Iteration 1052, loss = 2.90723046\n",
      "Iteration 1053, loss = 2.90643816\n",
      "Iteration 1054, loss = 2.90654795\n",
      "Iteration 1055, loss = 2.90649778\n",
      "Iteration 1056, loss = 2.90600561\n",
      "Iteration 1057, loss = 2.90535247\n",
      "Iteration 1058, loss = 2.90531799\n",
      "Iteration 1059, loss = 2.90469795\n",
      "Iteration 1060, loss = 2.90470270\n",
      "Iteration 1061, loss = 2.90437133\n",
      "Iteration 1062, loss = 2.90408110\n",
      "Iteration 1063, loss = 2.90430336\n",
      "Iteration 1064, loss = 2.90342841\n",
      "Iteration 1065, loss = 2.90374354\n",
      "Iteration 1066, loss = 2.90380711\n",
      "Iteration 1067, loss = 2.90337022\n",
      "Iteration 1068, loss = 2.90304022\n",
      "Iteration 1069, loss = 2.90297679\n",
      "Iteration 1070, loss = 2.90263724\n",
      "Iteration 1071, loss = 2.90254216\n",
      "Iteration 1072, loss = 2.90266145\n",
      "Iteration 1073, loss = 2.90245327\n",
      "Iteration 1074, loss = 2.90139943\n",
      "Iteration 1075, loss = 2.90259273\n",
      "Iteration 1076, loss = 2.90273461\n",
      "Iteration 1077, loss = 2.90148066\n",
      "Iteration 1078, loss = 2.90116595\n",
      "Iteration 1079, loss = 2.90076454\n",
      "Iteration 1080, loss = 2.90077007\n",
      "Iteration 1081, loss = 2.90015286\n",
      "Iteration 1082, loss = 2.90004124\n",
      "Iteration 1083, loss = 2.89919222\n",
      "Iteration 1084, loss = 2.89997833\n",
      "Iteration 1085, loss = 2.89994342\n",
      "Iteration 1086, loss = 2.89962206\n",
      "Iteration 1087, loss = 2.89917244\n",
      "Iteration 1088, loss = 2.89888985\n",
      "Iteration 1089, loss = 2.89841110\n",
      "Iteration 1090, loss = 2.89834426\n",
      "Iteration 1091, loss = 2.89764070\n",
      "Iteration 1092, loss = 2.89765071\n",
      "Iteration 1093, loss = 2.89750096\n",
      "Iteration 1094, loss = 2.89755899\n",
      "Iteration 1095, loss = 2.89683504\n",
      "Iteration 1096, loss = 2.89655955\n",
      "Iteration 1097, loss = 2.89695171\n",
      "Iteration 1098, loss = 2.89697648\n",
      "Iteration 1099, loss = 2.89605710\n",
      "Iteration 1100, loss = 2.89563667\n",
      "Iteration 1101, loss = 2.89558314\n",
      "Iteration 1102, loss = 2.89534232\n",
      "Iteration 1103, loss = 2.89508068\n",
      "Iteration 1104, loss = 2.89462681\n",
      "Iteration 1105, loss = 2.89481481\n",
      "Iteration 1106, loss = 2.89526797\n",
      "Iteration 1107, loss = 2.89456347\n",
      "Iteration 1108, loss = 2.89354452\n",
      "Iteration 1109, loss = 2.89392371\n",
      "Iteration 1110, loss = 2.89349079\n",
      "Iteration 1111, loss = 2.89368816\n",
      "Iteration 1112, loss = 2.89315513\n",
      "Iteration 1113, loss = 2.89273451\n",
      "Iteration 1114, loss = 2.89235223\n",
      "Iteration 1115, loss = 2.89212679\n",
      "Iteration 1116, loss = 2.89203614\n",
      "Iteration 1117, loss = 2.89204999\n",
      "Iteration 1118, loss = 2.89176066\n",
      "Iteration 1119, loss = 2.89193100\n",
      "Iteration 1120, loss = 2.89143045\n",
      "Iteration 1121, loss = 2.89114741\n",
      "Iteration 1122, loss = 2.89055563\n",
      "Iteration 1123, loss = 2.89043533\n",
      "Iteration 1124, loss = 2.89040818\n",
      "Iteration 1125, loss = 2.88978123\n",
      "Iteration 1126, loss = 2.88998722\n",
      "Iteration 1127, loss = 2.88950922\n",
      "Iteration 1128, loss = 2.88899909\n",
      "Iteration 1129, loss = 2.88961872\n",
      "Iteration 1130, loss = 2.88887297\n",
      "Iteration 1131, loss = 2.88836829\n",
      "Iteration 1132, loss = 2.88822938\n",
      "Iteration 1133, loss = 2.88831163\n",
      "Iteration 1134, loss = 2.88847076\n",
      "Iteration 1135, loss = 2.88838774\n",
      "Iteration 1136, loss = 2.88788598\n",
      "Iteration 1137, loss = 2.88796090\n",
      "Iteration 1138, loss = 2.88773338\n",
      "Iteration 1139, loss = 2.88677744\n",
      "Iteration 1140, loss = 2.88684453\n",
      "Iteration 1141, loss = 2.88707480\n",
      "Iteration 1142, loss = 2.88714589\n",
      "Iteration 1143, loss = 2.88683034\n",
      "Iteration 1144, loss = 2.88588202\n",
      "Iteration 1145, loss = 2.88600537\n",
      "Iteration 1146, loss = 2.88528587\n",
      "Iteration 1147, loss = 2.88502036\n",
      "Iteration 1148, loss = 2.88529410\n",
      "Iteration 1149, loss = 2.88510579\n",
      "Iteration 1150, loss = 2.88468952\n",
      "Iteration 1151, loss = 2.88531598\n",
      "Iteration 1152, loss = 2.88545202\n",
      "Iteration 1153, loss = 2.88445803\n",
      "Iteration 1154, loss = 2.88442402\n",
      "Iteration 1155, loss = 2.88410822\n",
      "Iteration 1156, loss = 2.88409449\n",
      "Iteration 1157, loss = 2.88345770\n",
      "Iteration 1158, loss = 2.88314443\n",
      "Iteration 1159, loss = 2.88313531\n",
      "Iteration 1160, loss = 2.88246106\n",
      "Iteration 1161, loss = 2.88246582\n",
      "Iteration 1162, loss = 2.88202503\n",
      "Iteration 1163, loss = 2.88215051\n",
      "Iteration 1164, loss = 2.88186450\n",
      "Iteration 1165, loss = 2.88139389\n",
      "Iteration 1166, loss = 2.88164718\n",
      "Iteration 1167, loss = 2.88132697\n",
      "Iteration 1168, loss = 2.88114001\n",
      "Iteration 1169, loss = 2.88082840\n",
      "Iteration 1170, loss = 2.88050569\n",
      "Iteration 1171, loss = 2.88047053\n",
      "Iteration 1172, loss = 2.87995409\n",
      "Iteration 1173, loss = 2.88043005\n",
      "Iteration 1174, loss = 2.87975173\n",
      "Iteration 1175, loss = 2.87989923\n",
      "Iteration 1176, loss = 2.88020820\n",
      "Iteration 1177, loss = 2.87953932\n",
      "Iteration 1178, loss = 2.87919714\n",
      "Iteration 1179, loss = 2.87910366\n",
      "Iteration 1180, loss = 2.87875877\n",
      "Iteration 1181, loss = 2.87872288\n",
      "Iteration 1182, loss = 2.87815899\n",
      "Iteration 1183, loss = 2.87776692\n",
      "Iteration 1184, loss = 2.87788034\n",
      "Iteration 1185, loss = 2.87727116\n",
      "Iteration 1186, loss = 2.87762331\n",
      "Iteration 1187, loss = 2.87730958\n",
      "Iteration 1188, loss = 2.87611044\n",
      "Iteration 1189, loss = 2.87635855\n",
      "Iteration 1190, loss = 2.87661102\n",
      "Iteration 1191, loss = 2.87605799\n",
      "Iteration 1192, loss = 2.87568286\n",
      "Iteration 1193, loss = 2.87600451\n",
      "Iteration 1194, loss = 2.87568663\n",
      "Iteration 1195, loss = 2.87586438\n",
      "Iteration 1196, loss = 2.87520715\n",
      "Iteration 1197, loss = 2.87505551\n",
      "Iteration 1198, loss = 2.87455425\n",
      "Iteration 1199, loss = 2.87451954\n",
      "Iteration 1200, loss = 2.87376669\n",
      "Iteration 1201, loss = 2.87391906\n",
      "Iteration 1202, loss = 2.87379280\n",
      "Iteration 1203, loss = 2.87354418\n",
      "Iteration 1204, loss = 2.87382925\n",
      "Iteration 1205, loss = 2.87300186\n",
      "Iteration 1206, loss = 2.87265751\n",
      "Iteration 1207, loss = 2.87306076\n",
      "Iteration 1208, loss = 2.87291905\n",
      "Iteration 1209, loss = 2.87246838\n",
      "Iteration 1210, loss = 2.87212369\n",
      "Iteration 1211, loss = 2.87189090\n",
      "Iteration 1212, loss = 2.87192825\n",
      "Iteration 1213, loss = 2.87177381\n",
      "Iteration 1214, loss = 2.87150171\n",
      "Iteration 1215, loss = 2.87098123\n",
      "Iteration 1216, loss = 2.87110077\n",
      "Iteration 1217, loss = 2.87074362\n",
      "Iteration 1218, loss = 2.87072071\n",
      "Iteration 1219, loss = 2.87099291\n",
      "Iteration 1220, loss = 2.87015993\n",
      "Iteration 1221, loss = 2.87004390\n",
      "Iteration 1222, loss = 2.86972222\n",
      "Iteration 1223, loss = 2.86911571\n",
      "Iteration 1224, loss = 2.86918694\n",
      "Iteration 1225, loss = 2.86892539\n",
      "Iteration 1226, loss = 2.86864330\n",
      "Iteration 1227, loss = 2.86866518\n",
      "Iteration 1228, loss = 2.86848398\n",
      "Iteration 1229, loss = 2.86831939\n",
      "Iteration 1230, loss = 2.86794403\n",
      "Iteration 1231, loss = 2.86772403\n",
      "Iteration 1232, loss = 2.86745569\n",
      "Iteration 1233, loss = 2.86741296\n",
      "Iteration 1234, loss = 2.86745337\n",
      "Iteration 1235, loss = 2.86719486\n",
      "Iteration 1236, loss = 2.86703125\n",
      "Iteration 1237, loss = 2.86711601\n",
      "Iteration 1238, loss = 2.86645862\n",
      "Iteration 1239, loss = 2.86629432\n",
      "Iteration 1240, loss = 2.86589160\n",
      "Iteration 1241, loss = 2.86602869\n",
      "Iteration 1242, loss = 2.86638707\n",
      "Iteration 1243, loss = 2.86545899\n",
      "Iteration 1244, loss = 2.86506616\n",
      "Iteration 1245, loss = 2.86523601\n",
      "Iteration 1246, loss = 2.86569572\n",
      "Iteration 1247, loss = 2.86501504\n",
      "Iteration 1248, loss = 2.86508563\n",
      "Iteration 1249, loss = 2.86498171\n",
      "Iteration 1250, loss = 2.86436697\n",
      "Iteration 1251, loss = 2.86403244\n",
      "Iteration 1252, loss = 2.86405979\n",
      "Iteration 1253, loss = 2.86402524\n",
      "Iteration 1254, loss = 2.86391005\n",
      "Iteration 1255, loss = 2.86397642\n",
      "Iteration 1256, loss = 2.86356725\n",
      "Iteration 1257, loss = 2.86264878\n",
      "Iteration 1258, loss = 2.86343251\n",
      "Iteration 1259, loss = 2.86328728\n",
      "Iteration 1260, loss = 2.86244041\n",
      "Iteration 1261, loss = 2.86243648\n",
      "Iteration 1262, loss = 2.86225781\n",
      "Iteration 1263, loss = 2.86161511\n",
      "Iteration 1264, loss = 2.86159449\n",
      "Iteration 1265, loss = 2.86124977\n",
      "Iteration 1266, loss = 2.86146845\n",
      "Iteration 1267, loss = 2.86130801\n",
      "Iteration 1268, loss = 2.86065425\n",
      "Iteration 1269, loss = 2.86069680\n",
      "Iteration 1270, loss = 2.86093702\n",
      "Iteration 1271, loss = 2.86089235\n",
      "Iteration 1272, loss = 2.86038260\n",
      "Iteration 1273, loss = 2.85976698\n",
      "Iteration 1274, loss = 2.85998982\n",
      "Iteration 1275, loss = 2.85985827\n",
      "Iteration 1276, loss = 2.85926264\n",
      "Iteration 1277, loss = 2.85902018\n",
      "Iteration 1278, loss = 2.85864293\n",
      "Iteration 1279, loss = 2.85870449\n",
      "Iteration 1280, loss = 2.85832069\n",
      "Iteration 1281, loss = 2.85775610\n",
      "Iteration 1282, loss = 2.85806763\n",
      "Iteration 1283, loss = 2.85790879\n",
      "Iteration 1284, loss = 2.85752181\n",
      "Iteration 1285, loss = 2.85959901\n",
      "Iteration 1286, loss = 2.85778471\n",
      "Iteration 1287, loss = 2.85714218\n",
      "Iteration 1288, loss = 2.85707686\n",
      "Iteration 1289, loss = 2.85727992\n",
      "Iteration 1290, loss = 2.85729395\n",
      "Iteration 1291, loss = 2.85638451\n",
      "Iteration 1292, loss = 2.85641270\n",
      "Iteration 1293, loss = 2.85615115\n",
      "Iteration 1294, loss = 2.85557284\n",
      "Iteration 1295, loss = 2.85540968\n",
      "Iteration 1296, loss = 2.85539516\n",
      "Iteration 1297, loss = 2.85503166\n",
      "Iteration 1298, loss = 2.85554010\n",
      "Iteration 1299, loss = 2.85491279\n",
      "Iteration 1300, loss = 2.85486961\n",
      "Iteration 1301, loss = 2.85450307\n",
      "Iteration 1302, loss = 2.85458977\n",
      "Iteration 1303, loss = 2.85408417\n",
      "Iteration 1304, loss = 2.85383711\n",
      "Iteration 1305, loss = 2.85360461\n",
      "Iteration 1306, loss = 2.85352931\n",
      "Iteration 1307, loss = 2.85305303\n",
      "Iteration 1308, loss = 2.85249588\n",
      "Iteration 1309, loss = 2.85306710\n",
      "Iteration 1310, loss = 2.85237121\n",
      "Iteration 1311, loss = 2.85212591\n",
      "Iteration 1312, loss = 2.85264372\n",
      "Iteration 1313, loss = 2.85229928\n",
      "Iteration 1314, loss = 2.85215763\n",
      "Iteration 1315, loss = 2.85197782\n",
      "Iteration 1316, loss = 2.85191920\n",
      "Iteration 1317, loss = 2.85195446\n",
      "Iteration 1318, loss = 2.85184170\n",
      "Iteration 1319, loss = 2.85194150\n",
      "Iteration 1320, loss = 2.85108584\n",
      "Iteration 1321, loss = 2.85076754\n",
      "Iteration 1322, loss = 2.85078170\n",
      "Iteration 1323, loss = 2.85029128\n",
      "Iteration 1324, loss = 2.85005858\n",
      "Iteration 1325, loss = 2.85038633\n",
      "Iteration 1326, loss = 2.85012415\n",
      "Iteration 1327, loss = 2.84978115\n",
      "Iteration 1328, loss = 2.84964181\n",
      "Iteration 1329, loss = 2.84888304\n",
      "Iteration 1330, loss = 2.84863729\n",
      "Iteration 1331, loss = 2.84820328\n",
      "Iteration 1332, loss = 2.84840125\n",
      "Iteration 1333, loss = 2.84792124\n",
      "Iteration 1334, loss = 2.84991936\n",
      "Iteration 1335, loss = 2.84912320\n",
      "Iteration 1336, loss = 2.84814128\n",
      "Iteration 1337, loss = 2.84822296\n",
      "Iteration 1338, loss = 2.84789706\n",
      "Iteration 1339, loss = 2.84706089\n",
      "Iteration 1340, loss = 2.84709634\n",
      "Iteration 1341, loss = 2.84695805\n",
      "Iteration 1342, loss = 2.84692178\n",
      "Iteration 1343, loss = 2.84682793\n",
      "Iteration 1344, loss = 2.84691534\n",
      "Iteration 1345, loss = 2.84693519\n",
      "Iteration 1346, loss = 2.84667404\n",
      "Iteration 1347, loss = 2.84602498\n",
      "Iteration 1348, loss = 2.84568296\n",
      "Iteration 1349, loss = 2.84575553\n",
      "Iteration 1350, loss = 2.84583572\n",
      "Iteration 1351, loss = 2.84595841\n",
      "Iteration 1352, loss = 2.84549579\n",
      "Iteration 1353, loss = 2.84445170\n",
      "Iteration 1354, loss = 2.84466461\n",
      "Iteration 1355, loss = 2.84484088\n",
      "Iteration 1356, loss = 2.84542548\n",
      "Iteration 1357, loss = 2.84415322\n",
      "Iteration 1358, loss = 2.84392496\n",
      "Iteration 1359, loss = 2.84334770\n",
      "Iteration 1360, loss = 2.84374186\n",
      "Iteration 1361, loss = 2.84404216\n",
      "Iteration 1362, loss = 2.84359410\n",
      "Iteration 1363, loss = 2.84313874\n",
      "Iteration 1364, loss = 2.84370434\n",
      "Iteration 1365, loss = 2.84267630\n",
      "Iteration 1366, loss = 2.84233540\n",
      "Iteration 1367, loss = 2.84210881\n",
      "Iteration 1368, loss = 2.84212204\n",
      "Iteration 1369, loss = 2.84189112\n",
      "Iteration 1370, loss = 2.84215816\n",
      "Iteration 1371, loss = 2.84166695\n",
      "Iteration 1372, loss = 2.84155510\n",
      "Iteration 1373, loss = 2.84129087\n",
      "Iteration 1374, loss = 2.84112819\n",
      "Iteration 1375, loss = 2.84091372\n",
      "Iteration 1376, loss = 2.84159414\n",
      "Iteration 1377, loss = 2.84128447\n",
      "Iteration 1378, loss = 2.84170698\n",
      "Iteration 1379, loss = 2.84056042\n",
      "Iteration 1380, loss = 2.84082137\n",
      "Iteration 1381, loss = 2.84079943\n",
      "Iteration 1382, loss = 2.84021313\n",
      "Iteration 1383, loss = 2.83966465\n",
      "Iteration 1384, loss = 2.83985945\n",
      "Iteration 1385, loss = 2.83911942\n",
      "Iteration 1386, loss = 2.83945479\n",
      "Iteration 1387, loss = 2.83911297\n",
      "Iteration 1388, loss = 2.83905063\n",
      "Iteration 1389, loss = 2.83848665\n",
      "Iteration 1390, loss = 2.83861044\n",
      "Iteration 1391, loss = 2.83835350\n",
      "Iteration 1392, loss = 2.83775395\n",
      "Iteration 1393, loss = 2.83779721\n",
      "Iteration 1394, loss = 2.83776152\n",
      "Iteration 1395, loss = 2.83778995\n",
      "Iteration 1396, loss = 2.83709731\n",
      "Iteration 1397, loss = 2.83823787\n",
      "Iteration 1398, loss = 2.83730559\n",
      "Iteration 1399, loss = 2.83644695\n",
      "Iteration 1400, loss = 2.83642475\n",
      "Iteration 1401, loss = 2.83625913\n",
      "Iteration 1402, loss = 2.83649049\n",
      "Iteration 1403, loss = 2.83661020\n",
      "Iteration 1404, loss = 2.83588496\n",
      "Iteration 1405, loss = 2.83527855\n",
      "Iteration 1406, loss = 2.83568109\n",
      "Iteration 1407, loss = 2.83532740\n",
      "Iteration 1408, loss = 2.83510823\n",
      "Iteration 1409, loss = 2.83530263\n",
      "Iteration 1410, loss = 2.83558786\n",
      "Iteration 1411, loss = 2.83576883\n",
      "Iteration 1412, loss = 2.83478905\n",
      "Iteration 1413, loss = 2.83395501\n",
      "Iteration 1414, loss = 2.83445343\n",
      "Iteration 1415, loss = 2.83584735\n",
      "Iteration 1416, loss = 2.83515886\n",
      "Iteration 1417, loss = 2.83420280\n",
      "Iteration 1418, loss = 2.83355542\n",
      "Iteration 1419, loss = 2.83325180\n",
      "Iteration 1420, loss = 2.83305347\n",
      "Iteration 1421, loss = 2.83292685\n",
      "Iteration 1422, loss = 2.83330056\n",
      "Iteration 1423, loss = 2.83315972\n",
      "Iteration 1424, loss = 2.83343820\n",
      "Iteration 1425, loss = 2.83295398\n",
      "Iteration 1426, loss = 2.83282716\n",
      "Iteration 1427, loss = 2.83265492\n",
      "Iteration 1428, loss = 2.83243517\n",
      "Iteration 1429, loss = 2.83213155\n",
      "Iteration 1430, loss = 2.83115553\n",
      "Iteration 1431, loss = 2.83205873\n",
      "Iteration 1432, loss = 2.83152937\n",
      "Iteration 1433, loss = 2.83135409\n",
      "Iteration 1434, loss = 2.83057002\n",
      "Iteration 1435, loss = 2.83096324\n",
      "Iteration 1436, loss = 2.83085924\n",
      "Iteration 1437, loss = 2.83059077\n",
      "Iteration 1438, loss = 2.83021280\n",
      "Iteration 1439, loss = 2.82957497\n",
      "Iteration 1440, loss = 2.82961161\n",
      "Iteration 1441, loss = 2.82957115\n",
      "Iteration 1442, loss = 2.82918664\n",
      "Iteration 1443, loss = 2.82845616\n",
      "Iteration 1444, loss = 2.82890771\n",
      "Iteration 1445, loss = 2.82900912\n",
      "Iteration 1446, loss = 2.82908865\n",
      "Iteration 1447, loss = 2.82905107\n",
      "Iteration 1448, loss = 2.82879200\n",
      "Iteration 1449, loss = 2.82851362\n",
      "Iteration 1450, loss = 2.82804196\n",
      "Iteration 1451, loss = 2.82776955\n",
      "Iteration 1452, loss = 2.82801593\n",
      "Iteration 1453, loss = 2.82759638\n",
      "Iteration 1454, loss = 2.82761728\n",
      "Iteration 1455, loss = 2.82688072\n",
      "Iteration 1456, loss = 2.82712143\n",
      "Iteration 1457, loss = 2.82686523\n",
      "Iteration 1458, loss = 2.82706548\n",
      "Iteration 1459, loss = 2.82681990\n",
      "Iteration 1460, loss = 2.82678693\n",
      "Iteration 1461, loss = 2.82661688\n",
      "Iteration 1462, loss = 2.82682271\n",
      "Iteration 1463, loss = 2.82692928\n",
      "Iteration 1464, loss = 2.82651173\n",
      "Iteration 1465, loss = 2.82611171\n",
      "Iteration 1466, loss = 2.82562666\n",
      "Iteration 1467, loss = 2.82508429\n",
      "Iteration 1468, loss = 2.82475015\n",
      "Iteration 1469, loss = 2.82492672\n",
      "Iteration 1470, loss = 2.82540092\n",
      "Iteration 1471, loss = 2.82554828\n",
      "Iteration 1472, loss = 2.82502071\n",
      "Iteration 1473, loss = 2.82412097\n",
      "Iteration 1474, loss = 2.82372347\n",
      "Iteration 1475, loss = 2.82349699\n",
      "Iteration 1476, loss = 2.82378257\n",
      "Iteration 1477, loss = 2.82415959\n",
      "Iteration 1478, loss = 2.82357278\n",
      "Iteration 1479, loss = 2.82371243\n",
      "Iteration 1480, loss = 2.82322238\n",
      "Iteration 1481, loss = 2.82293777\n",
      "Iteration 1482, loss = 2.82277190\n",
      "Iteration 1483, loss = 2.82270737\n",
      "Iteration 1484, loss = 2.82237690\n",
      "Iteration 1485, loss = 2.82184074\n",
      "Iteration 1486, loss = 2.82254921\n",
      "Iteration 1487, loss = 2.82264599\n",
      "Iteration 1488, loss = 2.82238195\n",
      "Iteration 1489, loss = 2.82195417\n",
      "Iteration 1490, loss = 2.82189758\n",
      "Iteration 1491, loss = 2.82147527\n",
      "Iteration 1492, loss = 2.82100237\n",
      "Iteration 1493, loss = 2.82071539\n",
      "Iteration 1494, loss = 2.82040426\n",
      "Iteration 1495, loss = 2.82095324\n",
      "Iteration 1496, loss = 2.82054926\n",
      "Iteration 1497, loss = 2.82000651\n",
      "Iteration 1498, loss = 2.81994730\n",
      "Iteration 1499, loss = 2.82004618\n",
      "Iteration 1500, loss = 2.81974170\n",
      "Iteration 1501, loss = 2.81930177\n",
      "Iteration 1502, loss = 2.81924328\n",
      "Iteration 1503, loss = 2.81939413\n",
      "Iteration 1504, loss = 2.81938681\n",
      "Iteration 1505, loss = 2.81898506\n",
      "Iteration 1506, loss = 2.81955235\n",
      "Iteration 1507, loss = 2.81934115\n",
      "Iteration 1508, loss = 2.81896805\n",
      "Iteration 1509, loss = 2.81854600\n",
      "Iteration 1510, loss = 2.81856513\n",
      "Iteration 1511, loss = 2.81853164\n",
      "Iteration 1512, loss = 2.81837490\n",
      "Iteration 1513, loss = 2.81776531\n",
      "Iteration 1514, loss = 2.81797350\n",
      "Iteration 1515, loss = 2.81755530\n",
      "Iteration 1516, loss = 2.81695734\n",
      "Iteration 1517, loss = 2.81687452\n",
      "Iteration 1518, loss = 2.81660239\n",
      "Iteration 1519, loss = 2.81693071\n",
      "Iteration 1520, loss = 2.81676878\n",
      "Iteration 1521, loss = 2.81690980\n",
      "Iteration 1522, loss = 2.81609285\n",
      "Iteration 1523, loss = 2.81640655\n",
      "Iteration 1524, loss = 2.81632223\n",
      "Iteration 1525, loss = 2.81647288\n",
      "Iteration 1526, loss = 2.81614622\n",
      "Iteration 1527, loss = 2.81645032\n",
      "Iteration 1528, loss = 2.81575227\n",
      "Iteration 1529, loss = 2.81520618\n",
      "Iteration 1530, loss = 2.81549105\n",
      "Iteration 1531, loss = 2.81600471\n",
      "Iteration 1532, loss = 2.81582342\n",
      "Iteration 1533, loss = 2.81528484\n",
      "Iteration 1534, loss = 2.81535965\n",
      "Iteration 1535, loss = 2.81635489\n",
      "Iteration 1536, loss = 2.81517936\n",
      "Iteration 1537, loss = 2.81420961\n",
      "Iteration 1538, loss = 2.81468565\n",
      "Iteration 1539, loss = 2.81457320\n",
      "Iteration 1540, loss = 2.81409404\n",
      "Iteration 1541, loss = 2.81394808\n",
      "Iteration 1542, loss = 2.81384208\n",
      "Iteration 1543, loss = 2.81336673\n",
      "Iteration 1544, loss = 2.81296737\n",
      "Iteration 1545, loss = 2.81275177\n",
      "Iteration 1546, loss = 2.81230749\n",
      "Iteration 1547, loss = 2.81229557\n",
      "Iteration 1548, loss = 2.81225051\n",
      "Iteration 1549, loss = 2.81228748\n",
      "Iteration 1550, loss = 2.81220569\n",
      "Iteration 1551, loss = 2.81178346\n",
      "Iteration 1552, loss = 2.81187761\n",
      "Iteration 1553, loss = 2.81183537\n",
      "Iteration 1554, loss = 2.81149646\n",
      "Iteration 1555, loss = 2.81120874\n",
      "Iteration 1556, loss = 2.81215280\n",
      "Iteration 1557, loss = 2.81196554\n",
      "Iteration 1558, loss = 2.81088503\n",
      "Iteration 1559, loss = 2.81055749\n",
      "Iteration 1560, loss = 2.81074238\n",
      "Iteration 1561, loss = 2.81109113\n",
      "Iteration 1562, loss = 2.81029790\n",
      "Iteration 1563, loss = 2.81033560\n",
      "Iteration 1564, loss = 2.81029420\n",
      "Iteration 1565, loss = 2.81022151\n",
      "Iteration 1566, loss = 2.80980000\n",
      "Iteration 1567, loss = 2.80926625\n",
      "Iteration 1568, loss = 2.80922673\n",
      "Iteration 1569, loss = 2.80906145\n",
      "Iteration 1570, loss = 2.80959388\n",
      "Iteration 1571, loss = 2.80921042\n",
      "Iteration 1572, loss = 2.80892792\n",
      "Iteration 1573, loss = 2.80877447\n",
      "Iteration 1574, loss = 2.80870277\n",
      "Iteration 1575, loss = 2.80895157\n",
      "Iteration 1576, loss = 2.80926580\n",
      "Iteration 1577, loss = 2.80871686\n",
      "Iteration 1578, loss = 2.80862933\n",
      "Iteration 1579, loss = 2.80808824\n",
      "Iteration 1580, loss = 2.80766382\n",
      "Iteration 1581, loss = 2.80808817\n",
      "Iteration 1582, loss = 2.80733698\n",
      "Iteration 1583, loss = 2.80751223\n",
      "Iteration 1584, loss = 2.80746429\n",
      "Iteration 1585, loss = 2.80840229\n",
      "Iteration 1586, loss = 2.80781331\n",
      "Iteration 1587, loss = 2.80747061\n",
      "Iteration 1588, loss = 2.80711164\n",
      "Iteration 1589, loss = 2.80621710\n",
      "Iteration 1590, loss = 2.80626940\n",
      "Iteration 1591, loss = 2.80696600\n",
      "Iteration 1592, loss = 2.80661671\n",
      "Iteration 1593, loss = 2.80651573\n",
      "Iteration 1594, loss = 2.80647747\n",
      "Iteration 1595, loss = 2.80666213\n",
      "Iteration 1596, loss = 2.80594827\n",
      "Iteration 1597, loss = 2.80513266\n",
      "Iteration 1598, loss = 2.80628735\n",
      "Iteration 1599, loss = 2.80541334\n",
      "Iteration 1600, loss = 2.80472951\n",
      "Iteration 1601, loss = 2.80456599\n",
      "Iteration 1602, loss = 2.80477666\n",
      "Iteration 1603, loss = 2.80571785\n",
      "Iteration 1604, loss = 2.80453200\n",
      "Iteration 1605, loss = 2.80441745\n",
      "Iteration 1606, loss = 2.80425905\n",
      "Iteration 1607, loss = 2.80452620\n",
      "Iteration 1608, loss = 2.80404810\n",
      "Iteration 1609, loss = 2.80408543\n",
      "Iteration 1610, loss = 2.80370970\n",
      "Iteration 1611, loss = 2.80332334\n",
      "Iteration 1612, loss = 2.80334542\n",
      "Iteration 1613, loss = 2.80389232\n",
      "Iteration 1614, loss = 2.80404548\n",
      "Iteration 1615, loss = 2.80295201\n",
      "Iteration 1616, loss = 2.80280025\n",
      "Iteration 1617, loss = 2.80231831\n",
      "Iteration 1618, loss = 2.80345487\n",
      "Iteration 1619, loss = 2.80261820\n",
      "Iteration 1620, loss = 2.80393358\n",
      "Iteration 1621, loss = 2.80274760\n",
      "Iteration 1622, loss = 2.80255433\n",
      "Iteration 1623, loss = 2.80181822\n",
      "Iteration 1624, loss = 2.80072703\n",
      "Iteration 1625, loss = 2.80087059\n",
      "Iteration 1626, loss = 2.80199243\n",
      "Iteration 1627, loss = 2.80248091\n",
      "Iteration 1628, loss = 2.80196816\n",
      "Iteration 1629, loss = 2.80074139\n",
      "Iteration 1630, loss = 2.80173383\n",
      "Iteration 1631, loss = 2.80102574\n",
      "Iteration 1632, loss = 2.80004453\n",
      "Iteration 1633, loss = 2.80068664\n",
      "Iteration 1634, loss = 2.79972360\n",
      "Iteration 1635, loss = 2.79938502\n",
      "Iteration 1636, loss = 2.79993337\n",
      "Iteration 1637, loss = 2.80034073\n",
      "Iteration 1638, loss = 2.79997662\n",
      "Iteration 1639, loss = 2.80082825\n",
      "Iteration 1640, loss = 2.80049952\n",
      "Iteration 1641, loss = 2.79938807\n",
      "Iteration 1642, loss = 2.79859353\n",
      "Iteration 1643, loss = 2.79852485\n",
      "Iteration 1644, loss = 2.79864854\n",
      "Iteration 1645, loss = 2.79884684\n",
      "Iteration 1646, loss = 2.79823340\n",
      "Iteration 1647, loss = 2.79790712\n",
      "Iteration 1648, loss = 2.79852920\n",
      "Iteration 1649, loss = 2.79790648\n",
      "Iteration 1650, loss = 2.79772426\n",
      "Iteration 1651, loss = 2.79739856\n",
      "Iteration 1652, loss = 2.79732935\n",
      "Iteration 1653, loss = 2.79771127\n",
      "Iteration 1654, loss = 2.79697586\n",
      "Iteration 1655, loss = 2.79713332\n",
      "Iteration 1656, loss = 2.79699544\n",
      "Iteration 1657, loss = 2.79692426\n",
      "Iteration 1658, loss = 2.79664757\n",
      "Iteration 1659, loss = 2.79655847\n",
      "Iteration 1660, loss = 2.79646255\n",
      "Iteration 1661, loss = 2.79621016\n",
      "Iteration 1662, loss = 2.79619294\n",
      "Iteration 1663, loss = 2.79586296\n",
      "Iteration 1664, loss = 2.79558735\n",
      "Iteration 1665, loss = 2.79561348\n",
      "Iteration 1666, loss = 2.79520174\n",
      "Iteration 1667, loss = 2.79558319\n",
      "Iteration 1668, loss = 2.79526603\n",
      "Iteration 1669, loss = 2.79553834\n",
      "Iteration 1670, loss = 2.79478241\n",
      "Iteration 1671, loss = 2.79445183\n",
      "Iteration 1672, loss = 2.79483307\n",
      "Iteration 1673, loss = 2.79457614\n",
      "Iteration 1674, loss = 2.79434813\n",
      "Iteration 1675, loss = 2.79441800\n",
      "Iteration 1676, loss = 2.79392121\n",
      "Iteration 1677, loss = 2.79453441\n",
      "Iteration 1678, loss = 2.79377387\n",
      "Iteration 1679, loss = 2.79349805\n",
      "Iteration 1680, loss = 2.79337042\n",
      "Iteration 1681, loss = 2.79288930\n",
      "Iteration 1682, loss = 2.79305749\n",
      "Iteration 1683, loss = 2.79325225\n",
      "Iteration 1684, loss = 2.79360830\n",
      "Iteration 1685, loss = 2.79273245\n",
      "Iteration 1686, loss = 2.79284719\n",
      "Iteration 1687, loss = 2.79292202\n",
      "Iteration 1688, loss = 2.79248282\n",
      "Iteration 1689, loss = 2.79263973\n",
      "Iteration 1690, loss = 2.79187068\n",
      "Iteration 1691, loss = 2.79188638\n",
      "Iteration 1692, loss = 2.79206618\n",
      "Iteration 1693, loss = 2.79207670\n",
      "Iteration 1694, loss = 2.79242697\n",
      "Iteration 1695, loss = 2.79157626\n",
      "Iteration 1696, loss = 2.79138762\n",
      "Iteration 1697, loss = 2.79140931\n",
      "Iteration 1698, loss = 2.79114317\n",
      "Iteration 1699, loss = 2.79081165\n",
      "Iteration 1700, loss = 2.79121060\n",
      "Iteration 1701, loss = 2.79100209\n",
      "Iteration 1702, loss = 2.79109105\n",
      "Iteration 1703, loss = 2.79135830\n",
      "Iteration 1704, loss = 2.79124897\n",
      "Iteration 1705, loss = 2.79018879\n",
      "Iteration 1706, loss = 2.78989398\n",
      "Iteration 1707, loss = 2.79009146\n",
      "Iteration 1708, loss = 2.78934915\n",
      "Iteration 1709, loss = 2.78974814\n",
      "Iteration 1710, loss = 2.79022157\n",
      "Iteration 1711, loss = 2.79012570\n",
      "Iteration 1712, loss = 2.78923331\n",
      "Iteration 1713, loss = 2.78894583\n",
      "Iteration 1714, loss = 2.78955633\n",
      "Iteration 1715, loss = 2.78948248\n",
      "Iteration 1716, loss = 2.78861429\n",
      "Iteration 1717, loss = 2.78943330\n",
      "Iteration 1718, loss = 2.78977840\n",
      "Iteration 1719, loss = 2.78902371\n",
      "Iteration 1720, loss = 2.78940083\n",
      "Iteration 1721, loss = 2.78986096\n",
      "Iteration 1722, loss = 2.78932382\n",
      "Iteration 1723, loss = 2.78813624\n",
      "Iteration 1724, loss = 2.78780594\n",
      "Iteration 1725, loss = 2.78791502\n",
      "Iteration 1726, loss = 2.78756260\n",
      "Iteration 1727, loss = 2.78738801\n",
      "Iteration 1728, loss = 2.78750878\n",
      "Iteration 1729, loss = 2.78744725\n",
      "Iteration 1730, loss = 2.78801888\n",
      "Iteration 1731, loss = 2.78763232\n",
      "Iteration 1732, loss = 2.78717515\n",
      "Iteration 1733, loss = 2.78656881\n",
      "Iteration 1734, loss = 2.78639607\n",
      "Iteration 1735, loss = 2.78664404\n",
      "Iteration 1736, loss = 2.78612558\n",
      "Iteration 1737, loss = 2.78608690\n",
      "Iteration 1738, loss = 2.78549653\n",
      "Iteration 1739, loss = 2.78602390\n",
      "Iteration 1740, loss = 2.78555649\n",
      "Iteration 1741, loss = 2.78465411\n",
      "Iteration 1742, loss = 2.78458827\n",
      "Iteration 1743, loss = 2.78575549\n",
      "Iteration 1744, loss = 2.78546098\n",
      "Iteration 1745, loss = 2.78500521\n",
      "Iteration 1746, loss = 2.78418141\n",
      "Iteration 1747, loss = 2.78437293\n",
      "Iteration 1748, loss = 2.78392967\n",
      "Iteration 1749, loss = 2.78436399\n",
      "Iteration 1750, loss = 2.78455479\n",
      "Iteration 1751, loss = 2.78465311\n",
      "Iteration 1752, loss = 2.78395781\n",
      "Iteration 1753, loss = 2.78398910\n",
      "Iteration 1754, loss = 2.78366140\n",
      "Iteration 1755, loss = 2.78351139\n",
      "Iteration 1756, loss = 2.78349255\n",
      "Iteration 1757, loss = 2.78371344\n",
      "Iteration 1758, loss = 2.78281801\n",
      "Iteration 1759, loss = 2.78222070\n",
      "Iteration 1760, loss = 2.78291840\n",
      "Iteration 1761, loss = 2.78208616\n",
      "Iteration 1762, loss = 2.78205657\n",
      "Iteration 1763, loss = 2.78205035\n",
      "Iteration 1764, loss = 2.78316930\n",
      "Iteration 1765, loss = 2.78294145\n",
      "Iteration 1766, loss = 2.78195746\n",
      "Iteration 1767, loss = 2.78413562\n",
      "Iteration 1768, loss = 2.78272912\n",
      "Iteration 1769, loss = 2.78203475\n",
      "Iteration 1770, loss = 2.78136682\n",
      "Iteration 1771, loss = 2.78084483\n",
      "Iteration 1772, loss = 2.78120342\n",
      "Iteration 1773, loss = 2.78120701\n",
      "Iteration 1774, loss = 2.78111134\n",
      "Iteration 1775, loss = 2.78109637\n",
      "Iteration 1776, loss = 2.78097505\n",
      "Iteration 1777, loss = 2.78162336\n",
      "Iteration 1778, loss = 2.78111109\n",
      "Iteration 1779, loss = 2.78021595\n",
      "Iteration 1780, loss = 2.78078205\n",
      "Iteration 1781, loss = 2.78019570\n",
      "Iteration 1782, loss = 2.78009782\n",
      "Iteration 1783, loss = 2.77981859\n",
      "Iteration 1784, loss = 2.77992822\n",
      "Iteration 1785, loss = 2.77963441\n",
      "Iteration 1786, loss = 2.77935847\n",
      "Iteration 1787, loss = 2.77931457\n",
      "Iteration 1788, loss = 2.77975705\n",
      "Iteration 1789, loss = 2.78135548\n",
      "Iteration 1790, loss = 2.77910296\n",
      "Iteration 1791, loss = 2.77888751\n",
      "Iteration 1792, loss = 2.78051332\n",
      "Iteration 1793, loss = 2.77937297\n",
      "Iteration 1794, loss = 2.77858031\n",
      "Iteration 1795, loss = 2.77994321\n",
      "Iteration 1796, loss = 2.77887575\n",
      "Iteration 1797, loss = 2.77897678\n",
      "Iteration 1798, loss = 2.77858575\n",
      "Iteration 1799, loss = 2.77802621\n",
      "Iteration 1800, loss = 2.77799370\n",
      "Iteration 1801, loss = 2.77794684\n",
      "Iteration 1802, loss = 2.77718891\n",
      "Iteration 1803, loss = 2.77749457\n",
      "Iteration 1804, loss = 2.77702134\n",
      "Iteration 1805, loss = 2.77667517\n",
      "Iteration 1806, loss = 2.77718293\n",
      "Iteration 1807, loss = 2.77672454\n",
      "Iteration 1808, loss = 2.77624644\n",
      "Iteration 1809, loss = 2.77608977\n",
      "Iteration 1810, loss = 2.77645758\n",
      "Iteration 1811, loss = 2.77595217\n",
      "Iteration 1812, loss = 2.77602423\n",
      "Iteration 1813, loss = 2.77626595\n",
      "Iteration 1814, loss = 2.77566560\n",
      "Iteration 1815, loss = 2.77574026\n",
      "Iteration 1816, loss = 2.77600524\n",
      "Iteration 1817, loss = 2.77547798\n",
      "Iteration 1818, loss = 2.77546392\n",
      "Iteration 1819, loss = 2.77509195\n",
      "Iteration 1820, loss = 2.77546615\n",
      "Iteration 1821, loss = 2.77577657\n",
      "Iteration 1822, loss = 2.77506060\n",
      "Iteration 1823, loss = 2.77508833\n",
      "Iteration 1824, loss = 2.77501691\n",
      "Iteration 1825, loss = 2.77465653\n",
      "Iteration 1826, loss = 2.77434424\n",
      "Iteration 1827, loss = 2.77407722\n",
      "Iteration 1828, loss = 2.77402613\n",
      "Iteration 1829, loss = 2.77481722\n",
      "Iteration 1830, loss = 2.77483652\n",
      "Iteration 1831, loss = 2.77568525\n",
      "Iteration 1832, loss = 2.77473365\n",
      "Iteration 1833, loss = 2.77328588\n",
      "Iteration 1834, loss = 2.77343592\n",
      "Iteration 1835, loss = 2.77338534\n",
      "Iteration 1836, loss = 2.77275571\n",
      "Iteration 1837, loss = 2.77266680\n",
      "Iteration 1838, loss = 2.77338208\n",
      "Iteration 1839, loss = 2.77301663\n",
      "Iteration 1840, loss = 2.77277496\n",
      "Iteration 1841, loss = 2.77303927\n",
      "Iteration 1842, loss = 2.77340690\n",
      "Iteration 1843, loss = 2.77263455\n",
      "Iteration 1844, loss = 2.77248793\n",
      "Iteration 1845, loss = 2.77229605\n",
      "Iteration 1846, loss = 2.77185824\n",
      "Iteration 1847, loss = 2.77218364\n",
      "Iteration 1848, loss = 2.77277647\n",
      "Iteration 1849, loss = 2.77250564\n",
      "Iteration 1850, loss = 2.77241756\n",
      "Iteration 1851, loss = 2.77153835\n",
      "Iteration 1852, loss = 2.77103993\n",
      "Iteration 1853, loss = 2.77107709\n",
      "Iteration 1854, loss = 2.77081209\n",
      "Iteration 1855, loss = 2.77113819\n",
      "Iteration 1856, loss = 2.77095275\n",
      "Iteration 1857, loss = 2.77120543\n",
      "Iteration 1858, loss = 2.77186872\n",
      "Iteration 1859, loss = 2.77168667\n",
      "Iteration 1860, loss = 2.77026439\n",
      "Iteration 1861, loss = 2.77015152\n",
      "Iteration 1862, loss = 2.77032390\n",
      "Iteration 1863, loss = 2.77035684\n",
      "Iteration 1864, loss = 2.77050972\n",
      "Iteration 1865, loss = 2.76999515\n",
      "Iteration 1866, loss = 2.76967625\n",
      "Iteration 1867, loss = 2.77019588\n",
      "Iteration 1868, loss = 2.77059356\n",
      "Iteration 1869, loss = 2.76967428\n",
      "Iteration 1870, loss = 2.76907714\n",
      "Iteration 1871, loss = 2.76933289\n",
      "Iteration 1872, loss = 2.76983039\n",
      "Iteration 1873, loss = 2.76899011\n",
      "Iteration 1874, loss = 2.76918030\n",
      "Iteration 1875, loss = 2.76863417\n",
      "Iteration 1876, loss = 2.76958909\n",
      "Iteration 1877, loss = 2.76847654\n",
      "Iteration 1878, loss = 2.76831102\n",
      "Iteration 1879, loss = 2.76841681\n",
      "Iteration 1880, loss = 2.76836700\n",
      "Iteration 1881, loss = 2.76806243\n",
      "Iteration 1882, loss = 2.76786420\n",
      "Iteration 1883, loss = 2.76727478\n",
      "Iteration 1884, loss = 2.76814245\n",
      "Iteration 1885, loss = 2.76771002\n",
      "Iteration 1886, loss = 2.76727401\n",
      "Iteration 1887, loss = 2.76793665\n",
      "Iteration 1888, loss = 2.76758156\n",
      "Iteration 1889, loss = 2.76702067\n",
      "Iteration 1890, loss = 2.76671254\n",
      "Iteration 1891, loss = 2.76674127\n",
      "Iteration 1892, loss = 2.76654611\n",
      "Iteration 1893, loss = 2.76630872\n",
      "Iteration 1894, loss = 2.76632204\n",
      "Iteration 1895, loss = 2.76654945\n",
      "Iteration 1896, loss = 2.76551759\n",
      "Iteration 1897, loss = 2.76583671\n",
      "Iteration 1898, loss = 2.76595356\n",
      "Iteration 1899, loss = 2.76624718\n",
      "Iteration 1900, loss = 2.76583762\n",
      "Iteration 1901, loss = 2.76538588\n",
      "Iteration 1902, loss = 2.76518177\n",
      "Iteration 1903, loss = 2.76539754\n",
      "Iteration 1904, loss = 2.76517010\n",
      "Iteration 1905, loss = 2.76552181\n",
      "Iteration 1906, loss = 2.76500232\n",
      "Iteration 1907, loss = 2.76480763\n",
      "Iteration 1908, loss = 2.76443448\n",
      "Iteration 1909, loss = 2.76431120\n",
      "Iteration 1910, loss = 2.76384824\n",
      "Iteration 1911, loss = 2.76420188\n",
      "Iteration 1912, loss = 2.76388879\n",
      "Iteration 1913, loss = 2.76413898\n",
      "Iteration 1914, loss = 2.76432198\n",
      "Iteration 1915, loss = 2.76399566\n",
      "Iteration 1916, loss = 2.76447904\n",
      "Iteration 1917, loss = 2.76405849\n",
      "Iteration 1918, loss = 2.76304978\n",
      "Iteration 1919, loss = 2.76430322\n",
      "Iteration 1920, loss = 2.76444608\n",
      "Iteration 1921, loss = 2.76288658\n",
      "Iteration 1922, loss = 2.76268041\n",
      "Iteration 1923, loss = 2.76284806\n",
      "Iteration 1924, loss = 2.76246901\n",
      "Iteration 1925, loss = 2.76247134\n",
      "Iteration 1926, loss = 2.76392630\n",
      "Iteration 1927, loss = 2.76353401\n",
      "Iteration 1928, loss = 2.76335623\n",
      "Iteration 1929, loss = 2.76314717\n",
      "Iteration 1930, loss = 2.76219031\n",
      "Iteration 1931, loss = 2.76224490\n",
      "Iteration 1932, loss = 2.76188733\n",
      "Iteration 1933, loss = 2.76171110\n",
      "Iteration 1934, loss = 2.76242547\n",
      "Iteration 1935, loss = 2.76184962\n",
      "Iteration 1936, loss = 2.76158182\n",
      "Iteration 1937, loss = 2.76163947\n",
      "Iteration 1938, loss = 2.76110091\n",
      "Iteration 1939, loss = 2.76075099\n",
      "Iteration 1940, loss = 2.76116587\n",
      "Iteration 1941, loss = 2.76125394\n",
      "Iteration 1942, loss = 2.76039378\n",
      "Iteration 1943, loss = 2.76031129\n",
      "Iteration 1944, loss = 2.76035243\n",
      "Iteration 1945, loss = 2.76006752\n",
      "Iteration 1946, loss = 2.76061919\n",
      "Iteration 1947, loss = 2.76038760\n",
      "Iteration 1948, loss = 2.75960110\n",
      "Iteration 1949, loss = 2.75957749\n",
      "Iteration 1950, loss = 2.75961202\n",
      "Iteration 1951, loss = 2.76000844\n",
      "Iteration 1952, loss = 2.75980403\n",
      "Iteration 1953, loss = 2.75948268\n",
      "Iteration 1954, loss = 2.75945985\n",
      "Iteration 1955, loss = 2.75969161\n",
      "Iteration 1956, loss = 2.75909097\n",
      "Iteration 1957, loss = 2.76018655\n",
      "Iteration 1958, loss = 2.76045926\n",
      "Iteration 1959, loss = 2.75935203\n",
      "Iteration 1960, loss = 2.75860529\n",
      "Iteration 1961, loss = 2.75863261\n",
      "Iteration 1962, loss = 2.75842279\n",
      "Iteration 1963, loss = 2.76028123\n",
      "Iteration 1964, loss = 2.75873965\n",
      "Iteration 1965, loss = 2.75859282\n",
      "Iteration 1966, loss = 2.75784382\n",
      "Iteration 1967, loss = 2.75932524\n",
      "Iteration 1968, loss = 2.75881008\n",
      "Iteration 1969, loss = 2.75816986\n",
      "Iteration 1970, loss = 2.75791674\n",
      "Iteration 1971, loss = 2.75745279\n",
      "Iteration 1972, loss = 2.75717134\n",
      "Iteration 1973, loss = 2.75718894\n",
      "Iteration 1974, loss = 2.75841687\n",
      "Iteration 1975, loss = 2.75833410\n",
      "Iteration 1976, loss = 2.75717849\n",
      "Iteration 1977, loss = 2.75681502\n",
      "Iteration 1978, loss = 2.75683341\n",
      "Iteration 1979, loss = 2.75660604\n",
      "Iteration 1980, loss = 2.75694759\n",
      "Iteration 1981, loss = 2.75597049\n",
      "Iteration 1982, loss = 2.75578771\n",
      "Iteration 1983, loss = 2.75579165\n",
      "Iteration 1984, loss = 2.75649010\n",
      "Iteration 1985, loss = 2.75551385\n",
      "Iteration 1986, loss = 2.75540655\n",
      "Iteration 1987, loss = 2.75543311\n",
      "Iteration 1988, loss = 2.75496504\n",
      "Iteration 1989, loss = 2.75570140\n",
      "Iteration 1990, loss = 2.75599405\n",
      "Iteration 1991, loss = 2.75598297\n",
      "Iteration 1992, loss = 2.75574547\n",
      "Iteration 1993, loss = 2.75532599\n",
      "Iteration 1994, loss = 2.75588799\n",
      "Iteration 1995, loss = 2.75573351\n",
      "Iteration 1996, loss = 2.75473227\n",
      "Iteration 1997, loss = 2.75447871\n",
      "Iteration 1998, loss = 2.75393177\n",
      "Iteration 1999, loss = 2.75452202\n",
      "Iteration 2000, loss = 2.75464987\n",
      "Iteration 2001, loss = 2.75433829\n",
      "Iteration 2002, loss = 2.75392103\n",
      "Iteration 2003, loss = 2.75421664\n",
      "Iteration 2004, loss = 2.75395355\n",
      "Iteration 2005, loss = 2.75359790\n",
      "Iteration 2006, loss = 2.75385340\n",
      "Iteration 2007, loss = 2.75351641\n",
      "Iteration 2008, loss = 2.75316096\n",
      "Iteration 2009, loss = 2.75314153\n",
      "Iteration 2010, loss = 2.75322832\n",
      "Iteration 2011, loss = 2.75311932\n",
      "Iteration 2012, loss = 2.75287385\n",
      "Iteration 2013, loss = 2.75271650\n",
      "Iteration 2014, loss = 2.75257111\n",
      "Iteration 2015, loss = 2.75319155\n",
      "Iteration 2016, loss = 2.75276958\n",
      "Iteration 2017, loss = 2.75207758\n",
      "Iteration 2018, loss = 2.75174950\n",
      "Iteration 2019, loss = 2.75280061\n",
      "Iteration 2020, loss = 2.75206875\n",
      "Iteration 2021, loss = 2.75203953\n",
      "Iteration 2022, loss = 2.75159234\n",
      "Iteration 2023, loss = 2.75120427\n",
      "Iteration 2024, loss = 2.75165862\n",
      "Iteration 2025, loss = 2.75249723\n",
      "Iteration 2026, loss = 2.75182948\n",
      "Iteration 2027, loss = 2.75257822\n",
      "Iteration 2028, loss = 2.75310023\n",
      "Iteration 2029, loss = 2.75286734\n",
      "Iteration 2030, loss = 2.75189534\n",
      "Iteration 2031, loss = 2.75140556\n",
      "Iteration 2032, loss = 2.75090667\n",
      "Iteration 2033, loss = 2.75080985\n",
      "Iteration 2034, loss = 2.75098661\n",
      "Iteration 2035, loss = 2.75122660\n",
      "Iteration 2036, loss = 2.75105566\n",
      "Iteration 2037, loss = 2.75102493\n",
      "Iteration 2038, loss = 2.74997136\n",
      "Iteration 2039, loss = 2.74947655\n",
      "Iteration 2040, loss = 2.74964888\n",
      "Iteration 2041, loss = 2.74918275\n",
      "Iteration 2042, loss = 2.75011470\n",
      "Iteration 2043, loss = 2.74970249\n",
      "Iteration 2044, loss = 2.74936021\n",
      "Iteration 2045, loss = 2.74992297\n",
      "Iteration 2046, loss = 2.75116424\n",
      "Iteration 2047, loss = 2.75053706\n",
      "Iteration 2048, loss = 2.74938700\n",
      "Iteration 2049, loss = 2.74929517\n",
      "Iteration 2050, loss = 2.74854077\n",
      "Iteration 2051, loss = 2.74941660\n",
      "Iteration 2052, loss = 2.74880049\n",
      "Iteration 2053, loss = 2.74840265\n",
      "Iteration 2054, loss = 2.74841590\n",
      "Iteration 2055, loss = 2.74798471\n",
      "Iteration 2056, loss = 2.74872829\n",
      "Iteration 2057, loss = 2.74820789\n",
      "Iteration 2058, loss = 2.75026013\n",
      "Iteration 2059, loss = 2.74994804\n",
      "Iteration 2060, loss = 2.74842047\n",
      "Iteration 2061, loss = 2.74811656\n",
      "Iteration 2062, loss = 2.74789931\n",
      "Iteration 2063, loss = 2.74732966\n",
      "Iteration 2064, loss = 2.74885286\n",
      "Iteration 2065, loss = 2.74690672\n",
      "Iteration 2066, loss = 2.74645665\n",
      "Iteration 2067, loss = 2.74737235\n",
      "Iteration 2068, loss = 2.74664872\n",
      "Iteration 2069, loss = 2.74795513\n",
      "Iteration 2070, loss = 2.74868951\n",
      "Iteration 2071, loss = 2.74685450\n",
      "Iteration 2072, loss = 2.74657983\n",
      "Iteration 2073, loss = 2.74613984\n",
      "Iteration 2074, loss = 2.74653395\n",
      "Iteration 2075, loss = 2.74655129\n",
      "Iteration 2076, loss = 2.74638819\n",
      "Iteration 2077, loss = 2.74538214\n",
      "Iteration 2078, loss = 2.74552080\n",
      "Iteration 2079, loss = 2.74570451\n",
      "Iteration 2080, loss = 2.74580069\n",
      "Iteration 2081, loss = 2.74592576\n",
      "Iteration 2082, loss = 2.74512312\n",
      "Iteration 2083, loss = 2.74494609\n",
      "Iteration 2084, loss = 2.74555256\n",
      "Iteration 2085, loss = 2.74502322\n",
      "Iteration 2086, loss = 2.74514499\n",
      "Iteration 2087, loss = 2.74566430\n",
      "Iteration 2088, loss = 2.74576961\n",
      "Iteration 2089, loss = 2.74532490\n",
      "Iteration 2090, loss = 2.74528417\n",
      "Iteration 2091, loss = 2.74448921\n",
      "Iteration 2092, loss = 2.74456822\n",
      "Iteration 2093, loss = 2.74443416\n",
      "Iteration 2094, loss = 2.74461661\n",
      "Iteration 2095, loss = 2.74497882\n",
      "Iteration 2096, loss = 2.74493837\n",
      "Iteration 2097, loss = 2.74365419\n",
      "Iteration 2098, loss = 2.74338575\n",
      "Iteration 2099, loss = 2.74377117\n",
      "Iteration 2100, loss = 2.74485129\n",
      "Iteration 2101, loss = 2.74400333\n",
      "Iteration 2102, loss = 2.74290699\n",
      "Iteration 2103, loss = 2.74288096\n",
      "Iteration 2104, loss = 2.74242364\n",
      "Iteration 2105, loss = 2.74227646\n",
      "Iteration 2106, loss = 2.74329371\n",
      "Iteration 2107, loss = 2.74326747\n",
      "Iteration 2108, loss = 2.74274644\n",
      "Iteration 2109, loss = 2.74304423\n",
      "Iteration 2110, loss = 2.74317348\n",
      "Iteration 2111, loss = 2.74297417\n",
      "Iteration 2112, loss = 2.74239898\n",
      "Iteration 2113, loss = 2.74219725\n",
      "Iteration 2114, loss = 2.74151824\n",
      "Iteration 2115, loss = 2.74272504\n",
      "Iteration 2116, loss = 2.74205566\n",
      "Iteration 2117, loss = 2.74230755\n",
      "Iteration 2118, loss = 2.74187348\n",
      "Iteration 2119, loss = 2.74195948\n",
      "Iteration 2120, loss = 2.74199516\n",
      "Iteration 2121, loss = 2.74193562\n",
      "Iteration 2122, loss = 2.74168501\n",
      "Iteration 2123, loss = 2.74135316\n",
      "Iteration 2124, loss = 2.74112522\n",
      "Iteration 2125, loss = 2.74095191\n",
      "Iteration 2126, loss = 2.74111145\n",
      "Iteration 2127, loss = 2.74073640\n",
      "Iteration 2128, loss = 2.74134479\n",
      "Iteration 2129, loss = 2.74130891\n",
      "Iteration 2130, loss = 2.74088922\n",
      "Iteration 2131, loss = 2.74108935\n",
      "Iteration 2132, loss = 2.74043068\n",
      "Iteration 2133, loss = 2.74064161\n",
      "Iteration 2134, loss = 2.74034433\n",
      "Iteration 2135, loss = 2.74068776\n",
      "Iteration 2136, loss = 2.74026440\n",
      "Iteration 2137, loss = 2.74038623\n",
      "Iteration 2138, loss = 2.74114436\n",
      "Iteration 2139, loss = 2.74064484\n",
      "Iteration 2140, loss = 2.73964385\n",
      "Iteration 2141, loss = 2.73948134\n",
      "Iteration 2142, loss = 2.73969605\n",
      "Iteration 2143, loss = 2.73878469\n",
      "Iteration 2144, loss = 2.73875775\n",
      "Iteration 2145, loss = 2.73952166\n",
      "Iteration 2146, loss = 2.73950317\n",
      "Iteration 2147, loss = 2.73985228\n",
      "Iteration 2148, loss = 2.73904322\n",
      "Iteration 2149, loss = 2.73844951\n",
      "Iteration 2150, loss = 2.73830395\n",
      "Iteration 2151, loss = 2.73846600\n",
      "Iteration 2152, loss = 2.73815017\n",
      "Iteration 2153, loss = 2.73782305\n",
      "Iteration 2154, loss = 2.73798889\n",
      "Iteration 2155, loss = 2.73764642\n",
      "Iteration 2156, loss = 2.73745391\n",
      "Iteration 2157, loss = 2.73751455\n",
      "Iteration 2158, loss = 2.73731036\n",
      "Iteration 2159, loss = 2.73777774\n",
      "Iteration 2160, loss = 2.73718041\n",
      "Iteration 2161, loss = 2.73767118\n",
      "Iteration 2162, loss = 2.73767421\n",
      "Iteration 2163, loss = 2.73815792\n",
      "Iteration 2164, loss = 2.73816135\n",
      "Iteration 2165, loss = 2.73747292\n",
      "Iteration 2166, loss = 2.73723691\n",
      "Iteration 2167, loss = 2.73944472\n",
      "Iteration 2168, loss = 2.73906465\n",
      "Iteration 2169, loss = 2.73751543\n",
      "Iteration 2170, loss = 2.73736956\n",
      "Iteration 2171, loss = 2.73735881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13283998\n",
      "Iteration 2, loss = 4.05352465\n",
      "Iteration 3, loss = 3.97816325\n",
      "Iteration 4, loss = 3.90522672\n",
      "Iteration 5, loss = 3.83313417\n",
      "Iteration 6, loss = 3.76128026\n",
      "Iteration 7, loss = 3.68983557\n",
      "Iteration 8, loss = 3.61875693\n",
      "Iteration 9, loss = 3.54884718\n",
      "Iteration 10, loss = 3.48387911\n",
      "Iteration 11, loss = 3.42262339\n",
      "Iteration 12, loss = 3.37307783\n",
      "Iteration 13, loss = 3.32854944\n",
      "Iteration 14, loss = 3.29389053\n",
      "Iteration 15, loss = 3.26794530\n",
      "Iteration 16, loss = 3.25118229\n",
      "Iteration 17, loss = 3.24186873\n",
      "Iteration 18, loss = 3.23743194\n",
      "Iteration 19, loss = 3.23491501\n",
      "Iteration 20, loss = 3.23396441\n",
      "Iteration 21, loss = 3.23284707\n",
      "Iteration 22, loss = 3.23188456\n",
      "Iteration 23, loss = 3.23100124\n",
      "Iteration 24, loss = 3.23030660\n",
      "Iteration 25, loss = 3.22925505\n",
      "Iteration 26, loss = 3.22883373\n",
      "Iteration 27, loss = 3.22828686\n",
      "Iteration 28, loss = 3.22759105\n",
      "Iteration 29, loss = 3.22692627\n",
      "Iteration 30, loss = 3.22640483\n",
      "Iteration 31, loss = 3.22598204\n",
      "Iteration 32, loss = 3.22582413\n",
      "Iteration 33, loss = 3.22544358\n",
      "Iteration 34, loss = 3.22502787\n",
      "Iteration 35, loss = 3.22401959\n",
      "Iteration 36, loss = 3.22396813\n",
      "Iteration 37, loss = 3.22329087\n",
      "Iteration 38, loss = 3.22283368\n",
      "Iteration 39, loss = 3.22243996\n",
      "Iteration 40, loss = 3.22185963\n",
      "Iteration 41, loss = 3.22142463\n",
      "Iteration 42, loss = 3.22127099\n",
      "Iteration 43, loss = 3.22138917\n",
      "Iteration 44, loss = 3.22085149\n",
      "Iteration 45, loss = 3.22007783\n",
      "Iteration 46, loss = 3.21945330\n",
      "Iteration 47, loss = 3.21920003\n",
      "Iteration 48, loss = 3.21888597\n",
      "Iteration 49, loss = 3.21858176\n",
      "Iteration 50, loss = 3.21819040\n",
      "Iteration 51, loss = 3.21820549\n",
      "Iteration 52, loss = 3.21813540\n",
      "Iteration 53, loss = 3.21797934\n",
      "Iteration 54, loss = 3.21765675\n",
      "Iteration 55, loss = 3.21749261\n",
      "Iteration 56, loss = 3.21668746\n",
      "Iteration 57, loss = 3.21636213\n",
      "Iteration 58, loss = 3.21579905\n",
      "Iteration 59, loss = 3.21602887\n",
      "Iteration 60, loss = 3.21561048\n",
      "Iteration 61, loss = 3.21582923\n",
      "Iteration 62, loss = 3.21539460\n",
      "Iteration 63, loss = 3.21506326\n",
      "Iteration 64, loss = 3.21406398\n",
      "Iteration 65, loss = 3.21398293\n",
      "Iteration 66, loss = 3.21381096\n",
      "Iteration 67, loss = 3.21402469\n",
      "Iteration 68, loss = 3.21365958\n",
      "Iteration 69, loss = 3.21315710\n",
      "Iteration 70, loss = 3.21278003\n",
      "Iteration 71, loss = 3.21254118\n",
      "Iteration 72, loss = 3.21184445\n",
      "Iteration 73, loss = 3.21183945\n",
      "Iteration 74, loss = 3.21167591\n",
      "Iteration 75, loss = 3.21144995\n",
      "Iteration 76, loss = 3.21105016\n",
      "Iteration 77, loss = 3.21054334\n",
      "Iteration 78, loss = 3.21013880\n",
      "Iteration 79, loss = 3.21018622\n",
      "Iteration 80, loss = 3.20941945\n",
      "Iteration 81, loss = 3.20922967\n",
      "Iteration 82, loss = 3.20860756\n",
      "Iteration 83, loss = 3.20846818\n",
      "Iteration 84, loss = 3.20842307\n",
      "Iteration 85, loss = 3.20794048\n",
      "Iteration 86, loss = 3.20749906\n",
      "Iteration 87, loss = 3.20743068\n",
      "Iteration 88, loss = 3.20690851\n",
      "Iteration 89, loss = 3.20651180\n",
      "Iteration 90, loss = 3.20603561\n",
      "Iteration 91, loss = 3.20553055\n",
      "Iteration 92, loss = 3.20549581\n",
      "Iteration 93, loss = 3.20536390\n",
      "Iteration 94, loss = 3.20512177\n",
      "Iteration 95, loss = 3.20463640\n",
      "Iteration 96, loss = 3.20444321\n",
      "Iteration 97, loss = 3.20428195\n",
      "Iteration 98, loss = 3.20435387\n",
      "Iteration 99, loss = 3.20382049\n",
      "Iteration 100, loss = 3.20383753\n",
      "Iteration 101, loss = 3.20341569\n",
      "Iteration 102, loss = 3.20291625\n",
      "Iteration 103, loss = 3.20301821\n",
      "Iteration 104, loss = 3.20259430\n",
      "Iteration 105, loss = 3.20235649\n",
      "Iteration 106, loss = 3.20193513\n",
      "Iteration 107, loss = 3.20169910\n",
      "Iteration 108, loss = 3.20128718\n",
      "Iteration 109, loss = 3.20087546\n",
      "Iteration 110, loss = 3.20070490\n",
      "Iteration 111, loss = 3.20048011\n",
      "Iteration 112, loss = 3.20050302\n",
      "Iteration 113, loss = 3.19997017\n",
      "Iteration 114, loss = 3.19936865\n",
      "Iteration 115, loss = 3.19886731\n",
      "Iteration 116, loss = 3.19891016\n",
      "Iteration 117, loss = 3.19834622\n",
      "Iteration 118, loss = 3.19816231\n",
      "Iteration 119, loss = 3.19800496\n",
      "Iteration 120, loss = 3.19782378\n",
      "Iteration 121, loss = 3.19743500\n",
      "Iteration 122, loss = 3.19739130\n",
      "Iteration 123, loss = 3.19753745\n",
      "Iteration 124, loss = 3.19753777\n",
      "Iteration 125, loss = 3.19705196\n",
      "Iteration 126, loss = 3.19642146\n",
      "Iteration 127, loss = 3.19605581\n",
      "Iteration 128, loss = 3.19623579\n",
      "Iteration 129, loss = 3.19547099\n",
      "Iteration 130, loss = 3.19507693\n",
      "Iteration 131, loss = 3.19459214\n",
      "Iteration 132, loss = 3.19446157\n",
      "Iteration 133, loss = 3.19429142\n",
      "Iteration 134, loss = 3.19397467\n",
      "Iteration 135, loss = 3.19351743\n",
      "Iteration 136, loss = 3.19298616\n",
      "Iteration 137, loss = 3.19277948\n",
      "Iteration 138, loss = 3.19262821\n",
      "Iteration 139, loss = 3.19249465\n",
      "Iteration 140, loss = 3.19235028\n",
      "Iteration 141, loss = 3.19200449\n",
      "Iteration 142, loss = 3.19202000\n",
      "Iteration 143, loss = 3.19163401\n",
      "Iteration 144, loss = 3.19091722\n",
      "Iteration 145, loss = 3.19117307\n",
      "Iteration 146, loss = 3.19167492\n",
      "Iteration 147, loss = 3.19148829\n",
      "Iteration 148, loss = 3.19050084\n",
      "Iteration 149, loss = 3.18986936\n",
      "Iteration 150, loss = 3.18970598\n",
      "Iteration 151, loss = 3.18942704\n",
      "Iteration 152, loss = 3.18913924\n",
      "Iteration 153, loss = 3.18998893\n",
      "Iteration 154, loss = 3.18939994\n",
      "Iteration 155, loss = 3.18890559\n",
      "Iteration 156, loss = 3.18845032\n",
      "Iteration 157, loss = 3.18802785\n",
      "Iteration 158, loss = 3.18760820\n",
      "Iteration 159, loss = 3.18728117\n",
      "Iteration 160, loss = 3.18687826\n",
      "Iteration 161, loss = 3.18667759\n",
      "Iteration 162, loss = 3.18659732\n",
      "Iteration 163, loss = 3.18626593\n",
      "Iteration 164, loss = 3.18617543\n",
      "Iteration 165, loss = 3.18521710\n",
      "Iteration 166, loss = 3.18503120\n",
      "Iteration 167, loss = 3.18513483\n",
      "Iteration 168, loss = 3.18451298\n",
      "Iteration 169, loss = 3.18423092\n",
      "Iteration 170, loss = 3.18349804\n",
      "Iteration 171, loss = 3.18341378\n",
      "Iteration 172, loss = 3.18334466\n",
      "Iteration 173, loss = 3.18382406\n",
      "Iteration 174, loss = 3.18358609\n",
      "Iteration 175, loss = 3.18299282\n",
      "Iteration 176, loss = 3.18252740\n",
      "Iteration 177, loss = 3.18171605\n",
      "Iteration 178, loss = 3.18180320\n",
      "Iteration 179, loss = 3.18153830\n",
      "Iteration 180, loss = 3.18153184\n",
      "Iteration 181, loss = 3.18110327\n",
      "Iteration 182, loss = 3.18031912\n",
      "Iteration 183, loss = 3.17973836\n",
      "Iteration 184, loss = 3.17959838\n",
      "Iteration 185, loss = 3.17938205\n",
      "Iteration 186, loss = 3.17917943\n",
      "Iteration 187, loss = 3.17883324\n",
      "Iteration 188, loss = 3.17857163\n",
      "Iteration 189, loss = 3.17804958\n",
      "Iteration 190, loss = 3.17760430\n",
      "Iteration 191, loss = 3.17728496\n",
      "Iteration 192, loss = 3.17714668\n",
      "Iteration 193, loss = 3.17690226\n",
      "Iteration 194, loss = 3.17678904\n",
      "Iteration 195, loss = 3.17702202\n",
      "Iteration 196, loss = 3.17675065\n",
      "Iteration 197, loss = 3.17626553\n",
      "Iteration 198, loss = 3.17568313\n",
      "Iteration 199, loss = 3.17527070\n",
      "Iteration 200, loss = 3.17455039\n",
      "Iteration 201, loss = 3.17427911\n",
      "Iteration 202, loss = 3.17381797\n",
      "Iteration 203, loss = 3.17410378\n",
      "Iteration 204, loss = 3.17374941\n",
      "Iteration 205, loss = 3.17339424\n",
      "Iteration 206, loss = 3.17313039\n",
      "Iteration 207, loss = 3.17279828\n",
      "Iteration 208, loss = 3.17187253\n",
      "Iteration 209, loss = 3.17194324\n",
      "Iteration 210, loss = 3.17223393\n",
      "Iteration 211, loss = 3.17180143\n",
      "Iteration 212, loss = 3.17174488\n",
      "Iteration 213, loss = 3.17165647\n",
      "Iteration 214, loss = 3.17077856\n",
      "Iteration 215, loss = 3.17042759\n",
      "Iteration 216, loss = 3.17019911\n",
      "Iteration 217, loss = 3.16967763\n",
      "Iteration 218, loss = 3.16908436\n",
      "Iteration 219, loss = 3.16846648\n",
      "Iteration 220, loss = 3.16840061\n",
      "Iteration 221, loss = 3.16796262\n",
      "Iteration 222, loss = 3.16782839\n",
      "Iteration 223, loss = 3.16710733\n",
      "Iteration 224, loss = 3.16721032\n",
      "Iteration 225, loss = 3.16723394\n",
      "Iteration 226, loss = 3.16704788\n",
      "Iteration 227, loss = 3.16624355\n",
      "Iteration 228, loss = 3.16609677\n",
      "Iteration 229, loss = 3.16568083\n",
      "Iteration 230, loss = 3.16553356\n",
      "Iteration 231, loss = 3.16512261\n",
      "Iteration 232, loss = 3.16487584\n",
      "Iteration 233, loss = 3.16414909\n",
      "Iteration 234, loss = 3.16370301\n",
      "Iteration 235, loss = 3.16371471\n",
      "Iteration 236, loss = 3.16309374\n",
      "Iteration 237, loss = 3.16288154\n",
      "Iteration 238, loss = 3.16243255\n",
      "Iteration 239, loss = 3.16219167\n",
      "Iteration 240, loss = 3.16186966\n",
      "Iteration 241, loss = 3.16154741\n",
      "Iteration 242, loss = 3.16120173\n",
      "Iteration 243, loss = 3.16082760\n",
      "Iteration 244, loss = 3.16043536\n",
      "Iteration 245, loss = 3.16003103\n",
      "Iteration 246, loss = 3.15995089\n",
      "Iteration 247, loss = 3.15983498\n",
      "Iteration 248, loss = 3.15925425\n",
      "Iteration 249, loss = 3.15914652\n",
      "Iteration 250, loss = 3.15874990\n",
      "Iteration 251, loss = 3.15869509\n",
      "Iteration 252, loss = 3.15832963\n",
      "Iteration 253, loss = 3.15781065\n",
      "Iteration 254, loss = 3.15745868\n",
      "Iteration 255, loss = 3.15726920\n",
      "Iteration 256, loss = 3.15655846\n",
      "Iteration 257, loss = 3.15623084\n",
      "Iteration 258, loss = 3.15594146\n",
      "Iteration 259, loss = 3.15544164\n",
      "Iteration 260, loss = 3.15518911\n",
      "Iteration 261, loss = 3.15511283\n",
      "Iteration 262, loss = 3.15385548\n",
      "Iteration 263, loss = 3.15401685\n",
      "Iteration 264, loss = 3.15324098\n",
      "Iteration 265, loss = 3.15341672\n",
      "Iteration 266, loss = 3.15282275\n",
      "Iteration 267, loss = 3.15246440\n",
      "Iteration 268, loss = 3.15198620\n",
      "Iteration 269, loss = 3.15221354\n",
      "Iteration 270, loss = 3.15181394\n",
      "Iteration 271, loss = 3.15116121\n",
      "Iteration 272, loss = 3.15085559\n",
      "Iteration 273, loss = 3.15012291\n",
      "Iteration 274, loss = 3.15007286\n",
      "Iteration 275, loss = 3.14968473\n",
      "Iteration 276, loss = 3.14946162\n",
      "Iteration 277, loss = 3.14871029\n",
      "Iteration 278, loss = 3.14852393\n",
      "Iteration 279, loss = 3.14814950\n",
      "Iteration 280, loss = 3.14769591\n",
      "Iteration 281, loss = 3.14809011\n",
      "Iteration 282, loss = 3.14780042\n",
      "Iteration 283, loss = 3.14715364\n",
      "Iteration 284, loss = 3.14671626\n",
      "Iteration 285, loss = 3.14651787\n",
      "Iteration 286, loss = 3.14650342\n",
      "Iteration 287, loss = 3.14618438\n",
      "Iteration 288, loss = 3.14526852\n",
      "Iteration 289, loss = 3.14483867\n",
      "Iteration 290, loss = 3.14443394\n",
      "Iteration 291, loss = 3.14413132\n",
      "Iteration 292, loss = 3.14338109\n",
      "Iteration 293, loss = 3.14305973\n",
      "Iteration 294, loss = 3.14300410\n",
      "Iteration 295, loss = 3.14257592\n",
      "Iteration 296, loss = 3.14186927\n",
      "Iteration 297, loss = 3.14177541\n",
      "Iteration 298, loss = 3.14137035\n",
      "Iteration 299, loss = 3.14116612\n",
      "Iteration 300, loss = 3.14053709\n",
      "Iteration 301, loss = 3.14004617\n",
      "Iteration 302, loss = 3.13929598\n",
      "Iteration 303, loss = 3.13939707\n",
      "Iteration 304, loss = 3.13892744\n",
      "Iteration 305, loss = 3.13828369\n",
      "Iteration 306, loss = 3.13792853\n",
      "Iteration 307, loss = 3.13777072\n",
      "Iteration 308, loss = 3.13750883\n",
      "Iteration 309, loss = 3.13718653\n",
      "Iteration 310, loss = 3.13642803\n",
      "Iteration 311, loss = 3.13646457\n",
      "Iteration 312, loss = 3.13650229\n",
      "Iteration 313, loss = 3.13524832\n",
      "Iteration 314, loss = 3.13526718\n",
      "Iteration 315, loss = 3.13492609\n",
      "Iteration 316, loss = 3.13447094\n",
      "Iteration 317, loss = 3.13374902\n",
      "Iteration 318, loss = 3.13318230\n",
      "Iteration 319, loss = 3.13260189\n",
      "Iteration 320, loss = 3.13225265\n",
      "Iteration 321, loss = 3.13204275\n",
      "Iteration 322, loss = 3.13162461\n",
      "Iteration 323, loss = 3.13105805\n",
      "Iteration 324, loss = 3.13081757\n",
      "Iteration 325, loss = 3.13075484\n",
      "Iteration 326, loss = 3.13052624\n",
      "Iteration 327, loss = 3.13065175\n",
      "Iteration 328, loss = 3.13019180\n",
      "Iteration 329, loss = 3.13001033\n",
      "Iteration 330, loss = 3.12873065\n",
      "Iteration 331, loss = 3.12810892\n",
      "Iteration 332, loss = 3.12779891\n",
      "Iteration 333, loss = 3.12723341\n",
      "Iteration 334, loss = 3.12682939\n",
      "Iteration 335, loss = 3.12614030\n",
      "Iteration 336, loss = 3.12643700\n",
      "Iteration 337, loss = 3.12560178\n",
      "Iteration 338, loss = 3.12503246\n",
      "Iteration 339, loss = 3.12530118\n",
      "Iteration 340, loss = 3.12464902\n",
      "Iteration 341, loss = 3.12367844\n",
      "Iteration 342, loss = 3.12349097\n",
      "Iteration 343, loss = 3.12308431\n",
      "Iteration 344, loss = 3.12283944\n",
      "Iteration 345, loss = 3.12282214\n",
      "Iteration 346, loss = 3.12276753\n",
      "Iteration 347, loss = 3.12205556\n",
      "Iteration 348, loss = 3.12217295\n",
      "Iteration 349, loss = 3.12123604\n",
      "Iteration 350, loss = 3.12079828\n",
      "Iteration 351, loss = 3.12037968\n",
      "Iteration 352, loss = 3.11957410\n",
      "Iteration 353, loss = 3.11905143\n",
      "Iteration 354, loss = 3.11905437\n",
      "Iteration 355, loss = 3.11827597\n",
      "Iteration 356, loss = 3.11855996\n",
      "Iteration 357, loss = 3.11753249\n",
      "Iteration 358, loss = 3.11700779\n",
      "Iteration 359, loss = 3.11633085\n",
      "Iteration 360, loss = 3.11647325\n",
      "Iteration 361, loss = 3.11624090\n",
      "Iteration 362, loss = 3.11662899\n",
      "Iteration 363, loss = 3.11490379\n",
      "Iteration 364, loss = 3.11464437\n",
      "Iteration 365, loss = 3.11399519\n",
      "Iteration 366, loss = 3.11377001\n",
      "Iteration 367, loss = 3.11346508\n",
      "Iteration 368, loss = 3.11327100\n",
      "Iteration 369, loss = 3.11296376\n",
      "Iteration 370, loss = 3.11252908\n",
      "Iteration 371, loss = 3.11201117\n",
      "Iteration 372, loss = 3.11161596\n",
      "Iteration 373, loss = 3.11105225\n",
      "Iteration 374, loss = 3.11078353\n",
      "Iteration 375, loss = 3.10987866\n",
      "Iteration 376, loss = 3.10940922\n",
      "Iteration 377, loss = 3.10946305\n",
      "Iteration 378, loss = 3.10902180\n",
      "Iteration 379, loss = 3.10869388\n",
      "Iteration 380, loss = 3.10888043\n",
      "Iteration 381, loss = 3.10796618\n",
      "Iteration 382, loss = 3.10735532\n",
      "Iteration 383, loss = 3.10652676\n",
      "Iteration 384, loss = 3.10645240\n",
      "Iteration 385, loss = 3.10595142\n",
      "Iteration 386, loss = 3.10538889\n",
      "Iteration 387, loss = 3.10452596\n",
      "Iteration 388, loss = 3.10405183\n",
      "Iteration 389, loss = 3.10364973\n",
      "Iteration 390, loss = 3.10331033\n",
      "Iteration 391, loss = 3.10301760\n",
      "Iteration 392, loss = 3.10319847\n",
      "Iteration 393, loss = 3.10251161\n",
      "Iteration 394, loss = 3.10264279\n",
      "Iteration 395, loss = 3.10151875\n",
      "Iteration 396, loss = 3.10121689\n",
      "Iteration 397, loss = 3.10119913\n",
      "Iteration 398, loss = 3.09992854\n",
      "Iteration 399, loss = 3.09945204\n",
      "Iteration 400, loss = 3.09977911\n",
      "Iteration 401, loss = 3.09976901\n",
      "Iteration 402, loss = 3.09935637\n",
      "Iteration 403, loss = 3.09766280\n",
      "Iteration 404, loss = 3.09733358\n",
      "Iteration 405, loss = 3.09685848\n",
      "Iteration 406, loss = 3.09639912\n",
      "Iteration 407, loss = 3.09617571\n",
      "Iteration 408, loss = 3.09576711\n",
      "Iteration 409, loss = 3.09517823\n",
      "Iteration 410, loss = 3.09494522\n",
      "Iteration 411, loss = 3.09439497\n",
      "Iteration 412, loss = 3.09419385\n",
      "Iteration 413, loss = 3.09367052\n",
      "Iteration 414, loss = 3.09342523\n",
      "Iteration 415, loss = 3.09318341\n",
      "Iteration 416, loss = 3.09238183\n",
      "Iteration 417, loss = 3.09181432\n",
      "Iteration 418, loss = 3.09124799\n",
      "Iteration 419, loss = 3.09116595\n",
      "Iteration 420, loss = 3.09020212\n",
      "Iteration 421, loss = 3.08986762\n",
      "Iteration 422, loss = 3.08949977\n",
      "Iteration 423, loss = 3.08915041\n",
      "Iteration 424, loss = 3.08895801\n",
      "Iteration 425, loss = 3.08857242\n",
      "Iteration 426, loss = 3.08821069\n",
      "Iteration 427, loss = 3.08797700\n",
      "Iteration 428, loss = 3.08780614\n",
      "Iteration 429, loss = 3.08673952\n",
      "Iteration 430, loss = 3.08687755\n",
      "Iteration 431, loss = 3.08592617\n",
      "Iteration 432, loss = 3.08574056\n",
      "Iteration 433, loss = 3.08634776\n",
      "Iteration 434, loss = 3.08584048\n",
      "Iteration 435, loss = 3.08410010\n",
      "Iteration 436, loss = 3.08355849\n",
      "Iteration 437, loss = 3.08366397\n",
      "Iteration 438, loss = 3.08310631\n",
      "Iteration 439, loss = 3.08410581\n",
      "Iteration 440, loss = 3.08322870\n",
      "Iteration 441, loss = 3.08162377\n",
      "Iteration 442, loss = 3.08103630\n",
      "Iteration 443, loss = 3.08064171\n",
      "Iteration 444, loss = 3.08027779\n",
      "Iteration 445, loss = 3.07969740\n",
      "Iteration 446, loss = 3.07877218\n",
      "Iteration 447, loss = 3.07941009\n",
      "Iteration 448, loss = 3.07876308\n",
      "Iteration 449, loss = 3.07782222\n",
      "Iteration 450, loss = 3.07804882\n",
      "Iteration 451, loss = 3.07771835\n",
      "Iteration 452, loss = 3.07687254\n",
      "Iteration 453, loss = 3.07725689\n",
      "Iteration 454, loss = 3.07743556\n",
      "Iteration 455, loss = 3.07582113\n",
      "Iteration 456, loss = 3.07657490\n",
      "Iteration 457, loss = 3.07620536\n",
      "Iteration 458, loss = 3.07536864\n",
      "Iteration 459, loss = 3.07450899\n",
      "Iteration 460, loss = 3.07331155\n",
      "Iteration 461, loss = 3.07312335\n",
      "Iteration 462, loss = 3.07263639\n",
      "Iteration 463, loss = 3.07224665\n",
      "Iteration 464, loss = 3.07158695\n",
      "Iteration 465, loss = 3.07120210\n",
      "Iteration 466, loss = 3.07114036\n",
      "Iteration 467, loss = 3.07062041\n",
      "Iteration 468, loss = 3.07047571\n",
      "Iteration 469, loss = 3.07002076\n",
      "Iteration 470, loss = 3.06936833\n",
      "Iteration 471, loss = 3.06931870\n",
      "Iteration 472, loss = 3.06889305\n",
      "Iteration 473, loss = 3.06816493\n",
      "Iteration 474, loss = 3.06734697\n",
      "Iteration 475, loss = 3.06718530\n",
      "Iteration 476, loss = 3.06681778\n",
      "Iteration 477, loss = 3.06645450\n",
      "Iteration 478, loss = 3.06571226\n",
      "Iteration 479, loss = 3.06588370\n",
      "Iteration 480, loss = 3.06538347\n",
      "Iteration 481, loss = 3.06463829\n",
      "Iteration 482, loss = 3.06472404\n",
      "Iteration 483, loss = 3.06371237\n",
      "Iteration 484, loss = 3.06412226\n",
      "Iteration 485, loss = 3.06315410\n",
      "Iteration 486, loss = 3.06253613\n",
      "Iteration 487, loss = 3.06191114\n",
      "Iteration 488, loss = 3.06189671\n",
      "Iteration 489, loss = 3.06132673\n",
      "Iteration 490, loss = 3.06170638\n",
      "Iteration 491, loss = 3.06095515\n",
      "Iteration 492, loss = 3.06018092\n",
      "Iteration 493, loss = 3.05970212\n",
      "Iteration 494, loss = 3.05901053\n",
      "Iteration 495, loss = 3.05956262\n",
      "Iteration 496, loss = 3.05939997\n",
      "Iteration 497, loss = 3.05877614\n",
      "Iteration 498, loss = 3.05812736\n",
      "Iteration 499, loss = 3.05712955\n",
      "Iteration 500, loss = 3.05740180\n",
      "Iteration 501, loss = 3.05687836\n",
      "Iteration 502, loss = 3.05603084\n",
      "Iteration 503, loss = 3.05592848\n",
      "Iteration 504, loss = 3.05609152\n",
      "Iteration 505, loss = 3.05514958\n",
      "Iteration 506, loss = 3.05501897\n",
      "Iteration 507, loss = 3.05486331\n",
      "Iteration 508, loss = 3.05432754\n",
      "Iteration 509, loss = 3.05365007\n",
      "Iteration 510, loss = 3.05300112\n",
      "Iteration 511, loss = 3.05314674\n",
      "Iteration 512, loss = 3.05309086\n",
      "Iteration 513, loss = 3.05240079\n",
      "Iteration 514, loss = 3.05131718\n",
      "Iteration 515, loss = 3.05076852\n",
      "Iteration 516, loss = 3.05053868\n",
      "Iteration 517, loss = 3.05013331\n",
      "Iteration 518, loss = 3.04982411\n",
      "Iteration 519, loss = 3.04977013\n",
      "Iteration 520, loss = 3.04897950\n",
      "Iteration 521, loss = 3.04853088\n",
      "Iteration 522, loss = 3.04788740\n",
      "Iteration 523, loss = 3.04740628\n",
      "Iteration 524, loss = 3.04843131\n",
      "Iteration 525, loss = 3.04813984\n",
      "Iteration 526, loss = 3.04720046\n",
      "Iteration 527, loss = 3.04640637\n",
      "Iteration 528, loss = 3.04628931\n",
      "Iteration 529, loss = 3.04555372\n",
      "Iteration 530, loss = 3.04515370\n",
      "Iteration 531, loss = 3.04439062\n",
      "Iteration 532, loss = 3.04398082\n",
      "Iteration 533, loss = 3.04393567\n",
      "Iteration 534, loss = 3.04366175\n",
      "Iteration 535, loss = 3.04279408\n",
      "Iteration 536, loss = 3.04252499\n",
      "Iteration 537, loss = 3.04214457\n",
      "Iteration 538, loss = 3.04202090\n",
      "Iteration 539, loss = 3.04178604\n",
      "Iteration 540, loss = 3.04064639\n",
      "Iteration 541, loss = 3.04119496\n",
      "Iteration 542, loss = 3.04112123\n",
      "Iteration 543, loss = 3.04039035\n",
      "Iteration 544, loss = 3.03967857\n",
      "Iteration 545, loss = 3.03953409\n",
      "Iteration 546, loss = 3.03950454\n",
      "Iteration 547, loss = 3.03939180\n",
      "Iteration 548, loss = 3.03862234\n",
      "Iteration 549, loss = 3.03839136\n",
      "Iteration 550, loss = 3.03772472\n",
      "Iteration 551, loss = 3.03707277\n",
      "Iteration 552, loss = 3.03711862\n",
      "Iteration 553, loss = 3.03662119\n",
      "Iteration 554, loss = 3.03613327\n",
      "Iteration 555, loss = 3.03535056\n",
      "Iteration 556, loss = 3.03514700\n",
      "Iteration 557, loss = 3.03460251\n",
      "Iteration 558, loss = 3.03421777\n",
      "Iteration 559, loss = 3.03459116\n",
      "Iteration 560, loss = 3.03375868\n",
      "Iteration 561, loss = 3.03313472\n",
      "Iteration 562, loss = 3.03284791\n",
      "Iteration 563, loss = 3.03289537\n",
      "Iteration 564, loss = 3.03222380\n",
      "Iteration 565, loss = 3.03148816\n",
      "Iteration 566, loss = 3.03118830\n",
      "Iteration 567, loss = 3.03068492\n",
      "Iteration 568, loss = 3.03035314\n",
      "Iteration 569, loss = 3.03017155\n",
      "Iteration 570, loss = 3.02973560\n",
      "Iteration 571, loss = 3.02939581\n",
      "Iteration 572, loss = 3.02902441\n",
      "Iteration 573, loss = 3.02873667\n",
      "Iteration 574, loss = 3.02825457\n",
      "Iteration 575, loss = 3.02832125\n",
      "Iteration 576, loss = 3.02741859\n",
      "Iteration 577, loss = 3.02735120\n",
      "Iteration 578, loss = 3.02683483\n",
      "Iteration 579, loss = 3.02695028\n",
      "Iteration 580, loss = 3.02696369\n",
      "Iteration 581, loss = 3.02639700\n",
      "Iteration 582, loss = 3.02565004\n",
      "Iteration 583, loss = 3.02550213\n",
      "Iteration 584, loss = 3.02482930\n",
      "Iteration 585, loss = 3.02430035\n",
      "Iteration 586, loss = 3.02407528\n",
      "Iteration 587, loss = 3.02381886\n",
      "Iteration 588, loss = 3.02309307\n",
      "Iteration 589, loss = 3.02271537\n",
      "Iteration 590, loss = 3.02247400\n",
      "Iteration 591, loss = 3.02317323\n",
      "Iteration 592, loss = 3.02200200\n",
      "Iteration 593, loss = 3.02161392\n",
      "Iteration 594, loss = 3.02147392\n",
      "Iteration 595, loss = 3.02126679\n",
      "Iteration 596, loss = 3.02075657\n",
      "Iteration 597, loss = 3.02070275\n",
      "Iteration 598, loss = 3.02043174\n",
      "Iteration 599, loss = 3.01918742\n",
      "Iteration 600, loss = 3.01930872\n",
      "Iteration 601, loss = 3.01897738\n",
      "Iteration 602, loss = 3.01933714\n",
      "Iteration 603, loss = 3.01845092\n",
      "Iteration 604, loss = 3.01816241\n",
      "Iteration 605, loss = 3.01771268\n",
      "Iteration 606, loss = 3.01717973\n",
      "Iteration 607, loss = 3.01691872\n",
      "Iteration 608, loss = 3.01676345\n",
      "Iteration 609, loss = 3.01678845\n",
      "Iteration 610, loss = 3.01603514\n",
      "Iteration 611, loss = 3.01538290\n",
      "Iteration 612, loss = 3.01476531\n",
      "Iteration 613, loss = 3.01444379\n",
      "Iteration 614, loss = 3.01403079\n",
      "Iteration 615, loss = 3.01428594\n",
      "Iteration 616, loss = 3.01351121\n",
      "Iteration 617, loss = 3.01448675\n",
      "Iteration 618, loss = 3.01357118\n",
      "Iteration 619, loss = 3.01278849\n",
      "Iteration 620, loss = 3.01229021\n",
      "Iteration 621, loss = 3.01184253\n",
      "Iteration 622, loss = 3.01179212\n",
      "Iteration 623, loss = 3.01135044\n",
      "Iteration 624, loss = 3.01104828\n",
      "Iteration 625, loss = 3.01037115\n",
      "Iteration 626, loss = 3.00986209\n",
      "Iteration 627, loss = 3.00978693\n",
      "Iteration 628, loss = 3.00923240\n",
      "Iteration 629, loss = 3.00881897\n",
      "Iteration 630, loss = 3.00890453\n",
      "Iteration 631, loss = 3.00870500\n",
      "Iteration 632, loss = 3.00748047\n",
      "Iteration 633, loss = 3.00742851\n",
      "Iteration 634, loss = 3.00727087\n",
      "Iteration 635, loss = 3.00726627\n",
      "Iteration 636, loss = 3.00631720\n",
      "Iteration 637, loss = 3.00612092\n",
      "Iteration 638, loss = 3.00650343\n",
      "Iteration 639, loss = 3.00522329\n",
      "Iteration 640, loss = 3.00497992\n",
      "Iteration 641, loss = 3.00486444\n",
      "Iteration 642, loss = 3.00458471\n",
      "Iteration 643, loss = 3.00504159\n",
      "Iteration 644, loss = 3.00397411\n",
      "Iteration 645, loss = 3.00388164\n",
      "Iteration 646, loss = 3.00313821\n",
      "Iteration 647, loss = 3.00291983\n",
      "Iteration 648, loss = 3.00334212\n",
      "Iteration 649, loss = 3.00267912\n",
      "Iteration 650, loss = 3.00176399\n",
      "Iteration 651, loss = 3.00167820\n",
      "Iteration 652, loss = 3.00113212\n",
      "Iteration 653, loss = 3.00074764\n",
      "Iteration 654, loss = 3.00033003\n",
      "Iteration 655, loss = 2.99977651\n",
      "Iteration 656, loss = 3.00003773\n",
      "Iteration 657, loss = 2.99914624\n",
      "Iteration 658, loss = 2.99928683\n",
      "Iteration 659, loss = 2.99872730\n",
      "Iteration 660, loss = 2.99836884\n",
      "Iteration 661, loss = 2.99823091\n",
      "Iteration 662, loss = 2.99779546\n",
      "Iteration 663, loss = 2.99734211\n",
      "Iteration 664, loss = 2.99693235\n",
      "Iteration 665, loss = 2.99662604\n",
      "Iteration 666, loss = 2.99635620\n",
      "Iteration 667, loss = 2.99661454\n",
      "Iteration 668, loss = 2.99679876\n",
      "Iteration 669, loss = 2.99578753\n",
      "Iteration 670, loss = 2.99481003\n",
      "Iteration 671, loss = 2.99454160\n",
      "Iteration 672, loss = 2.99434976\n",
      "Iteration 673, loss = 2.99433822\n",
      "Iteration 674, loss = 2.99442534\n",
      "Iteration 675, loss = 2.99354377\n",
      "Iteration 676, loss = 2.99355763\n",
      "Iteration 677, loss = 2.99309918\n",
      "Iteration 678, loss = 2.99254997\n",
      "Iteration 679, loss = 2.99217662\n",
      "Iteration 680, loss = 2.99236690\n",
      "Iteration 681, loss = 2.99222537\n",
      "Iteration 682, loss = 2.99127177\n",
      "Iteration 683, loss = 2.99130347\n",
      "Iteration 684, loss = 2.99197223\n",
      "Iteration 685, loss = 2.99099805\n",
      "Iteration 686, loss = 2.99034890\n",
      "Iteration 687, loss = 2.98986425\n",
      "Iteration 688, loss = 2.98898147\n",
      "Iteration 689, loss = 2.98943256\n",
      "Iteration 690, loss = 2.98917233\n",
      "Iteration 691, loss = 2.98865866\n",
      "Iteration 692, loss = 2.98823076\n",
      "Iteration 693, loss = 2.98803498\n",
      "Iteration 694, loss = 2.98736045\n",
      "Iteration 695, loss = 2.98771006\n",
      "Iteration 696, loss = 2.98674982\n",
      "Iteration 697, loss = 2.98630245\n",
      "Iteration 698, loss = 2.98632612\n",
      "Iteration 699, loss = 2.98554608\n",
      "Iteration 700, loss = 2.98528891\n",
      "Iteration 701, loss = 2.98511670\n",
      "Iteration 702, loss = 2.98486919\n",
      "Iteration 703, loss = 2.98478426\n",
      "Iteration 704, loss = 2.98452065\n",
      "Iteration 705, loss = 2.98427678\n",
      "Iteration 706, loss = 2.98359841\n",
      "Iteration 707, loss = 2.98357220\n",
      "Iteration 708, loss = 2.98391129\n",
      "Iteration 709, loss = 2.98389832\n",
      "Iteration 710, loss = 2.98267447\n",
      "Iteration 711, loss = 2.98198930\n",
      "Iteration 712, loss = 2.98213219\n",
      "Iteration 713, loss = 2.98216246\n",
      "Iteration 714, loss = 2.98111093\n",
      "Iteration 715, loss = 2.98108644\n",
      "Iteration 716, loss = 2.98097600\n",
      "Iteration 717, loss = 2.98062019\n",
      "Iteration 718, loss = 2.98032301\n",
      "Iteration 719, loss = 2.97938189\n",
      "Iteration 720, loss = 2.97897489\n",
      "Iteration 721, loss = 2.97882196\n",
      "Iteration 722, loss = 2.97872523\n",
      "Iteration 723, loss = 2.97830338\n",
      "Iteration 724, loss = 2.97813469\n",
      "Iteration 725, loss = 2.97798042\n",
      "Iteration 726, loss = 2.97735435\n",
      "Iteration 727, loss = 2.97705151\n",
      "Iteration 728, loss = 2.97675216\n",
      "Iteration 729, loss = 2.97705327\n",
      "Iteration 730, loss = 2.97635498\n",
      "Iteration 731, loss = 2.97614753\n",
      "Iteration 732, loss = 2.97587729\n",
      "Iteration 733, loss = 2.97558228\n",
      "Iteration 734, loss = 2.97562611\n",
      "Iteration 735, loss = 2.97533343\n",
      "Iteration 736, loss = 2.97565570\n",
      "Iteration 737, loss = 2.97458443\n",
      "Iteration 738, loss = 2.97373350\n",
      "Iteration 739, loss = 2.97422413\n",
      "Iteration 740, loss = 2.97283772\n",
      "Iteration 741, loss = 2.97309240\n",
      "Iteration 742, loss = 2.97360874\n",
      "Iteration 743, loss = 2.97392795\n",
      "Iteration 744, loss = 2.97278984\n",
      "Iteration 745, loss = 2.97127841\n",
      "Iteration 746, loss = 2.97162026\n",
      "Iteration 747, loss = 2.97146080\n",
      "Iteration 748, loss = 2.97117440\n",
      "Iteration 749, loss = 2.97121416\n",
      "Iteration 750, loss = 2.97079432\n",
      "Iteration 751, loss = 2.96973050\n",
      "Iteration 752, loss = 2.96947771\n",
      "Iteration 753, loss = 2.96892672\n",
      "Iteration 754, loss = 2.96872257\n",
      "Iteration 755, loss = 2.96820193\n",
      "Iteration 756, loss = 2.96865539\n",
      "Iteration 757, loss = 2.96878317\n",
      "Iteration 758, loss = 2.96861090\n",
      "Iteration 759, loss = 2.96798432\n",
      "Iteration 760, loss = 2.96705876\n",
      "Iteration 761, loss = 2.96656147\n",
      "Iteration 762, loss = 2.96689180\n",
      "Iteration 763, loss = 2.96650594\n",
      "Iteration 764, loss = 2.96583127\n",
      "Iteration 765, loss = 2.96565538\n",
      "Iteration 766, loss = 2.96474041\n",
      "Iteration 767, loss = 2.96476622\n",
      "Iteration 768, loss = 2.96469997\n",
      "Iteration 769, loss = 2.96437126\n",
      "Iteration 770, loss = 2.96419438\n",
      "Iteration 771, loss = 2.96339880\n",
      "Iteration 772, loss = 2.96306668\n",
      "Iteration 773, loss = 2.96285801\n",
      "Iteration 774, loss = 2.96375375\n",
      "Iteration 775, loss = 2.96359590\n",
      "Iteration 776, loss = 2.96250368\n",
      "Iteration 777, loss = 2.96176180\n",
      "Iteration 778, loss = 2.96186071\n",
      "Iteration 779, loss = 2.96141318\n",
      "Iteration 780, loss = 2.96092180\n",
      "Iteration 781, loss = 2.96059467\n",
      "Iteration 782, loss = 2.96040349\n",
      "Iteration 783, loss = 2.95986460\n",
      "Iteration 784, loss = 2.95941598\n",
      "Iteration 785, loss = 2.96000139\n",
      "Iteration 786, loss = 2.95941501\n",
      "Iteration 787, loss = 2.95852904\n",
      "Iteration 788, loss = 2.95839027\n",
      "Iteration 789, loss = 2.95827993\n",
      "Iteration 790, loss = 2.95762258\n",
      "Iteration 791, loss = 2.95836190\n",
      "Iteration 792, loss = 2.95759217\n",
      "Iteration 793, loss = 2.95825308\n",
      "Iteration 794, loss = 2.95736952\n",
      "Iteration 795, loss = 2.95579778\n",
      "Iteration 796, loss = 2.95580585\n",
      "Iteration 797, loss = 2.95613043\n",
      "Iteration 798, loss = 2.95601914\n",
      "Iteration 799, loss = 2.95548504\n",
      "Iteration 800, loss = 2.95504945\n",
      "Iteration 801, loss = 2.95474207\n",
      "Iteration 802, loss = 2.95502716\n",
      "Iteration 803, loss = 2.95400136\n",
      "Iteration 804, loss = 2.95403209\n",
      "Iteration 805, loss = 2.95399852\n",
      "Iteration 806, loss = 2.95334392\n",
      "Iteration 807, loss = 2.95291897\n",
      "Iteration 808, loss = 2.95266191\n",
      "Iteration 809, loss = 2.95254594\n",
      "Iteration 810, loss = 2.95200517\n",
      "Iteration 811, loss = 2.95146701\n",
      "Iteration 812, loss = 2.95143581\n",
      "Iteration 813, loss = 2.95094680\n",
      "Iteration 814, loss = 2.95075962\n",
      "Iteration 815, loss = 2.95126738\n",
      "Iteration 816, loss = 2.95086641\n",
      "Iteration 817, loss = 2.95056810\n",
      "Iteration 818, loss = 2.94969440\n",
      "Iteration 819, loss = 2.94901950\n",
      "Iteration 820, loss = 2.94914784\n",
      "Iteration 821, loss = 2.94876714\n",
      "Iteration 822, loss = 2.94856496\n",
      "Iteration 823, loss = 2.94829029\n",
      "Iteration 824, loss = 2.94783418\n",
      "Iteration 825, loss = 2.94776067\n",
      "Iteration 826, loss = 2.94751699\n",
      "Iteration 827, loss = 2.94717865\n",
      "Iteration 828, loss = 2.94687150\n",
      "Iteration 829, loss = 2.94637476\n",
      "Iteration 830, loss = 2.94656941\n",
      "Iteration 831, loss = 2.94656466\n",
      "Iteration 832, loss = 2.94592436\n",
      "Iteration 833, loss = 2.94559907\n",
      "Iteration 834, loss = 2.94568546\n",
      "Iteration 835, loss = 2.94498479\n",
      "Iteration 836, loss = 2.94477306\n",
      "Iteration 837, loss = 2.94427864\n",
      "Iteration 838, loss = 2.94369896\n",
      "Iteration 839, loss = 2.94355320\n",
      "Iteration 840, loss = 2.94324940\n",
      "Iteration 841, loss = 2.94290302\n",
      "Iteration 842, loss = 2.94256805\n",
      "Iteration 843, loss = 2.94250135\n",
      "Iteration 844, loss = 2.94305923\n",
      "Iteration 845, loss = 2.94363275\n",
      "Iteration 846, loss = 2.94194594\n",
      "Iteration 847, loss = 2.94208800\n",
      "Iteration 848, loss = 2.94246616\n",
      "Iteration 849, loss = 2.94076888\n",
      "Iteration 850, loss = 2.93992545\n",
      "Iteration 851, loss = 2.94077175\n",
      "Iteration 852, loss = 2.94018079\n",
      "Iteration 853, loss = 2.93927643\n",
      "Iteration 854, loss = 2.93939264\n",
      "Iteration 855, loss = 2.93897356\n",
      "Iteration 856, loss = 2.93873751\n",
      "Iteration 857, loss = 2.93867790\n",
      "Iteration 858, loss = 2.93855773\n",
      "Iteration 859, loss = 2.93823899\n",
      "Iteration 860, loss = 2.93803715\n",
      "Iteration 861, loss = 2.93776117\n",
      "Iteration 862, loss = 2.93697673\n",
      "Iteration 863, loss = 2.93656393\n",
      "Iteration 864, loss = 2.93701995\n",
      "Iteration 865, loss = 2.93673677\n",
      "Iteration 866, loss = 2.93590994\n",
      "Iteration 867, loss = 2.93542198\n",
      "Iteration 868, loss = 2.93521106\n",
      "Iteration 869, loss = 2.93549905\n",
      "Iteration 870, loss = 2.93481455\n",
      "Iteration 871, loss = 2.93475503\n",
      "Iteration 872, loss = 2.93472793\n",
      "Iteration 873, loss = 2.93491075\n",
      "Iteration 874, loss = 2.93373552\n",
      "Iteration 875, loss = 2.93334156\n",
      "Iteration 876, loss = 2.93295364\n",
      "Iteration 877, loss = 2.93281890\n",
      "Iteration 878, loss = 2.93301283\n",
      "Iteration 879, loss = 2.93258230\n",
      "Iteration 880, loss = 2.93259606\n",
      "Iteration 881, loss = 2.93211736\n",
      "Iteration 882, loss = 2.93164643\n",
      "Iteration 883, loss = 2.93116857\n",
      "Iteration 884, loss = 2.93062415\n",
      "Iteration 885, loss = 2.93028904\n",
      "Iteration 886, loss = 2.93037677\n",
      "Iteration 887, loss = 2.93063965\n",
      "Iteration 888, loss = 2.92999781\n",
      "Iteration 889, loss = 2.93007300\n",
      "Iteration 890, loss = 2.92971049\n",
      "Iteration 891, loss = 2.92948891\n",
      "Iteration 892, loss = 2.92876922\n",
      "Iteration 893, loss = 2.92883163\n",
      "Iteration 894, loss = 2.92775169\n",
      "Iteration 895, loss = 2.92775787\n",
      "Iteration 896, loss = 2.92741595\n",
      "Iteration 897, loss = 2.92752771\n",
      "Iteration 898, loss = 2.92713035\n",
      "Iteration 899, loss = 2.92667762\n",
      "Iteration 900, loss = 2.92655445\n",
      "Iteration 901, loss = 2.92621393\n",
      "Iteration 902, loss = 2.92598821\n",
      "Iteration 903, loss = 2.92501058\n",
      "Iteration 904, loss = 2.92576503\n",
      "Iteration 905, loss = 2.92532265\n",
      "Iteration 906, loss = 2.92465519\n",
      "Iteration 907, loss = 2.92394187\n",
      "Iteration 908, loss = 2.92422589\n",
      "Iteration 909, loss = 2.92364927\n",
      "Iteration 910, loss = 2.92355563\n",
      "Iteration 911, loss = 2.92322411\n",
      "Iteration 912, loss = 2.92334159\n",
      "Iteration 913, loss = 2.92328642\n",
      "Iteration 914, loss = 2.92285461\n",
      "Iteration 915, loss = 2.92237218\n",
      "Iteration 916, loss = 2.92328530\n",
      "Iteration 917, loss = 2.92189617\n",
      "Iteration 918, loss = 2.92150318\n",
      "Iteration 919, loss = 2.92221254\n",
      "Iteration 920, loss = 2.92242727\n",
      "Iteration 921, loss = 2.92125587\n",
      "Iteration 922, loss = 2.91997602\n",
      "Iteration 923, loss = 2.92103432\n",
      "Iteration 924, loss = 2.91977092\n",
      "Iteration 925, loss = 2.91933474\n",
      "Iteration 926, loss = 2.91940022\n",
      "Iteration 927, loss = 2.91867593\n",
      "Iteration 928, loss = 2.91914822\n",
      "Iteration 929, loss = 2.91852211\n",
      "Iteration 930, loss = 2.91844043\n",
      "Iteration 931, loss = 2.91857498\n",
      "Iteration 932, loss = 2.91767507\n",
      "Iteration 933, loss = 2.91732541\n",
      "Iteration 934, loss = 2.91687393\n",
      "Iteration 935, loss = 2.91672775\n",
      "Iteration 936, loss = 2.91632460\n",
      "Iteration 937, loss = 2.91639150\n",
      "Iteration 938, loss = 2.91594615\n",
      "Iteration 939, loss = 2.91598384\n",
      "Iteration 940, loss = 2.91554843\n",
      "Iteration 941, loss = 2.91523340\n",
      "Iteration 942, loss = 2.91539220\n",
      "Iteration 943, loss = 2.91479541\n",
      "Iteration 944, loss = 2.91455480\n",
      "Iteration 945, loss = 2.91433259\n",
      "Iteration 946, loss = 2.91409956\n",
      "Iteration 947, loss = 2.91365622\n",
      "Iteration 948, loss = 2.91402097\n",
      "Iteration 949, loss = 2.91339656\n",
      "Iteration 950, loss = 2.91319971\n",
      "Iteration 951, loss = 2.91253747\n",
      "Iteration 952, loss = 2.91240534\n",
      "Iteration 953, loss = 2.91218911\n",
      "Iteration 954, loss = 2.91215963\n",
      "Iteration 955, loss = 2.91156871\n",
      "Iteration 956, loss = 2.91151471\n",
      "Iteration 957, loss = 2.91113015\n",
      "Iteration 958, loss = 2.91095020\n",
      "Iteration 959, loss = 2.91055015\n",
      "Iteration 960, loss = 2.91009924\n",
      "Iteration 961, loss = 2.91073530\n",
      "Iteration 962, loss = 2.90988820\n",
      "Iteration 963, loss = 2.90904912\n",
      "Iteration 964, loss = 2.90923684\n",
      "Iteration 965, loss = 2.90888294\n",
      "Iteration 966, loss = 2.90864619\n",
      "Iteration 967, loss = 2.90861230\n",
      "Iteration 968, loss = 2.90838605\n",
      "Iteration 969, loss = 2.90789569\n",
      "Iteration 970, loss = 2.90827057\n",
      "Iteration 971, loss = 2.90858929\n",
      "Iteration 972, loss = 2.90883978\n",
      "Iteration 973, loss = 2.90746997\n",
      "Iteration 974, loss = 2.90652177\n",
      "Iteration 975, loss = 2.90647472\n",
      "Iteration 976, loss = 2.90754282\n",
      "Iteration 977, loss = 2.90656003\n",
      "Iteration 978, loss = 2.90564442\n",
      "Iteration 979, loss = 2.90545060\n",
      "Iteration 980, loss = 2.90498646\n",
      "Iteration 981, loss = 2.90543604\n",
      "Iteration 982, loss = 2.90473738\n",
      "Iteration 983, loss = 2.90440327\n",
      "Iteration 984, loss = 2.90387198\n",
      "Iteration 985, loss = 2.90406066\n",
      "Iteration 986, loss = 2.90349928\n",
      "Iteration 987, loss = 2.90324041\n",
      "Iteration 988, loss = 2.90438302\n",
      "Iteration 989, loss = 2.90396283\n",
      "Iteration 990, loss = 2.90291419\n",
      "Iteration 991, loss = 2.90207250\n",
      "Iteration 992, loss = 2.90194258\n",
      "Iteration 993, loss = 2.90182927\n",
      "Iteration 994, loss = 2.90209368\n",
      "Iteration 995, loss = 2.90154822\n",
      "Iteration 996, loss = 2.90071578\n",
      "Iteration 997, loss = 2.90065240\n",
      "Iteration 998, loss = 2.90006834\n",
      "Iteration 999, loss = 2.89958172\n",
      "Iteration 1000, loss = 2.89945493\n",
      "Iteration 1001, loss = 2.89927830\n",
      "Iteration 1002, loss = 2.89958543\n",
      "Iteration 1003, loss = 2.89893067\n",
      "Iteration 1004, loss = 2.89859450\n",
      "Iteration 1005, loss = 2.89814593\n",
      "Iteration 1006, loss = 2.89804841\n",
      "Iteration 1007, loss = 2.89811821\n",
      "Iteration 1008, loss = 2.89814548\n",
      "Iteration 1009, loss = 2.89832326\n",
      "Iteration 1010, loss = 2.89770939\n",
      "Iteration 1011, loss = 2.89676463\n",
      "Iteration 1012, loss = 2.89663845\n",
      "Iteration 1013, loss = 2.89588596\n",
      "Iteration 1014, loss = 2.89606261\n",
      "Iteration 1015, loss = 2.89590976\n",
      "Iteration 1016, loss = 2.89569360\n",
      "Iteration 1017, loss = 2.89530706\n",
      "Iteration 1018, loss = 2.89514668\n",
      "Iteration 1019, loss = 2.89522090\n",
      "Iteration 1020, loss = 2.89468117\n",
      "Iteration 1021, loss = 2.89463039\n",
      "Iteration 1022, loss = 2.89394083\n",
      "Iteration 1023, loss = 2.89343603\n",
      "Iteration 1024, loss = 2.89396176\n",
      "Iteration 1025, loss = 2.89400291\n",
      "Iteration 1026, loss = 2.89338271\n",
      "Iteration 1027, loss = 2.89344689\n",
      "Iteration 1028, loss = 2.89267253\n",
      "Iteration 1029, loss = 2.89302747\n",
      "Iteration 1030, loss = 2.89274565\n",
      "Iteration 1031, loss = 2.89201601\n",
      "Iteration 1032, loss = 2.89163249\n",
      "Iteration 1033, loss = 2.89159046\n",
      "Iteration 1034, loss = 2.89169485\n",
      "Iteration 1035, loss = 2.89121960\n",
      "Iteration 1036, loss = 2.89083643\n",
      "Iteration 1037, loss = 2.89045119\n",
      "Iteration 1038, loss = 2.88991204\n",
      "Iteration 1039, loss = 2.89032356\n",
      "Iteration 1040, loss = 2.88972557\n",
      "Iteration 1041, loss = 2.88931182\n",
      "Iteration 1042, loss = 2.88923314\n",
      "Iteration 1043, loss = 2.88887184\n",
      "Iteration 1044, loss = 2.88842811\n",
      "Iteration 1045, loss = 2.88817737\n",
      "Iteration 1046, loss = 2.88815923\n",
      "Iteration 1047, loss = 2.88798913\n",
      "Iteration 1048, loss = 2.88783908\n",
      "Iteration 1049, loss = 2.88819654\n",
      "Iteration 1050, loss = 2.88736601\n",
      "Iteration 1051, loss = 2.88715328\n",
      "Iteration 1052, loss = 2.88711826\n",
      "Iteration 1053, loss = 2.88705550\n",
      "Iteration 1054, loss = 2.88708296\n",
      "Iteration 1055, loss = 2.88743567\n",
      "Iteration 1056, loss = 2.88607782\n",
      "Iteration 1057, loss = 2.88612523\n",
      "Iteration 1058, loss = 2.88593220\n",
      "Iteration 1059, loss = 2.88562968\n",
      "Iteration 1060, loss = 2.88506022\n",
      "Iteration 1061, loss = 2.88467821\n",
      "Iteration 1062, loss = 2.88432431\n",
      "Iteration 1063, loss = 2.88433145\n",
      "Iteration 1064, loss = 2.88372523\n",
      "Iteration 1065, loss = 2.88373909\n",
      "Iteration 1066, loss = 2.88360954\n",
      "Iteration 1067, loss = 2.88352741\n",
      "Iteration 1068, loss = 2.88327278\n",
      "Iteration 1069, loss = 2.88252721\n",
      "Iteration 1070, loss = 2.88275573\n",
      "Iteration 1071, loss = 2.88219186\n",
      "Iteration 1072, loss = 2.88221699\n",
      "Iteration 1073, loss = 2.88182638\n",
      "Iteration 1074, loss = 2.88151340\n",
      "Iteration 1075, loss = 2.88223462\n",
      "Iteration 1076, loss = 2.88092831\n",
      "Iteration 1077, loss = 2.88133813\n",
      "Iteration 1078, loss = 2.88105052\n",
      "Iteration 1079, loss = 2.88013230\n",
      "Iteration 1080, loss = 2.87971093\n",
      "Iteration 1081, loss = 2.87999646\n",
      "Iteration 1082, loss = 2.87951591\n",
      "Iteration 1083, loss = 2.87913389\n",
      "Iteration 1084, loss = 2.87885073\n",
      "Iteration 1085, loss = 2.87939415\n",
      "Iteration 1086, loss = 2.87856533\n",
      "Iteration 1087, loss = 2.87873172\n",
      "Iteration 1088, loss = 2.87848346\n",
      "Iteration 1089, loss = 2.87814267\n",
      "Iteration 1090, loss = 2.87779099\n",
      "Iteration 1091, loss = 2.87757563\n",
      "Iteration 1092, loss = 2.87751488\n",
      "Iteration 1093, loss = 2.87727141\n",
      "Iteration 1094, loss = 2.87764485\n",
      "Iteration 1095, loss = 2.87729529\n",
      "Iteration 1096, loss = 2.87676847\n",
      "Iteration 1097, loss = 2.87608391\n",
      "Iteration 1098, loss = 2.87572838\n",
      "Iteration 1099, loss = 2.87551368\n",
      "Iteration 1100, loss = 2.87534716\n",
      "Iteration 1101, loss = 2.87526426\n",
      "Iteration 1102, loss = 2.87513238\n",
      "Iteration 1103, loss = 2.87474363\n",
      "Iteration 1104, loss = 2.87473516\n",
      "Iteration 1105, loss = 2.87421074\n",
      "Iteration 1106, loss = 2.87431131\n",
      "Iteration 1107, loss = 2.87429787\n",
      "Iteration 1108, loss = 2.87394208\n",
      "Iteration 1109, loss = 2.87387284\n",
      "Iteration 1110, loss = 2.87401825\n",
      "Iteration 1111, loss = 2.87367555\n",
      "Iteration 1112, loss = 2.87274295\n",
      "Iteration 1113, loss = 2.87234999\n",
      "Iteration 1114, loss = 2.87248722\n",
      "Iteration 1115, loss = 2.87228082\n",
      "Iteration 1116, loss = 2.87167234\n",
      "Iteration 1117, loss = 2.87222863\n",
      "Iteration 1118, loss = 2.87131873\n",
      "Iteration 1119, loss = 2.87158976\n",
      "Iteration 1120, loss = 2.87063877\n",
      "Iteration 1121, loss = 2.87108900\n",
      "Iteration 1122, loss = 2.87160434\n",
      "Iteration 1123, loss = 2.87062285\n",
      "Iteration 1124, loss = 2.87005253\n",
      "Iteration 1125, loss = 2.87031644\n",
      "Iteration 1126, loss = 2.87080980\n",
      "Iteration 1127, loss = 2.87141366\n",
      "Iteration 1128, loss = 2.87072810\n",
      "Iteration 1129, loss = 2.86973725\n",
      "Iteration 1130, loss = 2.86944005\n",
      "Iteration 1131, loss = 2.86944989\n",
      "Iteration 1132, loss = 2.86880116\n",
      "Iteration 1133, loss = 2.86791923\n",
      "Iteration 1134, loss = 2.86934293\n",
      "Iteration 1135, loss = 2.86839727\n",
      "Iteration 1136, loss = 2.86762056\n",
      "Iteration 1137, loss = 2.86799444\n",
      "Iteration 1138, loss = 2.86791615\n",
      "Iteration 1139, loss = 2.86696273\n",
      "Iteration 1140, loss = 2.86667423\n",
      "Iteration 1141, loss = 2.86658777\n",
      "Iteration 1142, loss = 2.86630249\n",
      "Iteration 1143, loss = 2.86623076\n",
      "Iteration 1144, loss = 2.86601214\n",
      "Iteration 1145, loss = 2.86544782\n",
      "Iteration 1146, loss = 2.86512656\n",
      "Iteration 1147, loss = 2.86499415\n",
      "Iteration 1148, loss = 2.86443807\n",
      "Iteration 1149, loss = 2.86452383\n",
      "Iteration 1150, loss = 2.86424675\n",
      "Iteration 1151, loss = 2.86477600\n",
      "Iteration 1152, loss = 2.86396803\n",
      "Iteration 1153, loss = 2.86349356\n",
      "Iteration 1154, loss = 2.86330167\n",
      "Iteration 1155, loss = 2.86394495\n",
      "Iteration 1156, loss = 2.86391256\n",
      "Iteration 1157, loss = 2.86340857\n",
      "Iteration 1158, loss = 2.86322963\n",
      "Iteration 1159, loss = 2.86205106\n",
      "Iteration 1160, loss = 2.86229506\n",
      "Iteration 1161, loss = 2.86177190\n",
      "Iteration 1162, loss = 2.86193043\n",
      "Iteration 1163, loss = 2.86154712\n",
      "Iteration 1164, loss = 2.86169376\n",
      "Iteration 1165, loss = 2.86144793\n",
      "Iteration 1166, loss = 2.86101115\n",
      "Iteration 1167, loss = 2.86032358\n",
      "Iteration 1168, loss = 2.86080079\n",
      "Iteration 1169, loss = 2.86064564\n",
      "Iteration 1170, loss = 2.86004085\n",
      "Iteration 1171, loss = 2.86003067\n",
      "Iteration 1172, loss = 2.86033928\n",
      "Iteration 1173, loss = 2.86032908\n",
      "Iteration 1174, loss = 2.85903358\n",
      "Iteration 1175, loss = 2.85888532\n",
      "Iteration 1176, loss = 2.85871411\n",
      "Iteration 1177, loss = 2.85890624\n",
      "Iteration 1178, loss = 2.85878709\n",
      "Iteration 1179, loss = 2.85882633\n",
      "Iteration 1180, loss = 2.85828181\n",
      "Iteration 1181, loss = 2.85746077\n",
      "Iteration 1182, loss = 2.85727467\n",
      "Iteration 1183, loss = 2.85721014\n",
      "Iteration 1184, loss = 2.85701895\n",
      "Iteration 1185, loss = 2.85684780\n",
      "Iteration 1186, loss = 2.85664135\n",
      "Iteration 1187, loss = 2.85634801\n",
      "Iteration 1188, loss = 2.85605606\n",
      "Iteration 1189, loss = 2.85574179\n",
      "Iteration 1190, loss = 2.85603664\n",
      "Iteration 1191, loss = 2.85533747\n",
      "Iteration 1192, loss = 2.85524874\n",
      "Iteration 1193, loss = 2.85477827\n",
      "Iteration 1194, loss = 2.85496134\n",
      "Iteration 1195, loss = 2.85474084\n",
      "Iteration 1196, loss = 2.85457403\n",
      "Iteration 1197, loss = 2.85464762\n",
      "Iteration 1198, loss = 2.85423067\n",
      "Iteration 1199, loss = 2.85465722\n",
      "Iteration 1200, loss = 2.85470333\n",
      "Iteration 1201, loss = 2.85381099\n",
      "Iteration 1202, loss = 2.85355725\n",
      "Iteration 1203, loss = 2.85291361\n",
      "Iteration 1204, loss = 2.85307573\n",
      "Iteration 1205, loss = 2.85284758\n",
      "Iteration 1206, loss = 2.85278597\n",
      "Iteration 1207, loss = 2.85211767\n",
      "Iteration 1208, loss = 2.85183089\n",
      "Iteration 1209, loss = 2.85149358\n",
      "Iteration 1210, loss = 2.85136293\n",
      "Iteration 1211, loss = 2.85158473\n",
      "Iteration 1212, loss = 2.85155566\n",
      "Iteration 1213, loss = 2.85145222\n",
      "Iteration 1214, loss = 2.85096770\n",
      "Iteration 1215, loss = 2.85041616\n",
      "Iteration 1216, loss = 2.85071891\n",
      "Iteration 1217, loss = 2.85007706\n",
      "Iteration 1218, loss = 2.85069831\n",
      "Iteration 1219, loss = 2.84985969\n",
      "Iteration 1220, loss = 2.84950907\n",
      "Iteration 1221, loss = 2.84946810\n",
      "Iteration 1222, loss = 2.84995832\n",
      "Iteration 1223, loss = 2.84896430\n",
      "Iteration 1224, loss = 2.84855853\n",
      "Iteration 1225, loss = 2.84787299\n",
      "Iteration 1226, loss = 2.84814487\n",
      "Iteration 1227, loss = 2.84790156\n",
      "Iteration 1228, loss = 2.84758425\n",
      "Iteration 1229, loss = 2.84721498\n",
      "Iteration 1230, loss = 2.84703954\n",
      "Iteration 1231, loss = 2.84673545\n",
      "Iteration 1232, loss = 2.84695270\n",
      "Iteration 1233, loss = 2.84654903\n",
      "Iteration 1234, loss = 2.84625522\n",
      "Iteration 1235, loss = 2.84616401\n",
      "Iteration 1236, loss = 2.84571796\n",
      "Iteration 1237, loss = 2.84563314\n",
      "Iteration 1238, loss = 2.84562115\n",
      "Iteration 1239, loss = 2.84542703\n",
      "Iteration 1240, loss = 2.84542367\n",
      "Iteration 1241, loss = 2.84480660\n",
      "Iteration 1242, loss = 2.84490305\n",
      "Iteration 1243, loss = 2.84483929\n",
      "Iteration 1244, loss = 2.84487258\n",
      "Iteration 1245, loss = 2.84414345\n",
      "Iteration 1246, loss = 2.84402860\n",
      "Iteration 1247, loss = 2.84427204\n",
      "Iteration 1248, loss = 2.84418429\n",
      "Iteration 1249, loss = 2.84349971\n",
      "Iteration 1250, loss = 2.84301279\n",
      "Iteration 1251, loss = 2.84318908\n",
      "Iteration 1252, loss = 2.84296320\n",
      "Iteration 1253, loss = 2.84283384\n",
      "Iteration 1254, loss = 2.84283532\n",
      "Iteration 1255, loss = 2.84251660\n",
      "Iteration 1256, loss = 2.84230236\n",
      "Iteration 1257, loss = 2.84230445\n",
      "Iteration 1258, loss = 2.84203957\n",
      "Iteration 1259, loss = 2.84157854\n",
      "Iteration 1260, loss = 2.84146406\n",
      "Iteration 1261, loss = 2.84102705\n",
      "Iteration 1262, loss = 2.84121001\n",
      "Iteration 1263, loss = 2.84129861\n",
      "Iteration 1264, loss = 2.84103494\n",
      "Iteration 1265, loss = 2.84090603\n",
      "Iteration 1266, loss = 2.84050883\n",
      "Iteration 1267, loss = 2.84012148\n",
      "Iteration 1268, loss = 2.83989856\n",
      "Iteration 1269, loss = 2.84002708\n",
      "Iteration 1270, loss = 2.83935418\n",
      "Iteration 1271, loss = 2.83926118\n",
      "Iteration 1272, loss = 2.83936766\n",
      "Iteration 1273, loss = 2.83936757\n",
      "Iteration 1274, loss = 2.83873966\n",
      "Iteration 1275, loss = 2.83803436\n",
      "Iteration 1276, loss = 2.83761523\n",
      "Iteration 1277, loss = 2.83886151\n",
      "Iteration 1278, loss = 2.83761128\n",
      "Iteration 1279, loss = 2.83713259\n",
      "Iteration 1280, loss = 2.83752409\n",
      "Iteration 1281, loss = 2.83734781\n",
      "Iteration 1282, loss = 2.83704368\n",
      "Iteration 1283, loss = 2.83691011\n",
      "Iteration 1284, loss = 2.83683436\n",
      "Iteration 1285, loss = 2.83758677\n",
      "Iteration 1286, loss = 2.83739737\n",
      "Iteration 1287, loss = 2.83625114\n",
      "Iteration 1288, loss = 2.83551120\n",
      "Iteration 1289, loss = 2.83572267\n",
      "Iteration 1290, loss = 2.83573291\n",
      "Iteration 1291, loss = 2.83505300\n",
      "Iteration 1292, loss = 2.83537793\n",
      "Iteration 1293, loss = 2.83555719\n",
      "Iteration 1294, loss = 2.83465535\n",
      "Iteration 1295, loss = 2.83625645\n",
      "Iteration 1296, loss = 2.83605272\n",
      "Iteration 1297, loss = 2.83556712\n",
      "Iteration 1298, loss = 2.83444075\n",
      "Iteration 1299, loss = 2.83394072\n",
      "Iteration 1300, loss = 2.83413442\n",
      "Iteration 1301, loss = 2.83374374\n",
      "Iteration 1302, loss = 2.83343928\n",
      "Iteration 1303, loss = 2.83399780\n",
      "Iteration 1304, loss = 2.83316508\n",
      "Iteration 1305, loss = 2.83201756\n",
      "Iteration 1306, loss = 2.83280916\n",
      "Iteration 1307, loss = 2.83255193\n",
      "Iteration 1308, loss = 2.83208327\n",
      "Iteration 1309, loss = 2.83173866\n",
      "Iteration 1310, loss = 2.83199394\n",
      "Iteration 1311, loss = 2.83172427\n",
      "Iteration 1312, loss = 2.83122224\n",
      "Iteration 1313, loss = 2.83108388\n",
      "Iteration 1314, loss = 2.83107792\n",
      "Iteration 1315, loss = 2.83081880\n",
      "Iteration 1316, loss = 2.83039451\n",
      "Iteration 1317, loss = 2.83088571\n",
      "Iteration 1318, loss = 2.83051945\n",
      "Iteration 1319, loss = 2.83039828\n",
      "Iteration 1320, loss = 2.83008653\n",
      "Iteration 1321, loss = 2.82946064\n",
      "Iteration 1322, loss = 2.82894776\n",
      "Iteration 1323, loss = 2.82919353\n",
      "Iteration 1324, loss = 2.82874698\n",
      "Iteration 1325, loss = 2.82939911\n",
      "Iteration 1326, loss = 2.82945874\n",
      "Iteration 1327, loss = 2.82863164\n",
      "Iteration 1328, loss = 2.82819658\n",
      "Iteration 1329, loss = 2.82830908\n",
      "Iteration 1330, loss = 2.82802694\n",
      "Iteration 1331, loss = 2.82800140\n",
      "Iteration 1332, loss = 2.82739801\n",
      "Iteration 1333, loss = 2.82783184\n",
      "Iteration 1334, loss = 2.82761959\n",
      "Iteration 1335, loss = 2.82690244\n",
      "Iteration 1336, loss = 2.82657890\n",
      "Iteration 1337, loss = 2.82651856\n",
      "Iteration 1338, loss = 2.82619904\n",
      "Iteration 1339, loss = 2.82577644\n",
      "Iteration 1340, loss = 2.82569992\n",
      "Iteration 1341, loss = 2.82562246\n",
      "Iteration 1342, loss = 2.82535779\n",
      "Iteration 1343, loss = 2.82518933\n",
      "Iteration 1344, loss = 2.82564261\n",
      "Iteration 1345, loss = 2.82599134\n",
      "Iteration 1346, loss = 2.82499443\n",
      "Iteration 1347, loss = 2.82515830\n",
      "Iteration 1348, loss = 2.82487073\n",
      "Iteration 1349, loss = 2.82467400\n",
      "Iteration 1350, loss = 2.82533324\n",
      "Iteration 1351, loss = 2.82456242\n",
      "Iteration 1352, loss = 2.82399219\n",
      "Iteration 1353, loss = 2.82360623\n",
      "Iteration 1354, loss = 2.82360733\n",
      "Iteration 1355, loss = 2.82347765\n",
      "Iteration 1356, loss = 2.82350670\n",
      "Iteration 1357, loss = 2.82377776\n",
      "Iteration 1358, loss = 2.82323681\n",
      "Iteration 1359, loss = 2.82294269\n",
      "Iteration 1360, loss = 2.82233465\n",
      "Iteration 1361, loss = 2.82230613\n",
      "Iteration 1362, loss = 2.82272773\n",
      "Iteration 1363, loss = 2.82211089\n",
      "Iteration 1364, loss = 2.82228938\n",
      "Iteration 1365, loss = 2.82185995\n",
      "Iteration 1366, loss = 2.82173638\n",
      "Iteration 1367, loss = 2.82171441\n",
      "Iteration 1368, loss = 2.82161777\n",
      "Iteration 1369, loss = 2.82115962\n",
      "Iteration 1370, loss = 2.82093358\n",
      "Iteration 1371, loss = 2.82060250\n",
      "Iteration 1372, loss = 2.82057248\n",
      "Iteration 1373, loss = 2.82034139\n",
      "Iteration 1374, loss = 2.82030216\n",
      "Iteration 1375, loss = 2.82013075\n",
      "Iteration 1376, loss = 2.82031805\n",
      "Iteration 1377, loss = 2.82081689\n",
      "Iteration 1378, loss = 2.82016849\n",
      "Iteration 1379, loss = 2.81936402\n",
      "Iteration 1380, loss = 2.81912161\n",
      "Iteration 1381, loss = 2.81954782\n",
      "Iteration 1382, loss = 2.81861062\n",
      "Iteration 1383, loss = 2.81824173\n",
      "Iteration 1384, loss = 2.81827749\n",
      "Iteration 1385, loss = 2.81826120\n",
      "Iteration 1386, loss = 2.81786139\n",
      "Iteration 1387, loss = 2.81744094\n",
      "Iteration 1388, loss = 2.81825711\n",
      "Iteration 1389, loss = 2.81804949\n",
      "Iteration 1390, loss = 2.81749891\n",
      "Iteration 1391, loss = 2.81703913\n",
      "Iteration 1392, loss = 2.81673799\n",
      "Iteration 1393, loss = 2.81679099\n",
      "Iteration 1394, loss = 2.81695323\n",
      "Iteration 1395, loss = 2.81649364\n",
      "Iteration 1396, loss = 2.81584203\n",
      "Iteration 1397, loss = 2.81601976\n",
      "Iteration 1398, loss = 2.81629396\n",
      "Iteration 1399, loss = 2.81584972\n",
      "Iteration 1400, loss = 2.81500609\n",
      "Iteration 1401, loss = 2.81502006\n",
      "Iteration 1402, loss = 2.81487581\n",
      "Iteration 1403, loss = 2.81557630\n",
      "Iteration 1404, loss = 2.81531025\n",
      "Iteration 1405, loss = 2.81552403\n",
      "Iteration 1406, loss = 2.81506410\n",
      "Iteration 1407, loss = 2.81450136\n",
      "Iteration 1408, loss = 2.81352870\n",
      "Iteration 1409, loss = 2.81399173\n",
      "Iteration 1410, loss = 2.81368420\n",
      "Iteration 1411, loss = 2.81317973\n",
      "Iteration 1412, loss = 2.81337688\n",
      "Iteration 1413, loss = 2.81301606\n",
      "Iteration 1414, loss = 2.81307632\n",
      "Iteration 1415, loss = 2.81282913\n",
      "Iteration 1416, loss = 2.81298436\n",
      "Iteration 1417, loss = 2.81236287\n",
      "Iteration 1418, loss = 2.81239877\n",
      "Iteration 1419, loss = 2.81235223\n",
      "Iteration 1420, loss = 2.81156322\n",
      "Iteration 1421, loss = 2.81150266\n",
      "Iteration 1422, loss = 2.81124163\n",
      "Iteration 1423, loss = 2.81160047\n",
      "Iteration 1424, loss = 2.81112826\n",
      "Iteration 1425, loss = 2.81151712\n",
      "Iteration 1426, loss = 2.81115232\n",
      "Iteration 1427, loss = 2.81065046\n",
      "Iteration 1428, loss = 2.81036440\n",
      "Iteration 1429, loss = 2.81058570\n",
      "Iteration 1430, loss = 2.81030338\n",
      "Iteration 1431, loss = 2.81042496\n",
      "Iteration 1432, loss = 2.81019700\n",
      "Iteration 1433, loss = 2.80971484\n",
      "Iteration 1434, loss = 2.80967663\n",
      "Iteration 1435, loss = 2.80976216\n",
      "Iteration 1436, loss = 2.80948017\n",
      "Iteration 1437, loss = 2.80910713\n",
      "Iteration 1438, loss = 2.80859676\n",
      "Iteration 1439, loss = 2.80848036\n",
      "Iteration 1440, loss = 2.80850319\n",
      "Iteration 1441, loss = 2.80815913\n",
      "Iteration 1442, loss = 2.80793059\n",
      "Iteration 1443, loss = 2.80843772\n",
      "Iteration 1444, loss = 2.80805790\n",
      "Iteration 1445, loss = 2.80776775\n",
      "Iteration 1446, loss = 2.80783702\n",
      "Iteration 1447, loss = 2.80799627\n",
      "Iteration 1448, loss = 2.80824007\n",
      "Iteration 1449, loss = 2.80903084\n",
      "Iteration 1450, loss = 2.80787364\n",
      "Iteration 1451, loss = 2.80649467\n",
      "Iteration 1452, loss = 2.80672899\n",
      "Iteration 1453, loss = 2.80673144\n",
      "Iteration 1454, loss = 2.80634145\n",
      "Iteration 1455, loss = 2.80605719\n",
      "Iteration 1456, loss = 2.80587608\n",
      "Iteration 1457, loss = 2.80591057\n",
      "Iteration 1458, loss = 2.80533482\n",
      "Iteration 1459, loss = 2.80495305\n",
      "Iteration 1460, loss = 2.80508411\n",
      "Iteration 1461, loss = 2.80516827\n",
      "Iteration 1462, loss = 2.80538561\n",
      "Iteration 1463, loss = 2.80544732\n",
      "Iteration 1464, loss = 2.80520692\n",
      "Iteration 1465, loss = 2.80429053\n",
      "Iteration 1466, loss = 2.80386652\n",
      "Iteration 1467, loss = 2.80370143\n",
      "Iteration 1468, loss = 2.80421608\n",
      "Iteration 1469, loss = 2.80437752\n",
      "Iteration 1470, loss = 2.80402778\n",
      "Iteration 1471, loss = 2.80409789\n",
      "Iteration 1472, loss = 2.80337523\n",
      "Iteration 1473, loss = 2.80323063\n",
      "Iteration 1474, loss = 2.80278726\n",
      "Iteration 1475, loss = 2.80270777\n",
      "Iteration 1476, loss = 2.80457709\n",
      "Iteration 1477, loss = 2.80446272\n",
      "Iteration 1478, loss = 2.80243944\n",
      "Iteration 1479, loss = 2.80236107\n",
      "Iteration 1480, loss = 2.80248878\n",
      "Iteration 1481, loss = 2.80274106\n",
      "Iteration 1482, loss = 2.80193813\n",
      "Iteration 1483, loss = 2.80216897\n",
      "Iteration 1484, loss = 2.80166029\n",
      "Iteration 1485, loss = 2.80131494\n",
      "Iteration 1486, loss = 2.80174166\n",
      "Iteration 1487, loss = 2.80126784\n",
      "Iteration 1488, loss = 2.80069509\n",
      "Iteration 1489, loss = 2.80117922\n",
      "Iteration 1490, loss = 2.80095788\n",
      "Iteration 1491, loss = 2.80037686\n",
      "Iteration 1492, loss = 2.80019196\n",
      "Iteration 1493, loss = 2.80025474\n",
      "Iteration 1494, loss = 2.79941402\n",
      "Iteration 1495, loss = 2.79972426\n",
      "Iteration 1496, loss = 2.79989188\n",
      "Iteration 1497, loss = 2.79951445\n",
      "Iteration 1498, loss = 2.79908525\n",
      "Iteration 1499, loss = 2.79889442\n",
      "Iteration 1500, loss = 2.79907984\n",
      "Iteration 1501, loss = 2.79884130\n",
      "Iteration 1502, loss = 2.79848852\n",
      "Iteration 1503, loss = 2.80010306\n",
      "Iteration 1504, loss = 2.79887416\n",
      "Iteration 1505, loss = 2.79738368\n",
      "Iteration 1506, loss = 2.79792931\n",
      "Iteration 1507, loss = 2.79769367\n",
      "Iteration 1508, loss = 2.79806644\n",
      "Iteration 1509, loss = 2.79756304\n",
      "Iteration 1510, loss = 2.79732833\n",
      "Iteration 1511, loss = 2.79665744\n",
      "Iteration 1512, loss = 2.79681407\n",
      "Iteration 1513, loss = 2.79693836\n",
      "Iteration 1514, loss = 2.79673786\n",
      "Iteration 1515, loss = 2.79757238\n",
      "Iteration 1516, loss = 2.79727272\n",
      "Iteration 1517, loss = 2.79657410\n",
      "Iteration 1518, loss = 2.79552352\n",
      "Iteration 1519, loss = 2.79620600\n",
      "Iteration 1520, loss = 2.79583926\n",
      "Iteration 1521, loss = 2.79565043\n",
      "Iteration 1522, loss = 2.79585446\n",
      "Iteration 1523, loss = 2.79554733\n",
      "Iteration 1524, loss = 2.79536205\n",
      "Iteration 1525, loss = 2.79502509\n",
      "Iteration 1526, loss = 2.79462198\n",
      "Iteration 1527, loss = 2.79453813\n",
      "Iteration 1528, loss = 2.79423307\n",
      "Iteration 1529, loss = 2.79452471\n",
      "Iteration 1530, loss = 2.79419316\n",
      "Iteration 1531, loss = 2.79398272\n",
      "Iteration 1532, loss = 2.79344381\n",
      "Iteration 1533, loss = 2.79455174\n",
      "Iteration 1534, loss = 2.79363858\n",
      "Iteration 1535, loss = 2.79326326\n",
      "Iteration 1536, loss = 2.79308757\n",
      "Iteration 1537, loss = 2.79282413\n",
      "Iteration 1538, loss = 2.79257414\n",
      "Iteration 1539, loss = 2.79275285\n",
      "Iteration 1540, loss = 2.79248869\n",
      "Iteration 1541, loss = 2.79241849\n",
      "Iteration 1542, loss = 2.79181544\n",
      "Iteration 1543, loss = 2.79188127\n",
      "Iteration 1544, loss = 2.79145445\n",
      "Iteration 1545, loss = 2.79169373\n",
      "Iteration 1546, loss = 2.79148452\n",
      "Iteration 1547, loss = 2.79146786\n",
      "Iteration 1548, loss = 2.79093736\n",
      "Iteration 1549, loss = 2.79083912\n",
      "Iteration 1550, loss = 2.79067477\n",
      "Iteration 1551, loss = 2.79015193\n",
      "Iteration 1552, loss = 2.78989792\n",
      "Iteration 1553, loss = 2.79019592\n",
      "Iteration 1554, loss = 2.79012494\n",
      "Iteration 1555, loss = 2.78985816\n",
      "Iteration 1556, loss = 2.79003392\n",
      "Iteration 1557, loss = 2.78963836\n",
      "Iteration 1558, loss = 2.78879201\n",
      "Iteration 1559, loss = 2.78922080\n",
      "Iteration 1560, loss = 2.79029998\n",
      "Iteration 1561, loss = 2.78977714\n",
      "Iteration 1562, loss = 2.78922421\n",
      "Iteration 1563, loss = 2.78849225\n",
      "Iteration 1564, loss = 2.78850524\n",
      "Iteration 1565, loss = 2.78882826\n",
      "Iteration 1566, loss = 2.78842954\n",
      "Iteration 1567, loss = 2.78817669\n",
      "Iteration 1568, loss = 2.78799138\n",
      "Iteration 1569, loss = 2.78800601\n",
      "Iteration 1570, loss = 2.78759249\n",
      "Iteration 1571, loss = 2.78755497\n",
      "Iteration 1572, loss = 2.78812576\n",
      "Iteration 1573, loss = 2.78739856\n",
      "Iteration 1574, loss = 2.78687118\n",
      "Iteration 1575, loss = 2.78775495\n",
      "Iteration 1576, loss = 2.78758188\n",
      "Iteration 1577, loss = 2.78786334\n",
      "Iteration 1578, loss = 2.78713016\n",
      "Iteration 1579, loss = 2.78723165\n",
      "Iteration 1580, loss = 2.78669210\n",
      "Iteration 1581, loss = 2.78565222\n",
      "Iteration 1582, loss = 2.78632979\n",
      "Iteration 1583, loss = 2.78617725\n",
      "Iteration 1584, loss = 2.78567730\n",
      "Iteration 1585, loss = 2.78599579\n",
      "Iteration 1586, loss = 2.78567244\n",
      "Iteration 1587, loss = 2.78536677\n",
      "Iteration 1588, loss = 2.78488157\n",
      "Iteration 1589, loss = 2.78499800\n",
      "Iteration 1590, loss = 2.78478211\n",
      "Iteration 1591, loss = 2.78469123\n",
      "Iteration 1592, loss = 2.78501279\n",
      "Iteration 1593, loss = 2.78467912\n",
      "Iteration 1594, loss = 2.78428459\n",
      "Iteration 1595, loss = 2.78422452\n",
      "Iteration 1596, loss = 2.78378053\n",
      "Iteration 1597, loss = 2.78386514\n",
      "Iteration 1598, loss = 2.78322829\n",
      "Iteration 1599, loss = 2.78354590\n",
      "Iteration 1600, loss = 2.78385975\n",
      "Iteration 1601, loss = 2.78414456\n",
      "Iteration 1602, loss = 2.78339870\n",
      "Iteration 1603, loss = 2.78259605\n",
      "Iteration 1604, loss = 2.78243802\n",
      "Iteration 1605, loss = 2.78309851\n",
      "Iteration 1606, loss = 2.78347592\n",
      "Iteration 1607, loss = 2.78267373\n",
      "Iteration 1608, loss = 2.78224222\n",
      "Iteration 1609, loss = 2.78290053\n",
      "Iteration 1610, loss = 2.78256119\n",
      "Iteration 1611, loss = 2.78155204\n",
      "Iteration 1612, loss = 2.78251210\n",
      "Iteration 1613, loss = 2.78246428\n",
      "Iteration 1614, loss = 2.78171633\n",
      "Iteration 1615, loss = 2.78133079\n",
      "Iteration 1616, loss = 2.78097227\n",
      "Iteration 1617, loss = 2.78082102\n",
      "Iteration 1618, loss = 2.78061704\n",
      "Iteration 1619, loss = 2.78067747\n",
      "Iteration 1620, loss = 2.78175103\n",
      "Iteration 1621, loss = 2.78208758\n",
      "Iteration 1622, loss = 2.78097304\n",
      "Iteration 1623, loss = 2.78029957\n",
      "Iteration 1624, loss = 2.77999101\n",
      "Iteration 1625, loss = 2.77954960\n",
      "Iteration 1626, loss = 2.77942760\n",
      "Iteration 1627, loss = 2.77931951\n",
      "Iteration 1628, loss = 2.77947364\n",
      "Iteration 1629, loss = 2.77872959\n",
      "Iteration 1630, loss = 2.77856043\n",
      "Iteration 1631, loss = 2.77849143\n",
      "Iteration 1632, loss = 2.77872503\n",
      "Iteration 1633, loss = 2.77808518\n",
      "Iteration 1634, loss = 2.77851556\n",
      "Iteration 1635, loss = 2.77811226\n",
      "Iteration 1636, loss = 2.77837377\n",
      "Iteration 1637, loss = 2.77775538\n",
      "Iteration 1638, loss = 2.77774020\n",
      "Iteration 1639, loss = 2.77753785\n",
      "Iteration 1640, loss = 2.77819676\n",
      "Iteration 1641, loss = 2.77711305\n",
      "Iteration 1642, loss = 2.77664441\n",
      "Iteration 1643, loss = 2.77693086\n",
      "Iteration 1644, loss = 2.77678526\n",
      "Iteration 1645, loss = 2.77638427\n",
      "Iteration 1646, loss = 2.77606982\n",
      "Iteration 1647, loss = 2.77591969\n",
      "Iteration 1648, loss = 2.77630707\n",
      "Iteration 1649, loss = 2.77589777\n",
      "Iteration 1650, loss = 2.77606094\n",
      "Iteration 1651, loss = 2.77526211\n",
      "Iteration 1652, loss = 2.77519761\n",
      "Iteration 1653, loss = 2.77506316\n",
      "Iteration 1654, loss = 2.77524844\n",
      "Iteration 1655, loss = 2.77498299\n",
      "Iteration 1656, loss = 2.77487020\n",
      "Iteration 1657, loss = 2.77468688\n",
      "Iteration 1658, loss = 2.77475003\n",
      "Iteration 1659, loss = 2.77475193\n",
      "Iteration 1660, loss = 2.77523578\n",
      "Iteration 1661, loss = 2.77511839\n",
      "Iteration 1662, loss = 2.77466591\n",
      "Iteration 1663, loss = 2.77376756\n",
      "Iteration 1664, loss = 2.77397668\n",
      "Iteration 1665, loss = 2.77337596\n",
      "Iteration 1666, loss = 2.77314193\n",
      "Iteration 1667, loss = 2.77313066\n",
      "Iteration 1668, loss = 2.77326139\n",
      "Iteration 1669, loss = 2.77303012\n",
      "Iteration 1670, loss = 2.77313508\n",
      "Iteration 1671, loss = 2.77258264\n",
      "Iteration 1672, loss = 2.77218892\n",
      "Iteration 1673, loss = 2.77273737\n",
      "Iteration 1674, loss = 2.77222143\n",
      "Iteration 1675, loss = 2.77215583\n",
      "Iteration 1676, loss = 2.77192564\n",
      "Iteration 1677, loss = 2.77225820\n",
      "Iteration 1678, loss = 2.77242337\n",
      "Iteration 1679, loss = 2.77156031\n",
      "Iteration 1680, loss = 2.77156845\n",
      "Iteration 1681, loss = 2.77132915\n",
      "Iteration 1682, loss = 2.77142927\n",
      "Iteration 1683, loss = 2.77179236\n",
      "Iteration 1684, loss = 2.77126120\n",
      "Iteration 1685, loss = 2.77108301\n",
      "Iteration 1686, loss = 2.77072472\n",
      "Iteration 1687, loss = 2.77239035\n",
      "Iteration 1688, loss = 2.77226780\n",
      "Iteration 1689, loss = 2.77078536\n",
      "Iteration 1690, loss = 2.77010116\n",
      "Iteration 1691, loss = 2.76988924\n",
      "Iteration 1692, loss = 2.77088917\n",
      "Iteration 1693, loss = 2.77092039\n",
      "Iteration 1694, loss = 2.77003679\n",
      "Iteration 1695, loss = 2.77005940\n",
      "Iteration 1696, loss = 2.77028933\n",
      "Iteration 1697, loss = 2.77000777\n",
      "Iteration 1698, loss = 2.77014229\n",
      "Iteration 1699, loss = 2.77023427\n",
      "Iteration 1700, loss = 2.76951976\n",
      "Iteration 1701, loss = 2.76882475\n",
      "Iteration 1702, loss = 2.76936137\n",
      "Iteration 1703, loss = 2.76893869\n",
      "Iteration 1704, loss = 2.76892420\n",
      "Iteration 1705, loss = 2.76818001\n",
      "Iteration 1706, loss = 2.76751395\n",
      "Iteration 1707, loss = 2.76807929\n",
      "Iteration 1708, loss = 2.76854123\n",
      "Iteration 1709, loss = 2.76893991\n",
      "Iteration 1710, loss = 2.76828204\n",
      "Iteration 1711, loss = 2.76807108\n",
      "Iteration 1712, loss = 2.76737185\n",
      "Iteration 1713, loss = 2.76722317\n",
      "Iteration 1714, loss = 2.76707260\n",
      "Iteration 1715, loss = 2.76688872\n",
      "Iteration 1716, loss = 2.76706805\n",
      "Iteration 1717, loss = 2.76669229\n",
      "Iteration 1718, loss = 2.76686096\n",
      "Iteration 1719, loss = 2.76653628\n",
      "Iteration 1720, loss = 2.76685970\n",
      "Iteration 1721, loss = 2.76616155\n",
      "Iteration 1722, loss = 2.76599268\n",
      "Iteration 1723, loss = 2.76589006\n",
      "Iteration 1724, loss = 2.76596951\n",
      "Iteration 1725, loss = 2.76614418\n",
      "Iteration 1726, loss = 2.76569299\n",
      "Iteration 1727, loss = 2.76527922\n",
      "Iteration 1728, loss = 2.76498713\n",
      "Iteration 1729, loss = 2.76551790\n",
      "Iteration 1730, loss = 2.76490691\n",
      "Iteration 1731, loss = 2.76493018\n",
      "Iteration 1732, loss = 2.76514428\n",
      "Iteration 1733, loss = 2.76455733\n",
      "Iteration 1734, loss = 2.76519007\n",
      "Iteration 1735, loss = 2.76525445\n",
      "Iteration 1736, loss = 2.76508766\n",
      "Iteration 1737, loss = 2.76445903\n",
      "Iteration 1738, loss = 2.76422982\n",
      "Iteration 1739, loss = 2.76366890\n",
      "Iteration 1740, loss = 2.76368289\n",
      "Iteration 1741, loss = 2.76347310\n",
      "Iteration 1742, loss = 2.76294926\n",
      "Iteration 1743, loss = 2.76317663\n",
      "Iteration 1744, loss = 2.76464718\n",
      "Iteration 1745, loss = 2.76343133\n",
      "Iteration 1746, loss = 2.76283485\n",
      "Iteration 1747, loss = 2.76354713\n",
      "Iteration 1748, loss = 2.76337626\n",
      "Iteration 1749, loss = 2.76315815\n",
      "Iteration 1750, loss = 2.76241364\n",
      "Iteration 1751, loss = 2.76181868\n",
      "Iteration 1752, loss = 2.76267950\n",
      "Iteration 1753, loss = 2.76272661\n",
      "Iteration 1754, loss = 2.76213030\n",
      "Iteration 1755, loss = 2.76188853\n",
      "Iteration 1756, loss = 2.76149446\n",
      "Iteration 1757, loss = 2.76184378\n",
      "Iteration 1758, loss = 2.76178367\n",
      "Iteration 1759, loss = 2.76067042\n",
      "Iteration 1760, loss = 2.76142689\n",
      "Iteration 1761, loss = 2.76092703\n",
      "Iteration 1762, loss = 2.76082535\n",
      "Iteration 1763, loss = 2.76167867\n",
      "Iteration 1764, loss = 2.76132607\n",
      "Iteration 1765, loss = 2.75991300\n",
      "Iteration 1766, loss = 2.76004788\n",
      "Iteration 1767, loss = 2.75993401\n",
      "Iteration 1768, loss = 2.75974378\n",
      "Iteration 1769, loss = 2.75969413\n",
      "Iteration 1770, loss = 2.75939493\n",
      "Iteration 1771, loss = 2.76016476\n",
      "Iteration 1772, loss = 2.75937756\n",
      "Iteration 1773, loss = 2.75871217\n",
      "Iteration 1774, loss = 2.75860255\n",
      "Iteration 1775, loss = 2.75881650\n",
      "Iteration 1776, loss = 2.75844159\n",
      "Iteration 1777, loss = 2.75863283\n",
      "Iteration 1778, loss = 2.75963778\n",
      "Iteration 1779, loss = 2.75906908\n",
      "Iteration 1780, loss = 2.75864529\n",
      "Iteration 1781, loss = 2.75806421\n",
      "Iteration 1782, loss = 2.75814602\n",
      "Iteration 1783, loss = 2.75790293\n",
      "Iteration 1784, loss = 2.75835765\n",
      "Iteration 1785, loss = 2.75769302\n",
      "Iteration 1786, loss = 2.75818444\n",
      "Iteration 1787, loss = 2.75772657\n",
      "Iteration 1788, loss = 2.75738751\n",
      "Iteration 1789, loss = 2.75760756\n",
      "Iteration 1790, loss = 2.75677456\n",
      "Iteration 1791, loss = 2.75687175\n",
      "Iteration 1792, loss = 2.75687939\n",
      "Iteration 1793, loss = 2.75677564\n",
      "Iteration 1794, loss = 2.75610922\n",
      "Iteration 1795, loss = 2.75646056\n",
      "Iteration 1796, loss = 2.75578562\n",
      "Iteration 1797, loss = 2.75560110\n",
      "Iteration 1798, loss = 2.75613370\n",
      "Iteration 1799, loss = 2.75561099\n",
      "Iteration 1800, loss = 2.75560486\n",
      "Iteration 1801, loss = 2.75534557\n",
      "Iteration 1802, loss = 2.75531874\n",
      "Iteration 1803, loss = 2.75517287\n",
      "Iteration 1804, loss = 2.75513336\n",
      "Iteration 1805, loss = 2.75535985\n",
      "Iteration 1806, loss = 2.75442864\n",
      "Iteration 1807, loss = 2.75485662\n",
      "Iteration 1808, loss = 2.75478787\n",
      "Iteration 1809, loss = 2.75449015\n",
      "Iteration 1810, loss = 2.75393337\n",
      "Iteration 1811, loss = 2.75396764\n",
      "Iteration 1812, loss = 2.75403221\n",
      "Iteration 1813, loss = 2.75373377\n",
      "Iteration 1814, loss = 2.75365142\n",
      "Iteration 1815, loss = 2.75344028\n",
      "Iteration 1816, loss = 2.75327699\n",
      "Iteration 1817, loss = 2.75351648\n",
      "Iteration 1818, loss = 2.75357931\n",
      "Iteration 1819, loss = 2.75370858\n",
      "Iteration 1820, loss = 2.75365495\n",
      "Iteration 1821, loss = 2.75334688\n",
      "Iteration 1822, loss = 2.75263907\n",
      "Iteration 1823, loss = 2.75282103\n",
      "Iteration 1824, loss = 2.75315298\n",
      "Iteration 1825, loss = 2.75314542\n",
      "Iteration 1826, loss = 2.75240999\n",
      "Iteration 1827, loss = 2.75194526\n",
      "Iteration 1828, loss = 2.75277514\n",
      "Iteration 1829, loss = 2.75241755\n",
      "Iteration 1830, loss = 2.75155904\n",
      "Iteration 1831, loss = 2.75162444\n",
      "Iteration 1832, loss = 2.75311667\n",
      "Iteration 1833, loss = 2.75197839\n",
      "Iteration 1834, loss = 2.75178551\n",
      "Iteration 1835, loss = 2.75149675\n",
      "Iteration 1836, loss = 2.75183951\n",
      "Iteration 1837, loss = 2.75199677\n",
      "Iteration 1838, loss = 2.75196867\n",
      "Iteration 1839, loss = 2.75036519\n",
      "Iteration 1840, loss = 2.75101236\n",
      "Iteration 1841, loss = 2.75170516\n",
      "Iteration 1842, loss = 2.75105475\n",
      "Iteration 1843, loss = 2.75064775\n",
      "Iteration 1844, loss = 2.75010757\n",
      "Iteration 1845, loss = 2.74973361\n",
      "Iteration 1846, loss = 2.74991275\n",
      "Iteration 1847, loss = 2.74988096\n",
      "Iteration 1848, loss = 2.75002021\n",
      "Iteration 1849, loss = 2.74969737\n",
      "Iteration 1850, loss = 2.74936806\n",
      "Iteration 1851, loss = 2.74907825\n",
      "Iteration 1852, loss = 2.74880377\n",
      "Iteration 1853, loss = 2.74896140\n",
      "Iteration 1854, loss = 2.74903903\n",
      "Iteration 1855, loss = 2.74866835\n",
      "Iteration 1856, loss = 2.74859194\n",
      "Iteration 1857, loss = 2.74856189\n",
      "Iteration 1858, loss = 2.74842572\n",
      "Iteration 1859, loss = 2.74788117\n",
      "Iteration 1860, loss = 2.74853069\n",
      "Iteration 1861, loss = 2.74839626\n",
      "Iteration 1862, loss = 2.74791053\n",
      "Iteration 1863, loss = 2.74806321\n",
      "Iteration 1864, loss = 2.74826977\n",
      "Iteration 1865, loss = 2.74777996\n",
      "Iteration 1866, loss = 2.74767152\n",
      "Iteration 1867, loss = 2.74767513\n",
      "Iteration 1868, loss = 2.74748048\n",
      "Iteration 1869, loss = 2.74753623\n",
      "Iteration 1870, loss = 2.74794695\n",
      "Iteration 1871, loss = 2.74667694\n",
      "Iteration 1872, loss = 2.74763353\n",
      "Iteration 1873, loss = 2.74736628\n",
      "Iteration 1874, loss = 2.74607228\n",
      "Iteration 1875, loss = 2.74575967\n",
      "Iteration 1876, loss = 2.74539731\n",
      "Iteration 1877, loss = 2.74556073\n",
      "Iteration 1878, loss = 2.74533320\n",
      "Iteration 1879, loss = 2.74589057\n",
      "Iteration 1880, loss = 2.74667490\n",
      "Iteration 1881, loss = 2.74539124\n",
      "Iteration 1882, loss = 2.74591163\n",
      "Iteration 1883, loss = 2.74571545\n",
      "Iteration 1884, loss = 2.74525789\n",
      "Iteration 1885, loss = 2.74487749\n",
      "Iteration 1886, loss = 2.74524348\n",
      "Iteration 1887, loss = 2.74539184\n",
      "Iteration 1888, loss = 2.74484518\n",
      "Iteration 1889, loss = 2.74489184\n",
      "Iteration 1890, loss = 2.74438373\n",
      "Iteration 1891, loss = 2.74423981\n",
      "Iteration 1892, loss = 2.74455827\n",
      "Iteration 1893, loss = 2.74398216\n",
      "Iteration 1894, loss = 2.74330040\n",
      "Iteration 1895, loss = 2.74351608\n",
      "Iteration 1896, loss = 2.74441073\n",
      "Iteration 1897, loss = 2.74347690\n",
      "Iteration 1898, loss = 2.74347042\n",
      "Iteration 1899, loss = 2.74335398\n",
      "Iteration 1900, loss = 2.74345791\n",
      "Iteration 1901, loss = 2.74325636\n",
      "Iteration 1902, loss = 2.74291934\n",
      "Iteration 1903, loss = 2.74481199\n",
      "Iteration 1904, loss = 2.74339222\n",
      "Iteration 1905, loss = 2.74333363\n",
      "Iteration 1906, loss = 2.74431281\n",
      "Iteration 1907, loss = 2.74232268\n",
      "Iteration 1908, loss = 2.74240209\n",
      "Iteration 1909, loss = 2.74229845\n",
      "Iteration 1910, loss = 2.74177054\n",
      "Iteration 1911, loss = 2.74201317\n",
      "Iteration 1912, loss = 2.74206140\n",
      "Iteration 1913, loss = 2.74164944\n",
      "Iteration 1914, loss = 2.74149812\n",
      "Iteration 1915, loss = 2.74142058\n",
      "Iteration 1916, loss = 2.74152464\n",
      "Iteration 1917, loss = 2.74109276\n",
      "Iteration 1918, loss = 2.74099763\n",
      "Iteration 1919, loss = 2.74141889\n",
      "Iteration 1920, loss = 2.74151850\n",
      "Iteration 1921, loss = 2.74129065\n",
      "Iteration 1922, loss = 2.74102317\n",
      "Iteration 1923, loss = 2.74115919\n",
      "Iteration 1924, loss = 2.74020336\n",
      "Iteration 1925, loss = 2.74005943\n",
      "Iteration 1926, loss = 2.74000114\n",
      "Iteration 1927, loss = 2.73957511\n",
      "Iteration 1928, loss = 2.73963502\n",
      "Iteration 1929, loss = 2.73991813\n",
      "Iteration 1930, loss = 2.73993281\n",
      "Iteration 1931, loss = 2.73945280\n",
      "Iteration 1932, loss = 2.74015010\n",
      "Iteration 1933, loss = 2.73969197\n",
      "Iteration 1934, loss = 2.73935404\n",
      "Iteration 1935, loss = 2.73972178\n",
      "Iteration 1936, loss = 2.73953901\n",
      "Iteration 1937, loss = 2.73930961\n",
      "Iteration 1938, loss = 2.73846610\n",
      "Iteration 1939, loss = 2.73851113\n",
      "Iteration 1940, loss = 2.73840765\n",
      "Iteration 1941, loss = 2.73800209\n",
      "Iteration 1942, loss = 2.73784222\n",
      "Iteration 1943, loss = 2.73839410\n",
      "Iteration 1944, loss = 2.73838434\n",
      "Iteration 1945, loss = 2.73791729\n",
      "Iteration 1946, loss = 2.73808537\n",
      "Iteration 1947, loss = 2.73826376\n",
      "Iteration 1948, loss = 2.73704736\n",
      "Iteration 1949, loss = 2.73703856\n",
      "Iteration 1950, loss = 2.73689563\n",
      "Iteration 1951, loss = 2.73725806\n",
      "Iteration 1952, loss = 2.73677230\n",
      "Iteration 1953, loss = 2.73651860\n",
      "Iteration 1954, loss = 2.73720321\n",
      "Iteration 1955, loss = 2.73748632\n",
      "Iteration 1956, loss = 2.73665602\n",
      "Iteration 1957, loss = 2.73746956\n",
      "Iteration 1958, loss = 2.73733274\n",
      "Iteration 1959, loss = 2.73637794\n",
      "Iteration 1960, loss = 2.73609468\n",
      "Iteration 1961, loss = 2.73556176\n",
      "Iteration 1962, loss = 2.73558550\n",
      "Iteration 1963, loss = 2.73561608\n",
      "Iteration 1964, loss = 2.73585129\n",
      "Iteration 1965, loss = 2.73546241\n",
      "Iteration 1966, loss = 2.73571813\n",
      "Iteration 1967, loss = 2.73529785\n",
      "Iteration 1968, loss = 2.73540051\n",
      "Iteration 1969, loss = 2.73539553\n",
      "Iteration 1970, loss = 2.73479688\n",
      "Iteration 1971, loss = 2.73449555\n",
      "Iteration 1972, loss = 2.73485218\n",
      "Iteration 1973, loss = 2.73473444\n",
      "Iteration 1974, loss = 2.73466882\n",
      "Iteration 1975, loss = 2.73446717\n",
      "Iteration 1976, loss = 2.73398577\n",
      "Iteration 1977, loss = 2.73386552\n",
      "Iteration 1978, loss = 2.73450415\n",
      "Iteration 1979, loss = 2.73363164\n",
      "Iteration 1980, loss = 2.73410810\n",
      "Iteration 1981, loss = 2.73401560\n",
      "Iteration 1982, loss = 2.73384428\n",
      "Iteration 1983, loss = 2.73347806\n",
      "Iteration 1984, loss = 2.73324320\n",
      "Iteration 1985, loss = 2.73288745\n",
      "Iteration 1986, loss = 2.73437056\n",
      "Iteration 1987, loss = 2.73447615\n",
      "Iteration 1988, loss = 2.73276403\n",
      "Iteration 1989, loss = 2.73234678\n",
      "Iteration 1990, loss = 2.73280313\n",
      "Iteration 1991, loss = 2.73272303\n",
      "Iteration 1992, loss = 2.73293516\n",
      "Iteration 1993, loss = 2.73357261\n",
      "Iteration 1994, loss = 2.73283468\n",
      "Iteration 1995, loss = 2.73277415\n",
      "Iteration 1996, loss = 2.73283479\n",
      "Iteration 1997, loss = 2.73223013\n",
      "Iteration 1998, loss = 2.73226924\n",
      "Iteration 1999, loss = 2.73184207\n",
      "Iteration 2000, loss = 2.73144546\n",
      "Iteration 2001, loss = 2.73142317\n",
      "Iteration 2002, loss = 2.73099758\n",
      "Iteration 2003, loss = 2.73148769\n",
      "Iteration 2004, loss = 2.73096708\n",
      "Iteration 2005, loss = 2.73106600\n",
      "Iteration 2006, loss = 2.73087712\n",
      "Iteration 2007, loss = 2.73095480\n",
      "Iteration 2008, loss = 2.73054801\n",
      "Iteration 2009, loss = 2.73035356\n",
      "Iteration 2010, loss = 2.73056094\n",
      "Iteration 2011, loss = 2.73126101\n",
      "Iteration 2012, loss = 2.73086185\n",
      "Iteration 2013, loss = 2.72998209\n",
      "Iteration 2014, loss = 2.72965716\n",
      "Iteration 2015, loss = 2.72956397\n",
      "Iteration 2016, loss = 2.72961681\n",
      "Iteration 2017, loss = 2.72946744\n",
      "Iteration 2018, loss = 2.72950615\n",
      "Iteration 2019, loss = 2.72949140\n",
      "Iteration 2020, loss = 2.72919131\n",
      "Iteration 2021, loss = 2.72915571\n",
      "Iteration 2022, loss = 2.72904082\n",
      "Iteration 2023, loss = 2.72858195\n",
      "Iteration 2024, loss = 2.72883038\n",
      "Iteration 2025, loss = 2.72853129\n",
      "Iteration 2026, loss = 2.72850900\n",
      "Iteration 2027, loss = 2.72839197\n",
      "Iteration 2028, loss = 2.72813032\n",
      "Iteration 2029, loss = 2.72834670\n",
      "Iteration 2030, loss = 2.72867223\n",
      "Iteration 2031, loss = 2.72809861\n",
      "Iteration 2032, loss = 2.72814380\n",
      "Iteration 2033, loss = 2.72937023\n",
      "Iteration 2034, loss = 2.72845204\n",
      "Iteration 2035, loss = 2.72783003\n",
      "Iteration 2036, loss = 2.72814922\n",
      "Iteration 2037, loss = 2.72755395\n",
      "Iteration 2038, loss = 2.72727595\n",
      "Iteration 2039, loss = 2.72667005\n",
      "Iteration 2040, loss = 2.72679908\n",
      "Iteration 2041, loss = 2.72669803\n",
      "Iteration 2042, loss = 2.72685662\n",
      "Iteration 2043, loss = 2.72790445\n",
      "Iteration 2044, loss = 2.72667898\n",
      "Iteration 2045, loss = 2.72669522\n",
      "Iteration 2046, loss = 2.72576680\n",
      "Iteration 2047, loss = 2.72643516\n",
      "Iteration 2048, loss = 2.72675640\n",
      "Iteration 2049, loss = 2.72626193\n",
      "Iteration 2050, loss = 2.72617777\n",
      "Iteration 2051, loss = 2.72581937\n",
      "Iteration 2052, loss = 2.72629153\n",
      "Iteration 2053, loss = 2.72579450\n",
      "Iteration 2054, loss = 2.72590056\n",
      "Iteration 2055, loss = 2.72555969\n",
      "Iteration 2056, loss = 2.72590056\n",
      "Iteration 2057, loss = 2.72603543\n",
      "Iteration 2058, loss = 2.72574185\n",
      "Iteration 2059, loss = 2.72476449\n",
      "Iteration 2060, loss = 2.72533163\n",
      "Iteration 2061, loss = 2.72480501\n",
      "Iteration 2062, loss = 2.72508780\n",
      "Iteration 2063, loss = 2.72519590\n",
      "Iteration 2064, loss = 2.72474996\n",
      "Iteration 2065, loss = 2.72462714\n",
      "Iteration 2066, loss = 2.72425802\n",
      "Iteration 2067, loss = 2.72522675\n",
      "Iteration 2068, loss = 2.72478006\n",
      "Iteration 2069, loss = 2.72405738\n",
      "Iteration 2070, loss = 2.72406421\n",
      "Iteration 2071, loss = 2.72364901\n",
      "Iteration 2072, loss = 2.72388433\n",
      "Iteration 2073, loss = 2.72356811\n",
      "Iteration 2074, loss = 2.72358970\n",
      "Iteration 2075, loss = 2.72384410\n",
      "Iteration 2076, loss = 2.72359236\n",
      "Iteration 2077, loss = 2.72371037\n",
      "Iteration 2078, loss = 2.72300341\n",
      "Iteration 2079, loss = 2.72362570\n",
      "Iteration 2080, loss = 2.72385364\n",
      "Iteration 2081, loss = 2.72271494\n",
      "Iteration 2082, loss = 2.72259019\n",
      "Iteration 2083, loss = 2.72291441\n",
      "Iteration 2084, loss = 2.72258515\n",
      "Iteration 2085, loss = 2.72243482\n",
      "Iteration 2086, loss = 2.72268969\n",
      "Iteration 2087, loss = 2.72360257\n",
      "Iteration 2088, loss = 2.72297514\n",
      "Iteration 2089, loss = 2.72187814\n",
      "Iteration 2090, loss = 2.72232163\n",
      "Iteration 2091, loss = 2.72163682\n",
      "Iteration 2092, loss = 2.72152746\n",
      "Iteration 2093, loss = 2.72105758\n",
      "Iteration 2094, loss = 2.72144434\n",
      "Iteration 2095, loss = 2.72198944\n",
      "Iteration 2096, loss = 2.72111396\n",
      "Iteration 2097, loss = 2.72061008\n",
      "Iteration 2098, loss = 2.72103146\n",
      "Iteration 2099, loss = 2.72130208\n",
      "Iteration 2100, loss = 2.72280369\n",
      "Iteration 2101, loss = 2.72140954\n",
      "Iteration 2102, loss = 2.72057106\n",
      "Iteration 2103, loss = 2.72240529\n",
      "Iteration 2104, loss = 2.72021050\n",
      "Iteration 2105, loss = 2.72002318\n",
      "Iteration 2106, loss = 2.72057169\n",
      "Iteration 2107, loss = 2.72054783\n",
      "Iteration 2108, loss = 2.72014216\n",
      "Iteration 2109, loss = 2.72000316\n",
      "Iteration 2110, loss = 2.71967600\n",
      "Iteration 2111, loss = 2.71945360\n",
      "Iteration 2112, loss = 2.71930472\n",
      "Iteration 2113, loss = 2.71921192\n",
      "Iteration 2114, loss = 2.71928724\n",
      "Iteration 2115, loss = 2.71987125\n",
      "Iteration 2116, loss = 2.72019682\n",
      "Iteration 2117, loss = 2.71868090\n",
      "Iteration 2118, loss = 2.71921712\n",
      "Iteration 2119, loss = 2.71876456\n",
      "Iteration 2120, loss = 2.71846516\n",
      "Iteration 2121, loss = 2.71829152\n",
      "Iteration 2122, loss = 2.71824647\n",
      "Iteration 2123, loss = 2.71802738\n",
      "Iteration 2124, loss = 2.71806548\n",
      "Iteration 2125, loss = 2.71814419\n",
      "Iteration 2126, loss = 2.71859505\n",
      "Iteration 2127, loss = 2.71883805\n",
      "Iteration 2128, loss = 2.71795528\n",
      "Iteration 2129, loss = 2.71809388\n",
      "Iteration 2130, loss = 2.71745998\n",
      "Iteration 2131, loss = 2.71820431\n",
      "Iteration 2132, loss = 2.71798216\n",
      "Iteration 2133, loss = 2.71767155\n",
      "Iteration 2134, loss = 2.71714481\n",
      "Iteration 2135, loss = 2.71747269\n",
      "Iteration 2136, loss = 2.71719525\n",
      "Iteration 2137, loss = 2.71696831\n",
      "Iteration 2138, loss = 2.71612772\n",
      "Iteration 2139, loss = 2.71739623\n",
      "Iteration 2140, loss = 2.71621981\n",
      "Iteration 2141, loss = 2.71588742\n",
      "Iteration 2142, loss = 2.71578523\n",
      "Iteration 2143, loss = 2.71571646\n",
      "Iteration 2144, loss = 2.71578221\n",
      "Iteration 2145, loss = 2.71538948\n",
      "Iteration 2146, loss = 2.71564144\n",
      "Iteration 2147, loss = 2.71561030\n",
      "Iteration 2148, loss = 2.71554322\n",
      "Iteration 2149, loss = 2.71537802\n",
      "Iteration 2150, loss = 2.71569643\n",
      "Iteration 2151, loss = 2.71532783\n",
      "Iteration 2152, loss = 2.71546232\n",
      "Iteration 2153, loss = 2.71548900\n",
      "Iteration 2154, loss = 2.71508558\n",
      "Iteration 2155, loss = 2.71477783\n",
      "Iteration 2156, loss = 2.71559439\n",
      "Iteration 2157, loss = 2.71568798\n",
      "Iteration 2158, loss = 2.71525628\n",
      "Iteration 2159, loss = 2.71477523\n",
      "Iteration 2160, loss = 2.71424080\n",
      "Iteration 2161, loss = 2.71411942\n",
      "Iteration 2162, loss = 2.71426412\n",
      "Iteration 2163, loss = 2.71393706\n",
      "Iteration 2164, loss = 2.71400623\n",
      "Iteration 2165, loss = 2.71371210\n",
      "Iteration 2166, loss = 2.71350635\n",
      "Iteration 2167, loss = 2.71386289\n",
      "Iteration 2168, loss = 2.71413354\n",
      "Iteration 2169, loss = 2.71329675\n",
      "Iteration 2170, loss = 2.71308879\n",
      "Iteration 2171, loss = 2.71344409\n",
      "Iteration 2172, loss = 2.71401565\n",
      "Iteration 2173, loss = 2.71323980\n",
      "Iteration 2174, loss = 2.71299377\n",
      "Iteration 2175, loss = 2.71280700\n",
      "Iteration 2176, loss = 2.71249454\n",
      "Iteration 2177, loss = 2.71521994\n",
      "Iteration 2178, loss = 2.71472448\n",
      "Iteration 2179, loss = 2.71327062\n",
      "Iteration 2180, loss = 2.71283559\n",
      "Iteration 2181, loss = 2.71277065\n",
      "Iteration 2182, loss = 2.71195925\n",
      "Iteration 2183, loss = 2.71261886\n",
      "Iteration 2184, loss = 2.71291717\n",
      "Iteration 2185, loss = 2.71262465\n",
      "Iteration 2186, loss = 2.71194822\n",
      "Iteration 2187, loss = 2.71267556\n",
      "Iteration 2188, loss = 2.71342119\n",
      "Iteration 2189, loss = 2.71245949\n",
      "Iteration 2190, loss = 2.71229744\n",
      "Iteration 2191, loss = 2.71129280\n",
      "Iteration 2192, loss = 2.71083049\n",
      "Iteration 2193, loss = 2.71082676\n",
      "Iteration 2194, loss = 2.71079020\n",
      "Iteration 2195, loss = 2.71070929\n",
      "Iteration 2196, loss = 2.71061675\n",
      "Iteration 2197, loss = 2.71123338\n",
      "Iteration 2198, loss = 2.71097492\n",
      "Iteration 2199, loss = 2.71028646\n",
      "Iteration 2200, loss = 2.70965202\n",
      "Iteration 2201, loss = 2.70939516\n",
      "Iteration 2202, loss = 2.70979155\n",
      "Iteration 2203, loss = 2.70958861\n",
      "Iteration 2204, loss = 2.70933546\n",
      "Iteration 2205, loss = 2.70960133\n",
      "Iteration 2206, loss = 2.70972158\n",
      "Iteration 2207, loss = 2.71034121\n",
      "Iteration 2208, loss = 2.70940079\n",
      "Iteration 2209, loss = 2.70912401\n",
      "Iteration 2210, loss = 2.70937367\n",
      "Iteration 2211, loss = 2.70919769\n",
      "Iteration 2212, loss = 2.70891857\n",
      "Iteration 2213, loss = 2.70841540\n",
      "Iteration 2214, loss = 2.70881584\n",
      "Iteration 2215, loss = 2.70940590\n",
      "Iteration 2216, loss = 2.70903837\n",
      "Iteration 2217, loss = 2.70866790\n",
      "Iteration 2218, loss = 2.70887414\n",
      "Iteration 2219, loss = 2.70855622\n",
      "Iteration 2220, loss = 2.70900297\n",
      "Iteration 2221, loss = 2.70879372\n",
      "Iteration 2222, loss = 2.70829990\n",
      "Iteration 2223, loss = 2.70803462\n",
      "Iteration 2224, loss = 2.70807487\n",
      "Iteration 2225, loss = 2.70815343\n",
      "Iteration 2226, loss = 2.70776900\n",
      "Iteration 2227, loss = 2.70766136\n",
      "Iteration 2228, loss = 2.70771435\n",
      "Iteration 2229, loss = 2.70809861\n",
      "Iteration 2230, loss = 2.70760233\n",
      "Iteration 2231, loss = 2.70714811\n",
      "Iteration 2232, loss = 2.70695390\n",
      "Iteration 2233, loss = 2.70627842\n",
      "Iteration 2234, loss = 2.70615595\n",
      "Iteration 2235, loss = 2.70658481\n",
      "Iteration 2236, loss = 2.70621884\n",
      "Iteration 2237, loss = 2.70657135\n",
      "Iteration 2238, loss = 2.70683149\n",
      "Iteration 2239, loss = 2.70735449\n",
      "Iteration 2240, loss = 2.70681714\n",
      "Iteration 2241, loss = 2.70599755\n",
      "Iteration 2242, loss = 2.70582103\n",
      "Iteration 2243, loss = 2.70587526\n",
      "Iteration 2244, loss = 2.70540944\n",
      "Iteration 2245, loss = 2.70581579\n",
      "Iteration 2246, loss = 2.70523455\n",
      "Iteration 2247, loss = 2.70530079\n",
      "Iteration 2248, loss = 2.70586887\n",
      "Iteration 2249, loss = 2.70500563\n",
      "Iteration 2250, loss = 2.70499766\n",
      "Iteration 2251, loss = 2.70478672\n",
      "Iteration 2252, loss = 2.70489339\n",
      "Iteration 2253, loss = 2.70453181\n",
      "Iteration 2254, loss = 2.70432960\n",
      "Iteration 2255, loss = 2.70436122\n",
      "Iteration 2256, loss = 2.70499614\n",
      "Iteration 2257, loss = 2.70458451\n",
      "Iteration 2258, loss = 2.70375088\n",
      "Iteration 2259, loss = 2.70373598\n",
      "Iteration 2260, loss = 2.70539946\n",
      "Iteration 2261, loss = 2.70417691\n",
      "Iteration 2262, loss = 2.70359960\n",
      "Iteration 2263, loss = 2.70381249\n",
      "Iteration 2264, loss = 2.70379186\n",
      "Iteration 2265, loss = 2.70347559\n",
      "Iteration 2266, loss = 2.70401129\n",
      "Iteration 2267, loss = 2.70526671\n",
      "Iteration 2268, loss = 2.70419625\n",
      "Iteration 2269, loss = 2.70381961\n",
      "Iteration 2270, loss = 2.70321822\n",
      "Iteration 2271, loss = 2.70304859\n",
      "Iteration 2272, loss = 2.70351372\n",
      "Iteration 2273, loss = 2.70260555\n",
      "Iteration 2274, loss = 2.70331999\n",
      "Iteration 2275, loss = 2.70361325\n",
      "Iteration 2276, loss = 2.70297312\n",
      "Iteration 2277, loss = 2.70276783\n",
      "Iteration 2278, loss = 2.70296593\n",
      "Iteration 2279, loss = 2.70217462\n",
      "Iteration 2280, loss = 2.70248841\n",
      "Iteration 2281, loss = 2.70190209\n",
      "Iteration 2282, loss = 2.70187900\n",
      "Iteration 2283, loss = 2.70203806\n",
      "Iteration 2284, loss = 2.70183104\n",
      "Iteration 2285, loss = 2.70197733\n",
      "Iteration 2286, loss = 2.70222715\n",
      "Iteration 2287, loss = 2.70146377\n",
      "Iteration 2288, loss = 2.70129720\n",
      "Iteration 2289, loss = 2.70101281\n",
      "Iteration 2290, loss = 2.70132604\n",
      "Iteration 2291, loss = 2.70109472\n",
      "Iteration 2292, loss = 2.70094645\n",
      "Iteration 2293, loss = 2.70087371\n",
      "Iteration 2294, loss = 2.70049079\n",
      "Iteration 2295, loss = 2.70086290\n",
      "Iteration 2296, loss = 2.70034870\n",
      "Iteration 2297, loss = 2.70069701\n",
      "Iteration 2298, loss = 2.70063930\n",
      "Iteration 2299, loss = 2.70048938\n",
      "Iteration 2300, loss = 2.70004934\n",
      "Iteration 2301, loss = 2.69998309\n",
      "Iteration 2302, loss = 2.70025874\n",
      "Iteration 2303, loss = 2.70005004\n",
      "Iteration 2304, loss = 2.69967936\n",
      "Iteration 2305, loss = 2.69966740\n",
      "Iteration 2306, loss = 2.70004001\n",
      "Iteration 2307, loss = 2.69942363\n",
      "Iteration 2308, loss = 2.69977340\n",
      "Iteration 2309, loss = 2.69969920\n",
      "Iteration 2310, loss = 2.69985837\n",
      "Iteration 2311, loss = 2.69956333\n",
      "Iteration 2312, loss = 2.69961712\n",
      "Iteration 2313, loss = 2.69930822\n",
      "Iteration 2314, loss = 2.69869743\n",
      "Iteration 2315, loss = 2.69867280\n",
      "Iteration 2316, loss = 2.69955493\n",
      "Iteration 2317, loss = 2.69900934\n",
      "Iteration 2318, loss = 2.69847184\n",
      "Iteration 2319, loss = 2.69839079\n",
      "Iteration 2320, loss = 2.69812631\n",
      "Iteration 2321, loss = 2.69800989\n",
      "Iteration 2322, loss = 2.69796214\n",
      "Iteration 2323, loss = 2.69790415\n",
      "Iteration 2324, loss = 2.69762188\n",
      "Iteration 2325, loss = 2.69761089\n",
      "Iteration 2326, loss = 2.69750791\n",
      "Iteration 2327, loss = 2.69728775\n",
      "Iteration 2328, loss = 2.69796942\n",
      "Iteration 2329, loss = 2.69759048\n",
      "Iteration 2330, loss = 2.69727316\n",
      "Iteration 2331, loss = 2.69742465\n",
      "Iteration 2332, loss = 2.69679104\n",
      "Iteration 2333, loss = 2.69807275\n",
      "Iteration 2334, loss = 2.69896849\n",
      "Iteration 2335, loss = 2.69837662\n",
      "Iteration 2336, loss = 2.69708280\n",
      "Iteration 2337, loss = 2.69701518\n",
      "Iteration 2338, loss = 2.69618533\n",
      "Iteration 2339, loss = 2.69707865\n",
      "Iteration 2340, loss = 2.69663387\n",
      "Iteration 2341, loss = 2.69648681\n",
      "Iteration 2342, loss = 2.69672127\n",
      "Iteration 2343, loss = 2.69663167\n",
      "Iteration 2344, loss = 2.69661325\n",
      "Iteration 2345, loss = 2.69600695\n",
      "Iteration 2346, loss = 2.69569801\n",
      "Iteration 2347, loss = 2.69561913\n",
      "Iteration 2348, loss = 2.69686913\n",
      "Iteration 2349, loss = 2.69573674\n",
      "Iteration 2350, loss = 2.69557016\n",
      "Iteration 2351, loss = 2.69621862\n",
      "Iteration 2352, loss = 2.69623229\n",
      "Iteration 2353, loss = 2.69509888\n",
      "Iteration 2354, loss = 2.69737751\n",
      "Iteration 2355, loss = 2.69639124\n",
      "Iteration 2356, loss = 2.69535512\n",
      "Iteration 2357, loss = 2.69504648\n",
      "Iteration 2358, loss = 2.69454177\n",
      "Iteration 2359, loss = 2.69476633\n",
      "Iteration 2360, loss = 2.69442586\n",
      "Iteration 2361, loss = 2.69413315\n",
      "Iteration 2362, loss = 2.69442182\n",
      "Iteration 2363, loss = 2.69476482\n",
      "Iteration 2364, loss = 2.69456987\n",
      "Iteration 2365, loss = 2.69469114\n",
      "Iteration 2366, loss = 2.69355738\n",
      "Iteration 2367, loss = 2.69332233\n",
      "Iteration 2368, loss = 2.69355675\n",
      "Iteration 2369, loss = 2.69329798\n",
      "Iteration 2370, loss = 2.69366987\n",
      "Iteration 2371, loss = 2.69423309\n",
      "Iteration 2372, loss = 2.69360966\n",
      "Iteration 2373, loss = 2.69366156\n",
      "Iteration 2374, loss = 2.69573340\n",
      "Iteration 2375, loss = 2.69546390\n",
      "Iteration 2376, loss = 2.69325519\n",
      "Iteration 2377, loss = 2.69422640\n",
      "Iteration 2378, loss = 2.69391775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13248507\n",
      "Iteration 2, loss = 4.05284203\n",
      "Iteration 3, loss = 3.97749126\n",
      "Iteration 4, loss = 3.90433379\n",
      "Iteration 5, loss = 3.83052564\n",
      "Iteration 6, loss = 3.75768368\n",
      "Iteration 7, loss = 3.68480112\n",
      "Iteration 8, loss = 3.61365010\n",
      "Iteration 9, loss = 3.54333702\n",
      "Iteration 10, loss = 3.47787776\n",
      "Iteration 11, loss = 3.41723533\n",
      "Iteration 12, loss = 3.36447035\n",
      "Iteration 13, loss = 3.32001589\n",
      "Iteration 14, loss = 3.28661195\n",
      "Iteration 15, loss = 3.26271003\n",
      "Iteration 16, loss = 3.24696079\n",
      "Iteration 17, loss = 3.23773347\n",
      "Iteration 18, loss = 3.23335241\n",
      "Iteration 19, loss = 3.23079460\n",
      "Iteration 20, loss = 3.22983558\n",
      "Iteration 21, loss = 3.22889778\n",
      "Iteration 22, loss = 3.22799216\n",
      "Iteration 23, loss = 3.22707217\n",
      "Iteration 24, loss = 3.22610662\n",
      "Iteration 25, loss = 3.22566564\n",
      "Iteration 26, loss = 3.22517243\n",
      "Iteration 27, loss = 3.22475076\n",
      "Iteration 28, loss = 3.22465700\n",
      "Iteration 29, loss = 3.22397849\n",
      "Iteration 30, loss = 3.22394197\n",
      "Iteration 31, loss = 3.22318185\n",
      "Iteration 32, loss = 3.22263151\n",
      "Iteration 33, loss = 3.22247090\n",
      "Iteration 34, loss = 3.22193691\n",
      "Iteration 35, loss = 3.22135935\n",
      "Iteration 36, loss = 3.22083783\n",
      "Iteration 37, loss = 3.22088441\n",
      "Iteration 38, loss = 3.22061298\n",
      "Iteration 39, loss = 3.21984530\n",
      "Iteration 40, loss = 3.21940345\n",
      "Iteration 41, loss = 3.21966474\n",
      "Iteration 42, loss = 3.21949263\n",
      "Iteration 43, loss = 3.21932415\n",
      "Iteration 44, loss = 3.21895155\n",
      "Iteration 45, loss = 3.21851459\n",
      "Iteration 46, loss = 3.21781989\n",
      "Iteration 47, loss = 3.21812599\n",
      "Iteration 48, loss = 3.21716384\n",
      "Iteration 49, loss = 3.21718239\n",
      "Iteration 50, loss = 3.21672217\n",
      "Iteration 51, loss = 3.21645619\n",
      "Iteration 52, loss = 3.21603058\n",
      "Iteration 53, loss = 3.21544520\n",
      "Iteration 54, loss = 3.21535798\n",
      "Iteration 55, loss = 3.21526426\n",
      "Iteration 56, loss = 3.21472930\n",
      "Iteration 57, loss = 3.21469029\n",
      "Iteration 58, loss = 3.21441802\n",
      "Iteration 59, loss = 3.21412285\n",
      "Iteration 60, loss = 3.21411066\n",
      "Iteration 61, loss = 3.21431742\n",
      "Iteration 62, loss = 3.21374340\n",
      "Iteration 63, loss = 3.21289428\n",
      "Iteration 64, loss = 3.21253734\n",
      "Iteration 65, loss = 3.21239931\n",
      "Iteration 66, loss = 3.21230826\n",
      "Iteration 67, loss = 3.21181694\n",
      "Iteration 68, loss = 3.21136976\n",
      "Iteration 69, loss = 3.21117235\n",
      "Iteration 70, loss = 3.21116424\n",
      "Iteration 71, loss = 3.21046239\n",
      "Iteration 72, loss = 3.21048014\n",
      "Iteration 73, loss = 3.20986323\n",
      "Iteration 74, loss = 3.20998071\n",
      "Iteration 75, loss = 3.20940166\n",
      "Iteration 76, loss = 3.20913072\n",
      "Iteration 77, loss = 3.20879993\n",
      "Iteration 78, loss = 3.20836834\n",
      "Iteration 79, loss = 3.20788367\n",
      "Iteration 80, loss = 3.20795108\n",
      "Iteration 81, loss = 3.20790135\n",
      "Iteration 82, loss = 3.20783484\n",
      "Iteration 83, loss = 3.20692977\n",
      "Iteration 84, loss = 3.20672621\n",
      "Iteration 85, loss = 3.20627231\n",
      "Iteration 86, loss = 3.20635283\n",
      "Iteration 87, loss = 3.20603387\n",
      "Iteration 88, loss = 3.20538629\n",
      "Iteration 89, loss = 3.20547112\n",
      "Iteration 90, loss = 3.20534168\n",
      "Iteration 91, loss = 3.20480978\n",
      "Iteration 92, loss = 3.20458989\n",
      "Iteration 93, loss = 3.20428731\n",
      "Iteration 94, loss = 3.20411208\n",
      "Iteration 95, loss = 3.20341911\n",
      "Iteration 96, loss = 3.20317268\n",
      "Iteration 97, loss = 3.20299901\n",
      "Iteration 98, loss = 3.20293522\n",
      "Iteration 99, loss = 3.20231199\n",
      "Iteration 100, loss = 3.20238283\n",
      "Iteration 101, loss = 3.20217417\n",
      "Iteration 102, loss = 3.20190646\n",
      "Iteration 103, loss = 3.20155651\n",
      "Iteration 104, loss = 3.20137011\n",
      "Iteration 105, loss = 3.20110028\n",
      "Iteration 106, loss = 3.20075877\n",
      "Iteration 107, loss = 3.20053715\n",
      "Iteration 108, loss = 3.20022437\n",
      "Iteration 109, loss = 3.19991062\n",
      "Iteration 110, loss = 3.19957881\n",
      "Iteration 111, loss = 3.19927510\n",
      "Iteration 112, loss = 3.19886898\n",
      "Iteration 113, loss = 3.19842541\n",
      "Iteration 114, loss = 3.19815494\n",
      "Iteration 115, loss = 3.19797912\n",
      "Iteration 116, loss = 3.19797835\n",
      "Iteration 117, loss = 3.19771436\n",
      "Iteration 118, loss = 3.19727528\n",
      "Iteration 119, loss = 3.19686693\n",
      "Iteration 120, loss = 3.19653915\n",
      "Iteration 121, loss = 3.19626888\n",
      "Iteration 122, loss = 3.19654452\n",
      "Iteration 123, loss = 3.19593313\n",
      "Iteration 124, loss = 3.19519628\n",
      "Iteration 125, loss = 3.19482726\n",
      "Iteration 126, loss = 3.19554017\n",
      "Iteration 127, loss = 3.19572089\n",
      "Iteration 128, loss = 3.19498544\n",
      "Iteration 129, loss = 3.19457965\n",
      "Iteration 130, loss = 3.19407541\n",
      "Iteration 131, loss = 3.19364014\n",
      "Iteration 132, loss = 3.19312803\n",
      "Iteration 133, loss = 3.19255883\n",
      "Iteration 134, loss = 3.19323736\n",
      "Iteration 135, loss = 3.19306343\n",
      "Iteration 136, loss = 3.19249844\n",
      "Iteration 137, loss = 3.19174903\n",
      "Iteration 138, loss = 3.19108631\n",
      "Iteration 139, loss = 3.19125540\n",
      "Iteration 140, loss = 3.19061134\n",
      "Iteration 141, loss = 3.19015127\n",
      "Iteration 142, loss = 3.19027993\n",
      "Iteration 143, loss = 3.18979559\n",
      "Iteration 144, loss = 3.18946281\n",
      "Iteration 145, loss = 3.18905055\n",
      "Iteration 146, loss = 3.18871602\n",
      "Iteration 147, loss = 3.18840867\n",
      "Iteration 148, loss = 3.18829616\n",
      "Iteration 149, loss = 3.18780356\n",
      "Iteration 150, loss = 3.18778852\n",
      "Iteration 151, loss = 3.18730043\n",
      "Iteration 152, loss = 3.18694109\n",
      "Iteration 153, loss = 3.18693430\n",
      "Iteration 154, loss = 3.18708516\n",
      "Iteration 155, loss = 3.18688600\n",
      "Iteration 156, loss = 3.18666304\n",
      "Iteration 157, loss = 3.18571895\n",
      "Iteration 158, loss = 3.18544027\n",
      "Iteration 159, loss = 3.18477826\n",
      "Iteration 160, loss = 3.18445566\n",
      "Iteration 161, loss = 3.18393324\n",
      "Iteration 162, loss = 3.18362624\n",
      "Iteration 163, loss = 3.18366829\n",
      "Iteration 164, loss = 3.18319995\n",
      "Iteration 165, loss = 3.18324084\n",
      "Iteration 166, loss = 3.18268364\n",
      "Iteration 167, loss = 3.18233029\n",
      "Iteration 168, loss = 3.18203658\n",
      "Iteration 169, loss = 3.18221580\n",
      "Iteration 170, loss = 3.18152960\n",
      "Iteration 171, loss = 3.18119282\n",
      "Iteration 172, loss = 3.18089317\n",
      "Iteration 173, loss = 3.18062090\n",
      "Iteration 174, loss = 3.18037817\n",
      "Iteration 175, loss = 3.17978311\n",
      "Iteration 176, loss = 3.17930019\n",
      "Iteration 177, loss = 3.17877210\n",
      "Iteration 178, loss = 3.17876168\n",
      "Iteration 179, loss = 3.17894879\n",
      "Iteration 180, loss = 3.17833787\n",
      "Iteration 181, loss = 3.17797746\n",
      "Iteration 182, loss = 3.17745785\n",
      "Iteration 183, loss = 3.17721315\n",
      "Iteration 184, loss = 3.17722454\n",
      "Iteration 185, loss = 3.17632321\n",
      "Iteration 186, loss = 3.17650586\n",
      "Iteration 187, loss = 3.17667056\n",
      "Iteration 188, loss = 3.17614116\n",
      "Iteration 189, loss = 3.17527388\n",
      "Iteration 190, loss = 3.17483467\n",
      "Iteration 191, loss = 3.17498420\n",
      "Iteration 192, loss = 3.17468340\n",
      "Iteration 193, loss = 3.17438865\n",
      "Iteration 194, loss = 3.17434887\n",
      "Iteration 195, loss = 3.17368755\n",
      "Iteration 196, loss = 3.17335722\n",
      "Iteration 197, loss = 3.17253782\n",
      "Iteration 198, loss = 3.17298146\n",
      "Iteration 199, loss = 3.17271234\n",
      "Iteration 200, loss = 3.17220557\n",
      "Iteration 201, loss = 3.17172127\n",
      "Iteration 202, loss = 3.17128034\n",
      "Iteration 203, loss = 3.17094665\n",
      "Iteration 204, loss = 3.17032362\n",
      "Iteration 205, loss = 3.17058121\n",
      "Iteration 206, loss = 3.17074298\n",
      "Iteration 207, loss = 3.17010901\n",
      "Iteration 208, loss = 3.16937616\n",
      "Iteration 209, loss = 3.16904964\n",
      "Iteration 210, loss = 3.16879029\n",
      "Iteration 211, loss = 3.16842135\n",
      "Iteration 212, loss = 3.16858176\n",
      "Iteration 213, loss = 3.16786447\n",
      "Iteration 214, loss = 3.16763439\n",
      "Iteration 215, loss = 3.16737927\n",
      "Iteration 216, loss = 3.16695331\n",
      "Iteration 217, loss = 3.16656154\n",
      "Iteration 218, loss = 3.16650080\n",
      "Iteration 219, loss = 3.16582914\n",
      "Iteration 220, loss = 3.16534677\n",
      "Iteration 221, loss = 3.16503371\n",
      "Iteration 222, loss = 3.16480117\n",
      "Iteration 223, loss = 3.16438760\n",
      "Iteration 224, loss = 3.16408489\n",
      "Iteration 225, loss = 3.16349900\n",
      "Iteration 226, loss = 3.16314751\n",
      "Iteration 227, loss = 3.16246519\n",
      "Iteration 228, loss = 3.16213725\n",
      "Iteration 229, loss = 3.16236240\n",
      "Iteration 230, loss = 3.16148453\n",
      "Iteration 231, loss = 3.16145881\n",
      "Iteration 232, loss = 3.16120863\n",
      "Iteration 233, loss = 3.16086258\n",
      "Iteration 234, loss = 3.16021859\n",
      "Iteration 235, loss = 3.15982043\n",
      "Iteration 236, loss = 3.15918833\n",
      "Iteration 237, loss = 3.15892777\n",
      "Iteration 238, loss = 3.15876719\n",
      "Iteration 239, loss = 3.15812745\n",
      "Iteration 240, loss = 3.15789385\n",
      "Iteration 241, loss = 3.15696957\n",
      "Iteration 242, loss = 3.15713903\n",
      "Iteration 243, loss = 3.15663168\n",
      "Iteration 244, loss = 3.15650491\n",
      "Iteration 245, loss = 3.15631718\n",
      "Iteration 246, loss = 3.15596410\n",
      "Iteration 247, loss = 3.15537784\n",
      "Iteration 248, loss = 3.15480847\n",
      "Iteration 249, loss = 3.15425382\n",
      "Iteration 250, loss = 3.15424852\n",
      "Iteration 251, loss = 3.15348591\n",
      "Iteration 252, loss = 3.15339361\n",
      "Iteration 253, loss = 3.15283934\n",
      "Iteration 254, loss = 3.15269346\n",
      "Iteration 255, loss = 3.15264087\n",
      "Iteration 256, loss = 3.15269225\n",
      "Iteration 257, loss = 3.15163312\n",
      "Iteration 258, loss = 3.15139752\n",
      "Iteration 259, loss = 3.15074682\n",
      "Iteration 260, loss = 3.15069696\n",
      "Iteration 261, loss = 3.15051522\n",
      "Iteration 262, loss = 3.14966505\n",
      "Iteration 263, loss = 3.14919816\n",
      "Iteration 264, loss = 3.14893273\n",
      "Iteration 265, loss = 3.14845261\n",
      "Iteration 266, loss = 3.14828981\n",
      "Iteration 267, loss = 3.14754681\n",
      "Iteration 268, loss = 3.14706577\n",
      "Iteration 269, loss = 3.14659456\n",
      "Iteration 270, loss = 3.14624759\n",
      "Iteration 271, loss = 3.14672349\n",
      "Iteration 272, loss = 3.14620862\n",
      "Iteration 273, loss = 3.14537943\n",
      "Iteration 274, loss = 3.14536471\n",
      "Iteration 275, loss = 3.14512874\n",
      "Iteration 276, loss = 3.14471311\n",
      "Iteration 277, loss = 3.14477769\n",
      "Iteration 278, loss = 3.14437457\n",
      "Iteration 279, loss = 3.14356229\n",
      "Iteration 280, loss = 3.14291730\n",
      "Iteration 281, loss = 3.14248445\n",
      "Iteration 282, loss = 3.14206078\n",
      "Iteration 283, loss = 3.14235696\n",
      "Iteration 284, loss = 3.14163503\n",
      "Iteration 285, loss = 3.14109596\n",
      "Iteration 286, loss = 3.14074012\n",
      "Iteration 287, loss = 3.14013253\n",
      "Iteration 288, loss = 3.13952072\n",
      "Iteration 289, loss = 3.13942432\n",
      "Iteration 290, loss = 3.13930605\n",
      "Iteration 291, loss = 3.13880341\n",
      "Iteration 292, loss = 3.13851169\n",
      "Iteration 293, loss = 3.13782659\n",
      "Iteration 294, loss = 3.13762476\n",
      "Iteration 295, loss = 3.13716738\n",
      "Iteration 296, loss = 3.13667382\n",
      "Iteration 297, loss = 3.13637609\n",
      "Iteration 298, loss = 3.13741766\n",
      "Iteration 299, loss = 3.13718492\n",
      "Iteration 300, loss = 3.13558071\n",
      "Iteration 301, loss = 3.13461716\n",
      "Iteration 302, loss = 3.13429237\n",
      "Iteration 303, loss = 3.13403494\n",
      "Iteration 304, loss = 3.13385044\n",
      "Iteration 305, loss = 3.13295232\n",
      "Iteration 306, loss = 3.13271901\n",
      "Iteration 307, loss = 3.13248316\n",
      "Iteration 308, loss = 3.13210585\n",
      "Iteration 309, loss = 3.13171675\n",
      "Iteration 310, loss = 3.13093695\n",
      "Iteration 311, loss = 3.13072444\n",
      "Iteration 312, loss = 3.13039474\n",
      "Iteration 313, loss = 3.12965434\n",
      "Iteration 314, loss = 3.12933317\n",
      "Iteration 315, loss = 3.12885593\n",
      "Iteration 316, loss = 3.12859199\n",
      "Iteration 317, loss = 3.12835418\n",
      "Iteration 318, loss = 3.12817144\n",
      "Iteration 319, loss = 3.12778022\n",
      "Iteration 320, loss = 3.12711436\n",
      "Iteration 321, loss = 3.12663614\n",
      "Iteration 322, loss = 3.12638979\n",
      "Iteration 323, loss = 3.12620075\n",
      "Iteration 324, loss = 3.12578192\n",
      "Iteration 325, loss = 3.12537969\n",
      "Iteration 326, loss = 3.12487928\n",
      "Iteration 327, loss = 3.12449942\n",
      "Iteration 328, loss = 3.12467160\n",
      "Iteration 329, loss = 3.12410405\n",
      "Iteration 330, loss = 3.12367533\n",
      "Iteration 331, loss = 3.12367890\n",
      "Iteration 332, loss = 3.12253442\n",
      "Iteration 333, loss = 3.12191145\n",
      "Iteration 334, loss = 3.12181806\n",
      "Iteration 335, loss = 3.12087018\n",
      "Iteration 336, loss = 3.12100220\n",
      "Iteration 337, loss = 3.12061579\n",
      "Iteration 338, loss = 3.12015597\n",
      "Iteration 339, loss = 3.11939372\n",
      "Iteration 340, loss = 3.11917453\n",
      "Iteration 341, loss = 3.11927090\n",
      "Iteration 342, loss = 3.11883233\n",
      "Iteration 343, loss = 3.11860848\n",
      "Iteration 344, loss = 3.11813079\n",
      "Iteration 345, loss = 3.11766373\n",
      "Iteration 346, loss = 3.11699349\n",
      "Iteration 347, loss = 3.11653168\n",
      "Iteration 348, loss = 3.11643650\n",
      "Iteration 349, loss = 3.11566019\n",
      "Iteration 350, loss = 3.11614733\n",
      "Iteration 351, loss = 3.11542519\n",
      "Iteration 352, loss = 3.11464982\n",
      "Iteration 353, loss = 3.11435383\n",
      "Iteration 354, loss = 3.11372767\n",
      "Iteration 355, loss = 3.11272417\n",
      "Iteration 356, loss = 3.11247976\n",
      "Iteration 357, loss = 3.11205137\n",
      "Iteration 358, loss = 3.11202942\n",
      "Iteration 359, loss = 3.11123508\n",
      "Iteration 360, loss = 3.11095063\n",
      "Iteration 361, loss = 3.11067642\n",
      "Iteration 362, loss = 3.11085878\n",
      "Iteration 363, loss = 3.11077163\n",
      "Iteration 364, loss = 3.10971799\n",
      "Iteration 365, loss = 3.10925966\n",
      "Iteration 366, loss = 3.10857437\n",
      "Iteration 367, loss = 3.10920358\n",
      "Iteration 368, loss = 3.10839184\n",
      "Iteration 369, loss = 3.10750776\n",
      "Iteration 370, loss = 3.10712783\n",
      "Iteration 371, loss = 3.10667065\n",
      "Iteration 372, loss = 3.10629597\n",
      "Iteration 373, loss = 3.10571211\n",
      "Iteration 374, loss = 3.10543432\n",
      "Iteration 375, loss = 3.10545090\n",
      "Iteration 376, loss = 3.10474324\n",
      "Iteration 377, loss = 3.10507919\n",
      "Iteration 378, loss = 3.10415765\n",
      "Iteration 379, loss = 3.10362863\n",
      "Iteration 380, loss = 3.10329800\n",
      "Iteration 381, loss = 3.10223794\n",
      "Iteration 382, loss = 3.10218887\n",
      "Iteration 383, loss = 3.10175922\n",
      "Iteration 384, loss = 3.10152254\n",
      "Iteration 385, loss = 3.10103090\n",
      "Iteration 386, loss = 3.10030938\n",
      "Iteration 387, loss = 3.09997148\n",
      "Iteration 388, loss = 3.09970634\n",
      "Iteration 389, loss = 3.09894888\n",
      "Iteration 390, loss = 3.09917898\n",
      "Iteration 391, loss = 3.09895564\n",
      "Iteration 392, loss = 3.09819826\n",
      "Iteration 393, loss = 3.09784142\n",
      "Iteration 394, loss = 3.09782932\n",
      "Iteration 395, loss = 3.09730694\n",
      "Iteration 396, loss = 3.09662197\n",
      "Iteration 397, loss = 3.09627521\n",
      "Iteration 398, loss = 3.09597544\n",
      "Iteration 399, loss = 3.09577787\n",
      "Iteration 400, loss = 3.09523259\n",
      "Iteration 401, loss = 3.09447084\n",
      "Iteration 402, loss = 3.09386199\n",
      "Iteration 403, loss = 3.09345545\n",
      "Iteration 404, loss = 3.09339604\n",
      "Iteration 405, loss = 3.09304656\n",
      "Iteration 406, loss = 3.09253037\n",
      "Iteration 407, loss = 3.09220961\n",
      "Iteration 408, loss = 3.09189773\n",
      "Iteration 409, loss = 3.09132334\n",
      "Iteration 410, loss = 3.09089387\n",
      "Iteration 411, loss = 3.09050055\n",
      "Iteration 412, loss = 3.09030638\n",
      "Iteration 413, loss = 3.08981924\n",
      "Iteration 414, loss = 3.08901439\n",
      "Iteration 415, loss = 3.08893295\n",
      "Iteration 416, loss = 3.08840652\n",
      "Iteration 417, loss = 3.08750333\n",
      "Iteration 418, loss = 3.08722121\n",
      "Iteration 419, loss = 3.08686681\n",
      "Iteration 420, loss = 3.08744443\n",
      "Iteration 421, loss = 3.08725259\n",
      "Iteration 422, loss = 3.08592592\n",
      "Iteration 423, loss = 3.08523253\n",
      "Iteration 424, loss = 3.08492972\n",
      "Iteration 425, loss = 3.08438044\n",
      "Iteration 426, loss = 3.08402803\n",
      "Iteration 427, loss = 3.08378681\n",
      "Iteration 428, loss = 3.08367502\n",
      "Iteration 429, loss = 3.08373678\n",
      "Iteration 430, loss = 3.08258965\n",
      "Iteration 431, loss = 3.08210837\n",
      "Iteration 432, loss = 3.08137092\n",
      "Iteration 433, loss = 3.08097032\n",
      "Iteration 434, loss = 3.08039973\n",
      "Iteration 435, loss = 3.08067326\n",
      "Iteration 436, loss = 3.08001082\n",
      "Iteration 437, loss = 3.07961311\n",
      "Iteration 438, loss = 3.07930629\n",
      "Iteration 439, loss = 3.07975607\n",
      "Iteration 440, loss = 3.07919966\n",
      "Iteration 441, loss = 3.07822325\n",
      "Iteration 442, loss = 3.07752583\n",
      "Iteration 443, loss = 3.07724729\n",
      "Iteration 444, loss = 3.07679298\n",
      "Iteration 445, loss = 3.07612151\n",
      "Iteration 446, loss = 3.07642798\n",
      "Iteration 447, loss = 3.07652846\n",
      "Iteration 448, loss = 3.07493681\n",
      "Iteration 449, loss = 3.07597360\n",
      "Iteration 450, loss = 3.07567137\n",
      "Iteration 451, loss = 3.07394558\n",
      "Iteration 452, loss = 3.07291947\n",
      "Iteration 453, loss = 3.07322277\n",
      "Iteration 454, loss = 3.07334243\n",
      "Iteration 455, loss = 3.07274109\n",
      "Iteration 456, loss = 3.07234344\n",
      "Iteration 457, loss = 3.07184552\n",
      "Iteration 458, loss = 3.07284129\n",
      "Iteration 459, loss = 3.07105580\n",
      "Iteration 460, loss = 3.07046623\n",
      "Iteration 461, loss = 3.07042871\n",
      "Iteration 462, loss = 3.06963092\n",
      "Iteration 463, loss = 3.06931520\n",
      "Iteration 464, loss = 3.06949585\n",
      "Iteration 465, loss = 3.06916232\n",
      "Iteration 466, loss = 3.06804491\n",
      "Iteration 467, loss = 3.06721078\n",
      "Iteration 468, loss = 3.06686072\n",
      "Iteration 469, loss = 3.06651154\n",
      "Iteration 470, loss = 3.06616708\n",
      "Iteration 471, loss = 3.06569687\n",
      "Iteration 472, loss = 3.06523599\n",
      "Iteration 473, loss = 3.06522677\n",
      "Iteration 474, loss = 3.06457635\n",
      "Iteration 475, loss = 3.06425679\n",
      "Iteration 476, loss = 3.06391066\n",
      "Iteration 477, loss = 3.06350805\n",
      "Iteration 478, loss = 3.06347612\n",
      "Iteration 479, loss = 3.06310512\n",
      "Iteration 480, loss = 3.06257651\n",
      "Iteration 481, loss = 3.06261463\n",
      "Iteration 482, loss = 3.06228903\n",
      "Iteration 483, loss = 3.06177508\n",
      "Iteration 484, loss = 3.06042330\n",
      "Iteration 485, loss = 3.05999004\n",
      "Iteration 486, loss = 3.05975629\n",
      "Iteration 487, loss = 3.06009869\n",
      "Iteration 488, loss = 3.05984665\n",
      "Iteration 489, loss = 3.05883886\n",
      "Iteration 490, loss = 3.05808687\n",
      "Iteration 491, loss = 3.05806306\n",
      "Iteration 492, loss = 3.05729930\n",
      "Iteration 493, loss = 3.05689515\n",
      "Iteration 494, loss = 3.05623607\n",
      "Iteration 495, loss = 3.05641814\n",
      "Iteration 496, loss = 3.05580443\n",
      "Iteration 497, loss = 3.05545810\n",
      "Iteration 498, loss = 3.05502944\n",
      "Iteration 499, loss = 3.05485318\n",
      "Iteration 500, loss = 3.05438840\n",
      "Iteration 501, loss = 3.05376258\n",
      "Iteration 502, loss = 3.05350214\n",
      "Iteration 503, loss = 3.05342776\n",
      "Iteration 504, loss = 3.05272882\n",
      "Iteration 505, loss = 3.05246124\n",
      "Iteration 506, loss = 3.05237461\n",
      "Iteration 507, loss = 3.05200411\n",
      "Iteration 508, loss = 3.05158598\n",
      "Iteration 509, loss = 3.05127438\n",
      "Iteration 510, loss = 3.05113081\n",
      "Iteration 511, loss = 3.05017966\n",
      "Iteration 512, loss = 3.04990487\n",
      "Iteration 513, loss = 3.04976470\n",
      "Iteration 514, loss = 3.04939784\n",
      "Iteration 515, loss = 3.04888175\n",
      "Iteration 516, loss = 3.04930357\n",
      "Iteration 517, loss = 3.04862818\n",
      "Iteration 518, loss = 3.04813802\n",
      "Iteration 519, loss = 3.04768139\n",
      "Iteration 520, loss = 3.04780724\n",
      "Iteration 521, loss = 3.04678985\n",
      "Iteration 522, loss = 3.04588940\n",
      "Iteration 523, loss = 3.04558045\n",
      "Iteration 524, loss = 3.04539445\n",
      "Iteration 525, loss = 3.04471539\n",
      "Iteration 526, loss = 3.04506760\n",
      "Iteration 527, loss = 3.04514574\n",
      "Iteration 528, loss = 3.04414169\n",
      "Iteration 529, loss = 3.04309235\n",
      "Iteration 530, loss = 3.04328902\n",
      "Iteration 531, loss = 3.04297859\n",
      "Iteration 532, loss = 3.04237341\n",
      "Iteration 533, loss = 3.04215803\n",
      "Iteration 534, loss = 3.04184244\n",
      "Iteration 535, loss = 3.04120990\n",
      "Iteration 536, loss = 3.04091602\n",
      "Iteration 537, loss = 3.04049909\n",
      "Iteration 538, loss = 3.04094780\n",
      "Iteration 539, loss = 3.04015436\n",
      "Iteration 540, loss = 3.03924616\n",
      "Iteration 541, loss = 3.03891466\n",
      "Iteration 542, loss = 3.03853567\n",
      "Iteration 543, loss = 3.03798776\n",
      "Iteration 544, loss = 3.03828543\n",
      "Iteration 545, loss = 3.03793439\n",
      "Iteration 546, loss = 3.03716320\n",
      "Iteration 547, loss = 3.03701534\n",
      "Iteration 548, loss = 3.03641115\n",
      "Iteration 549, loss = 3.03601263\n",
      "Iteration 550, loss = 3.03612388\n",
      "Iteration 551, loss = 3.03554464\n",
      "Iteration 552, loss = 3.03480025\n",
      "Iteration 553, loss = 3.03440159\n",
      "Iteration 554, loss = 3.03400016\n",
      "Iteration 555, loss = 3.03412884\n",
      "Iteration 556, loss = 3.03359433\n",
      "Iteration 557, loss = 3.03382668\n",
      "Iteration 558, loss = 3.03271080\n",
      "Iteration 559, loss = 3.03290658\n",
      "Iteration 560, loss = 3.03217294\n",
      "Iteration 561, loss = 3.03177980\n",
      "Iteration 562, loss = 3.03166029\n",
      "Iteration 563, loss = 3.03218212\n",
      "Iteration 564, loss = 3.03127966\n",
      "Iteration 565, loss = 3.03052035\n",
      "Iteration 566, loss = 3.03002495\n",
      "Iteration 567, loss = 3.02980828\n",
      "Iteration 568, loss = 3.02992002\n",
      "Iteration 569, loss = 3.02892247\n",
      "Iteration 570, loss = 3.02843542\n",
      "Iteration 571, loss = 3.02858900\n",
      "Iteration 572, loss = 3.02787824\n",
      "Iteration 573, loss = 3.02741378\n",
      "Iteration 574, loss = 3.02705245\n",
      "Iteration 575, loss = 3.02672511\n",
      "Iteration 576, loss = 3.02692146\n",
      "Iteration 577, loss = 3.02630886\n",
      "Iteration 578, loss = 3.02556150\n",
      "Iteration 579, loss = 3.02590742\n",
      "Iteration 580, loss = 3.02553630\n",
      "Iteration 581, loss = 3.02487409\n",
      "Iteration 582, loss = 3.02491118\n",
      "Iteration 583, loss = 3.02461885\n",
      "Iteration 584, loss = 3.02392035\n",
      "Iteration 585, loss = 3.02355273\n",
      "Iteration 586, loss = 3.02330203\n",
      "Iteration 587, loss = 3.02283245\n",
      "Iteration 588, loss = 3.02246051\n",
      "Iteration 589, loss = 3.02266337\n",
      "Iteration 590, loss = 3.02210789\n",
      "Iteration 591, loss = 3.02129944\n",
      "Iteration 592, loss = 3.02096286\n",
      "Iteration 593, loss = 3.02074708\n",
      "Iteration 594, loss = 3.02063838\n",
      "Iteration 595, loss = 3.01994858\n",
      "Iteration 596, loss = 3.01970883\n",
      "Iteration 597, loss = 3.01900377\n",
      "Iteration 598, loss = 3.02040122\n",
      "Iteration 599, loss = 3.01968363\n",
      "Iteration 600, loss = 3.01806137\n",
      "Iteration 601, loss = 3.01800132\n",
      "Iteration 602, loss = 3.01752237\n",
      "Iteration 603, loss = 3.01739376\n",
      "Iteration 604, loss = 3.01723668\n",
      "Iteration 605, loss = 3.01637110\n",
      "Iteration 606, loss = 3.01640402\n",
      "Iteration 607, loss = 3.01639625\n",
      "Iteration 608, loss = 3.01600169\n",
      "Iteration 609, loss = 3.01545923\n",
      "Iteration 610, loss = 3.01505109\n",
      "Iteration 611, loss = 3.01466955\n",
      "Iteration 612, loss = 3.01589046\n",
      "Iteration 613, loss = 3.01532201\n",
      "Iteration 614, loss = 3.01449810\n",
      "Iteration 615, loss = 3.01415621\n",
      "Iteration 616, loss = 3.01311912\n",
      "Iteration 617, loss = 3.01266317\n",
      "Iteration 618, loss = 3.01215591\n",
      "Iteration 619, loss = 3.01208987\n",
      "Iteration 620, loss = 3.01157477\n",
      "Iteration 621, loss = 3.01196941\n",
      "Iteration 622, loss = 3.01183625\n",
      "Iteration 623, loss = 3.01102242\n",
      "Iteration 624, loss = 3.01038496\n",
      "Iteration 625, loss = 3.01017130\n",
      "Iteration 626, loss = 3.00949473\n",
      "Iteration 627, loss = 3.00920761\n",
      "Iteration 628, loss = 3.00920489\n",
      "Iteration 629, loss = 3.00876310\n",
      "Iteration 630, loss = 3.00887318\n",
      "Iteration 631, loss = 3.00833050\n",
      "Iteration 632, loss = 3.00825541\n",
      "Iteration 633, loss = 3.00756433\n",
      "Iteration 634, loss = 3.00731953\n",
      "Iteration 635, loss = 3.00820168\n",
      "Iteration 636, loss = 3.00663019\n",
      "Iteration 637, loss = 3.00670756\n",
      "Iteration 638, loss = 3.00646996\n",
      "Iteration 639, loss = 3.00595755\n",
      "Iteration 640, loss = 3.00464851\n",
      "Iteration 641, loss = 3.00550094\n",
      "Iteration 642, loss = 3.00606233\n",
      "Iteration 643, loss = 3.00473427\n",
      "Iteration 644, loss = 3.00387967\n",
      "Iteration 645, loss = 3.00342906\n",
      "Iteration 646, loss = 3.00286939\n",
      "Iteration 647, loss = 3.00305939\n",
      "Iteration 648, loss = 3.00318394\n",
      "Iteration 649, loss = 3.00300242\n",
      "Iteration 650, loss = 3.00165094\n",
      "Iteration 651, loss = 3.00159879\n",
      "Iteration 652, loss = 3.00151959\n",
      "Iteration 653, loss = 3.00086767\n",
      "Iteration 654, loss = 3.00068335\n",
      "Iteration 655, loss = 3.00054495\n",
      "Iteration 656, loss = 2.99994934\n",
      "Iteration 657, loss = 2.99984424\n",
      "Iteration 658, loss = 2.99957726\n",
      "Iteration 659, loss = 2.99952821\n",
      "Iteration 660, loss = 2.99894673\n",
      "Iteration 661, loss = 2.99870536\n",
      "Iteration 662, loss = 2.99840849\n",
      "Iteration 663, loss = 2.99783743\n",
      "Iteration 664, loss = 2.99729326\n",
      "Iteration 665, loss = 2.99707729\n",
      "Iteration 666, loss = 2.99696089\n",
      "Iteration 667, loss = 2.99754646\n",
      "Iteration 668, loss = 2.99639052\n",
      "Iteration 669, loss = 2.99632512\n",
      "Iteration 670, loss = 2.99542571\n",
      "Iteration 671, loss = 2.99533226\n",
      "Iteration 672, loss = 2.99533542\n",
      "Iteration 673, loss = 2.99493657\n",
      "Iteration 674, loss = 2.99451333\n",
      "Iteration 675, loss = 2.99385456\n",
      "Iteration 676, loss = 2.99408414\n",
      "Iteration 677, loss = 2.99309684\n",
      "Iteration 678, loss = 2.99289814\n",
      "Iteration 679, loss = 2.99304687\n",
      "Iteration 680, loss = 2.99301965\n",
      "Iteration 681, loss = 2.99270502\n",
      "Iteration 682, loss = 2.99240528\n",
      "Iteration 683, loss = 2.99248080\n",
      "Iteration 684, loss = 2.99195752\n",
      "Iteration 685, loss = 2.99068399\n",
      "Iteration 686, loss = 2.99057173\n",
      "Iteration 687, loss = 2.99041955\n",
      "Iteration 688, loss = 2.99093533\n",
      "Iteration 689, loss = 2.99127632\n",
      "Iteration 690, loss = 2.98905549\n",
      "Iteration 691, loss = 2.98924957\n",
      "Iteration 692, loss = 2.98833132\n",
      "Iteration 693, loss = 2.98819276\n",
      "Iteration 694, loss = 2.98826716\n",
      "Iteration 695, loss = 2.98825402\n",
      "Iteration 696, loss = 2.98796667\n",
      "Iteration 697, loss = 2.98768544\n",
      "Iteration 698, loss = 2.98762350\n",
      "Iteration 699, loss = 2.98718623\n",
      "Iteration 700, loss = 2.98680834\n",
      "Iteration 701, loss = 2.98607714\n",
      "Iteration 702, loss = 2.98568442\n",
      "Iteration 703, loss = 2.98530155\n",
      "Iteration 704, loss = 2.98517117\n",
      "Iteration 705, loss = 2.98459857\n",
      "Iteration 706, loss = 2.98457801\n",
      "Iteration 707, loss = 2.98419327\n",
      "Iteration 708, loss = 2.98350114\n",
      "Iteration 709, loss = 2.98332225\n",
      "Iteration 710, loss = 2.98323363\n",
      "Iteration 711, loss = 2.98308732\n",
      "Iteration 712, loss = 2.98234336\n",
      "Iteration 713, loss = 2.98264016\n",
      "Iteration 714, loss = 2.98179751\n",
      "Iteration 715, loss = 2.98200709\n",
      "Iteration 716, loss = 2.98273864\n",
      "Iteration 717, loss = 2.98176816\n",
      "Iteration 718, loss = 2.98079496\n",
      "Iteration 719, loss = 2.98039388\n",
      "Iteration 720, loss = 2.98009097\n",
      "Iteration 721, loss = 2.98021705\n",
      "Iteration 722, loss = 2.97969569\n",
      "Iteration 723, loss = 2.97921730\n",
      "Iteration 724, loss = 2.97880622\n",
      "Iteration 725, loss = 2.97897311\n",
      "Iteration 726, loss = 2.97840385\n",
      "Iteration 727, loss = 2.97792148\n",
      "Iteration 728, loss = 2.97782494\n",
      "Iteration 729, loss = 2.97871752\n",
      "Iteration 730, loss = 2.97778576\n",
      "Iteration 731, loss = 2.97660256\n",
      "Iteration 732, loss = 2.97615261\n",
      "Iteration 733, loss = 2.97571949\n",
      "Iteration 734, loss = 2.97637539\n",
      "Iteration 735, loss = 2.97614062\n",
      "Iteration 736, loss = 2.97599418\n",
      "Iteration 737, loss = 2.97590487\n",
      "Iteration 738, loss = 2.97620881\n",
      "Iteration 739, loss = 2.97485210\n",
      "Iteration 740, loss = 2.97411764\n",
      "Iteration 741, loss = 2.97358821\n",
      "Iteration 742, loss = 2.97349248\n",
      "Iteration 743, loss = 2.97348291\n",
      "Iteration 744, loss = 2.97314419\n",
      "Iteration 745, loss = 2.97213066\n",
      "Iteration 746, loss = 2.97238134\n",
      "Iteration 747, loss = 2.97214360\n",
      "Iteration 748, loss = 2.97241077\n",
      "Iteration 749, loss = 2.97232251\n",
      "Iteration 750, loss = 2.97185197\n",
      "Iteration 751, loss = 2.97161385\n",
      "Iteration 752, loss = 2.97095089\n",
      "Iteration 753, loss = 2.97081376\n",
      "Iteration 754, loss = 2.97028183\n",
      "Iteration 755, loss = 2.96955288\n",
      "Iteration 756, loss = 2.96969508\n",
      "Iteration 757, loss = 2.96925009\n",
      "Iteration 758, loss = 2.96892208\n",
      "Iteration 759, loss = 2.96933695\n",
      "Iteration 760, loss = 2.96885795\n",
      "Iteration 761, loss = 2.96806010\n",
      "Iteration 762, loss = 2.96817000\n",
      "Iteration 763, loss = 2.96782506\n",
      "Iteration 764, loss = 2.96756750\n",
      "Iteration 765, loss = 2.96659718\n",
      "Iteration 766, loss = 2.96653548\n",
      "Iteration 767, loss = 2.96609507\n",
      "Iteration 768, loss = 2.96619462\n",
      "Iteration 769, loss = 2.96563301\n",
      "Iteration 770, loss = 2.96632534\n",
      "Iteration 771, loss = 2.96536750\n",
      "Iteration 772, loss = 2.96480774\n",
      "Iteration 773, loss = 2.96463009\n",
      "Iteration 774, loss = 2.96429918\n",
      "Iteration 775, loss = 2.96437768\n",
      "Iteration 776, loss = 2.96414755\n",
      "Iteration 777, loss = 2.96386289\n",
      "Iteration 778, loss = 2.96324879\n",
      "Iteration 779, loss = 2.96247216\n",
      "Iteration 780, loss = 2.96217568\n",
      "Iteration 781, loss = 2.96254720\n",
      "Iteration 782, loss = 2.96238027\n",
      "Iteration 783, loss = 2.96200210\n",
      "Iteration 784, loss = 2.96141260\n",
      "Iteration 785, loss = 2.96158016\n",
      "Iteration 786, loss = 2.96188089\n",
      "Iteration 787, loss = 2.96132460\n",
      "Iteration 788, loss = 2.96122262\n",
      "Iteration 789, loss = 2.96085656\n",
      "Iteration 790, loss = 2.96073281\n",
      "Iteration 791, loss = 2.96074109\n",
      "Iteration 792, loss = 2.95975630\n",
      "Iteration 793, loss = 2.95987798\n",
      "Iteration 794, loss = 2.95952293\n",
      "Iteration 795, loss = 2.95883636\n",
      "Iteration 796, loss = 2.95846262\n",
      "Iteration 797, loss = 2.95894407\n",
      "Iteration 798, loss = 2.95821896\n",
      "Iteration 799, loss = 2.95786628\n",
      "Iteration 800, loss = 2.95730817\n",
      "Iteration 801, loss = 2.95762720\n",
      "Iteration 802, loss = 2.95783195\n",
      "Iteration 803, loss = 2.95691982\n",
      "Iteration 804, loss = 2.95648090\n",
      "Iteration 805, loss = 2.95612776\n",
      "Iteration 806, loss = 2.95544743\n",
      "Iteration 807, loss = 2.95531410\n",
      "Iteration 808, loss = 2.95516369\n",
      "Iteration 809, loss = 2.95449847\n",
      "Iteration 810, loss = 2.95461953\n",
      "Iteration 811, loss = 2.95425990\n",
      "Iteration 812, loss = 2.95427045\n",
      "Iteration 813, loss = 2.95468910\n",
      "Iteration 814, loss = 2.95392958\n",
      "Iteration 815, loss = 2.95352005\n",
      "Iteration 816, loss = 2.95306120\n",
      "Iteration 817, loss = 2.95285242\n",
      "Iteration 818, loss = 2.95315708\n",
      "Iteration 819, loss = 2.95264894\n",
      "Iteration 820, loss = 2.95210143\n",
      "Iteration 821, loss = 2.95192098\n",
      "Iteration 822, loss = 2.95290782\n",
      "Iteration 823, loss = 2.95170406\n",
      "Iteration 824, loss = 2.95145664\n",
      "Iteration 825, loss = 2.95098725\n",
      "Iteration 826, loss = 2.95019835\n",
      "Iteration 827, loss = 2.95007148\n",
      "Iteration 828, loss = 2.94977204\n",
      "Iteration 829, loss = 2.94986826\n",
      "Iteration 830, loss = 2.94915124\n",
      "Iteration 831, loss = 2.94898140\n",
      "Iteration 832, loss = 2.94887588\n",
      "Iteration 833, loss = 2.94877176\n",
      "Iteration 834, loss = 2.94828954\n",
      "Iteration 835, loss = 2.94793728\n",
      "Iteration 836, loss = 2.94831787\n",
      "Iteration 837, loss = 2.94803785\n",
      "Iteration 838, loss = 2.94734076\n",
      "Iteration 839, loss = 2.94709077\n",
      "Iteration 840, loss = 2.94678931\n",
      "Iteration 841, loss = 2.94659690\n",
      "Iteration 842, loss = 2.94645398\n",
      "Iteration 843, loss = 2.94650450\n",
      "Iteration 844, loss = 2.94607576\n",
      "Iteration 845, loss = 2.94551694\n",
      "Iteration 846, loss = 2.94533640\n",
      "Iteration 847, loss = 2.94500049\n",
      "Iteration 848, loss = 2.94479083\n",
      "Iteration 849, loss = 2.94442435\n",
      "Iteration 850, loss = 2.94379971\n",
      "Iteration 851, loss = 2.94464042\n",
      "Iteration 852, loss = 2.94473451\n",
      "Iteration 853, loss = 2.94440376\n",
      "Iteration 854, loss = 2.94360444\n",
      "Iteration 855, loss = 2.94274817\n",
      "Iteration 856, loss = 2.94222553\n",
      "Iteration 857, loss = 2.94223692\n",
      "Iteration 858, loss = 2.94191642\n",
      "Iteration 859, loss = 2.94188824\n",
      "Iteration 860, loss = 2.94151835\n",
      "Iteration 861, loss = 2.94111771\n",
      "Iteration 862, loss = 2.94152068\n",
      "Iteration 863, loss = 2.94096404\n",
      "Iteration 864, loss = 2.94140485\n",
      "Iteration 865, loss = 2.94256340\n",
      "Iteration 866, loss = 2.94120708\n",
      "Iteration 867, loss = 2.94043551\n",
      "Iteration 868, loss = 2.94013605\n",
      "Iteration 869, loss = 2.93958632\n",
      "Iteration 870, loss = 2.94011761\n",
      "Iteration 871, loss = 2.93996615\n",
      "Iteration 872, loss = 2.93882397\n",
      "Iteration 873, loss = 2.93881321\n",
      "Iteration 874, loss = 2.93835184\n",
      "Iteration 875, loss = 2.93780815\n",
      "Iteration 876, loss = 2.93721273\n",
      "Iteration 877, loss = 2.93667968\n",
      "Iteration 878, loss = 2.93668766\n",
      "Iteration 879, loss = 2.93743086\n",
      "Iteration 880, loss = 2.93663585\n",
      "Iteration 881, loss = 2.93638397\n",
      "Iteration 882, loss = 2.93596249\n",
      "Iteration 883, loss = 2.93519706\n",
      "Iteration 884, loss = 2.93542960\n",
      "Iteration 885, loss = 2.93516423\n",
      "Iteration 886, loss = 2.93475655\n",
      "Iteration 887, loss = 2.93488437\n",
      "Iteration 888, loss = 2.93460493\n",
      "Iteration 889, loss = 2.93436796\n",
      "Iteration 890, loss = 2.93387431\n",
      "Iteration 891, loss = 2.93431665\n",
      "Iteration 892, loss = 2.93359716\n",
      "Iteration 893, loss = 2.93313542\n",
      "Iteration 894, loss = 2.93293510\n",
      "Iteration 895, loss = 2.93298796\n",
      "Iteration 896, loss = 2.93335410\n",
      "Iteration 897, loss = 2.93194303\n",
      "Iteration 898, loss = 2.93218906\n",
      "Iteration 899, loss = 2.93172785\n",
      "Iteration 900, loss = 2.93143724\n",
      "Iteration 901, loss = 2.93136938\n",
      "Iteration 902, loss = 2.93085679\n",
      "Iteration 903, loss = 2.93074087\n",
      "Iteration 904, loss = 2.93004144\n",
      "Iteration 905, loss = 2.93027557\n",
      "Iteration 906, loss = 2.92983223\n",
      "Iteration 907, loss = 2.92957928\n",
      "Iteration 908, loss = 2.92925221\n",
      "Iteration 909, loss = 2.92953664\n",
      "Iteration 910, loss = 2.92877212\n",
      "Iteration 911, loss = 2.92891172\n",
      "Iteration 912, loss = 2.92878249\n",
      "Iteration 913, loss = 2.92924146\n",
      "Iteration 914, loss = 2.92901601\n",
      "Iteration 915, loss = 2.92791075\n",
      "Iteration 916, loss = 2.92741001\n",
      "Iteration 917, loss = 2.92730917\n",
      "Iteration 918, loss = 2.92691395\n",
      "Iteration 919, loss = 2.92659350\n",
      "Iteration 920, loss = 2.92618589\n",
      "Iteration 921, loss = 2.92604297\n",
      "Iteration 922, loss = 2.92548798\n",
      "Iteration 923, loss = 2.92547447\n",
      "Iteration 924, loss = 2.92516057\n",
      "Iteration 925, loss = 2.92553926\n",
      "Iteration 926, loss = 2.92544749\n",
      "Iteration 927, loss = 2.92527044\n",
      "Iteration 928, loss = 2.92447561\n",
      "Iteration 929, loss = 2.92409702\n",
      "Iteration 930, loss = 2.92416749\n",
      "Iteration 931, loss = 2.92403855\n",
      "Iteration 932, loss = 2.92330928\n",
      "Iteration 933, loss = 2.92280081\n",
      "Iteration 934, loss = 2.92359969\n",
      "Iteration 935, loss = 2.92340381\n",
      "Iteration 936, loss = 2.92271703\n",
      "Iteration 937, loss = 2.92243929\n",
      "Iteration 938, loss = 2.92192292\n",
      "Iteration 939, loss = 2.92166127\n",
      "Iteration 940, loss = 2.92172160\n",
      "Iteration 941, loss = 2.92100375\n",
      "Iteration 942, loss = 2.92101736\n",
      "Iteration 943, loss = 2.92123538\n",
      "Iteration 944, loss = 2.92127431\n",
      "Iteration 945, loss = 2.92042807\n",
      "Iteration 946, loss = 2.92013222\n",
      "Iteration 947, loss = 2.92028105\n",
      "Iteration 948, loss = 2.92008473\n",
      "Iteration 949, loss = 2.91935275\n",
      "Iteration 950, loss = 2.91946365\n",
      "Iteration 951, loss = 2.91960290\n",
      "Iteration 952, loss = 2.91910693\n",
      "Iteration 953, loss = 2.91929694\n",
      "Iteration 954, loss = 2.91882798\n",
      "Iteration 955, loss = 2.91808824\n",
      "Iteration 956, loss = 2.91798482\n",
      "Iteration 957, loss = 2.91808140\n",
      "Iteration 958, loss = 2.91724352\n",
      "Iteration 959, loss = 2.91694202\n",
      "Iteration 960, loss = 2.91674817\n",
      "Iteration 961, loss = 2.91636130\n",
      "Iteration 962, loss = 2.91643658\n",
      "Iteration 963, loss = 2.91661495\n",
      "Iteration 964, loss = 2.91598505\n",
      "Iteration 965, loss = 2.91564951\n",
      "Iteration 966, loss = 2.91530436\n",
      "Iteration 967, loss = 2.91528113\n",
      "Iteration 968, loss = 2.91484732\n",
      "Iteration 969, loss = 2.91495212\n",
      "Iteration 970, loss = 2.91478629\n",
      "Iteration 971, loss = 2.91462411\n",
      "Iteration 972, loss = 2.91403680\n",
      "Iteration 973, loss = 2.91375853\n",
      "Iteration 974, loss = 2.91361240\n",
      "Iteration 975, loss = 2.91305923\n",
      "Iteration 976, loss = 2.91283054\n",
      "Iteration 977, loss = 2.91262677\n",
      "Iteration 978, loss = 2.91239073\n",
      "Iteration 979, loss = 2.91227242\n",
      "Iteration 980, loss = 2.91154866\n",
      "Iteration 981, loss = 2.91167808\n",
      "Iteration 982, loss = 2.91166901\n",
      "Iteration 983, loss = 2.91171378\n",
      "Iteration 984, loss = 2.91158641\n",
      "Iteration 985, loss = 2.91177220\n",
      "Iteration 986, loss = 2.91080824\n",
      "Iteration 987, loss = 2.91067478\n",
      "Iteration 988, loss = 2.91084999\n",
      "Iteration 989, loss = 2.91049229\n",
      "Iteration 990, loss = 2.90934430\n",
      "Iteration 991, loss = 2.90957989\n",
      "Iteration 992, loss = 2.90988263\n",
      "Iteration 993, loss = 2.90955277\n",
      "Iteration 994, loss = 2.91023880\n",
      "Iteration 995, loss = 2.91028901\n",
      "Iteration 996, loss = 2.90935923\n",
      "Iteration 997, loss = 2.90961567\n",
      "Iteration 998, loss = 2.90989667\n",
      "Iteration 999, loss = 2.90875802\n",
      "Iteration 1000, loss = 2.90807906\n",
      "Iteration 1001, loss = 2.90867357\n",
      "Iteration 1002, loss = 2.90839744\n",
      "Iteration 1003, loss = 2.90798477\n",
      "Iteration 1004, loss = 2.90727701\n",
      "Iteration 1005, loss = 2.90674654\n",
      "Iteration 1006, loss = 2.90565008\n",
      "Iteration 1007, loss = 2.90565106\n",
      "Iteration 1008, loss = 2.90546886\n",
      "Iteration 1009, loss = 2.90591560\n",
      "Iteration 1010, loss = 2.90503348\n",
      "Iteration 1011, loss = 2.90482085\n",
      "Iteration 1012, loss = 2.90514153\n",
      "Iteration 1013, loss = 2.90508959\n",
      "Iteration 1014, loss = 2.90455972\n",
      "Iteration 1015, loss = 2.90497291\n",
      "Iteration 1016, loss = 2.90447411\n",
      "Iteration 1017, loss = 2.90387600\n",
      "Iteration 1018, loss = 2.90397790\n",
      "Iteration 1019, loss = 2.90369005\n",
      "Iteration 1020, loss = 2.90354569\n",
      "Iteration 1021, loss = 2.90272590\n",
      "Iteration 1022, loss = 2.90313960\n",
      "Iteration 1023, loss = 2.90301036\n",
      "Iteration 1024, loss = 2.90198807\n",
      "Iteration 1025, loss = 2.90226621\n",
      "Iteration 1026, loss = 2.90172457\n",
      "Iteration 1027, loss = 2.90099862\n",
      "Iteration 1028, loss = 2.90026811\n",
      "Iteration 1029, loss = 2.90098765\n",
      "Iteration 1030, loss = 2.90074226\n",
      "Iteration 1031, loss = 2.90066712\n",
      "Iteration 1032, loss = 2.89990506\n",
      "Iteration 1033, loss = 2.89983792\n",
      "Iteration 1034, loss = 2.89975696\n",
      "Iteration 1035, loss = 2.89969471\n",
      "Iteration 1036, loss = 2.89990936\n",
      "Iteration 1037, loss = 2.89988717\n",
      "Iteration 1038, loss = 2.89905951\n",
      "Iteration 1039, loss = 2.89869559\n",
      "Iteration 1040, loss = 2.89946521\n",
      "Iteration 1041, loss = 2.89802422\n",
      "Iteration 1042, loss = 2.89804630\n",
      "Iteration 1043, loss = 2.89825969\n",
      "Iteration 1044, loss = 2.89769074\n",
      "Iteration 1045, loss = 2.89748310\n",
      "Iteration 1046, loss = 2.89842589\n",
      "Iteration 1047, loss = 2.89782919\n",
      "Iteration 1048, loss = 2.89690721\n",
      "Iteration 1049, loss = 2.89615073\n",
      "Iteration 1050, loss = 2.89631156\n",
      "Iteration 1051, loss = 2.89586701\n",
      "Iteration 1052, loss = 2.89571650\n",
      "Iteration 1053, loss = 2.89513401\n",
      "Iteration 1054, loss = 2.89538263\n",
      "Iteration 1055, loss = 2.89506956\n",
      "Iteration 1056, loss = 2.89462125\n",
      "Iteration 1057, loss = 2.89453018\n",
      "Iteration 1058, loss = 2.89415245\n",
      "Iteration 1059, loss = 2.89438885\n",
      "Iteration 1060, loss = 2.89387422\n",
      "Iteration 1061, loss = 2.89363923\n",
      "Iteration 1062, loss = 2.89342015\n",
      "Iteration 1063, loss = 2.89335162\n",
      "Iteration 1064, loss = 2.89292684\n",
      "Iteration 1065, loss = 2.89271362\n",
      "Iteration 1066, loss = 2.89236438\n",
      "Iteration 1067, loss = 2.89201721\n",
      "Iteration 1068, loss = 2.89209767\n",
      "Iteration 1069, loss = 2.89213931\n",
      "Iteration 1070, loss = 2.89233064\n",
      "Iteration 1071, loss = 2.89117812\n",
      "Iteration 1072, loss = 2.89128418\n",
      "Iteration 1073, loss = 2.89116441\n",
      "Iteration 1074, loss = 2.89109327\n",
      "Iteration 1075, loss = 2.89107555\n",
      "Iteration 1076, loss = 2.89012939\n",
      "Iteration 1077, loss = 2.89027396\n",
      "Iteration 1078, loss = 2.89244766\n",
      "Iteration 1079, loss = 2.89155239\n",
      "Iteration 1080, loss = 2.88952715\n",
      "Iteration 1081, loss = 2.88907332\n",
      "Iteration 1082, loss = 2.88944497\n",
      "Iteration 1083, loss = 2.88875525\n",
      "Iteration 1084, loss = 2.88984650\n",
      "Iteration 1085, loss = 2.88950754\n",
      "Iteration 1086, loss = 2.88849475\n",
      "Iteration 1087, loss = 2.88775374\n",
      "Iteration 1088, loss = 2.88742106\n",
      "Iteration 1089, loss = 2.88764160\n",
      "Iteration 1090, loss = 2.88754912\n",
      "Iteration 1091, loss = 2.88738635\n",
      "Iteration 1092, loss = 2.88659198\n",
      "Iteration 1093, loss = 2.88694377\n",
      "Iteration 1094, loss = 2.88698059\n",
      "Iteration 1095, loss = 2.88663729\n",
      "Iteration 1096, loss = 2.88635621\n",
      "Iteration 1097, loss = 2.88602659\n",
      "Iteration 1098, loss = 2.88578792\n",
      "Iteration 1099, loss = 2.88605560\n",
      "Iteration 1100, loss = 2.88552480\n",
      "Iteration 1101, loss = 2.88547141\n",
      "Iteration 1102, loss = 2.88530918\n",
      "Iteration 1103, loss = 2.88486427\n",
      "Iteration 1104, loss = 2.88466201\n",
      "Iteration 1105, loss = 2.88616122\n",
      "Iteration 1106, loss = 2.88519753\n",
      "Iteration 1107, loss = 2.88414433\n",
      "Iteration 1108, loss = 2.88353224\n",
      "Iteration 1109, loss = 2.88334363\n",
      "Iteration 1110, loss = 2.88330572\n",
      "Iteration 1111, loss = 2.88310130\n",
      "Iteration 1112, loss = 2.88360474\n",
      "Iteration 1113, loss = 2.88316164\n",
      "Iteration 1114, loss = 2.88237610\n",
      "Iteration 1115, loss = 2.88229477\n",
      "Iteration 1116, loss = 2.88215181\n",
      "Iteration 1117, loss = 2.88195888\n",
      "Iteration 1118, loss = 2.88121869\n",
      "Iteration 1119, loss = 2.88137520\n",
      "Iteration 1120, loss = 2.88140009\n",
      "Iteration 1121, loss = 2.88063923\n",
      "Iteration 1122, loss = 2.88065045\n",
      "Iteration 1123, loss = 2.88048096\n",
      "Iteration 1124, loss = 2.88045115\n",
      "Iteration 1125, loss = 2.88015014\n",
      "Iteration 1126, loss = 2.88017256\n",
      "Iteration 1127, loss = 2.88071991\n",
      "Iteration 1128, loss = 2.88008250\n",
      "Iteration 1129, loss = 2.87937020\n",
      "Iteration 1130, loss = 2.87922096\n",
      "Iteration 1131, loss = 2.87881621\n",
      "Iteration 1132, loss = 2.87850950\n",
      "Iteration 1133, loss = 2.87813335\n",
      "Iteration 1134, loss = 2.87802975\n",
      "Iteration 1135, loss = 2.87835783\n",
      "Iteration 1136, loss = 2.87813766\n",
      "Iteration 1137, loss = 2.87750449\n",
      "Iteration 1138, loss = 2.87713110\n",
      "Iteration 1139, loss = 2.87725646\n",
      "Iteration 1140, loss = 2.87764664\n",
      "Iteration 1141, loss = 2.87763661\n",
      "Iteration 1142, loss = 2.87623724\n",
      "Iteration 1143, loss = 2.87722763\n",
      "Iteration 1144, loss = 2.87699008\n",
      "Iteration 1145, loss = 2.87590145\n",
      "Iteration 1146, loss = 2.87620335\n",
      "Iteration 1147, loss = 2.87638044\n",
      "Iteration 1148, loss = 2.87567807\n",
      "Iteration 1149, loss = 2.87505056\n",
      "Iteration 1150, loss = 2.87505232\n",
      "Iteration 1151, loss = 2.87479910\n",
      "Iteration 1152, loss = 2.87492882\n",
      "Iteration 1153, loss = 2.87456930\n",
      "Iteration 1154, loss = 2.87471802\n",
      "Iteration 1155, loss = 2.87440438\n",
      "Iteration 1156, loss = 2.87383022\n",
      "Iteration 1157, loss = 2.87380506\n",
      "Iteration 1158, loss = 2.87311156\n",
      "Iteration 1159, loss = 2.87288113\n",
      "Iteration 1160, loss = 2.87315061\n",
      "Iteration 1161, loss = 2.87271324\n",
      "Iteration 1162, loss = 2.87214768\n",
      "Iteration 1163, loss = 2.87219580\n",
      "Iteration 1164, loss = 2.87202653\n",
      "Iteration 1165, loss = 2.87173375\n",
      "Iteration 1166, loss = 2.87179768\n",
      "Iteration 1167, loss = 2.87177150\n",
      "Iteration 1168, loss = 2.87059863\n",
      "Iteration 1169, loss = 2.87276004\n",
      "Iteration 1170, loss = 2.87151551\n",
      "Iteration 1171, loss = 2.87013248\n",
      "Iteration 1172, loss = 2.87107625\n",
      "Iteration 1173, loss = 2.87055939\n",
      "Iteration 1174, loss = 2.86959517\n",
      "Iteration 1175, loss = 2.87062859\n",
      "Iteration 1176, loss = 2.87084844\n",
      "Iteration 1177, loss = 2.86991290\n",
      "Iteration 1178, loss = 2.86985342\n",
      "Iteration 1179, loss = 2.86985101\n",
      "Iteration 1180, loss = 2.86836461\n",
      "Iteration 1181, loss = 2.86882635\n",
      "Iteration 1182, loss = 2.86820759\n",
      "Iteration 1183, loss = 2.86786383\n",
      "Iteration 1184, loss = 2.86758822\n",
      "Iteration 1185, loss = 2.86793330\n",
      "Iteration 1186, loss = 2.86767888\n",
      "Iteration 1187, loss = 2.86723699\n",
      "Iteration 1188, loss = 2.86690060\n",
      "Iteration 1189, loss = 2.86680588\n",
      "Iteration 1190, loss = 2.86710843\n",
      "Iteration 1191, loss = 2.86624590\n",
      "Iteration 1192, loss = 2.86597466\n",
      "Iteration 1193, loss = 2.86564193\n",
      "Iteration 1194, loss = 2.86719635\n",
      "Iteration 1195, loss = 2.86626193\n",
      "Iteration 1196, loss = 2.86640254\n",
      "Iteration 1197, loss = 2.86624516\n",
      "Iteration 1198, loss = 2.86574603\n",
      "Iteration 1199, loss = 2.86527651\n",
      "Iteration 1200, loss = 2.86503413\n",
      "Iteration 1201, loss = 2.86494009\n",
      "Iteration 1202, loss = 2.86514469\n",
      "Iteration 1203, loss = 2.86445182\n",
      "Iteration 1204, loss = 2.86409890\n",
      "Iteration 1205, loss = 2.86357292\n",
      "Iteration 1206, loss = 2.86364301\n",
      "Iteration 1207, loss = 2.86399038\n",
      "Iteration 1208, loss = 2.86367045\n",
      "Iteration 1209, loss = 2.86319163\n",
      "Iteration 1210, loss = 2.86433321\n",
      "Iteration 1211, loss = 2.86364342\n",
      "Iteration 1212, loss = 2.86288624\n",
      "Iteration 1213, loss = 2.86234251\n",
      "Iteration 1214, loss = 2.86251786\n",
      "Iteration 1215, loss = 2.86211480\n",
      "Iteration 1216, loss = 2.86200726\n",
      "Iteration 1217, loss = 2.86145421\n",
      "Iteration 1218, loss = 2.86138352\n",
      "Iteration 1219, loss = 2.86109031\n",
      "Iteration 1220, loss = 2.86112308\n",
      "Iteration 1221, loss = 2.86151476\n",
      "Iteration 1222, loss = 2.86273186\n",
      "Iteration 1223, loss = 2.86024291\n",
      "Iteration 1224, loss = 2.86019155\n",
      "Iteration 1225, loss = 2.85965830\n",
      "Iteration 1226, loss = 2.86003938\n",
      "Iteration 1227, loss = 2.86014456\n",
      "Iteration 1228, loss = 2.85952808\n",
      "Iteration 1229, loss = 2.85927626\n",
      "Iteration 1230, loss = 2.85870790\n",
      "Iteration 1231, loss = 2.85922649\n",
      "Iteration 1232, loss = 2.85868017\n",
      "Iteration 1233, loss = 2.85844329\n",
      "Iteration 1234, loss = 2.85885481\n",
      "Iteration 1235, loss = 2.85839320\n",
      "Iteration 1236, loss = 2.85761344\n",
      "Iteration 1237, loss = 2.85727346\n",
      "Iteration 1238, loss = 2.85685750\n",
      "Iteration 1239, loss = 2.85686622\n",
      "Iteration 1240, loss = 2.85672845\n",
      "Iteration 1241, loss = 2.85680180\n",
      "Iteration 1242, loss = 2.85661480\n",
      "Iteration 1243, loss = 2.85611497\n",
      "Iteration 1244, loss = 2.85621962\n",
      "Iteration 1245, loss = 2.85569211\n",
      "Iteration 1246, loss = 2.85597220\n",
      "Iteration 1247, loss = 2.85539890\n",
      "Iteration 1248, loss = 2.85543600\n",
      "Iteration 1249, loss = 2.85564442\n",
      "Iteration 1250, loss = 2.85540155\n",
      "Iteration 1251, loss = 2.85498389\n",
      "Iteration 1252, loss = 2.85500504\n",
      "Iteration 1253, loss = 2.85513237\n",
      "Iteration 1254, loss = 2.85432888\n",
      "Iteration 1255, loss = 2.85484913\n",
      "Iteration 1256, loss = 2.85491991\n",
      "Iteration 1257, loss = 2.85551291\n",
      "Iteration 1258, loss = 2.85374629\n",
      "Iteration 1259, loss = 2.85351768\n",
      "Iteration 1260, loss = 2.85345548\n",
      "Iteration 1261, loss = 2.85288424\n",
      "Iteration 1262, loss = 2.85323586\n",
      "Iteration 1263, loss = 2.85244093\n",
      "Iteration 1264, loss = 2.85285689\n",
      "Iteration 1265, loss = 2.85263343\n",
      "Iteration 1266, loss = 2.85230900\n",
      "Iteration 1267, loss = 2.85378725\n",
      "Iteration 1268, loss = 2.85194327\n",
      "Iteration 1269, loss = 2.85214813\n",
      "Iteration 1270, loss = 2.85248785\n",
      "Iteration 1271, loss = 2.85118432\n",
      "Iteration 1272, loss = 2.85112002\n",
      "Iteration 1273, loss = 2.85091571\n",
      "Iteration 1274, loss = 2.85078342\n",
      "Iteration 1275, loss = 2.85055636\n",
      "Iteration 1276, loss = 2.85039076\n",
      "Iteration 1277, loss = 2.85019017\n",
      "Iteration 1278, loss = 2.84987445\n",
      "Iteration 1279, loss = 2.84927255\n",
      "Iteration 1280, loss = 2.84883601\n",
      "Iteration 1281, loss = 2.84884510\n",
      "Iteration 1282, loss = 2.84923436\n",
      "Iteration 1283, loss = 2.84929181\n",
      "Iteration 1284, loss = 2.84898350\n",
      "Iteration 1285, loss = 2.84827153\n",
      "Iteration 1286, loss = 2.84828959\n",
      "Iteration 1287, loss = 2.84820624\n",
      "Iteration 1288, loss = 2.84816138\n",
      "Iteration 1289, loss = 2.84768018\n",
      "Iteration 1290, loss = 2.84766029\n",
      "Iteration 1291, loss = 2.84759721\n",
      "Iteration 1292, loss = 2.84763485\n",
      "Iteration 1293, loss = 2.84750558\n",
      "Iteration 1294, loss = 2.84706556\n",
      "Iteration 1295, loss = 2.84626816\n",
      "Iteration 1296, loss = 2.84588043\n",
      "Iteration 1297, loss = 2.84633419\n",
      "Iteration 1298, loss = 2.84631073\n",
      "Iteration 1299, loss = 2.84591391\n",
      "Iteration 1300, loss = 2.84565569\n",
      "Iteration 1301, loss = 2.84550675\n",
      "Iteration 1302, loss = 2.84510364\n",
      "Iteration 1303, loss = 2.84522978\n",
      "Iteration 1304, loss = 2.84499892\n",
      "Iteration 1305, loss = 2.84463967\n",
      "Iteration 1306, loss = 2.84444338\n",
      "Iteration 1307, loss = 2.84411827\n",
      "Iteration 1308, loss = 2.84451734\n",
      "Iteration 1309, loss = 2.84420332\n",
      "Iteration 1310, loss = 2.84372331\n",
      "Iteration 1311, loss = 2.84435943\n",
      "Iteration 1312, loss = 2.84412362\n",
      "Iteration 1313, loss = 2.84393079\n",
      "Iteration 1314, loss = 2.84350302\n",
      "Iteration 1315, loss = 2.84339695\n",
      "Iteration 1316, loss = 2.84330471\n",
      "Iteration 1317, loss = 2.84243995\n",
      "Iteration 1318, loss = 2.84308065\n",
      "Iteration 1319, loss = 2.84301394\n",
      "Iteration 1320, loss = 2.84264323\n",
      "Iteration 1321, loss = 2.84261672\n",
      "Iteration 1322, loss = 2.84177490\n",
      "Iteration 1323, loss = 2.84161975\n",
      "Iteration 1324, loss = 2.84117742\n",
      "Iteration 1325, loss = 2.84162296\n",
      "Iteration 1326, loss = 2.84118348\n",
      "Iteration 1327, loss = 2.84087907\n",
      "Iteration 1328, loss = 2.84053784\n",
      "Iteration 1329, loss = 2.84017513\n",
      "Iteration 1330, loss = 2.83988049\n",
      "Iteration 1331, loss = 2.84125012\n",
      "Iteration 1332, loss = 2.84050577\n",
      "Iteration 1333, loss = 2.83975249\n",
      "Iteration 1334, loss = 2.83968718\n",
      "Iteration 1335, loss = 2.84045715\n",
      "Iteration 1336, loss = 2.83978987\n",
      "Iteration 1337, loss = 2.83920444\n",
      "Iteration 1338, loss = 2.83902519\n",
      "Iteration 1339, loss = 2.83913964\n",
      "Iteration 1340, loss = 2.83891603\n",
      "Iteration 1341, loss = 2.83912310\n",
      "Iteration 1342, loss = 2.83789612\n",
      "Iteration 1343, loss = 2.83816510\n",
      "Iteration 1344, loss = 2.83750154\n",
      "Iteration 1345, loss = 2.83745710\n",
      "Iteration 1346, loss = 2.83792363\n",
      "Iteration 1347, loss = 2.83784439\n",
      "Iteration 1348, loss = 2.83703898\n",
      "Iteration 1349, loss = 2.83685935\n",
      "Iteration 1350, loss = 2.83647011\n",
      "Iteration 1351, loss = 2.83678481\n",
      "Iteration 1352, loss = 2.83693563\n",
      "Iteration 1353, loss = 2.83670132\n",
      "Iteration 1354, loss = 2.83645109\n",
      "Iteration 1355, loss = 2.83634745\n",
      "Iteration 1356, loss = 2.83627540\n",
      "Iteration 1357, loss = 2.83586357\n",
      "Iteration 1358, loss = 2.83512123\n",
      "Iteration 1359, loss = 2.83600160\n",
      "Iteration 1360, loss = 2.83532001\n",
      "Iteration 1361, loss = 2.83488498\n",
      "Iteration 1362, loss = 2.83481405\n",
      "Iteration 1363, loss = 2.83511308\n",
      "Iteration 1364, loss = 2.83433347\n",
      "Iteration 1365, loss = 2.83470139\n",
      "Iteration 1366, loss = 2.83411921\n",
      "Iteration 1367, loss = 2.83410588\n",
      "Iteration 1368, loss = 2.83393715\n",
      "Iteration 1369, loss = 2.83396136\n",
      "Iteration 1370, loss = 2.83354335\n",
      "Iteration 1371, loss = 2.83308147\n",
      "Iteration 1372, loss = 2.83299138\n",
      "Iteration 1373, loss = 2.83255222\n",
      "Iteration 1374, loss = 2.83231834\n",
      "Iteration 1375, loss = 2.83181214\n",
      "Iteration 1376, loss = 2.83237032\n",
      "Iteration 1377, loss = 2.83255114\n",
      "Iteration 1378, loss = 2.83181248\n",
      "Iteration 1379, loss = 2.83126896\n",
      "Iteration 1380, loss = 2.83105937\n",
      "Iteration 1381, loss = 2.83104163\n",
      "Iteration 1382, loss = 2.83124492\n",
      "Iteration 1383, loss = 2.83074009\n",
      "Iteration 1384, loss = 2.83094674\n",
      "Iteration 1385, loss = 2.83057035\n",
      "Iteration 1386, loss = 2.83022576\n",
      "Iteration 1387, loss = 2.82997532\n",
      "Iteration 1388, loss = 2.82973514\n",
      "Iteration 1389, loss = 2.83026450\n",
      "Iteration 1390, loss = 2.82957283\n",
      "Iteration 1391, loss = 2.82936658\n",
      "Iteration 1392, loss = 2.82948558\n",
      "Iteration 1393, loss = 2.82906957\n",
      "Iteration 1394, loss = 2.82884781\n",
      "Iteration 1395, loss = 2.82881972\n",
      "Iteration 1396, loss = 2.82854787\n",
      "Iteration 1397, loss = 2.82832631\n",
      "Iteration 1398, loss = 2.82789955\n",
      "Iteration 1399, loss = 2.82765377\n",
      "Iteration 1400, loss = 2.82755801\n",
      "Iteration 1401, loss = 2.82738711\n",
      "Iteration 1402, loss = 2.82784219\n",
      "Iteration 1403, loss = 2.82731912\n",
      "Iteration 1404, loss = 2.82734829\n",
      "Iteration 1405, loss = 2.82652470\n",
      "Iteration 1406, loss = 2.82753095\n",
      "Iteration 1407, loss = 2.82681414\n",
      "Iteration 1408, loss = 2.82627804\n",
      "Iteration 1409, loss = 2.82650358\n",
      "Iteration 1410, loss = 2.82609479\n",
      "Iteration 1411, loss = 2.82548463\n",
      "Iteration 1412, loss = 2.82584681\n",
      "Iteration 1413, loss = 2.82561732\n",
      "Iteration 1414, loss = 2.82587515\n",
      "Iteration 1415, loss = 2.82534220\n",
      "Iteration 1416, loss = 2.82515129\n",
      "Iteration 1417, loss = 2.82486215\n",
      "Iteration 1418, loss = 2.82475894\n",
      "Iteration 1419, loss = 2.82465276\n",
      "Iteration 1420, loss = 2.82440367\n",
      "Iteration 1421, loss = 2.82446946\n",
      "Iteration 1422, loss = 2.82419531\n",
      "Iteration 1423, loss = 2.82410374\n",
      "Iteration 1424, loss = 2.82381963\n",
      "Iteration 1425, loss = 2.82388198\n",
      "Iteration 1426, loss = 2.82351590\n",
      "Iteration 1427, loss = 2.82339440\n",
      "Iteration 1428, loss = 2.82376161\n",
      "Iteration 1429, loss = 2.82400953\n",
      "Iteration 1430, loss = 2.82368136\n",
      "Iteration 1431, loss = 2.82332758\n",
      "Iteration 1432, loss = 2.82342507\n",
      "Iteration 1433, loss = 2.82260452\n",
      "Iteration 1434, loss = 2.82205002\n",
      "Iteration 1435, loss = 2.82285905\n",
      "Iteration 1436, loss = 2.82288662\n",
      "Iteration 1437, loss = 2.82197610\n",
      "Iteration 1438, loss = 2.82142010\n",
      "Iteration 1439, loss = 2.82149710\n",
      "Iteration 1440, loss = 2.82106886\n",
      "Iteration 1441, loss = 2.82082140\n",
      "Iteration 1442, loss = 2.82135284\n",
      "Iteration 1443, loss = 2.82069728\n",
      "Iteration 1444, loss = 2.82073131\n",
      "Iteration 1445, loss = 2.82020421\n",
      "Iteration 1446, loss = 2.81967588\n",
      "Iteration 1447, loss = 2.82022616\n",
      "Iteration 1448, loss = 2.81939514\n",
      "Iteration 1449, loss = 2.81945404\n",
      "Iteration 1450, loss = 2.82009429\n",
      "Iteration 1451, loss = 2.81947978\n",
      "Iteration 1452, loss = 2.81885091\n",
      "Iteration 1453, loss = 2.81907548\n",
      "Iteration 1454, loss = 2.81941750\n",
      "Iteration 1455, loss = 2.81881564\n",
      "Iteration 1456, loss = 2.81907018\n",
      "Iteration 1457, loss = 2.81843172\n",
      "Iteration 1458, loss = 2.81776056\n",
      "Iteration 1459, loss = 2.81814757\n",
      "Iteration 1460, loss = 2.81770098\n",
      "Iteration 1461, loss = 2.81740819\n",
      "Iteration 1462, loss = 2.81769125\n",
      "Iteration 1463, loss = 2.81780507\n",
      "Iteration 1464, loss = 2.81695822\n",
      "Iteration 1465, loss = 2.81708064\n",
      "Iteration 1466, loss = 2.81671222\n",
      "Iteration 1467, loss = 2.81646167\n",
      "Iteration 1468, loss = 2.81644364\n",
      "Iteration 1469, loss = 2.81642760\n",
      "Iteration 1470, loss = 2.81632307\n",
      "Iteration 1471, loss = 2.81574234\n",
      "Iteration 1472, loss = 2.81609737\n",
      "Iteration 1473, loss = 2.81575191\n",
      "Iteration 1474, loss = 2.81578588\n",
      "Iteration 1475, loss = 2.81566048\n",
      "Iteration 1476, loss = 2.81599339\n",
      "Iteration 1477, loss = 2.81545703\n",
      "Iteration 1478, loss = 2.81441646\n",
      "Iteration 1479, loss = 2.81459170\n",
      "Iteration 1480, loss = 2.81411096\n",
      "Iteration 1481, loss = 2.81462702\n",
      "Iteration 1482, loss = 2.81437497\n",
      "Iteration 1483, loss = 2.81422295\n",
      "Iteration 1484, loss = 2.81378183\n",
      "Iteration 1485, loss = 2.81394483\n",
      "Iteration 1486, loss = 2.81379358\n",
      "Iteration 1487, loss = 2.81349436\n",
      "Iteration 1488, loss = 2.81271780\n",
      "Iteration 1489, loss = 2.81273276\n",
      "Iteration 1490, loss = 2.81267220\n",
      "Iteration 1491, loss = 2.81345335\n",
      "Iteration 1492, loss = 2.81266840\n",
      "Iteration 1493, loss = 2.81216769\n",
      "Iteration 1494, loss = 2.81197235\n",
      "Iteration 1495, loss = 2.81213881\n",
      "Iteration 1496, loss = 2.81167195\n",
      "Iteration 1497, loss = 2.81164339\n",
      "Iteration 1498, loss = 2.81143822\n",
      "Iteration 1499, loss = 2.81120316\n",
      "Iteration 1500, loss = 2.81226296\n",
      "Iteration 1501, loss = 2.81167663\n",
      "Iteration 1502, loss = 2.81125583\n",
      "Iteration 1503, loss = 2.81150406\n",
      "Iteration 1504, loss = 2.81120825\n",
      "Iteration 1505, loss = 2.81103026\n",
      "Iteration 1506, loss = 2.81100196\n",
      "Iteration 1507, loss = 2.81121154\n",
      "Iteration 1508, loss = 2.81135232\n",
      "Iteration 1509, loss = 2.81150788\n",
      "Iteration 1510, loss = 2.81026842\n",
      "Iteration 1511, loss = 2.80974925\n",
      "Iteration 1512, loss = 2.80947220\n",
      "Iteration 1513, loss = 2.80932802\n",
      "Iteration 1514, loss = 2.80931316\n",
      "Iteration 1515, loss = 2.80910734\n",
      "Iteration 1516, loss = 2.80843859\n",
      "Iteration 1517, loss = 2.80835802\n",
      "Iteration 1518, loss = 2.80911055\n",
      "Iteration 1519, loss = 2.80796607\n",
      "Iteration 1520, loss = 2.80774321\n",
      "Iteration 1521, loss = 2.80785419\n",
      "Iteration 1522, loss = 2.80780582\n",
      "Iteration 1523, loss = 2.80734817\n",
      "Iteration 1524, loss = 2.80746004\n",
      "Iteration 1525, loss = 2.80737914\n",
      "Iteration 1526, loss = 2.80649361\n",
      "Iteration 1527, loss = 2.80660387\n",
      "Iteration 1528, loss = 2.80649524\n",
      "Iteration 1529, loss = 2.80684307\n",
      "Iteration 1530, loss = 2.80729386\n",
      "Iteration 1531, loss = 2.80595427\n",
      "Iteration 1532, loss = 2.80590130\n",
      "Iteration 1533, loss = 2.80557790\n",
      "Iteration 1534, loss = 2.80549581\n",
      "Iteration 1535, loss = 2.80538933\n",
      "Iteration 1536, loss = 2.80498580\n",
      "Iteration 1537, loss = 2.80531060\n",
      "Iteration 1538, loss = 2.80494778\n",
      "Iteration 1539, loss = 2.80506600\n",
      "Iteration 1540, loss = 2.80676434\n",
      "Iteration 1541, loss = 2.80507950\n",
      "Iteration 1542, loss = 2.80479292\n",
      "Iteration 1543, loss = 2.80497569\n",
      "Iteration 1544, loss = 2.80406972\n",
      "Iteration 1545, loss = 2.80395155\n",
      "Iteration 1546, loss = 2.80342412\n",
      "Iteration 1547, loss = 2.80321015\n",
      "Iteration 1548, loss = 2.80399528\n",
      "Iteration 1549, loss = 2.80364100\n",
      "Iteration 1550, loss = 2.80286223\n",
      "Iteration 1551, loss = 2.80270254\n",
      "Iteration 1552, loss = 2.80384348\n",
      "Iteration 1553, loss = 2.80360385\n",
      "Iteration 1554, loss = 2.80288875\n",
      "Iteration 1555, loss = 2.80252172\n",
      "Iteration 1556, loss = 2.80272032\n",
      "Iteration 1557, loss = 2.80218866\n",
      "Iteration 1558, loss = 2.80201564\n",
      "Iteration 1559, loss = 2.80154201\n",
      "Iteration 1560, loss = 2.80229141\n",
      "Iteration 1561, loss = 2.80214784\n",
      "Iteration 1562, loss = 2.80219792\n",
      "Iteration 1563, loss = 2.80231890\n",
      "Iteration 1564, loss = 2.80114654\n",
      "Iteration 1565, loss = 2.80068207\n",
      "Iteration 1566, loss = 2.80120092\n",
      "Iteration 1567, loss = 2.80048444\n",
      "Iteration 1568, loss = 2.80081735\n",
      "Iteration 1569, loss = 2.80055643\n",
      "Iteration 1570, loss = 2.80104502\n",
      "Iteration 1571, loss = 2.80096323\n",
      "Iteration 1572, loss = 2.80165339\n",
      "Iteration 1573, loss = 2.80034907\n",
      "Iteration 1574, loss = 2.79989219\n",
      "Iteration 1575, loss = 2.80005437\n",
      "Iteration 1576, loss = 2.79990583\n",
      "Iteration 1577, loss = 2.79946053\n",
      "Iteration 1578, loss = 2.79899145\n",
      "Iteration 1579, loss = 2.79876958\n",
      "Iteration 1580, loss = 2.79828734\n",
      "Iteration 1581, loss = 2.79872763\n",
      "Iteration 1582, loss = 2.79951484\n",
      "Iteration 1583, loss = 2.79862927\n",
      "Iteration 1584, loss = 2.79882006\n",
      "Iteration 1585, loss = 2.79905251\n",
      "Iteration 1586, loss = 2.79838471\n",
      "Iteration 1587, loss = 2.79823719\n",
      "Iteration 1588, loss = 2.79802571\n",
      "Iteration 1589, loss = 2.79729432\n",
      "Iteration 1590, loss = 2.79690478\n",
      "Iteration 1591, loss = 2.79781665\n",
      "Iteration 1592, loss = 2.79805711\n",
      "Iteration 1593, loss = 2.79744445\n",
      "Iteration 1594, loss = 2.79644322\n",
      "Iteration 1595, loss = 2.79633922\n",
      "Iteration 1596, loss = 2.79633104\n",
      "Iteration 1597, loss = 2.79661495\n",
      "Iteration 1598, loss = 2.79595796\n",
      "Iteration 1599, loss = 2.79604579\n",
      "Iteration 1600, loss = 2.79589328\n",
      "Iteration 1601, loss = 2.79616751\n",
      "Iteration 1602, loss = 2.79687598\n",
      "Iteration 1603, loss = 2.79550972\n",
      "Iteration 1604, loss = 2.79568383\n",
      "Iteration 1605, loss = 2.79534442\n",
      "Iteration 1606, loss = 2.79571994\n",
      "Iteration 1607, loss = 2.79703234\n",
      "Iteration 1608, loss = 2.79582833\n",
      "Iteration 1609, loss = 2.79444076\n",
      "Iteration 1610, loss = 2.79471275\n",
      "Iteration 1611, loss = 2.79488179\n",
      "Iteration 1612, loss = 2.79375319\n",
      "Iteration 1613, loss = 2.79479982\n",
      "Iteration 1614, loss = 2.79370341\n",
      "Iteration 1615, loss = 2.79414600\n",
      "Iteration 1616, loss = 2.79327114\n",
      "Iteration 1617, loss = 2.79348202\n",
      "Iteration 1618, loss = 2.79286136\n",
      "Iteration 1619, loss = 2.79291989\n",
      "Iteration 1620, loss = 2.79336634\n",
      "Iteration 1621, loss = 2.79387167\n",
      "Iteration 1622, loss = 2.79308717\n",
      "Iteration 1623, loss = 2.79294096\n",
      "Iteration 1624, loss = 2.79214038\n",
      "Iteration 1625, loss = 2.79178716\n",
      "Iteration 1626, loss = 2.79157481\n",
      "Iteration 1627, loss = 2.79144325\n",
      "Iteration 1628, loss = 2.79143813\n",
      "Iteration 1629, loss = 2.79175902\n",
      "Iteration 1630, loss = 2.79189386\n",
      "Iteration 1631, loss = 2.79096148\n",
      "Iteration 1632, loss = 2.79076363\n",
      "Iteration 1633, loss = 2.79046129\n",
      "Iteration 1634, loss = 2.79143618\n",
      "Iteration 1635, loss = 2.79127299\n",
      "Iteration 1636, loss = 2.79034281\n",
      "Iteration 1637, loss = 2.79059912\n",
      "Iteration 1638, loss = 2.79027345\n",
      "Iteration 1639, loss = 2.78976344\n",
      "Iteration 1640, loss = 2.78936547\n",
      "Iteration 1641, loss = 2.78987070\n",
      "Iteration 1642, loss = 2.78942620\n",
      "Iteration 1643, loss = 2.78952781\n",
      "Iteration 1644, loss = 2.78969890\n",
      "Iteration 1645, loss = 2.78966993\n",
      "Iteration 1646, loss = 2.78888989\n",
      "Iteration 1647, loss = 2.78899405\n",
      "Iteration 1648, loss = 2.78915429\n",
      "Iteration 1649, loss = 2.78945748\n",
      "Iteration 1650, loss = 2.78777054\n",
      "Iteration 1651, loss = 2.78836069\n",
      "Iteration 1652, loss = 2.78796339\n",
      "Iteration 1653, loss = 2.78777007\n",
      "Iteration 1654, loss = 2.78796713\n",
      "Iteration 1655, loss = 2.78801170\n",
      "Iteration 1656, loss = 2.78738431\n",
      "Iteration 1657, loss = 2.78717643\n",
      "Iteration 1658, loss = 2.78682440\n",
      "Iteration 1659, loss = 2.78693176\n",
      "Iteration 1660, loss = 2.78664287\n",
      "Iteration 1661, loss = 2.78656206\n",
      "Iteration 1662, loss = 2.78658416\n",
      "Iteration 1663, loss = 2.78752061\n",
      "Iteration 1664, loss = 2.78698779\n",
      "Iteration 1665, loss = 2.78606905\n",
      "Iteration 1666, loss = 2.78604177\n",
      "Iteration 1667, loss = 2.78571272\n",
      "Iteration 1668, loss = 2.78578528\n",
      "Iteration 1669, loss = 2.78557648\n",
      "Iteration 1670, loss = 2.78559305\n",
      "Iteration 1671, loss = 2.78550845\n",
      "Iteration 1672, loss = 2.78499631\n",
      "Iteration 1673, loss = 2.78485299\n",
      "Iteration 1674, loss = 2.78472974\n",
      "Iteration 1675, loss = 2.78406203\n",
      "Iteration 1676, loss = 2.78393668\n",
      "Iteration 1677, loss = 2.78359248\n",
      "Iteration 1678, loss = 2.78398164\n",
      "Iteration 1679, loss = 2.78351271\n",
      "Iteration 1680, loss = 2.78352483\n",
      "Iteration 1681, loss = 2.78340444\n",
      "Iteration 1682, loss = 2.78381633\n",
      "Iteration 1683, loss = 2.78319997\n",
      "Iteration 1684, loss = 2.78336942\n",
      "Iteration 1685, loss = 2.78329859\n",
      "Iteration 1686, loss = 2.78344327\n",
      "Iteration 1687, loss = 2.78262706\n",
      "Iteration 1688, loss = 2.78292259\n",
      "Iteration 1689, loss = 2.78322337\n",
      "Iteration 1690, loss = 2.78213056\n",
      "Iteration 1691, loss = 2.78249687\n",
      "Iteration 1692, loss = 2.78245749\n",
      "Iteration 1693, loss = 2.78171288\n",
      "Iteration 1694, loss = 2.78247435\n",
      "Iteration 1695, loss = 2.78210617\n",
      "Iteration 1696, loss = 2.78355737\n",
      "Iteration 1697, loss = 2.78292863\n",
      "Iteration 1698, loss = 2.78212421\n",
      "Iteration 1699, loss = 2.78244650\n",
      "Iteration 1700, loss = 2.78227579\n",
      "Iteration 1701, loss = 2.78173965\n",
      "Iteration 1702, loss = 2.78117902\n",
      "Iteration 1703, loss = 2.78135470\n",
      "Iteration 1704, loss = 2.78117706\n",
      "Iteration 1705, loss = 2.78033744\n",
      "Iteration 1706, loss = 2.78075713\n",
      "Iteration 1707, loss = 2.78073818\n",
      "Iteration 1708, loss = 2.77931408\n",
      "Iteration 1709, loss = 2.77984544\n",
      "Iteration 1710, loss = 2.77942572\n",
      "Iteration 1711, loss = 2.78059052\n",
      "Iteration 1712, loss = 2.77977926\n",
      "Iteration 1713, loss = 2.77917850\n",
      "Iteration 1714, loss = 2.77889940\n",
      "Iteration 1715, loss = 2.77871779\n",
      "Iteration 1716, loss = 2.77812418\n",
      "Iteration 1717, loss = 2.77844995\n",
      "Iteration 1718, loss = 2.77848396\n",
      "Iteration 1719, loss = 2.77819512\n",
      "Iteration 1720, loss = 2.77805148\n",
      "Iteration 1721, loss = 2.77788178\n",
      "Iteration 1722, loss = 2.77910759\n",
      "Iteration 1723, loss = 2.77834748\n",
      "Iteration 1724, loss = 2.77755437\n",
      "Iteration 1725, loss = 2.77729339\n",
      "Iteration 1726, loss = 2.77770174\n",
      "Iteration 1727, loss = 2.77784024\n",
      "Iteration 1728, loss = 2.77785076\n",
      "Iteration 1729, loss = 2.77755045\n",
      "Iteration 1730, loss = 2.77665925\n",
      "Iteration 1731, loss = 2.77623084\n",
      "Iteration 1732, loss = 2.77637385\n",
      "Iteration 1733, loss = 2.77673136\n",
      "Iteration 1734, loss = 2.77683990\n",
      "Iteration 1735, loss = 2.77728319\n",
      "Iteration 1736, loss = 2.77699057\n",
      "Iteration 1737, loss = 2.77635124\n",
      "Iteration 1738, loss = 2.77649615\n",
      "Iteration 1739, loss = 2.77598829\n",
      "Iteration 1740, loss = 2.77559993\n",
      "Iteration 1741, loss = 2.77539153\n",
      "Iteration 1742, loss = 2.77475901\n",
      "Iteration 1743, loss = 2.77510009\n",
      "Iteration 1744, loss = 2.77547388\n",
      "Iteration 1745, loss = 2.77588614\n",
      "Iteration 1746, loss = 2.77519806\n",
      "Iteration 1747, loss = 2.77404166\n",
      "Iteration 1748, loss = 2.77384545\n",
      "Iteration 1749, loss = 2.77387475\n",
      "Iteration 1750, loss = 2.77377227\n",
      "Iteration 1751, loss = 2.77362205\n",
      "Iteration 1752, loss = 2.77508780\n",
      "Iteration 1753, loss = 2.77594230\n",
      "Iteration 1754, loss = 2.77326982\n",
      "Iteration 1755, loss = 2.77418591\n",
      "Iteration 1756, loss = 2.77327304\n",
      "Iteration 1757, loss = 2.77301021\n",
      "Iteration 1758, loss = 2.77312701\n",
      "Iteration 1759, loss = 2.77299667\n",
      "Iteration 1760, loss = 2.77368485\n",
      "Iteration 1761, loss = 2.77316092\n",
      "Iteration 1762, loss = 2.77273264\n",
      "Iteration 1763, loss = 2.77239164\n",
      "Iteration 1764, loss = 2.77401060\n",
      "Iteration 1765, loss = 2.77355842\n",
      "Iteration 1766, loss = 2.77222207\n",
      "Iteration 1767, loss = 2.77127136\n",
      "Iteration 1768, loss = 2.77115840\n",
      "Iteration 1769, loss = 2.77099892\n",
      "Iteration 1770, loss = 2.77110232\n",
      "Iteration 1771, loss = 2.77130525\n",
      "Iteration 1772, loss = 2.77097582\n",
      "Iteration 1773, loss = 2.77190062\n",
      "Iteration 1774, loss = 2.77142265\n",
      "Iteration 1775, loss = 2.77098692\n",
      "Iteration 1776, loss = 2.77035759\n",
      "Iteration 1777, loss = 2.76998642\n",
      "Iteration 1778, loss = 2.77086613\n",
      "Iteration 1779, loss = 2.77022349\n",
      "Iteration 1780, loss = 2.77035954\n",
      "Iteration 1781, loss = 2.76953481\n",
      "Iteration 1782, loss = 2.77042062\n",
      "Iteration 1783, loss = 2.76940335\n",
      "Iteration 1784, loss = 2.77065915\n",
      "Iteration 1785, loss = 2.77074006\n",
      "Iteration 1786, loss = 2.76912413\n",
      "Iteration 1787, loss = 2.76876561\n",
      "Iteration 1788, loss = 2.77011830\n",
      "Iteration 1789, loss = 2.76904628\n",
      "Iteration 1790, loss = 2.76780118\n",
      "Iteration 1791, loss = 2.76961581\n",
      "Iteration 1792, loss = 2.76963332\n",
      "Iteration 1793, loss = 2.76844987\n",
      "Iteration 1794, loss = 2.76790471\n",
      "Iteration 1795, loss = 2.76795984\n",
      "Iteration 1796, loss = 2.76789245\n",
      "Iteration 1797, loss = 2.76811906\n",
      "Iteration 1798, loss = 2.76865816\n",
      "Iteration 1799, loss = 2.76781843\n",
      "Iteration 1800, loss = 2.76827936\n",
      "Iteration 1801, loss = 2.76730804\n",
      "Iteration 1802, loss = 2.76644227\n",
      "Iteration 1803, loss = 2.76688025\n",
      "Iteration 1804, loss = 2.76731963\n",
      "Iteration 1805, loss = 2.76761521\n",
      "Iteration 1806, loss = 2.76722185\n",
      "Iteration 1807, loss = 2.76657024\n",
      "Iteration 1808, loss = 2.76647824\n",
      "Iteration 1809, loss = 2.76594059\n",
      "Iteration 1810, loss = 2.76729287\n",
      "Iteration 1811, loss = 2.76713590\n",
      "Iteration 1812, loss = 2.76641819\n",
      "Iteration 1813, loss = 2.76526675\n",
      "Iteration 1814, loss = 2.76633588\n",
      "Iteration 1815, loss = 2.76539822\n",
      "Iteration 1816, loss = 2.76430531\n",
      "Iteration 1817, loss = 2.76454509\n",
      "Iteration 1818, loss = 2.76454984\n",
      "Iteration 1819, loss = 2.76436871\n",
      "Iteration 1820, loss = 2.76482709\n",
      "Iteration 1821, loss = 2.76435472\n",
      "Iteration 1822, loss = 2.76409591\n",
      "Iteration 1823, loss = 2.76398776\n",
      "Iteration 1824, loss = 2.76376817\n",
      "Iteration 1825, loss = 2.76374818\n",
      "Iteration 1826, loss = 2.76369858\n",
      "Iteration 1827, loss = 2.76386854\n",
      "Iteration 1828, loss = 2.76402740\n",
      "Iteration 1829, loss = 2.76352485\n",
      "Iteration 1830, loss = 2.76325069\n",
      "Iteration 1831, loss = 2.76373542\n",
      "Iteration 1832, loss = 2.76363393\n",
      "Iteration 1833, loss = 2.76317790\n",
      "Iteration 1834, loss = 2.76302021\n",
      "Iteration 1835, loss = 2.76225415\n",
      "Iteration 1836, loss = 2.76194188\n",
      "Iteration 1837, loss = 2.76183013\n",
      "Iteration 1838, loss = 2.76238436\n",
      "Iteration 1839, loss = 2.76214042\n",
      "Iteration 1840, loss = 2.76144578\n",
      "Iteration 1841, loss = 2.76201062\n",
      "Iteration 1842, loss = 2.76183960\n",
      "Iteration 1843, loss = 2.76267841\n",
      "Iteration 1844, loss = 2.76174243\n",
      "Iteration 1845, loss = 2.76132271\n",
      "Iteration 1846, loss = 2.76214820\n",
      "Iteration 1847, loss = 2.76132635\n",
      "Iteration 1848, loss = 2.76014720\n",
      "Iteration 1849, loss = 2.76077855\n",
      "Iteration 1850, loss = 2.76164060\n",
      "Iteration 1851, loss = 2.76022661\n",
      "Iteration 1852, loss = 2.76000277\n",
      "Iteration 1853, loss = 2.76077599\n",
      "Iteration 1854, loss = 2.76060211\n",
      "Iteration 1855, loss = 2.76066025\n",
      "Iteration 1856, loss = 2.76048764\n",
      "Iteration 1857, loss = 2.76037944\n",
      "Iteration 1858, loss = 2.75950279\n",
      "Iteration 1859, loss = 2.75929188\n",
      "Iteration 1860, loss = 2.75870252\n",
      "Iteration 1861, loss = 2.75860605\n",
      "Iteration 1862, loss = 2.75872434\n",
      "Iteration 1863, loss = 2.75887823\n",
      "Iteration 1864, loss = 2.75850486\n",
      "Iteration 1865, loss = 2.75853799\n",
      "Iteration 1866, loss = 2.75829607\n",
      "Iteration 1867, loss = 2.75820582\n",
      "Iteration 1868, loss = 2.75804357\n",
      "Iteration 1869, loss = 2.75804429\n",
      "Iteration 1870, loss = 2.75801179\n",
      "Iteration 1871, loss = 2.75740497\n",
      "Iteration 1872, loss = 2.75761992\n",
      "Iteration 1873, loss = 2.75825071\n",
      "Iteration 1874, loss = 2.75777778\n",
      "Iteration 1875, loss = 2.75731069\n",
      "Iteration 1876, loss = 2.75746102\n",
      "Iteration 1877, loss = 2.75735568\n",
      "Iteration 1878, loss = 2.75739878\n",
      "Iteration 1879, loss = 2.75667680\n",
      "Iteration 1880, loss = 2.75671753\n",
      "Iteration 1881, loss = 2.75636406\n",
      "Iteration 1882, loss = 2.75623817\n",
      "Iteration 1883, loss = 2.75627336\n",
      "Iteration 1884, loss = 2.75643659\n",
      "Iteration 1885, loss = 2.75642969\n",
      "Iteration 1886, loss = 2.75610428\n",
      "Iteration 1887, loss = 2.75631409\n",
      "Iteration 1888, loss = 2.75588720\n",
      "Iteration 1889, loss = 2.75590351\n",
      "Iteration 1890, loss = 2.75543521\n",
      "Iteration 1891, loss = 2.75540451\n",
      "Iteration 1892, loss = 2.75610987\n",
      "Iteration 1893, loss = 2.75548704\n",
      "Iteration 1894, loss = 2.75484706\n",
      "Iteration 1895, loss = 2.75509226\n",
      "Iteration 1896, loss = 2.75500934\n",
      "Iteration 1897, loss = 2.75534189\n",
      "Iteration 1898, loss = 2.75438274\n",
      "Iteration 1899, loss = 2.75415197\n",
      "Iteration 1900, loss = 2.75373450\n",
      "Iteration 1901, loss = 2.75365982\n",
      "Iteration 1902, loss = 2.75337876\n",
      "Iteration 1903, loss = 2.75329911\n",
      "Iteration 1904, loss = 2.75378507\n",
      "Iteration 1905, loss = 2.75408160\n",
      "Iteration 1906, loss = 2.75380881\n",
      "Iteration 1907, loss = 2.75291422\n",
      "Iteration 1908, loss = 2.75256053\n",
      "Iteration 1909, loss = 2.75282707\n",
      "Iteration 1910, loss = 2.75331801\n",
      "Iteration 1911, loss = 2.75239234\n",
      "Iteration 1912, loss = 2.75287420\n",
      "Iteration 1913, loss = 2.75323999\n",
      "Iteration 1914, loss = 2.75272960\n",
      "Iteration 1915, loss = 2.75222360\n",
      "Iteration 1916, loss = 2.75248702\n",
      "Iteration 1917, loss = 2.75214750\n",
      "Iteration 1918, loss = 2.75210033\n",
      "Iteration 1919, loss = 2.75151327\n",
      "Iteration 1920, loss = 2.75252269\n",
      "Iteration 1921, loss = 2.75187169\n",
      "Iteration 1922, loss = 2.75147536\n",
      "Iteration 1923, loss = 2.75135709\n",
      "Iteration 1924, loss = 2.75065891\n",
      "Iteration 1925, loss = 2.75149625\n",
      "Iteration 1926, loss = 2.75195348\n",
      "Iteration 1927, loss = 2.75111831\n",
      "Iteration 1928, loss = 2.75053840\n",
      "Iteration 1929, loss = 2.75040910\n",
      "Iteration 1930, loss = 2.75003831\n",
      "Iteration 1931, loss = 2.74978184\n",
      "Iteration 1932, loss = 2.74935424\n",
      "Iteration 1933, loss = 2.74964972\n",
      "Iteration 1934, loss = 2.75071305\n",
      "Iteration 1935, loss = 2.74975159\n",
      "Iteration 1936, loss = 2.74918384\n",
      "Iteration 1937, loss = 2.74921189\n",
      "Iteration 1938, loss = 2.74904261\n",
      "Iteration 1939, loss = 2.74922793\n",
      "Iteration 1940, loss = 2.74898558\n",
      "Iteration 1941, loss = 2.74931968\n",
      "Iteration 1942, loss = 2.74855232\n",
      "Iteration 1943, loss = 2.74819733\n",
      "Iteration 1944, loss = 2.74794145\n",
      "Iteration 1945, loss = 2.74913214\n",
      "Iteration 1946, loss = 2.74896609\n",
      "Iteration 1947, loss = 2.74854291\n",
      "Iteration 1948, loss = 2.74821739\n",
      "Iteration 1949, loss = 2.74833336\n",
      "Iteration 1950, loss = 2.74819691\n",
      "Iteration 1951, loss = 2.74836991\n",
      "Iteration 1952, loss = 2.74780606\n",
      "Iteration 1953, loss = 2.74739006\n",
      "Iteration 1954, loss = 2.74726939\n",
      "Iteration 1955, loss = 2.74774158\n",
      "Iteration 1956, loss = 2.74707114\n",
      "Iteration 1957, loss = 2.74718555\n",
      "Iteration 1958, loss = 2.74690883\n",
      "Iteration 1959, loss = 2.74648278\n",
      "Iteration 1960, loss = 2.74611847\n",
      "Iteration 1961, loss = 2.74619678\n",
      "Iteration 1962, loss = 2.74618883\n",
      "Iteration 1963, loss = 2.74627404\n",
      "Iteration 1964, loss = 2.74576848\n",
      "Iteration 1965, loss = 2.74629342\n",
      "Iteration 1966, loss = 2.74554452\n",
      "Iteration 1967, loss = 2.74565552\n",
      "Iteration 1968, loss = 2.74504088\n",
      "Iteration 1969, loss = 2.74573365\n",
      "Iteration 1970, loss = 2.74549744\n",
      "Iteration 1971, loss = 2.74551971\n",
      "Iteration 1972, loss = 2.74491397\n",
      "Iteration 1973, loss = 2.74568131\n",
      "Iteration 1974, loss = 2.74470583\n",
      "Iteration 1975, loss = 2.74529320\n",
      "Iteration 1976, loss = 2.74515916\n",
      "Iteration 1977, loss = 2.74415149\n",
      "Iteration 1978, loss = 2.74437207\n",
      "Iteration 1979, loss = 2.74437888\n",
      "Iteration 1980, loss = 2.74531709\n",
      "Iteration 1981, loss = 2.74515004\n",
      "Iteration 1982, loss = 2.74412572\n",
      "Iteration 1983, loss = 2.74341778\n",
      "Iteration 1984, loss = 2.74375092\n",
      "Iteration 1985, loss = 2.74418956\n",
      "Iteration 1986, loss = 2.74407200\n",
      "Iteration 1987, loss = 2.74395355\n",
      "Iteration 1988, loss = 2.74330388\n",
      "Iteration 1989, loss = 2.74309105\n",
      "Iteration 1990, loss = 2.74307339\n",
      "Iteration 1991, loss = 2.74246029\n",
      "Iteration 1992, loss = 2.74276532\n",
      "Iteration 1993, loss = 2.74230248\n",
      "Iteration 1994, loss = 2.74290867\n",
      "Iteration 1995, loss = 2.74263934\n",
      "Iteration 1996, loss = 2.74243973\n",
      "Iteration 1997, loss = 2.74258725\n",
      "Iteration 1998, loss = 2.74280918\n",
      "Iteration 1999, loss = 2.74298188\n",
      "Iteration 2000, loss = 2.74195169\n",
      "Iteration 2001, loss = 2.74149345\n",
      "Iteration 2002, loss = 2.74175900\n",
      "Iteration 2003, loss = 2.74130101\n",
      "Iteration 2004, loss = 2.74124472\n",
      "Iteration 2005, loss = 2.74081711\n",
      "Iteration 2006, loss = 2.74129537\n",
      "Iteration 2007, loss = 2.74015525\n",
      "Iteration 2008, loss = 2.74010898\n",
      "Iteration 2009, loss = 2.74031877\n",
      "Iteration 2010, loss = 2.74066197\n",
      "Iteration 2011, loss = 2.74049828\n",
      "Iteration 2012, loss = 2.74105643\n",
      "Iteration 2013, loss = 2.73999906\n",
      "Iteration 2014, loss = 2.74046185\n",
      "Iteration 2015, loss = 2.74069765\n",
      "Iteration 2016, loss = 2.74046243\n",
      "Iteration 2017, loss = 2.74013746\n",
      "Iteration 2018, loss = 2.73926049\n",
      "Iteration 2019, loss = 2.74043757\n",
      "Iteration 2020, loss = 2.73960863\n",
      "Iteration 2021, loss = 2.74012115\n",
      "Iteration 2022, loss = 2.74008878\n",
      "Iteration 2023, loss = 2.73927241\n",
      "Iteration 2024, loss = 2.73916986\n",
      "Iteration 2025, loss = 2.73803448\n",
      "Iteration 2026, loss = 2.73893159\n",
      "Iteration 2027, loss = 2.73865809\n",
      "Iteration 2028, loss = 2.73895440\n",
      "Iteration 2029, loss = 2.73926969\n",
      "Iteration 2030, loss = 2.73889459\n",
      "Iteration 2031, loss = 2.73867058\n",
      "Iteration 2032, loss = 2.73781448\n",
      "Iteration 2033, loss = 2.73849857\n",
      "Iteration 2034, loss = 2.73807812\n",
      "Iteration 2035, loss = 2.73764394\n",
      "Iteration 2036, loss = 2.73713292\n",
      "Iteration 2037, loss = 2.73668440\n",
      "Iteration 2038, loss = 2.73793083\n",
      "Iteration 2039, loss = 2.73746245\n",
      "Iteration 2040, loss = 2.73670466\n",
      "Iteration 2041, loss = 2.73791298\n",
      "Iteration 2042, loss = 2.73872848\n",
      "Iteration 2043, loss = 2.73784627\n",
      "Iteration 2044, loss = 2.73680435\n",
      "Iteration 2045, loss = 2.73686883\n",
      "Iteration 2046, loss = 2.73639699\n",
      "Iteration 2047, loss = 2.73647518\n",
      "Iteration 2048, loss = 2.73706724\n",
      "Iteration 2049, loss = 2.73666051\n",
      "Iteration 2050, loss = 2.73594796\n",
      "Iteration 2051, loss = 2.73634897\n",
      "Iteration 2052, loss = 2.73546486\n",
      "Iteration 2053, loss = 2.73533278\n",
      "Iteration 2054, loss = 2.73622687\n",
      "Iteration 2055, loss = 2.73502835\n",
      "Iteration 2056, loss = 2.73565051\n",
      "Iteration 2057, loss = 2.73601657\n",
      "Iteration 2058, loss = 2.73536027\n",
      "Iteration 2059, loss = 2.73502221\n",
      "Iteration 2060, loss = 2.73517084\n",
      "Iteration 2061, loss = 2.73484969\n",
      "Iteration 2062, loss = 2.73468525\n",
      "Iteration 2063, loss = 2.73428802\n",
      "Iteration 2064, loss = 2.73464556\n",
      "Iteration 2065, loss = 2.73489069\n",
      "Iteration 2066, loss = 2.73450630\n",
      "Iteration 2067, loss = 2.73415428\n",
      "Iteration 2068, loss = 2.73460035\n",
      "Iteration 2069, loss = 2.73398175\n",
      "Iteration 2070, loss = 2.73385153\n",
      "Iteration 2071, loss = 2.73381144\n",
      "Iteration 2072, loss = 2.73337262\n",
      "Iteration 2073, loss = 2.73246150\n",
      "Iteration 2074, loss = 2.73284740\n",
      "Iteration 2075, loss = 2.73197353\n",
      "Iteration 2076, loss = 2.73233545\n",
      "Iteration 2077, loss = 2.73200218\n",
      "Iteration 2078, loss = 2.73221802\n",
      "Iteration 2079, loss = 2.73224518\n",
      "Iteration 2080, loss = 2.73221747\n",
      "Iteration 2081, loss = 2.73277761\n",
      "Iteration 2082, loss = 2.73244382\n",
      "Iteration 2083, loss = 2.73283636\n",
      "Iteration 2084, loss = 2.73228264\n",
      "Iteration 2085, loss = 2.73177492\n",
      "Iteration 2086, loss = 2.73189281\n",
      "Iteration 2087, loss = 2.73226633\n",
      "Iteration 2088, loss = 2.73173911\n",
      "Iteration 2089, loss = 2.73213281\n",
      "Iteration 2090, loss = 2.73153433\n",
      "Iteration 2091, loss = 2.73077219\n",
      "Iteration 2092, loss = 2.73048771\n",
      "Iteration 2093, loss = 2.73198818\n",
      "Iteration 2094, loss = 2.73241317\n",
      "Iteration 2095, loss = 2.73191472\n",
      "Iteration 2096, loss = 2.73102140\n",
      "Iteration 2097, loss = 2.72985743\n",
      "Iteration 2098, loss = 2.72995428\n",
      "Iteration 2099, loss = 2.72964556\n",
      "Iteration 2100, loss = 2.73017908\n",
      "Iteration 2101, loss = 2.72943578\n",
      "Iteration 2102, loss = 2.72956894\n",
      "Iteration 2103, loss = 2.72952260\n",
      "Iteration 2104, loss = 2.73028386\n",
      "Iteration 2105, loss = 2.72999670\n",
      "Iteration 2106, loss = 2.72945022\n",
      "Iteration 2107, loss = 2.72946080\n",
      "Iteration 2108, loss = 2.72886241\n",
      "Iteration 2109, loss = 2.72884648\n",
      "Iteration 2110, loss = 2.72892644\n",
      "Iteration 2111, loss = 2.72900409\n",
      "Iteration 2112, loss = 2.72824247\n",
      "Iteration 2113, loss = 2.72821927\n",
      "Iteration 2114, loss = 2.72853506\n",
      "Iteration 2115, loss = 2.72869186\n",
      "Iteration 2116, loss = 2.72809716\n",
      "Iteration 2117, loss = 2.72834189\n",
      "Iteration 2118, loss = 2.72887627\n",
      "Iteration 2119, loss = 2.72838178\n",
      "Iteration 2120, loss = 2.72874870\n",
      "Iteration 2121, loss = 2.72818703\n",
      "Iteration 2122, loss = 2.72773835\n",
      "Iteration 2123, loss = 2.72700826\n",
      "Iteration 2124, loss = 2.72725198\n",
      "Iteration 2125, loss = 2.72701475\n",
      "Iteration 2126, loss = 2.72743988\n",
      "Iteration 2127, loss = 2.72673923\n",
      "Iteration 2128, loss = 2.72656861\n",
      "Iteration 2129, loss = 2.72766363\n",
      "Iteration 2130, loss = 2.72782817\n",
      "Iteration 2131, loss = 2.72647917\n",
      "Iteration 2132, loss = 2.72597044\n",
      "Iteration 2133, loss = 2.72563386\n",
      "Iteration 2134, loss = 2.72621789\n",
      "Iteration 2135, loss = 2.72566809\n",
      "Iteration 2136, loss = 2.72605615\n",
      "Iteration 2137, loss = 2.72677812\n",
      "Iteration 2138, loss = 2.72581205\n",
      "Iteration 2139, loss = 2.72628611\n",
      "Iteration 2140, loss = 2.72542855\n",
      "Iteration 2141, loss = 2.72461708\n",
      "Iteration 2142, loss = 2.72526497\n",
      "Iteration 2143, loss = 2.72514896\n",
      "Iteration 2144, loss = 2.72530520\n",
      "Iteration 2145, loss = 2.72462329\n",
      "Iteration 2146, loss = 2.72474262\n",
      "Iteration 2147, loss = 2.72468783\n",
      "Iteration 2148, loss = 2.72488839\n",
      "Iteration 2149, loss = 2.72467310\n",
      "Iteration 2150, loss = 2.72463734\n",
      "Iteration 2151, loss = 2.72460887\n",
      "Iteration 2152, loss = 2.72435387\n",
      "Iteration 2153, loss = 2.72547542\n",
      "Iteration 2154, loss = 2.72553589\n",
      "Iteration 2155, loss = 2.72366914\n",
      "Iteration 2156, loss = 2.72377298\n",
      "Iteration 2157, loss = 2.72365937\n",
      "Iteration 2158, loss = 2.72368032\n",
      "Iteration 2159, loss = 2.72474790\n",
      "Iteration 2160, loss = 2.72354682\n",
      "Iteration 2161, loss = 2.72267602\n",
      "Iteration 2162, loss = 2.72322237\n",
      "Iteration 2163, loss = 2.72342355\n",
      "Iteration 2164, loss = 2.72300806\n",
      "Iteration 2165, loss = 2.72232485\n",
      "Iteration 2166, loss = 2.72255073\n",
      "Iteration 2167, loss = 2.72252956\n",
      "Iteration 2168, loss = 2.72303070\n",
      "Iteration 2169, loss = 2.72289485\n",
      "Iteration 2170, loss = 2.72204744\n",
      "Iteration 2171, loss = 2.72180553\n",
      "Iteration 2172, loss = 2.72303533\n",
      "Iteration 2173, loss = 2.72195946\n",
      "Iteration 2174, loss = 2.72172986\n",
      "Iteration 2175, loss = 2.72155694\n",
      "Iteration 2176, loss = 2.72260559\n",
      "Iteration 2177, loss = 2.72179601\n",
      "Iteration 2178, loss = 2.72147590\n",
      "Iteration 2179, loss = 2.72204732\n",
      "Iteration 2180, loss = 2.72017577\n",
      "Iteration 2181, loss = 2.72096339\n",
      "Iteration 2182, loss = 2.72053285\n",
      "Iteration 2183, loss = 2.72253567\n",
      "Iteration 2184, loss = 2.72139588\n",
      "Iteration 2185, loss = 2.72061159\n",
      "Iteration 2186, loss = 2.72057847\n",
      "Iteration 2187, loss = 2.72080066\n",
      "Iteration 2188, loss = 2.72071657\n",
      "Iteration 2189, loss = 2.72040521\n",
      "Iteration 2190, loss = 2.71943543\n",
      "Iteration 2191, loss = 2.71950354\n",
      "Iteration 2192, loss = 2.72013142\n",
      "Iteration 2193, loss = 2.71919502\n",
      "Iteration 2194, loss = 2.71981701\n",
      "Iteration 2195, loss = 2.72099744\n",
      "Iteration 2196, loss = 2.71944653\n",
      "Iteration 2197, loss = 2.71908016\n",
      "Iteration 2198, loss = 2.71927508\n",
      "Iteration 2199, loss = 2.71898337\n",
      "Iteration 2200, loss = 2.71876151\n",
      "Iteration 2201, loss = 2.71804882\n",
      "Iteration 2202, loss = 2.71829413\n",
      "Iteration 2203, loss = 2.71808429\n",
      "Iteration 2204, loss = 2.71809186\n",
      "Iteration 2205, loss = 2.71875132\n",
      "Iteration 2206, loss = 2.71934600\n",
      "Iteration 2207, loss = 2.71792887\n",
      "Iteration 2208, loss = 2.71826205\n",
      "Iteration 2209, loss = 2.71878199\n",
      "Iteration 2210, loss = 2.71802595\n",
      "Iteration 2211, loss = 2.71709170\n",
      "Iteration 2212, loss = 2.71767549\n",
      "Iteration 2213, loss = 2.71699271\n",
      "Iteration 2214, loss = 2.71738647\n",
      "Iteration 2215, loss = 2.71772153\n",
      "Iteration 2216, loss = 2.71783208\n",
      "Iteration 2217, loss = 2.71710420\n",
      "Iteration 2218, loss = 2.71684169\n",
      "Iteration 2219, loss = 2.71647704\n",
      "Iteration 2220, loss = 2.71685498\n",
      "Iteration 2221, loss = 2.71710488\n",
      "Iteration 2222, loss = 2.71744114\n",
      "Iteration 2223, loss = 2.71662247\n",
      "Iteration 2224, loss = 2.71687428\n",
      "Iteration 2225, loss = 2.71646391\n",
      "Iteration 2226, loss = 2.71660923\n",
      "Iteration 2227, loss = 2.71607837\n",
      "Iteration 2228, loss = 2.71624069\n",
      "Iteration 2229, loss = 2.71649438\n",
      "Iteration 2230, loss = 2.71546814\n",
      "Iteration 2231, loss = 2.71515835\n",
      "Iteration 2232, loss = 2.71492976\n",
      "Iteration 2233, loss = 2.71491308\n",
      "Iteration 2234, loss = 2.71545251\n",
      "Iteration 2235, loss = 2.71538081\n",
      "Iteration 2236, loss = 2.71501315\n",
      "Iteration 2237, loss = 2.71486417\n",
      "Iteration 2238, loss = 2.71515609\n",
      "Iteration 2239, loss = 2.71429716\n",
      "Iteration 2240, loss = 2.71420049\n",
      "Iteration 2241, loss = 2.71403154\n",
      "Iteration 2242, loss = 2.71786469\n",
      "Iteration 2243, loss = 2.71632324\n",
      "Iteration 2244, loss = 2.71485289\n",
      "Iteration 2245, loss = 2.71525415\n",
      "Iteration 2246, loss = 2.71356306\n",
      "Iteration 2247, loss = 2.71396787\n",
      "Iteration 2248, loss = 2.71396903\n",
      "Iteration 2249, loss = 2.71357634\n",
      "Iteration 2250, loss = 2.71299120\n",
      "Iteration 2251, loss = 2.71341146\n",
      "Iteration 2252, loss = 2.71364988\n",
      "Iteration 2253, loss = 2.71361229\n",
      "Iteration 2254, loss = 2.71269990\n",
      "Iteration 2255, loss = 2.71286857\n",
      "Iteration 2256, loss = 2.71333452\n",
      "Iteration 2257, loss = 2.71229106\n",
      "Iteration 2258, loss = 2.71390661\n",
      "Iteration 2259, loss = 2.71533890\n",
      "Iteration 2260, loss = 2.71228955\n",
      "Iteration 2261, loss = 2.71256694\n",
      "Iteration 2262, loss = 2.71313921\n",
      "Iteration 2263, loss = 2.71295050\n",
      "Iteration 2264, loss = 2.71278911\n",
      "Iteration 2265, loss = 2.71202375\n",
      "Iteration 2266, loss = 2.71222833\n",
      "Iteration 2267, loss = 2.71244137\n",
      "Iteration 2268, loss = 2.71199356\n",
      "Iteration 2269, loss = 2.71181335\n",
      "Iteration 2270, loss = 2.71180452\n",
      "Iteration 2271, loss = 2.71129268\n",
      "Iteration 2272, loss = 2.71148659\n",
      "Iteration 2273, loss = 2.71110085\n",
      "Iteration 2274, loss = 2.71081407\n",
      "Iteration 2275, loss = 2.71066598\n",
      "Iteration 2276, loss = 2.71083630\n",
      "Iteration 2277, loss = 2.71055206\n",
      "Iteration 2278, loss = 2.71100837\n",
      "Iteration 2279, loss = 2.71061489\n",
      "Iteration 2280, loss = 2.71028317\n",
      "Iteration 2281, loss = 2.71098300\n",
      "Iteration 2282, loss = 2.71074675\n",
      "Iteration 2283, loss = 2.71011417\n",
      "Iteration 2284, loss = 2.70951119\n",
      "Iteration 2285, loss = 2.70988878\n",
      "Iteration 2286, loss = 2.71100996\n",
      "Iteration 2287, loss = 2.71056220\n",
      "Iteration 2288, loss = 2.70961403\n",
      "Iteration 2289, loss = 2.70967710\n",
      "Iteration 2290, loss = 2.70966851\n",
      "Iteration 2291, loss = 2.70915486\n",
      "Iteration 2292, loss = 2.71034515\n",
      "Iteration 2293, loss = 2.70961339\n",
      "Iteration 2294, loss = 2.70882481\n",
      "Iteration 2295, loss = 2.70870427\n",
      "Iteration 2296, loss = 2.70890872\n",
      "Iteration 2297, loss = 2.70855044\n",
      "Iteration 2298, loss = 2.70781353\n",
      "Iteration 2299, loss = 2.70846935\n",
      "Iteration 2300, loss = 2.70838570\n",
      "Iteration 2301, loss = 2.70832467\n",
      "Iteration 2302, loss = 2.70848899\n",
      "Iteration 2303, loss = 2.70818800\n",
      "Iteration 2304, loss = 2.70755220\n",
      "Iteration 2305, loss = 2.70777593\n",
      "Iteration 2306, loss = 2.70761612\n",
      "Iteration 2307, loss = 2.70776911\n",
      "Iteration 2308, loss = 2.70763841\n",
      "Iteration 2309, loss = 2.70898326\n",
      "Iteration 2310, loss = 2.70777771\n",
      "Iteration 2311, loss = 2.70768203\n",
      "Iteration 2312, loss = 2.70751094\n",
      "Iteration 2313, loss = 2.70777059\n",
      "Iteration 2314, loss = 2.70885469\n",
      "Iteration 2315, loss = 2.70787287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13205172\n",
      "Iteration 2, loss = 4.05247596\n",
      "Iteration 3, loss = 3.97674094\n",
      "Iteration 4, loss = 3.90282853\n",
      "Iteration 5, loss = 3.82929692\n",
      "Iteration 6, loss = 3.75703724\n",
      "Iteration 7, loss = 3.68463272\n",
      "Iteration 8, loss = 3.61414305\n",
      "Iteration 9, loss = 3.54663536\n",
      "Iteration 10, loss = 3.48051146\n",
      "Iteration 11, loss = 3.42085495\n",
      "Iteration 12, loss = 3.36749339\n",
      "Iteration 13, loss = 3.32458214\n",
      "Iteration 14, loss = 3.28781905\n",
      "Iteration 15, loss = 3.26200902\n",
      "Iteration 16, loss = 3.24414060\n",
      "Iteration 17, loss = 3.23434952\n",
      "Iteration 18, loss = 3.23037757\n",
      "Iteration 19, loss = 3.22812666\n",
      "Iteration 20, loss = 3.22656420\n",
      "Iteration 21, loss = 3.22512310\n",
      "Iteration 22, loss = 3.22438564\n",
      "Iteration 23, loss = 3.22332998\n",
      "Iteration 24, loss = 3.22281329\n",
      "Iteration 25, loss = 3.22213785\n",
      "Iteration 26, loss = 3.22157897\n",
      "Iteration 27, loss = 3.22083318\n",
      "Iteration 28, loss = 3.22037035\n",
      "Iteration 29, loss = 3.21948713\n",
      "Iteration 30, loss = 3.21914390\n",
      "Iteration 31, loss = 3.21862535\n",
      "Iteration 32, loss = 3.21820417\n",
      "Iteration 33, loss = 3.21794713\n",
      "Iteration 34, loss = 3.21788312\n",
      "Iteration 35, loss = 3.21710638\n",
      "Iteration 36, loss = 3.21680217\n",
      "Iteration 37, loss = 3.21664785\n",
      "Iteration 38, loss = 3.21622569\n",
      "Iteration 39, loss = 3.21592768\n",
      "Iteration 40, loss = 3.21589131\n",
      "Iteration 41, loss = 3.21552264\n",
      "Iteration 42, loss = 3.21548924\n",
      "Iteration 43, loss = 3.21501401\n",
      "Iteration 44, loss = 3.21436045\n",
      "Iteration 45, loss = 3.21414859\n",
      "Iteration 46, loss = 3.21374561\n",
      "Iteration 47, loss = 3.21374144\n",
      "Iteration 48, loss = 3.21366468\n",
      "Iteration 49, loss = 3.21319291\n",
      "Iteration 50, loss = 3.21281499\n",
      "Iteration 51, loss = 3.21214075\n",
      "Iteration 52, loss = 3.21197577\n",
      "Iteration 53, loss = 3.21179406\n",
      "Iteration 54, loss = 3.21164386\n",
      "Iteration 55, loss = 3.21162339\n",
      "Iteration 56, loss = 3.21138220\n",
      "Iteration 57, loss = 3.21108927\n",
      "Iteration 58, loss = 3.21053804\n",
      "Iteration 59, loss = 3.20990244\n",
      "Iteration 60, loss = 3.20988545\n",
      "Iteration 61, loss = 3.20981322\n",
      "Iteration 62, loss = 3.20993688\n",
      "Iteration 63, loss = 3.20956614\n",
      "Iteration 64, loss = 3.20964093\n",
      "Iteration 65, loss = 3.20952975\n",
      "Iteration 66, loss = 3.20887828\n",
      "Iteration 67, loss = 3.20809327\n",
      "Iteration 68, loss = 3.20781756\n",
      "Iteration 69, loss = 3.20791376\n",
      "Iteration 70, loss = 3.20791176\n",
      "Iteration 71, loss = 3.20746023\n",
      "Iteration 72, loss = 3.20728206\n",
      "Iteration 73, loss = 3.20696392\n",
      "Iteration 74, loss = 3.20697840\n",
      "Iteration 75, loss = 3.20673060\n",
      "Iteration 76, loss = 3.20644937\n",
      "Iteration 77, loss = 3.20630381\n",
      "Iteration 78, loss = 3.20577657\n",
      "Iteration 79, loss = 3.20577152\n",
      "Iteration 80, loss = 3.20551481\n",
      "Iteration 81, loss = 3.20483589\n",
      "Iteration 82, loss = 3.20468341\n",
      "Iteration 83, loss = 3.20401521\n",
      "Iteration 84, loss = 3.20368834\n",
      "Iteration 85, loss = 3.20364764\n",
      "Iteration 86, loss = 3.20358218\n",
      "Iteration 87, loss = 3.20345875\n",
      "Iteration 88, loss = 3.20340402\n",
      "Iteration 89, loss = 3.20273351\n",
      "Iteration 90, loss = 3.20233325\n",
      "Iteration 91, loss = 3.20192445\n",
      "Iteration 92, loss = 3.20177401\n",
      "Iteration 93, loss = 3.20135637\n",
      "Iteration 94, loss = 3.20164213\n",
      "Iteration 95, loss = 3.20118130\n",
      "Iteration 96, loss = 3.20094808\n",
      "Iteration 97, loss = 3.20053968\n",
      "Iteration 98, loss = 3.20033724\n",
      "Iteration 99, loss = 3.20030363\n",
      "Iteration 100, loss = 3.19984152\n",
      "Iteration 101, loss = 3.19946350\n",
      "Iteration 102, loss = 3.19919381\n",
      "Iteration 103, loss = 3.19883052\n",
      "Iteration 104, loss = 3.19846133\n",
      "Iteration 105, loss = 3.19837757\n",
      "Iteration 106, loss = 3.19826657\n",
      "Iteration 107, loss = 3.19848021\n",
      "Iteration 108, loss = 3.19799457\n",
      "Iteration 109, loss = 3.19768121\n",
      "Iteration 110, loss = 3.19714275\n",
      "Iteration 111, loss = 3.19666492\n",
      "Iteration 112, loss = 3.19628141\n",
      "Iteration 113, loss = 3.19586163\n",
      "Iteration 114, loss = 3.19560658\n",
      "Iteration 115, loss = 3.19513968\n",
      "Iteration 116, loss = 3.19519533\n",
      "Iteration 117, loss = 3.19475651\n",
      "Iteration 118, loss = 3.19458334\n",
      "Iteration 119, loss = 3.19410501\n",
      "Iteration 120, loss = 3.19395580\n",
      "Iteration 121, loss = 3.19390353\n",
      "Iteration 122, loss = 3.19330179\n",
      "Iteration 123, loss = 3.19356521\n",
      "Iteration 124, loss = 3.19381066\n",
      "Iteration 125, loss = 3.19290625\n",
      "Iteration 126, loss = 3.19252185\n",
      "Iteration 127, loss = 3.19218267\n",
      "Iteration 128, loss = 3.19204369\n",
      "Iteration 129, loss = 3.19148907\n",
      "Iteration 130, loss = 3.19144501\n",
      "Iteration 131, loss = 3.19110177\n",
      "Iteration 132, loss = 3.19058801\n",
      "Iteration 133, loss = 3.19039214\n",
      "Iteration 134, loss = 3.19059582\n",
      "Iteration 135, loss = 3.19033629\n",
      "Iteration 136, loss = 3.18997156\n",
      "Iteration 137, loss = 3.18910920\n",
      "Iteration 138, loss = 3.18880065\n",
      "Iteration 139, loss = 3.18871568\n",
      "Iteration 140, loss = 3.18935358\n",
      "Iteration 141, loss = 3.18885802\n",
      "Iteration 142, loss = 3.18836291\n",
      "Iteration 143, loss = 3.18810296\n",
      "Iteration 144, loss = 3.18811263\n",
      "Iteration 145, loss = 3.18732203\n",
      "Iteration 146, loss = 3.18661481\n",
      "Iteration 147, loss = 3.18631154\n",
      "Iteration 148, loss = 3.18609840\n",
      "Iteration 149, loss = 3.18617628\n",
      "Iteration 150, loss = 3.18564242\n",
      "Iteration 151, loss = 3.18527749\n",
      "Iteration 152, loss = 3.18546718\n",
      "Iteration 153, loss = 3.18483758\n",
      "Iteration 154, loss = 3.18442531\n",
      "Iteration 155, loss = 3.18403728\n",
      "Iteration 156, loss = 3.18389187\n",
      "Iteration 157, loss = 3.18430391\n",
      "Iteration 158, loss = 3.18412518\n",
      "Iteration 159, loss = 3.18304615\n",
      "Iteration 160, loss = 3.18298094\n",
      "Iteration 161, loss = 3.18352584\n",
      "Iteration 162, loss = 3.18334260\n",
      "Iteration 163, loss = 3.18235554\n",
      "Iteration 164, loss = 3.18182960\n",
      "Iteration 165, loss = 3.18203834\n",
      "Iteration 166, loss = 3.18171377\n",
      "Iteration 167, loss = 3.18115206\n",
      "Iteration 168, loss = 3.18066942\n",
      "Iteration 169, loss = 3.18077317\n",
      "Iteration 170, loss = 3.18014497\n",
      "Iteration 171, loss = 3.17941201\n",
      "Iteration 172, loss = 3.17946812\n",
      "Iteration 173, loss = 3.17931174\n",
      "Iteration 174, loss = 3.17909124\n",
      "Iteration 175, loss = 3.17843008\n",
      "Iteration 176, loss = 3.17792042\n",
      "Iteration 177, loss = 3.17787661\n",
      "Iteration 178, loss = 3.17826309\n",
      "Iteration 179, loss = 3.17748259\n",
      "Iteration 180, loss = 3.17743302\n",
      "Iteration 181, loss = 3.17649480\n",
      "Iteration 182, loss = 3.17636191\n",
      "Iteration 183, loss = 3.17539763\n",
      "Iteration 184, loss = 3.17534982\n",
      "Iteration 185, loss = 3.17511895\n",
      "Iteration 186, loss = 3.17489975\n",
      "Iteration 187, loss = 3.17445086\n",
      "Iteration 188, loss = 3.17437908\n",
      "Iteration 189, loss = 3.17442234\n",
      "Iteration 190, loss = 3.17420401\n",
      "Iteration 191, loss = 3.17372482\n",
      "Iteration 192, loss = 3.17317548\n",
      "Iteration 193, loss = 3.17302002\n",
      "Iteration 194, loss = 3.17284536\n",
      "Iteration 195, loss = 3.17253360\n",
      "Iteration 196, loss = 3.17214031\n",
      "Iteration 197, loss = 3.17166387\n",
      "Iteration 198, loss = 3.17165827\n",
      "Iteration 199, loss = 3.17139600\n",
      "Iteration 200, loss = 3.17077699\n",
      "Iteration 201, loss = 3.17066181\n",
      "Iteration 202, loss = 3.17014350\n",
      "Iteration 203, loss = 3.16964823\n",
      "Iteration 204, loss = 3.16914045\n",
      "Iteration 205, loss = 3.16903915\n",
      "Iteration 206, loss = 3.16902808\n",
      "Iteration 207, loss = 3.16823942\n",
      "Iteration 208, loss = 3.16804608\n",
      "Iteration 209, loss = 3.16788912\n",
      "Iteration 210, loss = 3.16785205\n",
      "Iteration 211, loss = 3.16772538\n",
      "Iteration 212, loss = 3.16762201\n",
      "Iteration 213, loss = 3.16717931\n",
      "Iteration 214, loss = 3.16678275\n",
      "Iteration 215, loss = 3.16642405\n",
      "Iteration 216, loss = 3.16638697\n",
      "Iteration 217, loss = 3.16541362\n",
      "Iteration 218, loss = 3.16564941\n",
      "Iteration 219, loss = 3.16551662\n",
      "Iteration 220, loss = 3.16486523\n",
      "Iteration 221, loss = 3.16486648\n",
      "Iteration 222, loss = 3.16390899\n",
      "Iteration 223, loss = 3.16358562\n",
      "Iteration 224, loss = 3.16410505\n",
      "Iteration 225, loss = 3.16455908\n",
      "Iteration 226, loss = 3.16424704\n",
      "Iteration 227, loss = 3.16392089\n",
      "Iteration 228, loss = 3.16287529\n",
      "Iteration 229, loss = 3.16259378\n",
      "Iteration 230, loss = 3.16202207\n",
      "Iteration 231, loss = 3.16156664\n",
      "Iteration 232, loss = 3.16131902\n",
      "Iteration 233, loss = 3.16056736\n",
      "Iteration 234, loss = 3.16013691\n",
      "Iteration 235, loss = 3.15980608\n",
      "Iteration 236, loss = 3.15951969\n",
      "Iteration 237, loss = 3.15930342\n",
      "Iteration 238, loss = 3.15867266\n",
      "Iteration 239, loss = 3.15871578\n",
      "Iteration 240, loss = 3.15826746\n",
      "Iteration 241, loss = 3.15778828\n",
      "Iteration 242, loss = 3.15781763\n",
      "Iteration 243, loss = 3.15748397\n",
      "Iteration 244, loss = 3.15699960\n",
      "Iteration 245, loss = 3.15673766\n",
      "Iteration 246, loss = 3.15730222\n",
      "Iteration 247, loss = 3.15692154\n",
      "Iteration 248, loss = 3.15634516\n",
      "Iteration 249, loss = 3.15586715\n",
      "Iteration 250, loss = 3.15553649\n",
      "Iteration 251, loss = 3.15494151\n",
      "Iteration 252, loss = 3.15448526\n",
      "Iteration 253, loss = 3.15469085\n",
      "Iteration 254, loss = 3.15432547\n",
      "Iteration 255, loss = 3.15375032\n",
      "Iteration 256, loss = 3.15342400\n",
      "Iteration 257, loss = 3.15316076\n",
      "Iteration 258, loss = 3.15301289\n",
      "Iteration 259, loss = 3.15243037\n",
      "Iteration 260, loss = 3.15225648\n",
      "Iteration 261, loss = 3.15248752\n",
      "Iteration 262, loss = 3.15213072\n",
      "Iteration 263, loss = 3.15198887\n",
      "Iteration 264, loss = 3.15133053\n",
      "Iteration 265, loss = 3.15116331\n",
      "Iteration 266, loss = 3.15093679\n",
      "Iteration 267, loss = 3.15043652\n",
      "Iteration 268, loss = 3.14995287\n",
      "Iteration 269, loss = 3.14974655\n",
      "Iteration 270, loss = 3.14983232\n",
      "Iteration 271, loss = 3.14951957\n",
      "Iteration 272, loss = 3.14871175\n",
      "Iteration 273, loss = 3.14843588\n",
      "Iteration 274, loss = 3.14838571\n",
      "Iteration 275, loss = 3.14849483\n",
      "Iteration 276, loss = 3.14802096\n",
      "Iteration 277, loss = 3.14758184\n",
      "Iteration 278, loss = 3.14686187\n",
      "Iteration 279, loss = 3.14644316\n",
      "Iteration 280, loss = 3.14629313\n",
      "Iteration 281, loss = 3.14578412\n",
      "Iteration 282, loss = 3.14540194\n",
      "Iteration 283, loss = 3.14507393\n",
      "Iteration 284, loss = 3.14466743\n",
      "Iteration 285, loss = 3.14430897\n",
      "Iteration 286, loss = 3.14374848\n",
      "Iteration 287, loss = 3.14312798\n",
      "Iteration 288, loss = 3.14306690\n",
      "Iteration 289, loss = 3.14326001\n",
      "Iteration 290, loss = 3.14266382\n",
      "Iteration 291, loss = 3.14227709\n",
      "Iteration 292, loss = 3.14197473\n",
      "Iteration 293, loss = 3.14174204\n",
      "Iteration 294, loss = 3.14140765\n",
      "Iteration 295, loss = 3.14159710\n",
      "Iteration 296, loss = 3.14152628\n",
      "Iteration 297, loss = 3.14086852\n",
      "Iteration 298, loss = 3.14075428\n",
      "Iteration 299, loss = 3.14071627\n",
      "Iteration 300, loss = 3.14047071\n",
      "Iteration 301, loss = 3.13932258\n",
      "Iteration 302, loss = 3.13929856\n",
      "Iteration 303, loss = 3.13916341\n",
      "Iteration 304, loss = 3.13861372\n",
      "Iteration 305, loss = 3.13819529\n",
      "Iteration 306, loss = 3.13769578\n",
      "Iteration 307, loss = 3.13720877\n",
      "Iteration 308, loss = 3.13683933\n",
      "Iteration 309, loss = 3.13641087\n",
      "Iteration 310, loss = 3.13594090\n",
      "Iteration 311, loss = 3.13579029\n",
      "Iteration 312, loss = 3.13629290\n",
      "Iteration 313, loss = 3.13593497\n",
      "Iteration 314, loss = 3.13517758\n",
      "Iteration 315, loss = 3.13486096\n",
      "Iteration 316, loss = 3.13482163\n",
      "Iteration 317, loss = 3.13429644\n",
      "Iteration 318, loss = 3.13389433\n",
      "Iteration 319, loss = 3.13402531\n",
      "Iteration 320, loss = 3.13320397\n",
      "Iteration 321, loss = 3.13278046\n",
      "Iteration 322, loss = 3.13253211\n",
      "Iteration 323, loss = 3.13251750\n",
      "Iteration 324, loss = 3.13196188\n",
      "Iteration 325, loss = 3.13146192\n",
      "Iteration 326, loss = 3.13157705\n",
      "Iteration 327, loss = 3.13069981\n",
      "Iteration 328, loss = 3.13044498\n",
      "Iteration 329, loss = 3.13044831\n",
      "Iteration 330, loss = 3.13046782\n",
      "Iteration 331, loss = 3.13014822\n",
      "Iteration 332, loss = 3.12932451\n",
      "Iteration 333, loss = 3.12847129\n",
      "Iteration 334, loss = 3.12828137\n",
      "Iteration 335, loss = 3.12818919\n",
      "Iteration 336, loss = 3.12792752\n",
      "Iteration 337, loss = 3.12707172\n",
      "Iteration 338, loss = 3.12698475\n",
      "Iteration 339, loss = 3.12663334\n",
      "Iteration 340, loss = 3.12669388\n",
      "Iteration 341, loss = 3.12662051\n",
      "Iteration 342, loss = 3.12687846\n",
      "Iteration 343, loss = 3.12627929\n",
      "Iteration 344, loss = 3.12558291\n",
      "Iteration 345, loss = 3.12524444\n",
      "Iteration 346, loss = 3.12475875\n",
      "Iteration 347, loss = 3.12482869\n",
      "Iteration 348, loss = 3.12477993\n",
      "Iteration 349, loss = 3.12506068\n",
      "Iteration 350, loss = 3.12453880\n",
      "Iteration 351, loss = 3.12310325\n",
      "Iteration 352, loss = 3.12260349\n",
      "Iteration 353, loss = 3.12218662\n",
      "Iteration 354, loss = 3.12233245\n",
      "Iteration 355, loss = 3.12161056\n",
      "Iteration 356, loss = 3.12154081\n",
      "Iteration 357, loss = 3.12095874\n",
      "Iteration 358, loss = 3.12057783\n",
      "Iteration 359, loss = 3.12041310\n",
      "Iteration 360, loss = 3.11994752\n",
      "Iteration 361, loss = 3.12000722\n",
      "Iteration 362, loss = 3.11963688\n",
      "Iteration 363, loss = 3.11891598\n",
      "Iteration 364, loss = 3.11848976\n",
      "Iteration 365, loss = 3.11818972\n",
      "Iteration 366, loss = 3.11787701\n",
      "Iteration 367, loss = 3.11765737\n",
      "Iteration 368, loss = 3.11741799\n",
      "Iteration 369, loss = 3.11727145\n",
      "Iteration 370, loss = 3.11709096\n",
      "Iteration 371, loss = 3.11666116\n",
      "Iteration 372, loss = 3.11626988\n",
      "Iteration 373, loss = 3.11556019\n",
      "Iteration 374, loss = 3.11493079\n",
      "Iteration 375, loss = 3.11464446\n",
      "Iteration 376, loss = 3.11671871\n",
      "Iteration 377, loss = 3.11643673\n",
      "Iteration 378, loss = 3.11385781\n",
      "Iteration 379, loss = 3.11374012\n",
      "Iteration 380, loss = 3.11354334\n",
      "Iteration 381, loss = 3.11259475\n",
      "Iteration 382, loss = 3.11192924\n",
      "Iteration 383, loss = 3.11163582\n",
      "Iteration 384, loss = 3.11141354\n",
      "Iteration 385, loss = 3.11139940\n",
      "Iteration 386, loss = 3.11104068\n",
      "Iteration 387, loss = 3.11066091\n",
      "Iteration 388, loss = 3.11049635\n",
      "Iteration 389, loss = 3.10993349\n",
      "Iteration 390, loss = 3.10947879\n",
      "Iteration 391, loss = 3.10919908\n",
      "Iteration 392, loss = 3.10881366\n",
      "Iteration 393, loss = 3.10877098\n",
      "Iteration 394, loss = 3.10810657\n",
      "Iteration 395, loss = 3.10775084\n",
      "Iteration 396, loss = 3.10806606\n",
      "Iteration 397, loss = 3.10778562\n",
      "Iteration 398, loss = 3.10697514\n",
      "Iteration 399, loss = 3.10785283\n",
      "Iteration 400, loss = 3.10702567\n",
      "Iteration 401, loss = 3.10632103\n",
      "Iteration 402, loss = 3.10533612\n",
      "Iteration 403, loss = 3.10511673\n",
      "Iteration 404, loss = 3.10520425\n",
      "Iteration 405, loss = 3.10502892\n",
      "Iteration 406, loss = 3.10471259\n",
      "Iteration 407, loss = 3.10462935\n",
      "Iteration 408, loss = 3.10402228\n",
      "Iteration 409, loss = 3.10346615\n",
      "Iteration 410, loss = 3.10281128\n",
      "Iteration 411, loss = 3.10275959\n",
      "Iteration 412, loss = 3.10245001\n",
      "Iteration 413, loss = 3.10194542\n",
      "Iteration 414, loss = 3.10128817\n",
      "Iteration 415, loss = 3.10097375\n",
      "Iteration 416, loss = 3.10066121\n",
      "Iteration 417, loss = 3.10001994\n",
      "Iteration 418, loss = 3.09997562\n",
      "Iteration 419, loss = 3.09961376\n",
      "Iteration 420, loss = 3.09902299\n",
      "Iteration 421, loss = 3.09941545\n",
      "Iteration 422, loss = 3.09919796\n",
      "Iteration 423, loss = 3.09841185\n",
      "Iteration 424, loss = 3.09780622\n",
      "Iteration 425, loss = 3.09781653\n",
      "Iteration 426, loss = 3.09763032\n",
      "Iteration 427, loss = 3.09751856\n",
      "Iteration 428, loss = 3.09654355\n",
      "Iteration 429, loss = 3.09597595\n",
      "Iteration 430, loss = 3.09543942\n",
      "Iteration 431, loss = 3.09499120\n",
      "Iteration 432, loss = 3.09489838\n",
      "Iteration 433, loss = 3.09436738\n",
      "Iteration 434, loss = 3.09441550\n",
      "Iteration 435, loss = 3.09375807\n",
      "Iteration 436, loss = 3.09296577\n",
      "Iteration 437, loss = 3.09291573\n",
      "Iteration 438, loss = 3.09273120\n",
      "Iteration 439, loss = 3.09242467\n",
      "Iteration 440, loss = 3.09179756\n",
      "Iteration 441, loss = 3.09143712\n",
      "Iteration 442, loss = 3.09094288\n",
      "Iteration 443, loss = 3.09071908\n",
      "Iteration 444, loss = 3.09054347\n",
      "Iteration 445, loss = 3.08978384\n",
      "Iteration 446, loss = 3.08995810\n",
      "Iteration 447, loss = 3.08937772\n",
      "Iteration 448, loss = 3.08917815\n",
      "Iteration 449, loss = 3.08877726\n",
      "Iteration 450, loss = 3.08856550\n",
      "Iteration 451, loss = 3.08789379\n",
      "Iteration 452, loss = 3.08732940\n",
      "Iteration 453, loss = 3.08714741\n",
      "Iteration 454, loss = 3.08664557\n",
      "Iteration 455, loss = 3.08602716\n",
      "Iteration 456, loss = 3.08635210\n",
      "Iteration 457, loss = 3.08601766\n",
      "Iteration 458, loss = 3.08548088\n",
      "Iteration 459, loss = 3.08498246\n",
      "Iteration 460, loss = 3.08496902\n",
      "Iteration 461, loss = 3.08416606\n",
      "Iteration 462, loss = 3.08422093\n",
      "Iteration 463, loss = 3.08439115\n",
      "Iteration 464, loss = 3.08356594\n",
      "Iteration 465, loss = 3.08312439\n",
      "Iteration 466, loss = 3.08279619\n",
      "Iteration 467, loss = 3.08241850\n",
      "Iteration 468, loss = 3.08181671\n",
      "Iteration 469, loss = 3.08152465\n",
      "Iteration 470, loss = 3.08117566\n",
      "Iteration 471, loss = 3.08089143\n",
      "Iteration 472, loss = 3.08074632\n",
      "Iteration 473, loss = 3.08024231\n",
      "Iteration 474, loss = 3.07997832\n",
      "Iteration 475, loss = 3.07972332\n",
      "Iteration 476, loss = 3.07935121\n",
      "Iteration 477, loss = 3.07891745\n",
      "Iteration 478, loss = 3.07840744\n",
      "Iteration 479, loss = 3.07791141\n",
      "Iteration 480, loss = 3.07761246\n",
      "Iteration 481, loss = 3.07710488\n",
      "Iteration 482, loss = 3.07696837\n",
      "Iteration 483, loss = 3.07687964\n",
      "Iteration 484, loss = 3.07682920\n",
      "Iteration 485, loss = 3.07632725\n",
      "Iteration 486, loss = 3.07555992\n",
      "Iteration 487, loss = 3.07555986\n",
      "Iteration 488, loss = 3.07570771\n",
      "Iteration 489, loss = 3.07476174\n",
      "Iteration 490, loss = 3.07413481\n",
      "Iteration 491, loss = 3.07458749\n",
      "Iteration 492, loss = 3.07401569\n",
      "Iteration 493, loss = 3.07326463\n",
      "Iteration 494, loss = 3.07274393\n",
      "Iteration 495, loss = 3.07223055\n",
      "Iteration 496, loss = 3.07175267\n",
      "Iteration 497, loss = 3.07158009\n",
      "Iteration 498, loss = 3.07140117\n",
      "Iteration 499, loss = 3.07077691\n",
      "Iteration 500, loss = 3.07082898\n",
      "Iteration 501, loss = 3.07043239\n",
      "Iteration 502, loss = 3.07015485\n",
      "Iteration 503, loss = 3.06942907\n",
      "Iteration 504, loss = 3.06925030\n",
      "Iteration 505, loss = 3.07104135\n",
      "Iteration 506, loss = 3.07041957\n",
      "Iteration 507, loss = 3.06879621\n",
      "Iteration 508, loss = 3.06794021\n",
      "Iteration 509, loss = 3.06784813\n",
      "Iteration 510, loss = 3.06726366\n",
      "Iteration 511, loss = 3.06647598\n",
      "Iteration 512, loss = 3.06676270\n",
      "Iteration 513, loss = 3.06593492\n",
      "Iteration 514, loss = 3.06561111\n",
      "Iteration 515, loss = 3.06509340\n",
      "Iteration 516, loss = 3.06487793\n",
      "Iteration 517, loss = 3.06458943\n",
      "Iteration 518, loss = 3.06425426\n",
      "Iteration 519, loss = 3.06403770\n",
      "Iteration 520, loss = 3.06363400\n",
      "Iteration 521, loss = 3.06276764\n",
      "Iteration 522, loss = 3.06267101\n",
      "Iteration 523, loss = 3.06233355\n",
      "Iteration 524, loss = 3.06204967\n",
      "Iteration 525, loss = 3.06170432\n",
      "Iteration 526, loss = 3.06118622\n",
      "Iteration 527, loss = 3.06069695\n",
      "Iteration 528, loss = 3.06026244\n",
      "Iteration 529, loss = 3.05984336\n",
      "Iteration 530, loss = 3.05953992\n",
      "Iteration 531, loss = 3.05915436\n",
      "Iteration 532, loss = 3.05895737\n",
      "Iteration 533, loss = 3.05819947\n",
      "Iteration 534, loss = 3.05824698\n",
      "Iteration 535, loss = 3.05775329\n",
      "Iteration 536, loss = 3.05756910\n",
      "Iteration 537, loss = 3.05711310\n",
      "Iteration 538, loss = 3.05708115\n",
      "Iteration 539, loss = 3.05623227\n",
      "Iteration 540, loss = 3.05593539\n",
      "Iteration 541, loss = 3.05573149\n",
      "Iteration 542, loss = 3.05512470\n",
      "Iteration 543, loss = 3.05509517\n",
      "Iteration 544, loss = 3.05571227\n",
      "Iteration 545, loss = 3.05451367\n",
      "Iteration 546, loss = 3.05441941\n",
      "Iteration 547, loss = 3.05432068\n",
      "Iteration 548, loss = 3.05314272\n",
      "Iteration 549, loss = 3.05384966\n",
      "Iteration 550, loss = 3.05277922\n",
      "Iteration 551, loss = 3.05268677\n",
      "Iteration 552, loss = 3.05195768\n",
      "Iteration 553, loss = 3.05154839\n",
      "Iteration 554, loss = 3.05104996\n",
      "Iteration 555, loss = 3.05054096\n",
      "Iteration 556, loss = 3.05072222\n",
      "Iteration 557, loss = 3.05026635\n",
      "Iteration 558, loss = 3.04937378\n",
      "Iteration 559, loss = 3.04924771\n",
      "Iteration 560, loss = 3.04925380\n",
      "Iteration 561, loss = 3.04874833\n",
      "Iteration 562, loss = 3.04868481\n",
      "Iteration 563, loss = 3.04826789\n",
      "Iteration 564, loss = 3.04769942\n",
      "Iteration 565, loss = 3.04758340\n",
      "Iteration 566, loss = 3.04692872\n",
      "Iteration 567, loss = 3.04648265\n",
      "Iteration 568, loss = 3.04631945\n",
      "Iteration 569, loss = 3.04555344\n",
      "Iteration 570, loss = 3.04560465\n",
      "Iteration 571, loss = 3.04609006\n",
      "Iteration 572, loss = 3.04515724\n",
      "Iteration 573, loss = 3.04520204\n",
      "Iteration 574, loss = 3.04451703\n",
      "Iteration 575, loss = 3.04389428\n",
      "Iteration 576, loss = 3.04336891\n",
      "Iteration 577, loss = 3.04296320\n",
      "Iteration 578, loss = 3.04293774\n",
      "Iteration 579, loss = 3.04261677\n",
      "Iteration 580, loss = 3.04225164\n",
      "Iteration 581, loss = 3.04194052\n",
      "Iteration 582, loss = 3.04194241\n",
      "Iteration 583, loss = 3.04131878\n",
      "Iteration 584, loss = 3.04091438\n",
      "Iteration 585, loss = 3.04079449\n",
      "Iteration 586, loss = 3.04010770\n",
      "Iteration 587, loss = 3.03988288\n",
      "Iteration 588, loss = 3.04017909\n",
      "Iteration 589, loss = 3.03968364\n",
      "Iteration 590, loss = 3.03913019\n",
      "Iteration 591, loss = 3.03923464\n",
      "Iteration 592, loss = 3.03877570\n",
      "Iteration 593, loss = 3.03821593\n",
      "Iteration 594, loss = 3.03795647\n",
      "Iteration 595, loss = 3.03767462\n",
      "Iteration 596, loss = 3.03723163\n",
      "Iteration 597, loss = 3.03693106\n",
      "Iteration 598, loss = 3.03672880\n",
      "Iteration 599, loss = 3.03634352\n",
      "Iteration 600, loss = 3.03581301\n",
      "Iteration 601, loss = 3.03527313\n",
      "Iteration 602, loss = 3.03511159\n",
      "Iteration 603, loss = 3.03474925\n",
      "Iteration 604, loss = 3.03449978\n",
      "Iteration 605, loss = 3.03446480\n",
      "Iteration 606, loss = 3.03383650\n",
      "Iteration 607, loss = 3.03354960\n",
      "Iteration 608, loss = 3.03330719\n",
      "Iteration 609, loss = 3.03294238\n",
      "Iteration 610, loss = 3.03211383\n",
      "Iteration 611, loss = 3.03230663\n",
      "Iteration 612, loss = 3.03203336\n",
      "Iteration 613, loss = 3.03191135\n",
      "Iteration 614, loss = 3.03087256\n",
      "Iteration 615, loss = 3.03071626\n",
      "Iteration 616, loss = 3.03037061\n",
      "Iteration 617, loss = 3.03061461\n",
      "Iteration 618, loss = 3.02958686\n",
      "Iteration 619, loss = 3.02936410\n",
      "Iteration 620, loss = 3.02845390\n",
      "Iteration 621, loss = 3.02928297\n",
      "Iteration 622, loss = 3.02892711\n",
      "Iteration 623, loss = 3.02785349\n",
      "Iteration 624, loss = 3.02768166\n",
      "Iteration 625, loss = 3.02785692\n",
      "Iteration 626, loss = 3.02719103\n",
      "Iteration 627, loss = 3.02690196\n",
      "Iteration 628, loss = 3.02640312\n",
      "Iteration 629, loss = 3.02613074\n",
      "Iteration 630, loss = 3.02633846\n",
      "Iteration 631, loss = 3.02612756\n",
      "Iteration 632, loss = 3.02560837\n",
      "Iteration 633, loss = 3.02516803\n",
      "Iteration 634, loss = 3.02515945\n",
      "Iteration 635, loss = 3.02422751\n",
      "Iteration 636, loss = 3.02390186\n",
      "Iteration 637, loss = 3.02302287\n",
      "Iteration 638, loss = 3.02248076\n",
      "Iteration 639, loss = 3.02244583\n",
      "Iteration 640, loss = 3.02268745\n",
      "Iteration 641, loss = 3.02263782\n",
      "Iteration 642, loss = 3.02198577\n",
      "Iteration 643, loss = 3.02139639\n",
      "Iteration 644, loss = 3.02142546\n",
      "Iteration 645, loss = 3.02118698\n",
      "Iteration 646, loss = 3.02066692\n",
      "Iteration 647, loss = 3.01983343\n",
      "Iteration 648, loss = 3.02011903\n",
      "Iteration 649, loss = 3.01990634\n",
      "Iteration 650, loss = 3.01899993\n",
      "Iteration 651, loss = 3.01892665\n",
      "Iteration 652, loss = 3.01815300\n",
      "Iteration 653, loss = 3.01811236\n",
      "Iteration 654, loss = 3.01786404\n",
      "Iteration 655, loss = 3.01777183\n",
      "Iteration 656, loss = 3.01759657\n",
      "Iteration 657, loss = 3.01744380\n",
      "Iteration 658, loss = 3.01694420\n",
      "Iteration 659, loss = 3.01624984\n",
      "Iteration 660, loss = 3.01613741\n",
      "Iteration 661, loss = 3.01564312\n",
      "Iteration 662, loss = 3.01533955\n",
      "Iteration 663, loss = 3.01531013\n",
      "Iteration 664, loss = 3.01499619\n",
      "Iteration 665, loss = 3.01429955\n",
      "Iteration 666, loss = 3.01386835\n",
      "Iteration 667, loss = 3.01336999\n",
      "Iteration 668, loss = 3.01381361\n",
      "Iteration 669, loss = 3.01407836\n",
      "Iteration 670, loss = 3.01331537\n",
      "Iteration 671, loss = 3.01274289\n",
      "Iteration 672, loss = 3.01207471\n",
      "Iteration 673, loss = 3.01169150\n",
      "Iteration 674, loss = 3.01155342\n",
      "Iteration 675, loss = 3.01113613\n",
      "Iteration 676, loss = 3.01184142\n",
      "Iteration 677, loss = 3.01091761\n",
      "Iteration 678, loss = 3.01052713\n",
      "Iteration 679, loss = 3.01009157\n",
      "Iteration 680, loss = 3.01080020\n",
      "Iteration 681, loss = 3.00975265\n",
      "Iteration 682, loss = 3.00897192\n",
      "Iteration 683, loss = 3.00872927\n",
      "Iteration 684, loss = 3.00837867\n",
      "Iteration 685, loss = 3.00837040\n",
      "Iteration 686, loss = 3.00756359\n",
      "Iteration 687, loss = 3.00741116\n",
      "Iteration 688, loss = 3.00716019\n",
      "Iteration 689, loss = 3.00656798\n",
      "Iteration 690, loss = 3.00641915\n",
      "Iteration 691, loss = 3.00706983\n",
      "Iteration 692, loss = 3.00591819\n",
      "Iteration 693, loss = 3.00550853\n",
      "Iteration 694, loss = 3.00532157\n",
      "Iteration 695, loss = 3.00526662\n",
      "Iteration 696, loss = 3.00511850\n",
      "Iteration 697, loss = 3.00468336\n",
      "Iteration 698, loss = 3.00387424\n",
      "Iteration 699, loss = 3.00430733\n",
      "Iteration 700, loss = 3.00358501\n",
      "Iteration 701, loss = 3.00308487\n",
      "Iteration 702, loss = 3.00268224\n",
      "Iteration 703, loss = 3.00234171\n",
      "Iteration 704, loss = 3.00208779\n",
      "Iteration 705, loss = 3.00246286\n",
      "Iteration 706, loss = 3.00159269\n",
      "Iteration 707, loss = 3.00137095\n",
      "Iteration 708, loss = 3.00109137\n",
      "Iteration 709, loss = 3.00069348\n",
      "Iteration 710, loss = 2.99994658\n",
      "Iteration 711, loss = 2.99976369\n",
      "Iteration 712, loss = 2.99946824\n",
      "Iteration 713, loss = 2.99947024\n",
      "Iteration 714, loss = 2.99904754\n",
      "Iteration 715, loss = 2.99817513\n",
      "Iteration 716, loss = 2.99866068\n",
      "Iteration 717, loss = 2.99819100\n",
      "Iteration 718, loss = 2.99830677\n",
      "Iteration 719, loss = 2.99714078\n",
      "Iteration 720, loss = 2.99726658\n",
      "Iteration 721, loss = 2.99662411\n",
      "Iteration 722, loss = 2.99655410\n",
      "Iteration 723, loss = 2.99637466\n",
      "Iteration 724, loss = 2.99605165\n",
      "Iteration 725, loss = 2.99541634\n",
      "Iteration 726, loss = 2.99546861\n",
      "Iteration 727, loss = 2.99507260\n",
      "Iteration 728, loss = 2.99447022\n",
      "Iteration 729, loss = 2.99479755\n",
      "Iteration 730, loss = 2.99432519\n",
      "Iteration 731, loss = 2.99373455\n",
      "Iteration 732, loss = 2.99344912\n",
      "Iteration 733, loss = 2.99370264\n",
      "Iteration 734, loss = 2.99311585\n",
      "Iteration 735, loss = 2.99297421\n",
      "Iteration 736, loss = 2.99218019\n",
      "Iteration 737, loss = 2.99175056\n",
      "Iteration 738, loss = 2.99155525\n",
      "Iteration 739, loss = 2.99167890\n",
      "Iteration 740, loss = 2.99152100\n",
      "Iteration 741, loss = 2.99130712\n",
      "Iteration 742, loss = 2.99070133\n",
      "Iteration 743, loss = 2.99014849\n",
      "Iteration 744, loss = 2.98963809\n",
      "Iteration 745, loss = 2.98963059\n",
      "Iteration 746, loss = 2.98910543\n",
      "Iteration 747, loss = 2.98890289\n",
      "Iteration 748, loss = 2.98856660\n",
      "Iteration 749, loss = 2.98829606\n",
      "Iteration 750, loss = 2.98792411\n",
      "Iteration 751, loss = 2.98802599\n",
      "Iteration 752, loss = 2.98733634\n",
      "Iteration 753, loss = 2.98750038\n",
      "Iteration 754, loss = 2.98710383\n",
      "Iteration 755, loss = 2.98684694\n",
      "Iteration 756, loss = 2.98614057\n",
      "Iteration 757, loss = 2.98572728\n",
      "Iteration 758, loss = 2.98560653\n",
      "Iteration 759, loss = 2.98513198\n",
      "Iteration 760, loss = 2.98538498\n",
      "Iteration 761, loss = 2.98494192\n",
      "Iteration 762, loss = 2.98455629\n",
      "Iteration 763, loss = 2.98474693\n",
      "Iteration 764, loss = 2.98470198\n",
      "Iteration 765, loss = 2.98420039\n",
      "Iteration 766, loss = 2.98357238\n",
      "Iteration 767, loss = 2.98349022\n",
      "Iteration 768, loss = 2.98288491\n",
      "Iteration 769, loss = 2.98201874\n",
      "Iteration 770, loss = 2.98202861\n",
      "Iteration 771, loss = 2.98203472\n",
      "Iteration 772, loss = 2.98181886\n",
      "Iteration 773, loss = 2.98170821\n",
      "Iteration 774, loss = 2.98101511\n",
      "Iteration 775, loss = 2.98058322\n",
      "Iteration 776, loss = 2.98018092\n",
      "Iteration 777, loss = 2.98013266\n",
      "Iteration 778, loss = 2.97937862\n",
      "Iteration 779, loss = 2.97937355\n",
      "Iteration 780, loss = 2.97927880\n",
      "Iteration 781, loss = 2.97893607\n",
      "Iteration 782, loss = 2.97839176\n",
      "Iteration 783, loss = 2.97816887\n",
      "Iteration 784, loss = 2.97805927\n",
      "Iteration 785, loss = 2.97768640\n",
      "Iteration 786, loss = 2.97732288\n",
      "Iteration 787, loss = 2.97743249\n",
      "Iteration 788, loss = 2.97690040\n",
      "Iteration 789, loss = 2.97774904\n",
      "Iteration 790, loss = 2.97621252\n",
      "Iteration 791, loss = 2.97544884\n",
      "Iteration 792, loss = 2.97527341\n",
      "Iteration 793, loss = 2.97540373\n",
      "Iteration 794, loss = 2.97513329\n",
      "Iteration 795, loss = 2.97510362\n",
      "Iteration 796, loss = 2.97461805\n",
      "Iteration 797, loss = 2.97397932\n",
      "Iteration 798, loss = 2.97371435\n",
      "Iteration 799, loss = 2.97355067\n",
      "Iteration 800, loss = 2.97299484\n",
      "Iteration 801, loss = 2.97300878\n",
      "Iteration 802, loss = 2.97325033\n",
      "Iteration 803, loss = 2.97296027\n",
      "Iteration 804, loss = 2.97257050\n",
      "Iteration 805, loss = 2.97173445\n",
      "Iteration 806, loss = 2.97194944\n",
      "Iteration 807, loss = 2.97174606\n",
      "Iteration 808, loss = 2.97098826\n",
      "Iteration 809, loss = 2.97069665\n",
      "Iteration 810, loss = 2.97004752\n",
      "Iteration 811, loss = 2.97009407\n",
      "Iteration 812, loss = 2.96972354\n",
      "Iteration 813, loss = 2.96962520\n",
      "Iteration 814, loss = 2.96950915\n",
      "Iteration 815, loss = 2.96990767\n",
      "Iteration 816, loss = 2.96907221\n",
      "Iteration 817, loss = 2.96880519\n",
      "Iteration 818, loss = 2.96828428\n",
      "Iteration 819, loss = 2.96792383\n",
      "Iteration 820, loss = 2.96779828\n",
      "Iteration 821, loss = 2.96784184\n",
      "Iteration 822, loss = 2.96685659\n",
      "Iteration 823, loss = 2.96667183\n",
      "Iteration 824, loss = 2.96667283\n",
      "Iteration 825, loss = 2.96660783\n",
      "Iteration 826, loss = 2.96740796\n",
      "Iteration 827, loss = 2.96642391\n",
      "Iteration 828, loss = 2.96583324\n",
      "Iteration 829, loss = 2.96545748\n",
      "Iteration 830, loss = 2.96548079\n",
      "Iteration 831, loss = 2.96443528\n",
      "Iteration 832, loss = 2.96411279\n",
      "Iteration 833, loss = 2.96399869\n",
      "Iteration 834, loss = 2.96399253\n",
      "Iteration 835, loss = 2.96364993\n",
      "Iteration 836, loss = 2.96379077\n",
      "Iteration 837, loss = 2.96380944\n",
      "Iteration 838, loss = 2.96294861\n",
      "Iteration 839, loss = 2.96242130\n",
      "Iteration 840, loss = 2.96271331\n",
      "Iteration 841, loss = 2.96189104\n",
      "Iteration 842, loss = 2.96167441\n",
      "Iteration 843, loss = 2.96138345\n",
      "Iteration 844, loss = 2.96136290\n",
      "Iteration 845, loss = 2.96100034\n",
      "Iteration 846, loss = 2.96061222\n",
      "Iteration 847, loss = 2.96017993\n",
      "Iteration 848, loss = 2.96006021\n",
      "Iteration 849, loss = 2.96000445\n",
      "Iteration 850, loss = 2.95974206\n",
      "Iteration 851, loss = 2.95954169\n",
      "Iteration 852, loss = 2.95938782\n",
      "Iteration 853, loss = 2.95888074\n",
      "Iteration 854, loss = 2.95880500\n",
      "Iteration 855, loss = 2.95860994\n",
      "Iteration 856, loss = 2.95812213\n",
      "Iteration 857, loss = 2.95796138\n",
      "Iteration 858, loss = 2.95713333\n",
      "Iteration 859, loss = 2.95675389\n",
      "Iteration 860, loss = 2.95642018\n",
      "Iteration 861, loss = 2.95642214\n",
      "Iteration 862, loss = 2.95638191\n",
      "Iteration 863, loss = 2.95618932\n",
      "Iteration 864, loss = 2.95587364\n",
      "Iteration 865, loss = 2.95594948\n",
      "Iteration 866, loss = 2.95592962\n",
      "Iteration 867, loss = 2.95559443\n",
      "Iteration 868, loss = 2.95498355\n",
      "Iteration 869, loss = 2.95449385\n",
      "Iteration 870, loss = 2.95412984\n",
      "Iteration 871, loss = 2.95390848\n",
      "Iteration 872, loss = 2.95350506\n",
      "Iteration 873, loss = 2.95322479\n",
      "Iteration 874, loss = 2.95328481\n",
      "Iteration 875, loss = 2.95300875\n",
      "Iteration 876, loss = 2.95273133\n",
      "Iteration 877, loss = 2.95292380\n",
      "Iteration 878, loss = 2.95258985\n",
      "Iteration 879, loss = 2.95208565\n",
      "Iteration 880, loss = 2.95146876\n",
      "Iteration 881, loss = 2.95098208\n",
      "Iteration 882, loss = 2.95074214\n",
      "Iteration 883, loss = 2.95073534\n",
      "Iteration 884, loss = 2.95042819\n",
      "Iteration 885, loss = 2.95022684\n",
      "Iteration 886, loss = 2.94997944\n",
      "Iteration 887, loss = 2.94978197\n",
      "Iteration 888, loss = 2.94937915\n",
      "Iteration 889, loss = 2.94897733\n",
      "Iteration 890, loss = 2.94905818\n",
      "Iteration 891, loss = 2.94873275\n",
      "Iteration 892, loss = 2.94795981\n",
      "Iteration 893, loss = 2.94796183\n",
      "Iteration 894, loss = 2.94789149\n",
      "Iteration 895, loss = 2.94794230\n",
      "Iteration 896, loss = 2.94766690\n",
      "Iteration 897, loss = 2.94718957\n",
      "Iteration 898, loss = 2.94778640\n",
      "Iteration 899, loss = 2.94751301\n",
      "Iteration 900, loss = 2.94678530\n",
      "Iteration 901, loss = 2.94634004\n",
      "Iteration 902, loss = 2.94590921\n",
      "Iteration 903, loss = 2.94569128\n",
      "Iteration 904, loss = 2.94491181\n",
      "Iteration 905, loss = 2.94493565\n",
      "Iteration 906, loss = 2.94472614\n",
      "Iteration 907, loss = 2.94453880\n",
      "Iteration 908, loss = 2.94383200\n",
      "Iteration 909, loss = 2.94366685\n",
      "Iteration 910, loss = 2.94347884\n",
      "Iteration 911, loss = 2.94319265\n",
      "Iteration 912, loss = 2.94364667\n",
      "Iteration 913, loss = 2.94321889\n",
      "Iteration 914, loss = 2.94315096\n",
      "Iteration 915, loss = 2.94289015\n",
      "Iteration 916, loss = 2.94268791\n",
      "Iteration 917, loss = 2.94218730\n",
      "Iteration 918, loss = 2.94155743\n",
      "Iteration 919, loss = 2.94113286\n",
      "Iteration 920, loss = 2.94140988\n",
      "Iteration 921, loss = 2.94082476\n",
      "Iteration 922, loss = 2.94082863\n",
      "Iteration 923, loss = 2.94087628\n",
      "Iteration 924, loss = 2.93972472\n",
      "Iteration 925, loss = 2.94032426\n",
      "Iteration 926, loss = 2.94009540\n",
      "Iteration 927, loss = 2.93942086\n",
      "Iteration 928, loss = 2.93902769\n",
      "Iteration 929, loss = 2.93897557\n",
      "Iteration 930, loss = 2.93917576\n",
      "Iteration 931, loss = 2.93860513\n",
      "Iteration 932, loss = 2.93857141\n",
      "Iteration 933, loss = 2.93842702\n",
      "Iteration 934, loss = 2.93807105\n",
      "Iteration 935, loss = 2.93803705\n",
      "Iteration 936, loss = 2.93771244\n",
      "Iteration 937, loss = 2.93692630\n",
      "Iteration 938, loss = 2.93650493\n",
      "Iteration 939, loss = 2.93610365\n",
      "Iteration 940, loss = 2.93619920\n",
      "Iteration 941, loss = 2.93644768\n",
      "Iteration 942, loss = 2.93600036\n",
      "Iteration 943, loss = 2.93570579\n",
      "Iteration 944, loss = 2.93532715\n",
      "Iteration 945, loss = 2.93493572\n",
      "Iteration 946, loss = 2.93462723\n",
      "Iteration 947, loss = 2.93484804\n",
      "Iteration 948, loss = 2.93443815\n",
      "Iteration 949, loss = 2.93365607\n",
      "Iteration 950, loss = 2.93321272\n",
      "Iteration 951, loss = 2.93318586\n",
      "Iteration 952, loss = 2.93284291\n",
      "Iteration 953, loss = 2.93302742\n",
      "Iteration 954, loss = 2.93250535\n",
      "Iteration 955, loss = 2.93226172\n",
      "Iteration 956, loss = 2.93191765\n",
      "Iteration 957, loss = 2.93154400\n",
      "Iteration 958, loss = 2.93139778\n",
      "Iteration 959, loss = 2.93135851\n",
      "Iteration 960, loss = 2.93096067\n",
      "Iteration 961, loss = 2.93111129\n",
      "Iteration 962, loss = 2.93061622\n",
      "Iteration 963, loss = 2.93082499\n",
      "Iteration 964, loss = 2.93005579\n",
      "Iteration 965, loss = 2.92994792\n",
      "Iteration 966, loss = 2.92991109\n",
      "Iteration 967, loss = 2.92913987\n",
      "Iteration 968, loss = 2.92896918\n",
      "Iteration 969, loss = 2.92890047\n",
      "Iteration 970, loss = 2.92925855\n",
      "Iteration 971, loss = 2.92885257\n",
      "Iteration 972, loss = 2.92826637\n",
      "Iteration 973, loss = 2.92743376\n",
      "Iteration 974, loss = 2.92842754\n",
      "Iteration 975, loss = 2.92732112\n",
      "Iteration 976, loss = 2.92670033\n",
      "Iteration 977, loss = 2.92652354\n",
      "Iteration 978, loss = 2.92713554\n",
      "Iteration 979, loss = 2.92625724\n",
      "Iteration 980, loss = 2.92717992\n",
      "Iteration 981, loss = 2.92600288\n",
      "Iteration 982, loss = 2.92571462\n",
      "Iteration 983, loss = 2.92606492\n",
      "Iteration 984, loss = 2.92532341\n",
      "Iteration 985, loss = 2.92489277\n",
      "Iteration 986, loss = 2.92477024\n",
      "Iteration 987, loss = 2.92450259\n",
      "Iteration 988, loss = 2.92486844\n",
      "Iteration 989, loss = 2.92479056\n",
      "Iteration 990, loss = 2.92406040\n",
      "Iteration 991, loss = 2.92376268\n",
      "Iteration 992, loss = 2.92348539\n",
      "Iteration 993, loss = 2.92271355\n",
      "Iteration 994, loss = 2.92272021\n",
      "Iteration 995, loss = 2.92274119\n",
      "Iteration 996, loss = 2.92270777\n",
      "Iteration 997, loss = 2.92199693\n",
      "Iteration 998, loss = 2.92242783\n",
      "Iteration 999, loss = 2.92224366\n",
      "Iteration 1000, loss = 2.92189110\n",
      "Iteration 1001, loss = 2.92166221\n",
      "Iteration 1002, loss = 2.92106827\n",
      "Iteration 1003, loss = 2.92080653\n",
      "Iteration 1004, loss = 2.92080947\n",
      "Iteration 1005, loss = 2.92061955\n",
      "Iteration 1006, loss = 2.92024345\n",
      "Iteration 1007, loss = 2.91995743\n",
      "Iteration 1008, loss = 2.91941231\n",
      "Iteration 1009, loss = 2.91938326\n",
      "Iteration 1010, loss = 2.91952007\n",
      "Iteration 1011, loss = 2.91908367\n",
      "Iteration 1012, loss = 2.91875015\n",
      "Iteration 1013, loss = 2.91790979\n",
      "Iteration 1014, loss = 2.91765373\n",
      "Iteration 1015, loss = 2.91801316\n",
      "Iteration 1016, loss = 2.91813318\n",
      "Iteration 1017, loss = 2.91760788\n",
      "Iteration 1018, loss = 2.91747887\n",
      "Iteration 1019, loss = 2.91714928\n",
      "Iteration 1020, loss = 2.91731971\n",
      "Iteration 1021, loss = 2.91697561\n",
      "Iteration 1022, loss = 2.91666537\n",
      "Iteration 1023, loss = 2.91644578\n",
      "Iteration 1024, loss = 2.91581927\n",
      "Iteration 1025, loss = 2.91622196\n",
      "Iteration 1026, loss = 2.91625030\n",
      "Iteration 1027, loss = 2.91529808\n",
      "Iteration 1028, loss = 2.91481693\n",
      "Iteration 1029, loss = 2.91435248\n",
      "Iteration 1030, loss = 2.91407806\n",
      "Iteration 1031, loss = 2.91372767\n",
      "Iteration 1032, loss = 2.91428427\n",
      "Iteration 1033, loss = 2.91363701\n",
      "Iteration 1034, loss = 2.91301071\n",
      "Iteration 1035, loss = 2.91267021\n",
      "Iteration 1036, loss = 2.91268530\n",
      "Iteration 1037, loss = 2.91254782\n",
      "Iteration 1038, loss = 2.91236251\n",
      "Iteration 1039, loss = 2.91187097\n",
      "Iteration 1040, loss = 2.91201002\n",
      "Iteration 1041, loss = 2.91218182\n",
      "Iteration 1042, loss = 2.91210756\n",
      "Iteration 1043, loss = 2.91242225\n",
      "Iteration 1044, loss = 2.91170440\n",
      "Iteration 1045, loss = 2.91110994\n",
      "Iteration 1046, loss = 2.91120745\n",
      "Iteration 1047, loss = 2.91035404\n",
      "Iteration 1048, loss = 2.91000954\n",
      "Iteration 1049, loss = 2.90992832\n",
      "Iteration 1050, loss = 2.91015145\n",
      "Iteration 1051, loss = 2.90958178\n",
      "Iteration 1052, loss = 2.90907954\n",
      "Iteration 1053, loss = 2.90902414\n",
      "Iteration 1054, loss = 2.90896872\n",
      "Iteration 1055, loss = 2.90884358\n",
      "Iteration 1056, loss = 2.90897022\n",
      "Iteration 1057, loss = 2.90855201\n",
      "Iteration 1058, loss = 2.90797088\n",
      "Iteration 1059, loss = 2.90792422\n",
      "Iteration 1060, loss = 2.90782684\n",
      "Iteration 1061, loss = 2.90706806\n",
      "Iteration 1062, loss = 2.90659042\n",
      "Iteration 1063, loss = 2.90675902\n",
      "Iteration 1064, loss = 2.90701035\n",
      "Iteration 1065, loss = 2.90611356\n",
      "Iteration 1066, loss = 2.90586316\n",
      "Iteration 1067, loss = 2.90578181\n",
      "Iteration 1068, loss = 2.90538704\n",
      "Iteration 1069, loss = 2.90545504\n",
      "Iteration 1070, loss = 2.90546711\n",
      "Iteration 1071, loss = 2.90529677\n",
      "Iteration 1072, loss = 2.90495749\n",
      "Iteration 1073, loss = 2.90477498\n",
      "Iteration 1074, loss = 2.90413734\n",
      "Iteration 1075, loss = 2.90391702\n",
      "Iteration 1076, loss = 2.90442703\n",
      "Iteration 1077, loss = 2.90455948\n",
      "Iteration 1078, loss = 2.90394749\n",
      "Iteration 1079, loss = 2.90455991\n",
      "Iteration 1080, loss = 2.90459227\n",
      "Iteration 1081, loss = 2.90351326\n",
      "Iteration 1082, loss = 2.90263738\n",
      "Iteration 1083, loss = 2.90292337\n",
      "Iteration 1084, loss = 2.90206053\n",
      "Iteration 1085, loss = 2.90233706\n",
      "Iteration 1086, loss = 2.90202251\n",
      "Iteration 1087, loss = 2.90147624\n",
      "Iteration 1088, loss = 2.90135577\n",
      "Iteration 1089, loss = 2.90108214\n",
      "Iteration 1090, loss = 2.90097380\n",
      "Iteration 1091, loss = 2.90124012\n",
      "Iteration 1092, loss = 2.90067629\n",
      "Iteration 1093, loss = 2.90073430\n",
      "Iteration 1094, loss = 2.90035266\n",
      "Iteration 1095, loss = 2.89973364\n",
      "Iteration 1096, loss = 2.89981653\n",
      "Iteration 1097, loss = 2.89950887\n",
      "Iteration 1098, loss = 2.89886491\n",
      "Iteration 1099, loss = 2.89832910\n",
      "Iteration 1100, loss = 2.89856294\n",
      "Iteration 1101, loss = 2.89888906\n",
      "Iteration 1102, loss = 2.89839099\n",
      "Iteration 1103, loss = 2.89822750\n",
      "Iteration 1104, loss = 2.89799843\n",
      "Iteration 1105, loss = 2.89777847\n",
      "Iteration 1106, loss = 2.89800879\n",
      "Iteration 1107, loss = 2.89730104\n",
      "Iteration 1108, loss = 2.89702566\n",
      "Iteration 1109, loss = 2.89711096\n",
      "Iteration 1110, loss = 2.89703936\n",
      "Iteration 1111, loss = 2.89627886\n",
      "Iteration 1112, loss = 2.89640276\n",
      "Iteration 1113, loss = 2.89664016\n",
      "Iteration 1114, loss = 2.89663934\n",
      "Iteration 1115, loss = 2.89625799\n",
      "Iteration 1116, loss = 2.89620725\n",
      "Iteration 1117, loss = 2.89628792\n",
      "Iteration 1118, loss = 2.89536857\n",
      "Iteration 1119, loss = 2.89560929\n",
      "Iteration 1120, loss = 2.89446767\n",
      "Iteration 1121, loss = 2.89415785\n",
      "Iteration 1122, loss = 2.89453597\n",
      "Iteration 1123, loss = 2.89352981\n",
      "Iteration 1124, loss = 2.89369702\n",
      "Iteration 1125, loss = 2.89325598\n",
      "Iteration 1126, loss = 2.89343986\n",
      "Iteration 1127, loss = 2.89293539\n",
      "Iteration 1128, loss = 2.89246367\n",
      "Iteration 1129, loss = 2.89214573\n",
      "Iteration 1130, loss = 2.89191688\n",
      "Iteration 1131, loss = 2.89213697\n",
      "Iteration 1132, loss = 2.89203337\n",
      "Iteration 1133, loss = 2.89166525\n",
      "Iteration 1134, loss = 2.89148885\n",
      "Iteration 1135, loss = 2.89068639\n",
      "Iteration 1136, loss = 2.89077074\n",
      "Iteration 1137, loss = 2.89052186\n",
      "Iteration 1138, loss = 2.89029873\n",
      "Iteration 1139, loss = 2.89055504\n",
      "Iteration 1140, loss = 2.89024735\n",
      "Iteration 1141, loss = 2.89042169\n",
      "Iteration 1142, loss = 2.88961112\n",
      "Iteration 1143, loss = 2.89088713\n",
      "Iteration 1144, loss = 2.89030439\n",
      "Iteration 1145, loss = 2.88917805\n",
      "Iteration 1146, loss = 2.88901304\n",
      "Iteration 1147, loss = 2.88892544\n",
      "Iteration 1148, loss = 2.88835788\n",
      "Iteration 1149, loss = 2.88851368\n",
      "Iteration 1150, loss = 2.88803051\n",
      "Iteration 1151, loss = 2.88837809\n",
      "Iteration 1152, loss = 2.88816078\n",
      "Iteration 1153, loss = 2.88786998\n",
      "Iteration 1154, loss = 2.88783130\n",
      "Iteration 1155, loss = 2.88759286\n",
      "Iteration 1156, loss = 2.88686711\n",
      "Iteration 1157, loss = 2.88631644\n",
      "Iteration 1158, loss = 2.88662724\n",
      "Iteration 1159, loss = 2.88632932\n",
      "Iteration 1160, loss = 2.88556701\n",
      "Iteration 1161, loss = 2.88578171\n",
      "Iteration 1162, loss = 2.88557044\n",
      "Iteration 1163, loss = 2.88555453\n",
      "Iteration 1164, loss = 2.88516816\n",
      "Iteration 1165, loss = 2.88469694\n",
      "Iteration 1166, loss = 2.88492032\n",
      "Iteration 1167, loss = 2.88449250\n",
      "Iteration 1168, loss = 2.88454409\n",
      "Iteration 1169, loss = 2.88439914\n",
      "Iteration 1170, loss = 2.88406330\n",
      "Iteration 1171, loss = 2.88338617\n",
      "Iteration 1172, loss = 2.88393087\n",
      "Iteration 1173, loss = 2.88447785\n",
      "Iteration 1174, loss = 2.88349930\n",
      "Iteration 1175, loss = 2.88360380\n",
      "Iteration 1176, loss = 2.88341235\n",
      "Iteration 1177, loss = 2.88320490\n",
      "Iteration 1178, loss = 2.88250965\n",
      "Iteration 1179, loss = 2.88165025\n",
      "Iteration 1180, loss = 2.88170138\n",
      "Iteration 1181, loss = 2.88203906\n",
      "Iteration 1182, loss = 2.88168166\n",
      "Iteration 1183, loss = 2.88160630\n",
      "Iteration 1184, loss = 2.88125937\n",
      "Iteration 1185, loss = 2.88070693\n",
      "Iteration 1186, loss = 2.88074347\n",
      "Iteration 1187, loss = 2.88010847\n",
      "Iteration 1188, loss = 2.88004642\n",
      "Iteration 1189, loss = 2.87983683\n",
      "Iteration 1190, loss = 2.88003014\n",
      "Iteration 1191, loss = 2.88004757\n",
      "Iteration 1192, loss = 2.87967610\n",
      "Iteration 1193, loss = 2.87930662\n",
      "Iteration 1194, loss = 2.87968950\n",
      "Iteration 1195, loss = 2.87930310\n",
      "Iteration 1196, loss = 2.87932612\n",
      "Iteration 1197, loss = 2.87944665\n",
      "Iteration 1198, loss = 2.87881800\n",
      "Iteration 1199, loss = 2.87849326\n",
      "Iteration 1200, loss = 2.87821075\n",
      "Iteration 1201, loss = 2.87797543\n",
      "Iteration 1202, loss = 2.87751258\n",
      "Iteration 1203, loss = 2.87786619\n",
      "Iteration 1204, loss = 2.87780895\n",
      "Iteration 1205, loss = 2.87681271\n",
      "Iteration 1206, loss = 2.87652884\n",
      "Iteration 1207, loss = 2.87594485\n",
      "Iteration 1208, loss = 2.87649681\n",
      "Iteration 1209, loss = 2.87577849\n",
      "Iteration 1210, loss = 2.87587361\n",
      "Iteration 1211, loss = 2.87570019\n",
      "Iteration 1212, loss = 2.87657093\n",
      "Iteration 1213, loss = 2.87705634\n",
      "Iteration 1214, loss = 2.87542585\n",
      "Iteration 1215, loss = 2.87453700\n",
      "Iteration 1216, loss = 2.87426135\n",
      "Iteration 1217, loss = 2.87450737\n",
      "Iteration 1218, loss = 2.87448410\n",
      "Iteration 1219, loss = 2.87369141\n",
      "Iteration 1220, loss = 2.87304990\n",
      "Iteration 1221, loss = 2.87471518\n",
      "Iteration 1222, loss = 2.87365786\n",
      "Iteration 1223, loss = 2.87313273\n",
      "Iteration 1224, loss = 2.87285236\n",
      "Iteration 1225, loss = 2.87234627\n",
      "Iteration 1226, loss = 2.87248277\n",
      "Iteration 1227, loss = 2.87219218\n",
      "Iteration 1228, loss = 2.87204748\n",
      "Iteration 1229, loss = 2.87263763\n",
      "Iteration 1230, loss = 2.87163326\n",
      "Iteration 1231, loss = 2.87104722\n",
      "Iteration 1232, loss = 2.87191137\n",
      "Iteration 1233, loss = 2.87149858\n",
      "Iteration 1234, loss = 2.87105943\n",
      "Iteration 1235, loss = 2.87144875\n",
      "Iteration 1236, loss = 2.87059703\n",
      "Iteration 1237, loss = 2.87010108\n",
      "Iteration 1238, loss = 2.86996062\n",
      "Iteration 1239, loss = 2.86983348\n",
      "Iteration 1240, loss = 2.86901109\n",
      "Iteration 1241, loss = 2.86916779\n",
      "Iteration 1242, loss = 2.86995178\n",
      "Iteration 1243, loss = 2.86979864\n",
      "Iteration 1244, loss = 2.86943318\n",
      "Iteration 1245, loss = 2.86918141\n",
      "Iteration 1246, loss = 2.86903777\n",
      "Iteration 1247, loss = 2.86894439\n",
      "Iteration 1248, loss = 2.86839351\n",
      "Iteration 1249, loss = 2.86811208\n",
      "Iteration 1250, loss = 2.86792604\n",
      "Iteration 1251, loss = 2.86759142\n",
      "Iteration 1252, loss = 2.86752677\n",
      "Iteration 1253, loss = 2.86718743\n",
      "Iteration 1254, loss = 2.86711118\n",
      "Iteration 1255, loss = 2.86680818\n",
      "Iteration 1256, loss = 2.86721379\n",
      "Iteration 1257, loss = 2.86655003\n",
      "Iteration 1258, loss = 2.86572960\n",
      "Iteration 1259, loss = 2.86575282\n",
      "Iteration 1260, loss = 2.86589320\n",
      "Iteration 1261, loss = 2.86544455\n",
      "Iteration 1262, loss = 2.86554736\n",
      "Iteration 1263, loss = 2.86565150\n",
      "Iteration 1264, loss = 2.86488981\n",
      "Iteration 1265, loss = 2.86496948\n",
      "Iteration 1266, loss = 2.86467526\n",
      "Iteration 1267, loss = 2.86453473\n",
      "Iteration 1268, loss = 2.86386708\n",
      "Iteration 1269, loss = 2.86383014\n",
      "Iteration 1270, loss = 2.86361774\n",
      "Iteration 1271, loss = 2.86430744\n",
      "Iteration 1272, loss = 2.86387222\n",
      "Iteration 1273, loss = 2.86323499\n",
      "Iteration 1274, loss = 2.86350618\n",
      "Iteration 1275, loss = 2.86291990\n",
      "Iteration 1276, loss = 2.86271431\n",
      "Iteration 1277, loss = 2.86233226\n",
      "Iteration 1278, loss = 2.86242764\n",
      "Iteration 1279, loss = 2.86204911\n",
      "Iteration 1280, loss = 2.86169794\n",
      "Iteration 1281, loss = 2.86141630\n",
      "Iteration 1282, loss = 2.86130383\n",
      "Iteration 1283, loss = 2.86145403\n",
      "Iteration 1284, loss = 2.86137359\n",
      "Iteration 1285, loss = 2.86120888\n",
      "Iteration 1286, loss = 2.86087959\n",
      "Iteration 1287, loss = 2.86057798\n",
      "Iteration 1288, loss = 2.86004037\n",
      "Iteration 1289, loss = 2.86012960\n",
      "Iteration 1290, loss = 2.86044023\n",
      "Iteration 1291, loss = 2.86029233\n",
      "Iteration 1292, loss = 2.85931147\n",
      "Iteration 1293, loss = 2.85891791\n",
      "Iteration 1294, loss = 2.85927259\n",
      "Iteration 1295, loss = 2.85956239\n",
      "Iteration 1296, loss = 2.85951872\n",
      "Iteration 1297, loss = 2.85949137\n",
      "Iteration 1298, loss = 2.85913659\n",
      "Iteration 1299, loss = 2.85830940\n",
      "Iteration 1300, loss = 2.85830493\n",
      "Iteration 1301, loss = 2.85812905\n",
      "Iteration 1302, loss = 2.85771552\n",
      "Iteration 1303, loss = 2.85763521\n",
      "Iteration 1304, loss = 2.85735632\n",
      "Iteration 1305, loss = 2.85707199\n",
      "Iteration 1306, loss = 2.85644869\n",
      "Iteration 1307, loss = 2.85687872\n",
      "Iteration 1308, loss = 2.85689501\n",
      "Iteration 1309, loss = 2.85624828\n",
      "Iteration 1310, loss = 2.85598035\n",
      "Iteration 1311, loss = 2.85590948\n",
      "Iteration 1312, loss = 2.85607293\n",
      "Iteration 1313, loss = 2.85605288\n",
      "Iteration 1314, loss = 2.85568483\n",
      "Iteration 1315, loss = 2.85510437\n",
      "Iteration 1316, loss = 2.85494026\n",
      "Iteration 1317, loss = 2.85526810\n",
      "Iteration 1318, loss = 2.85455002\n",
      "Iteration 1319, loss = 2.85508492\n",
      "Iteration 1320, loss = 2.85490895\n",
      "Iteration 1321, loss = 2.85417773\n",
      "Iteration 1322, loss = 2.85400382\n",
      "Iteration 1323, loss = 2.85395465\n",
      "Iteration 1324, loss = 2.85367379\n",
      "Iteration 1325, loss = 2.85326144\n",
      "Iteration 1326, loss = 2.85274806\n",
      "Iteration 1327, loss = 2.85275826\n",
      "Iteration 1328, loss = 2.85296366\n",
      "Iteration 1329, loss = 2.85309098\n",
      "Iteration 1330, loss = 2.85253161\n",
      "Iteration 1331, loss = 2.85265519\n",
      "Iteration 1332, loss = 2.85176639\n",
      "Iteration 1333, loss = 2.85207649\n",
      "Iteration 1334, loss = 2.85148880\n",
      "Iteration 1335, loss = 2.85128164\n",
      "Iteration 1336, loss = 2.85152337\n",
      "Iteration 1337, loss = 2.85137201\n",
      "Iteration 1338, loss = 2.85101092\n",
      "Iteration 1339, loss = 2.85049078\n",
      "Iteration 1340, loss = 2.85004063\n",
      "Iteration 1341, loss = 2.85068480\n",
      "Iteration 1342, loss = 2.85027645\n",
      "Iteration 1343, loss = 2.84987906\n",
      "Iteration 1344, loss = 2.85018783\n",
      "Iteration 1345, loss = 2.84926402\n",
      "Iteration 1346, loss = 2.84938006\n",
      "Iteration 1347, loss = 2.84894186\n",
      "Iteration 1348, loss = 2.84899008\n",
      "Iteration 1349, loss = 2.84984457\n",
      "Iteration 1350, loss = 2.84834263\n",
      "Iteration 1351, loss = 2.84928339\n",
      "Iteration 1352, loss = 2.84910728\n",
      "Iteration 1353, loss = 2.84844641\n",
      "Iteration 1354, loss = 2.84844117\n",
      "Iteration 1355, loss = 2.84827990\n",
      "Iteration 1356, loss = 2.84809203\n",
      "Iteration 1357, loss = 2.84792436\n",
      "Iteration 1358, loss = 2.84782110\n",
      "Iteration 1359, loss = 2.84785981\n",
      "Iteration 1360, loss = 2.84711450\n",
      "Iteration 1361, loss = 2.84669289\n",
      "Iteration 1362, loss = 2.84655333\n",
      "Iteration 1363, loss = 2.84728799\n",
      "Iteration 1364, loss = 2.84728429\n",
      "Iteration 1365, loss = 2.84694168\n",
      "Iteration 1366, loss = 2.84610802\n",
      "Iteration 1367, loss = 2.84562107\n",
      "Iteration 1368, loss = 2.84582523\n",
      "Iteration 1369, loss = 2.84558189\n",
      "Iteration 1370, loss = 2.84507014\n",
      "Iteration 1371, loss = 2.84475129\n",
      "Iteration 1372, loss = 2.84447522\n",
      "Iteration 1373, loss = 2.84517178\n",
      "Iteration 1374, loss = 2.84486224\n",
      "Iteration 1375, loss = 2.84449644\n",
      "Iteration 1376, loss = 2.84405859\n",
      "Iteration 1377, loss = 2.84399787\n",
      "Iteration 1378, loss = 2.84340584\n",
      "Iteration 1379, loss = 2.84308866\n",
      "Iteration 1380, loss = 2.84279486\n",
      "Iteration 1381, loss = 2.84271954\n",
      "Iteration 1382, loss = 2.84260382\n",
      "Iteration 1383, loss = 2.84233099\n",
      "Iteration 1384, loss = 2.84234728\n",
      "Iteration 1385, loss = 2.84259625\n",
      "Iteration 1386, loss = 2.84288941\n",
      "Iteration 1387, loss = 2.84217392\n",
      "Iteration 1388, loss = 2.84182969\n",
      "Iteration 1389, loss = 2.84175868\n",
      "Iteration 1390, loss = 2.84160280\n",
      "Iteration 1391, loss = 2.84122868\n",
      "Iteration 1392, loss = 2.84103674\n",
      "Iteration 1393, loss = 2.84090700\n",
      "Iteration 1394, loss = 2.84108344\n",
      "Iteration 1395, loss = 2.84105436\n",
      "Iteration 1396, loss = 2.84086031\n",
      "Iteration 1397, loss = 2.84045186\n",
      "Iteration 1398, loss = 2.84018891\n",
      "Iteration 1399, loss = 2.83975562\n",
      "Iteration 1400, loss = 2.83929788\n",
      "Iteration 1401, loss = 2.84056124\n",
      "Iteration 1402, loss = 2.84066099\n",
      "Iteration 1403, loss = 2.84017033\n",
      "Iteration 1404, loss = 2.83969019\n",
      "Iteration 1405, loss = 2.83962479\n",
      "Iteration 1406, loss = 2.83972720\n",
      "Iteration 1407, loss = 2.83920086\n",
      "Iteration 1408, loss = 2.83854346\n",
      "Iteration 1409, loss = 2.83809553\n",
      "Iteration 1410, loss = 2.83832669\n",
      "Iteration 1411, loss = 2.83840702\n",
      "Iteration 1412, loss = 2.83752496\n",
      "Iteration 1413, loss = 2.83713775\n",
      "Iteration 1414, loss = 2.83691786\n",
      "Iteration 1415, loss = 2.83694831\n",
      "Iteration 1416, loss = 2.83735605\n",
      "Iteration 1417, loss = 2.83692601\n",
      "Iteration 1418, loss = 2.83645398\n",
      "Iteration 1419, loss = 2.83635863\n",
      "Iteration 1420, loss = 2.83620041\n",
      "Iteration 1421, loss = 2.83631539\n",
      "Iteration 1422, loss = 2.83609643\n",
      "Iteration 1423, loss = 2.83545686\n",
      "Iteration 1424, loss = 2.83562371\n",
      "Iteration 1425, loss = 2.83569310\n",
      "Iteration 1426, loss = 2.83502330\n",
      "Iteration 1427, loss = 2.83501841\n",
      "Iteration 1428, loss = 2.83455145\n",
      "Iteration 1429, loss = 2.83511383\n",
      "Iteration 1430, loss = 2.83403509\n",
      "Iteration 1431, loss = 2.83409623\n",
      "Iteration 1432, loss = 2.83347503\n",
      "Iteration 1433, loss = 2.83371641\n",
      "Iteration 1434, loss = 2.83332226\n",
      "Iteration 1435, loss = 2.83314574\n",
      "Iteration 1436, loss = 2.83349393\n",
      "Iteration 1437, loss = 2.83391646\n",
      "Iteration 1438, loss = 2.83287907\n",
      "Iteration 1439, loss = 2.83300984\n",
      "Iteration 1440, loss = 2.83253527\n",
      "Iteration 1441, loss = 2.83256627\n",
      "Iteration 1442, loss = 2.83295915\n",
      "Iteration 1443, loss = 2.83272534\n",
      "Iteration 1444, loss = 2.83251133\n",
      "Iteration 1445, loss = 2.83204286\n",
      "Iteration 1446, loss = 2.83164043\n",
      "Iteration 1447, loss = 2.83151598\n",
      "Iteration 1448, loss = 2.83087437\n",
      "Iteration 1449, loss = 2.83109959\n",
      "Iteration 1450, loss = 2.83120250\n",
      "Iteration 1451, loss = 2.83168772\n",
      "Iteration 1452, loss = 2.83151453\n",
      "Iteration 1453, loss = 2.83068948\n",
      "Iteration 1454, loss = 2.83014535\n",
      "Iteration 1455, loss = 2.82992846\n",
      "Iteration 1456, loss = 2.82934842\n",
      "Iteration 1457, loss = 2.83003025\n",
      "Iteration 1458, loss = 2.82972024\n",
      "Iteration 1459, loss = 2.82903153\n",
      "Iteration 1460, loss = 2.82878053\n",
      "Iteration 1461, loss = 2.82917441\n",
      "Iteration 1462, loss = 2.82889564\n",
      "Iteration 1463, loss = 2.82854319\n",
      "Iteration 1464, loss = 2.82841901\n",
      "Iteration 1465, loss = 2.82822972\n",
      "Iteration 1466, loss = 2.82811680\n",
      "Iteration 1467, loss = 2.82866991\n",
      "Iteration 1468, loss = 2.82818990\n",
      "Iteration 1469, loss = 2.82798032\n",
      "Iteration 1470, loss = 2.82761199\n",
      "Iteration 1471, loss = 2.82760536\n",
      "Iteration 1472, loss = 2.82722566\n",
      "Iteration 1473, loss = 2.82664739\n",
      "Iteration 1474, loss = 2.82681117\n",
      "Iteration 1475, loss = 2.82626769\n",
      "Iteration 1476, loss = 2.82629421\n",
      "Iteration 1477, loss = 2.82605801\n",
      "Iteration 1478, loss = 2.82569421\n",
      "Iteration 1479, loss = 2.82549865\n",
      "Iteration 1480, loss = 2.82577364\n",
      "Iteration 1481, loss = 2.82562095\n",
      "Iteration 1482, loss = 2.82603142\n",
      "Iteration 1483, loss = 2.82573748\n",
      "Iteration 1484, loss = 2.82565931\n",
      "Iteration 1485, loss = 2.82557779\n",
      "Iteration 1486, loss = 2.82470938\n",
      "Iteration 1487, loss = 2.82519083\n",
      "Iteration 1488, loss = 2.82493017\n",
      "Iteration 1489, loss = 2.82491432\n",
      "Iteration 1490, loss = 2.82426625\n",
      "Iteration 1491, loss = 2.82432641\n",
      "Iteration 1492, loss = 2.82433122\n",
      "Iteration 1493, loss = 2.82428280\n",
      "Iteration 1494, loss = 2.82412080\n",
      "Iteration 1495, loss = 2.82338424\n",
      "Iteration 1496, loss = 2.82355773\n",
      "Iteration 1497, loss = 2.82251809\n",
      "Iteration 1498, loss = 2.82265077\n",
      "Iteration 1499, loss = 2.82373681\n",
      "Iteration 1500, loss = 2.82355005\n",
      "Iteration 1501, loss = 2.82270525\n",
      "Iteration 1502, loss = 2.82262896\n",
      "Iteration 1503, loss = 2.82254586\n",
      "Iteration 1504, loss = 2.82225224\n",
      "Iteration 1505, loss = 2.82202779\n",
      "Iteration 1506, loss = 2.82205671\n",
      "Iteration 1507, loss = 2.82202109\n",
      "Iteration 1508, loss = 2.82144906\n",
      "Iteration 1509, loss = 2.82112931\n",
      "Iteration 1510, loss = 2.82126047\n",
      "Iteration 1511, loss = 2.82053698\n",
      "Iteration 1512, loss = 2.82098935\n",
      "Iteration 1513, loss = 2.82094913\n",
      "Iteration 1514, loss = 2.82033897\n",
      "Iteration 1515, loss = 2.82004183\n",
      "Iteration 1516, loss = 2.82040406\n",
      "Iteration 1517, loss = 2.82071925\n",
      "Iteration 1518, loss = 2.82058241\n",
      "Iteration 1519, loss = 2.81925681\n",
      "Iteration 1520, loss = 2.81991021\n",
      "Iteration 1521, loss = 2.81978670\n",
      "Iteration 1522, loss = 2.81964571\n",
      "Iteration 1523, loss = 2.81893164\n",
      "Iteration 1524, loss = 2.81851824\n",
      "Iteration 1525, loss = 2.81840251\n",
      "Iteration 1526, loss = 2.81921178\n",
      "Iteration 1527, loss = 2.81817745\n",
      "Iteration 1528, loss = 2.81802168\n",
      "Iteration 1529, loss = 2.81831038\n",
      "Iteration 1530, loss = 2.81886021\n",
      "Iteration 1531, loss = 2.81722182\n",
      "Iteration 1532, loss = 2.81700649\n",
      "Iteration 1533, loss = 2.81771398\n",
      "Iteration 1534, loss = 2.81695918\n",
      "Iteration 1535, loss = 2.81665164\n",
      "Iteration 1536, loss = 2.81657988\n",
      "Iteration 1537, loss = 2.81642027\n",
      "Iteration 1538, loss = 2.81598240\n",
      "Iteration 1539, loss = 2.81628127\n",
      "Iteration 1540, loss = 2.81618361\n",
      "Iteration 1541, loss = 2.81578903\n",
      "Iteration 1542, loss = 2.81552569\n",
      "Iteration 1543, loss = 2.81556621\n",
      "Iteration 1544, loss = 2.81537973\n",
      "Iteration 1545, loss = 2.81496692\n",
      "Iteration 1546, loss = 2.81470063\n",
      "Iteration 1547, loss = 2.81530846\n",
      "Iteration 1548, loss = 2.81437883\n",
      "Iteration 1549, loss = 2.81463269\n",
      "Iteration 1550, loss = 2.81465055\n",
      "Iteration 1551, loss = 2.81431823\n",
      "Iteration 1552, loss = 2.81450498\n",
      "Iteration 1553, loss = 2.81394030\n",
      "Iteration 1554, loss = 2.81396770\n",
      "Iteration 1555, loss = 2.81352901\n",
      "Iteration 1556, loss = 2.81331402\n",
      "Iteration 1557, loss = 2.81306867\n",
      "Iteration 1558, loss = 2.81283893\n",
      "Iteration 1559, loss = 2.81251893\n",
      "Iteration 1560, loss = 2.81225796\n",
      "Iteration 1561, loss = 2.81218481\n",
      "Iteration 1562, loss = 2.81201470\n",
      "Iteration 1563, loss = 2.81216987\n",
      "Iteration 1564, loss = 2.81224079\n",
      "Iteration 1565, loss = 2.81196140\n",
      "Iteration 1566, loss = 2.81157515\n",
      "Iteration 1567, loss = 2.81180166\n",
      "Iteration 1568, loss = 2.81127877\n",
      "Iteration 1569, loss = 2.81152668\n",
      "Iteration 1570, loss = 2.81130650\n",
      "Iteration 1571, loss = 2.81074730\n",
      "Iteration 1572, loss = 2.81112578\n",
      "Iteration 1573, loss = 2.81077624\n",
      "Iteration 1574, loss = 2.81055902\n",
      "Iteration 1575, loss = 2.81019099\n",
      "Iteration 1576, loss = 2.81000263\n",
      "Iteration 1577, loss = 2.81003050\n",
      "Iteration 1578, loss = 2.80966178\n",
      "Iteration 1579, loss = 2.80955327\n",
      "Iteration 1580, loss = 2.80918834\n",
      "Iteration 1581, loss = 2.80878833\n",
      "Iteration 1582, loss = 2.80910274\n",
      "Iteration 1583, loss = 2.80988979\n",
      "Iteration 1584, loss = 2.80939589\n",
      "Iteration 1585, loss = 2.80988779\n",
      "Iteration 1586, loss = 2.80859656\n",
      "Iteration 1587, loss = 2.80830650\n",
      "Iteration 1588, loss = 2.80879755\n",
      "Iteration 1589, loss = 2.80843171\n",
      "Iteration 1590, loss = 2.80730474\n",
      "Iteration 1591, loss = 2.80846127\n",
      "Iteration 1592, loss = 2.80854945\n",
      "Iteration 1593, loss = 2.80771056\n",
      "Iteration 1594, loss = 2.80722198\n",
      "Iteration 1595, loss = 2.80732263\n",
      "Iteration 1596, loss = 2.80684819\n",
      "Iteration 1597, loss = 2.80679739\n",
      "Iteration 1598, loss = 2.80693850\n",
      "Iteration 1599, loss = 2.80642308\n",
      "Iteration 1600, loss = 2.80661096\n",
      "Iteration 1601, loss = 2.80709969\n",
      "Iteration 1602, loss = 2.80656308\n",
      "Iteration 1603, loss = 2.80592542\n",
      "Iteration 1604, loss = 2.80589365\n",
      "Iteration 1605, loss = 2.80582386\n",
      "Iteration 1606, loss = 2.80430106\n",
      "Iteration 1607, loss = 2.80657268\n",
      "Iteration 1608, loss = 2.80524693\n",
      "Iteration 1609, loss = 2.80553955\n",
      "Iteration 1610, loss = 2.80477699\n",
      "Iteration 1611, loss = 2.80419865\n",
      "Iteration 1612, loss = 2.80426130\n",
      "Iteration 1613, loss = 2.80472912\n",
      "Iteration 1614, loss = 2.80432899\n",
      "Iteration 1615, loss = 2.80389929\n",
      "Iteration 1616, loss = 2.80337004\n",
      "Iteration 1617, loss = 2.80415146\n",
      "Iteration 1618, loss = 2.80432611\n",
      "Iteration 1619, loss = 2.80313501\n",
      "Iteration 1620, loss = 2.80342329\n",
      "Iteration 1621, loss = 2.80391680\n",
      "Iteration 1622, loss = 2.80322053\n",
      "Iteration 1623, loss = 2.80255537\n",
      "Iteration 1624, loss = 2.80259080\n",
      "Iteration 1625, loss = 2.80220515\n",
      "Iteration 1626, loss = 2.80213203\n",
      "Iteration 1627, loss = 2.80210777\n",
      "Iteration 1628, loss = 2.80199975\n",
      "Iteration 1629, loss = 2.80173483\n",
      "Iteration 1630, loss = 2.80176501\n",
      "Iteration 1631, loss = 2.80180416\n",
      "Iteration 1632, loss = 2.80183429\n",
      "Iteration 1633, loss = 2.80091695\n",
      "Iteration 1634, loss = 2.80101472\n",
      "Iteration 1635, loss = 2.80125082\n",
      "Iteration 1636, loss = 2.80187037\n",
      "Iteration 1637, loss = 2.80148329\n",
      "Iteration 1638, loss = 2.80034279\n",
      "Iteration 1639, loss = 2.80075894\n",
      "Iteration 1640, loss = 2.80010879\n",
      "Iteration 1641, loss = 2.79995406\n",
      "Iteration 1642, loss = 2.80019905\n",
      "Iteration 1643, loss = 2.80012687\n",
      "Iteration 1644, loss = 2.79979832\n",
      "Iteration 1645, loss = 2.79985742\n",
      "Iteration 1646, loss = 2.80015663\n",
      "Iteration 1647, loss = 2.79985501\n",
      "Iteration 1648, loss = 2.79895288\n",
      "Iteration 1649, loss = 2.79896142\n",
      "Iteration 1650, loss = 2.80042493\n",
      "Iteration 1651, loss = 2.79875904\n",
      "Iteration 1652, loss = 2.79900924\n",
      "Iteration 1653, loss = 2.79883073\n",
      "Iteration 1654, loss = 2.79826437\n",
      "Iteration 1655, loss = 2.79770631\n",
      "Iteration 1656, loss = 2.79847026\n",
      "Iteration 1657, loss = 2.79742466\n",
      "Iteration 1658, loss = 2.79727006\n",
      "Iteration 1659, loss = 2.79682437\n",
      "Iteration 1660, loss = 2.79761786\n",
      "Iteration 1661, loss = 2.79739417\n",
      "Iteration 1662, loss = 2.79775309\n",
      "Iteration 1663, loss = 2.79604830\n",
      "Iteration 1664, loss = 2.79594799\n",
      "Iteration 1665, loss = 2.79668292\n",
      "Iteration 1666, loss = 2.79578065\n",
      "Iteration 1667, loss = 2.79590841\n",
      "Iteration 1668, loss = 2.79524391\n",
      "Iteration 1669, loss = 2.79625339\n",
      "Iteration 1670, loss = 2.79618476\n",
      "Iteration 1671, loss = 2.79484467\n",
      "Iteration 1672, loss = 2.79561389\n",
      "Iteration 1673, loss = 2.79551594\n",
      "Iteration 1674, loss = 2.79579915\n",
      "Iteration 1675, loss = 2.79506789\n",
      "Iteration 1676, loss = 2.79470706\n",
      "Iteration 1677, loss = 2.79411658\n",
      "Iteration 1678, loss = 2.79431971\n",
      "Iteration 1679, loss = 2.79470993\n",
      "Iteration 1680, loss = 2.79420910\n",
      "Iteration 1681, loss = 2.79353477\n",
      "Iteration 1682, loss = 2.79372286\n",
      "Iteration 1683, loss = 2.79364388\n",
      "Iteration 1684, loss = 2.79308581\n",
      "Iteration 1685, loss = 2.79274775\n",
      "Iteration 1686, loss = 2.79318611\n",
      "Iteration 1687, loss = 2.79275887\n",
      "Iteration 1688, loss = 2.79239652\n",
      "Iteration 1689, loss = 2.79236412\n",
      "Iteration 1690, loss = 2.79245948\n",
      "Iteration 1691, loss = 2.79250618\n",
      "Iteration 1692, loss = 2.79304157\n",
      "Iteration 1693, loss = 2.79195568\n",
      "Iteration 1694, loss = 2.79232957\n",
      "Iteration 1695, loss = 2.79258774\n",
      "Iteration 1696, loss = 2.79124995\n",
      "Iteration 1697, loss = 2.79161288\n",
      "Iteration 1698, loss = 2.79106720\n",
      "Iteration 1699, loss = 2.79114941\n",
      "Iteration 1700, loss = 2.79055290\n",
      "Iteration 1701, loss = 2.79046568\n",
      "Iteration 1702, loss = 2.79039260\n",
      "Iteration 1703, loss = 2.79079072\n",
      "Iteration 1704, loss = 2.79040102\n",
      "Iteration 1705, loss = 2.79047682\n",
      "Iteration 1706, loss = 2.79062014\n",
      "Iteration 1707, loss = 2.79015434\n",
      "Iteration 1708, loss = 2.79052508\n",
      "Iteration 1709, loss = 2.79098399\n",
      "Iteration 1710, loss = 2.78998066\n",
      "Iteration 1711, loss = 2.78944262\n",
      "Iteration 1712, loss = 2.78905499\n",
      "Iteration 1713, loss = 2.78946861\n",
      "Iteration 1714, loss = 2.78913626\n",
      "Iteration 1715, loss = 2.78913190\n",
      "Iteration 1716, loss = 2.78875769\n",
      "Iteration 1717, loss = 2.78884701\n",
      "Iteration 1718, loss = 2.78859360\n",
      "Iteration 1719, loss = 2.78886853\n",
      "Iteration 1720, loss = 2.78917587\n",
      "Iteration 1721, loss = 2.78912410\n",
      "Iteration 1722, loss = 2.78863133\n",
      "Iteration 1723, loss = 2.78812843\n",
      "Iteration 1724, loss = 2.78726536\n",
      "Iteration 1725, loss = 2.78750758\n",
      "Iteration 1726, loss = 2.78704266\n",
      "Iteration 1727, loss = 2.78665747\n",
      "Iteration 1728, loss = 2.78685219\n",
      "Iteration 1729, loss = 2.78664563\n",
      "Iteration 1730, loss = 2.78697955\n",
      "Iteration 1731, loss = 2.78654597\n",
      "Iteration 1732, loss = 2.78617321\n",
      "Iteration 1733, loss = 2.78620211\n",
      "Iteration 1734, loss = 2.78729440\n",
      "Iteration 1735, loss = 2.78637334\n",
      "Iteration 1736, loss = 2.78576988\n",
      "Iteration 1737, loss = 2.78531097\n",
      "Iteration 1738, loss = 2.78517228\n",
      "Iteration 1739, loss = 2.78506428\n",
      "Iteration 1740, loss = 2.78485368\n",
      "Iteration 1741, loss = 2.78509560\n",
      "Iteration 1742, loss = 2.78531785\n",
      "Iteration 1743, loss = 2.78505907\n",
      "Iteration 1744, loss = 2.78482952\n",
      "Iteration 1745, loss = 2.78460610\n",
      "Iteration 1746, loss = 2.78486007\n",
      "Iteration 1747, loss = 2.78469624\n",
      "Iteration 1748, loss = 2.78446198\n",
      "Iteration 1749, loss = 2.78418624\n",
      "Iteration 1750, loss = 2.78423415\n",
      "Iteration 1751, loss = 2.78321362\n",
      "Iteration 1752, loss = 2.78359963\n",
      "Iteration 1753, loss = 2.78352672\n",
      "Iteration 1754, loss = 2.78355286\n",
      "Iteration 1755, loss = 2.78276948\n",
      "Iteration 1756, loss = 2.78304875\n",
      "Iteration 1757, loss = 2.78301481\n",
      "Iteration 1758, loss = 2.78319589\n",
      "Iteration 1759, loss = 2.78271143\n",
      "Iteration 1760, loss = 2.78269889\n",
      "Iteration 1761, loss = 2.78240121\n",
      "Iteration 1762, loss = 2.78208722\n",
      "Iteration 1763, loss = 2.78227650\n",
      "Iteration 1764, loss = 2.78203492\n",
      "Iteration 1765, loss = 2.78194631\n",
      "Iteration 1766, loss = 2.78172639\n",
      "Iteration 1767, loss = 2.78092135\n",
      "Iteration 1768, loss = 2.78111487\n",
      "Iteration 1769, loss = 2.78103981\n",
      "Iteration 1770, loss = 2.78065024\n",
      "Iteration 1771, loss = 2.78142203\n",
      "Iteration 1772, loss = 2.78117699\n",
      "Iteration 1773, loss = 2.78031753\n",
      "Iteration 1774, loss = 2.78018576\n",
      "Iteration 1775, loss = 2.78007582\n",
      "Iteration 1776, loss = 2.78033004\n",
      "Iteration 1777, loss = 2.78022760\n",
      "Iteration 1778, loss = 2.78013165\n",
      "Iteration 1779, loss = 2.78045637\n",
      "Iteration 1780, loss = 2.78033205\n",
      "Iteration 1781, loss = 2.77974724\n",
      "Iteration 1782, loss = 2.77981345\n",
      "Iteration 1783, loss = 2.77985815\n",
      "Iteration 1784, loss = 2.77918239\n",
      "Iteration 1785, loss = 2.77869054\n",
      "Iteration 1786, loss = 2.77856043\n",
      "Iteration 1787, loss = 2.77853532\n",
      "Iteration 1788, loss = 2.77881284\n",
      "Iteration 1789, loss = 2.77888144\n",
      "Iteration 1790, loss = 2.78047988\n",
      "Iteration 1791, loss = 2.78006988\n",
      "Iteration 1792, loss = 2.77813627\n",
      "Iteration 1793, loss = 2.77804950\n",
      "Iteration 1794, loss = 2.77765542\n",
      "Iteration 1795, loss = 2.77865602\n",
      "Iteration 1796, loss = 2.77765115\n",
      "Iteration 1797, loss = 2.77772656\n",
      "Iteration 1798, loss = 2.77827456\n",
      "Iteration 1799, loss = 2.77733504\n",
      "Iteration 1800, loss = 2.77676452\n",
      "Iteration 1801, loss = 2.77691770\n",
      "Iteration 1802, loss = 2.77682504\n",
      "Iteration 1803, loss = 2.77723414\n",
      "Iteration 1804, loss = 2.77708825\n",
      "Iteration 1805, loss = 2.77664599\n",
      "Iteration 1806, loss = 2.77644525\n",
      "Iteration 1807, loss = 2.77579979\n",
      "Iteration 1808, loss = 2.77619972\n",
      "Iteration 1809, loss = 2.77585821\n",
      "Iteration 1810, loss = 2.77574338\n",
      "Iteration 1811, loss = 2.77590770\n",
      "Iteration 1812, loss = 2.77496920\n",
      "Iteration 1813, loss = 2.77556439\n",
      "Iteration 1814, loss = 2.77486696\n",
      "Iteration 1815, loss = 2.77487834\n",
      "Iteration 1816, loss = 2.77454586\n",
      "Iteration 1817, loss = 2.77454539\n",
      "Iteration 1818, loss = 2.77411601\n",
      "Iteration 1819, loss = 2.77452246\n",
      "Iteration 1820, loss = 2.77400752\n",
      "Iteration 1821, loss = 2.77446121\n",
      "Iteration 1822, loss = 2.77482234\n",
      "Iteration 1823, loss = 2.77432971\n",
      "Iteration 1824, loss = 2.77355240\n",
      "Iteration 1825, loss = 2.77330089\n",
      "Iteration 1826, loss = 2.77317211\n",
      "Iteration 1827, loss = 2.77311626\n",
      "Iteration 1828, loss = 2.77292912\n",
      "Iteration 1829, loss = 2.77281171\n",
      "Iteration 1830, loss = 2.77293956\n",
      "Iteration 1831, loss = 2.77293253\n",
      "Iteration 1832, loss = 2.77317350\n",
      "Iteration 1833, loss = 2.77284279\n",
      "Iteration 1834, loss = 2.77231246\n",
      "Iteration 1835, loss = 2.77224343\n",
      "Iteration 1836, loss = 2.77167992\n",
      "Iteration 1837, loss = 2.77120651\n",
      "Iteration 1838, loss = 2.77165704\n",
      "Iteration 1839, loss = 2.77201635\n",
      "Iteration 1840, loss = 2.77200188\n",
      "Iteration 1841, loss = 2.77188321\n",
      "Iteration 1842, loss = 2.77236651\n",
      "Iteration 1843, loss = 2.77065684\n",
      "Iteration 1844, loss = 2.77112335\n",
      "Iteration 1845, loss = 2.77113188\n",
      "Iteration 1846, loss = 2.77034319\n",
      "Iteration 1847, loss = 2.77027730\n",
      "Iteration 1848, loss = 2.77041663\n",
      "Iteration 1849, loss = 2.76965374\n",
      "Iteration 1850, loss = 2.76991477\n",
      "Iteration 1851, loss = 2.76965677\n",
      "Iteration 1852, loss = 2.76922853\n",
      "Iteration 1853, loss = 2.76963666\n",
      "Iteration 1854, loss = 2.76973690\n",
      "Iteration 1855, loss = 2.76963162\n",
      "Iteration 1856, loss = 2.76955263\n",
      "Iteration 1857, loss = 2.77026195\n",
      "Iteration 1858, loss = 2.76998960\n",
      "Iteration 1859, loss = 2.76919383\n",
      "Iteration 1860, loss = 2.76832818\n",
      "Iteration 1861, loss = 2.76861919\n",
      "Iteration 1862, loss = 2.76993393\n",
      "Iteration 1863, loss = 2.76865202\n",
      "Iteration 1864, loss = 2.76797417\n",
      "Iteration 1865, loss = 2.76785906\n",
      "Iteration 1866, loss = 2.76773462\n",
      "Iteration 1867, loss = 2.76872567\n",
      "Iteration 1868, loss = 2.76937001\n",
      "Iteration 1869, loss = 2.76801974\n",
      "Iteration 1870, loss = 2.76751740\n",
      "Iteration 1871, loss = 2.76713140\n",
      "Iteration 1872, loss = 2.76763627\n",
      "Iteration 1873, loss = 2.76692415\n",
      "Iteration 1874, loss = 2.76712435\n",
      "Iteration 1875, loss = 2.76696989\n",
      "Iteration 1876, loss = 2.76705973\n",
      "Iteration 1877, loss = 2.76683712\n",
      "Iteration 1878, loss = 2.76624777\n",
      "Iteration 1879, loss = 2.76680521\n",
      "Iteration 1880, loss = 2.76609502\n",
      "Iteration 1881, loss = 2.76722982\n",
      "Iteration 1882, loss = 2.76637847\n",
      "Iteration 1883, loss = 2.76531219\n",
      "Iteration 1884, loss = 2.76586434\n",
      "Iteration 1885, loss = 2.76533897\n",
      "Iteration 1886, loss = 2.76487922\n",
      "Iteration 1887, loss = 2.76463214\n",
      "Iteration 1888, loss = 2.76472179\n",
      "Iteration 1889, loss = 2.76534467\n",
      "Iteration 1890, loss = 2.76506861\n",
      "Iteration 1891, loss = 2.76489325\n",
      "Iteration 1892, loss = 2.76476467\n",
      "Iteration 1893, loss = 2.76472070\n",
      "Iteration 1894, loss = 2.76422724\n",
      "Iteration 1895, loss = 2.76405100\n",
      "Iteration 1896, loss = 2.76375013\n",
      "Iteration 1897, loss = 2.76351750\n",
      "Iteration 1898, loss = 2.76371004\n",
      "Iteration 1899, loss = 2.76352768\n",
      "Iteration 1900, loss = 2.76306236\n",
      "Iteration 1901, loss = 2.76304200\n",
      "Iteration 1902, loss = 2.76330660\n",
      "Iteration 1903, loss = 2.76282598\n",
      "Iteration 1904, loss = 2.76317067\n",
      "Iteration 1905, loss = 2.76327437\n",
      "Iteration 1906, loss = 2.76273575\n",
      "Iteration 1907, loss = 2.76267375\n",
      "Iteration 1908, loss = 2.76343353\n",
      "Iteration 1909, loss = 2.76258139\n",
      "Iteration 1910, loss = 2.76183623\n",
      "Iteration 1911, loss = 2.76216847\n",
      "Iteration 1912, loss = 2.76221330\n",
      "Iteration 1913, loss = 2.76159726\n",
      "Iteration 1914, loss = 2.76187006\n",
      "Iteration 1915, loss = 2.76135860\n",
      "Iteration 1916, loss = 2.76162981\n",
      "Iteration 1917, loss = 2.76183375\n",
      "Iteration 1918, loss = 2.76030741\n",
      "Iteration 1919, loss = 2.76175542\n",
      "Iteration 1920, loss = 2.76131822\n",
      "Iteration 1921, loss = 2.76095936\n",
      "Iteration 1922, loss = 2.76091326\n",
      "Iteration 1923, loss = 2.75955847\n",
      "Iteration 1924, loss = 2.76055082\n",
      "Iteration 1925, loss = 2.76105766\n",
      "Iteration 1926, loss = 2.76068934\n",
      "Iteration 1927, loss = 2.76067999\n",
      "Iteration 1928, loss = 2.76042176\n",
      "Iteration 1929, loss = 2.75972081\n",
      "Iteration 1930, loss = 2.75933408\n",
      "Iteration 1931, loss = 2.75914073\n",
      "Iteration 1932, loss = 2.75997213\n",
      "Iteration 1933, loss = 2.75951423\n",
      "Iteration 1934, loss = 2.75898159\n",
      "Iteration 1935, loss = 2.75865242\n",
      "Iteration 1936, loss = 2.75827094\n",
      "Iteration 1937, loss = 2.75929825\n",
      "Iteration 1938, loss = 2.75986596\n",
      "Iteration 1939, loss = 2.76034311\n",
      "Iteration 1940, loss = 2.75960757\n",
      "Iteration 1941, loss = 2.75821226\n",
      "Iteration 1942, loss = 2.75804663\n",
      "Iteration 1943, loss = 2.75783149\n",
      "Iteration 1944, loss = 2.75766604\n",
      "Iteration 1945, loss = 2.75825732\n",
      "Iteration 1946, loss = 2.75826763\n",
      "Iteration 1947, loss = 2.75699004\n",
      "Iteration 1948, loss = 2.75713910\n",
      "Iteration 1949, loss = 2.75717353\n",
      "Iteration 1950, loss = 2.75712289\n",
      "Iteration 1951, loss = 2.75730702\n",
      "Iteration 1952, loss = 2.75674487\n",
      "Iteration 1953, loss = 2.75657854\n",
      "Iteration 1954, loss = 2.75714627\n",
      "Iteration 1955, loss = 2.75659439\n",
      "Iteration 1956, loss = 2.75566028\n",
      "Iteration 1957, loss = 2.75605561\n",
      "Iteration 1958, loss = 2.75658413\n",
      "Iteration 1959, loss = 2.75620465\n",
      "Iteration 1960, loss = 2.75643283\n",
      "Iteration 1961, loss = 2.75616499\n",
      "Iteration 1962, loss = 2.75574764\n",
      "Iteration 1963, loss = 2.75512781\n",
      "Iteration 1964, loss = 2.75507197\n",
      "Iteration 1965, loss = 2.75551187\n",
      "Iteration 1966, loss = 2.75516070\n",
      "Iteration 1967, loss = 2.75514051\n",
      "Iteration 1968, loss = 2.75548484\n",
      "Iteration 1969, loss = 2.75511391\n",
      "Iteration 1970, loss = 2.75499524\n",
      "Iteration 1971, loss = 2.75530266\n",
      "Iteration 1972, loss = 2.75522636\n",
      "Iteration 1973, loss = 2.75369995\n",
      "Iteration 1974, loss = 2.75443630\n",
      "Iteration 1975, loss = 2.75393027\n",
      "Iteration 1976, loss = 2.75332225\n",
      "Iteration 1977, loss = 2.75305010\n",
      "Iteration 1978, loss = 2.75287987\n",
      "Iteration 1979, loss = 2.75264876\n",
      "Iteration 1980, loss = 2.75269201\n",
      "Iteration 1981, loss = 2.75236343\n",
      "Iteration 1982, loss = 2.75252393\n",
      "Iteration 1983, loss = 2.75239471\n",
      "Iteration 1984, loss = 2.75256786\n",
      "Iteration 1985, loss = 2.75201436\n",
      "Iteration 1986, loss = 2.75254990\n",
      "Iteration 1987, loss = 2.75198691\n",
      "Iteration 1988, loss = 2.75192288\n",
      "Iteration 1989, loss = 2.75174301\n",
      "Iteration 1990, loss = 2.75159430\n",
      "Iteration 1991, loss = 2.75135661\n",
      "Iteration 1992, loss = 2.75186209\n",
      "Iteration 1993, loss = 2.75182836\n",
      "Iteration 1994, loss = 2.75164223\n",
      "Iteration 1995, loss = 2.75131828\n",
      "Iteration 1996, loss = 2.75124119\n",
      "Iteration 1997, loss = 2.75074126\n",
      "Iteration 1998, loss = 2.75065120\n",
      "Iteration 1999, loss = 2.75060533\n",
      "Iteration 2000, loss = 2.75097012\n",
      "Iteration 2001, loss = 2.75031008\n",
      "Iteration 2002, loss = 2.74963869\n",
      "Iteration 2003, loss = 2.75042767\n",
      "Iteration 2004, loss = 2.75073735\n",
      "Iteration 2005, loss = 2.75018244\n",
      "Iteration 2006, loss = 2.74990585\n",
      "Iteration 2007, loss = 2.75004120\n",
      "Iteration 2008, loss = 2.74934432\n",
      "Iteration 2009, loss = 2.74891769\n",
      "Iteration 2010, loss = 2.74939949\n",
      "Iteration 2011, loss = 2.74968034\n",
      "Iteration 2012, loss = 2.74999931\n",
      "Iteration 2013, loss = 2.74927701\n",
      "Iteration 2014, loss = 2.74872905\n",
      "Iteration 2015, loss = 2.74872548\n",
      "Iteration 2016, loss = 2.74923702\n",
      "Iteration 2017, loss = 2.74851672\n",
      "Iteration 2018, loss = 2.74836066\n",
      "Iteration 2019, loss = 2.74884064\n",
      "Iteration 2020, loss = 2.74853004\n",
      "Iteration 2021, loss = 2.74846809\n",
      "Iteration 2022, loss = 2.74822861\n",
      "Iteration 2023, loss = 2.74833355\n",
      "Iteration 2024, loss = 2.74777378\n",
      "Iteration 2025, loss = 2.74726441\n",
      "Iteration 2026, loss = 2.74742117\n",
      "Iteration 2027, loss = 2.74744083\n",
      "Iteration 2028, loss = 2.74749755\n",
      "Iteration 2029, loss = 2.74737824\n",
      "Iteration 2030, loss = 2.74676359\n",
      "Iteration 2031, loss = 2.74669084\n",
      "Iteration 2032, loss = 2.74629899\n",
      "Iteration 2033, loss = 2.74728526\n",
      "Iteration 2034, loss = 2.74775190\n",
      "Iteration 2035, loss = 2.74633850\n",
      "Iteration 2036, loss = 2.74663164\n",
      "Iteration 2037, loss = 2.74746542\n",
      "Iteration 2038, loss = 2.74690950\n",
      "Iteration 2039, loss = 2.74622809\n",
      "Iteration 2040, loss = 2.74581889\n",
      "Iteration 2041, loss = 2.74559893\n",
      "Iteration 2042, loss = 2.74513157\n",
      "Iteration 2043, loss = 2.74510115\n",
      "Iteration 2044, loss = 2.74511636\n",
      "Iteration 2045, loss = 2.74600906\n",
      "Iteration 2046, loss = 2.74598467\n",
      "Iteration 2047, loss = 2.74497792\n",
      "Iteration 2048, loss = 2.74598830\n",
      "Iteration 2049, loss = 2.74579617\n",
      "Iteration 2050, loss = 2.74506718\n",
      "Iteration 2051, loss = 2.74422846\n",
      "Iteration 2052, loss = 2.74392332\n",
      "Iteration 2053, loss = 2.74423440\n",
      "Iteration 2054, loss = 2.74371454\n",
      "Iteration 2055, loss = 2.74395518\n",
      "Iteration 2056, loss = 2.74461301\n",
      "Iteration 2057, loss = 2.74430376\n",
      "Iteration 2058, loss = 2.74327221\n",
      "Iteration 2059, loss = 2.74367802\n",
      "Iteration 2060, loss = 2.74245617\n",
      "Iteration 2061, loss = 2.74578820\n",
      "Iteration 2062, loss = 2.74486829\n",
      "Iteration 2063, loss = 2.74304797\n",
      "Iteration 2064, loss = 2.74261390\n",
      "Iteration 2065, loss = 2.74260348\n",
      "Iteration 2066, loss = 2.74277321\n",
      "Iteration 2067, loss = 2.74293820\n",
      "Iteration 2068, loss = 2.74316003\n",
      "Iteration 2069, loss = 2.74280721\n",
      "Iteration 2070, loss = 2.74305954\n",
      "Iteration 2071, loss = 2.74266868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13292990\n",
      "Iteration 2, loss = 4.05495777\n",
      "Iteration 3, loss = 3.98305047\n",
      "Iteration 4, loss = 3.91061267\n",
      "Iteration 5, loss = 3.83901187\n",
      "Iteration 6, loss = 3.76700778\n",
      "Iteration 7, loss = 3.69438167\n",
      "Iteration 8, loss = 3.62323091\n",
      "Iteration 9, loss = 3.55425331\n",
      "Iteration 10, loss = 3.48803233\n",
      "Iteration 11, loss = 3.42566936\n",
      "Iteration 12, loss = 3.37442021\n",
      "Iteration 13, loss = 3.32984839\n",
      "Iteration 14, loss = 3.29398932\n",
      "Iteration 15, loss = 3.26810483\n",
      "Iteration 16, loss = 3.25247827\n",
      "Iteration 17, loss = 3.24338986\n",
      "Iteration 18, loss = 3.23804419\n",
      "Iteration 19, loss = 3.23559387\n",
      "Iteration 20, loss = 3.23371560\n",
      "Iteration 21, loss = 3.23270323\n",
      "Iteration 22, loss = 3.23154202\n",
      "Iteration 23, loss = 3.23089235\n",
      "Iteration 24, loss = 3.22983375\n",
      "Iteration 25, loss = 3.22946053\n",
      "Iteration 26, loss = 3.22893558\n",
      "Iteration 27, loss = 3.22816161\n",
      "Iteration 28, loss = 3.22773179\n",
      "Iteration 29, loss = 3.22700930\n",
      "Iteration 30, loss = 3.22632613\n",
      "Iteration 31, loss = 3.22598202\n",
      "Iteration 32, loss = 3.22564424\n",
      "Iteration 33, loss = 3.22544476\n",
      "Iteration 34, loss = 3.22464936\n",
      "Iteration 35, loss = 3.22453555\n",
      "Iteration 36, loss = 3.22414466\n",
      "Iteration 37, loss = 3.22400197\n",
      "Iteration 38, loss = 3.22320440\n",
      "Iteration 39, loss = 3.22331257\n",
      "Iteration 40, loss = 3.22260369\n",
      "Iteration 41, loss = 3.22241910\n",
      "Iteration 42, loss = 3.22212355\n",
      "Iteration 43, loss = 3.22140963\n",
      "Iteration 44, loss = 3.22155952\n",
      "Iteration 45, loss = 3.22137041\n",
      "Iteration 46, loss = 3.22101912\n",
      "Iteration 47, loss = 3.22092715\n",
      "Iteration 48, loss = 3.22017792\n",
      "Iteration 49, loss = 3.22006875\n",
      "Iteration 50, loss = 3.21942931\n",
      "Iteration 51, loss = 3.21937533\n",
      "Iteration 52, loss = 3.21872822\n",
      "Iteration 53, loss = 3.21874782\n",
      "Iteration 54, loss = 3.21863722\n",
      "Iteration 55, loss = 3.21806297\n",
      "Iteration 56, loss = 3.21823715\n",
      "Iteration 57, loss = 3.21808968\n",
      "Iteration 58, loss = 3.21763630\n",
      "Iteration 59, loss = 3.21720573\n",
      "Iteration 60, loss = 3.21673802\n",
      "Iteration 61, loss = 3.21691228\n",
      "Iteration 62, loss = 3.21629899\n",
      "Iteration 63, loss = 3.21624737\n",
      "Iteration 64, loss = 3.21602375\n",
      "Iteration 65, loss = 3.21542059\n",
      "Iteration 66, loss = 3.21494779\n",
      "Iteration 67, loss = 3.21471006\n",
      "Iteration 68, loss = 3.21450422\n",
      "Iteration 69, loss = 3.21441062\n",
      "Iteration 70, loss = 3.21378040\n",
      "Iteration 71, loss = 3.21376697\n",
      "Iteration 72, loss = 3.21361788\n",
      "Iteration 73, loss = 3.21355779\n",
      "Iteration 74, loss = 3.21330217\n",
      "Iteration 75, loss = 3.21289438\n",
      "Iteration 76, loss = 3.21237698\n",
      "Iteration 77, loss = 3.21236269\n",
      "Iteration 78, loss = 3.21222844\n",
      "Iteration 79, loss = 3.21210218\n",
      "Iteration 80, loss = 3.21181438\n",
      "Iteration 81, loss = 3.21153065\n",
      "Iteration 82, loss = 3.21111343\n",
      "Iteration 83, loss = 3.21105496\n",
      "Iteration 84, loss = 3.21121984\n",
      "Iteration 85, loss = 3.21081847\n",
      "Iteration 86, loss = 3.21051672\n",
      "Iteration 87, loss = 3.20998049\n",
      "Iteration 88, loss = 3.20978165\n",
      "Iteration 89, loss = 3.20942305\n",
      "Iteration 90, loss = 3.20938736\n",
      "Iteration 91, loss = 3.20917676\n",
      "Iteration 92, loss = 3.20878234\n",
      "Iteration 93, loss = 3.20851557\n",
      "Iteration 94, loss = 3.20814054\n",
      "Iteration 95, loss = 3.20802933\n",
      "Iteration 96, loss = 3.20763554\n",
      "Iteration 97, loss = 3.20736440\n",
      "Iteration 98, loss = 3.20733250\n",
      "Iteration 99, loss = 3.20675516\n",
      "Iteration 100, loss = 3.20642715\n",
      "Iteration 101, loss = 3.20640793\n",
      "Iteration 102, loss = 3.20667384\n",
      "Iteration 103, loss = 3.20613051\n",
      "Iteration 104, loss = 3.20600775\n",
      "Iteration 105, loss = 3.20540975\n",
      "Iteration 106, loss = 3.20527718\n",
      "Iteration 107, loss = 3.20476579\n",
      "Iteration 108, loss = 3.20477722\n",
      "Iteration 109, loss = 3.20433037\n",
      "Iteration 110, loss = 3.20414529\n",
      "Iteration 111, loss = 3.20416362\n",
      "Iteration 112, loss = 3.20408055\n",
      "Iteration 113, loss = 3.20366427\n",
      "Iteration 114, loss = 3.20375727\n",
      "Iteration 115, loss = 3.20393808\n",
      "Iteration 116, loss = 3.20324791\n",
      "Iteration 117, loss = 3.20290961\n",
      "Iteration 118, loss = 3.20250990\n",
      "Iteration 119, loss = 3.20183618\n",
      "Iteration 120, loss = 3.20156806\n",
      "Iteration 121, loss = 3.20140870\n",
      "Iteration 122, loss = 3.20122220\n",
      "Iteration 123, loss = 3.20127621\n",
      "Iteration 124, loss = 3.20061637\n",
      "Iteration 125, loss = 3.20022241\n",
      "Iteration 126, loss = 3.20007587\n",
      "Iteration 127, loss = 3.19980442\n",
      "Iteration 128, loss = 3.19938391\n",
      "Iteration 129, loss = 3.19932359\n",
      "Iteration 130, loss = 3.19911738\n",
      "Iteration 131, loss = 3.19873368\n",
      "Iteration 132, loss = 3.19869286\n",
      "Iteration 133, loss = 3.19840701\n",
      "Iteration 134, loss = 3.19904468\n",
      "Iteration 135, loss = 3.19858506\n",
      "Iteration 136, loss = 3.19798356\n",
      "Iteration 137, loss = 3.19763283\n",
      "Iteration 138, loss = 3.19704898\n",
      "Iteration 139, loss = 3.19684020\n",
      "Iteration 140, loss = 3.19629919\n",
      "Iteration 141, loss = 3.19622188\n",
      "Iteration 142, loss = 3.19570950\n",
      "Iteration 143, loss = 3.19604535\n",
      "Iteration 144, loss = 3.19554223\n",
      "Iteration 145, loss = 3.19540805\n",
      "Iteration 146, loss = 3.19465337\n",
      "Iteration 147, loss = 3.19438536\n",
      "Iteration 148, loss = 3.19473340\n",
      "Iteration 149, loss = 3.19443353\n",
      "Iteration 150, loss = 3.19394561\n",
      "Iteration 151, loss = 3.19408862\n",
      "Iteration 152, loss = 3.19352636\n",
      "Iteration 153, loss = 3.19298727\n",
      "Iteration 154, loss = 3.19291608\n",
      "Iteration 155, loss = 3.19205668\n",
      "Iteration 156, loss = 3.19191803\n",
      "Iteration 157, loss = 3.19201939\n",
      "Iteration 158, loss = 3.19162210\n",
      "Iteration 159, loss = 3.19128538\n",
      "Iteration 160, loss = 3.19114884\n",
      "Iteration 161, loss = 3.19084077\n",
      "Iteration 162, loss = 3.19071228\n",
      "Iteration 163, loss = 3.19038332\n",
      "Iteration 164, loss = 3.19027087\n",
      "Iteration 165, loss = 3.18993688\n",
      "Iteration 166, loss = 3.18975540\n",
      "Iteration 167, loss = 3.18939479\n",
      "Iteration 168, loss = 3.18893114\n",
      "Iteration 169, loss = 3.18846375\n",
      "Iteration 170, loss = 3.18763296\n",
      "Iteration 171, loss = 3.18775277\n",
      "Iteration 172, loss = 3.18733116\n",
      "Iteration 173, loss = 3.18721624\n",
      "Iteration 174, loss = 3.18722364\n",
      "Iteration 175, loss = 3.18679410\n",
      "Iteration 176, loss = 3.18649774\n",
      "Iteration 177, loss = 3.18621005\n",
      "Iteration 178, loss = 3.18588641\n",
      "Iteration 179, loss = 3.18536110\n",
      "Iteration 180, loss = 3.18528183\n",
      "Iteration 181, loss = 3.18469793\n",
      "Iteration 182, loss = 3.18422897\n",
      "Iteration 183, loss = 3.18420655\n",
      "Iteration 184, loss = 3.18391121\n",
      "Iteration 185, loss = 3.18351279\n",
      "Iteration 186, loss = 3.18314338\n",
      "Iteration 187, loss = 3.18286384\n",
      "Iteration 188, loss = 3.18262696\n",
      "Iteration 189, loss = 3.18225887\n",
      "Iteration 190, loss = 3.18195835\n",
      "Iteration 191, loss = 3.18176018\n",
      "Iteration 192, loss = 3.18144904\n",
      "Iteration 193, loss = 3.18119981\n",
      "Iteration 194, loss = 3.18106939\n",
      "Iteration 195, loss = 3.18110109\n",
      "Iteration 196, loss = 3.18031844\n",
      "Iteration 197, loss = 3.17992061\n",
      "Iteration 198, loss = 3.18031437\n",
      "Iteration 199, loss = 3.17997010\n",
      "Iteration 200, loss = 3.17917203\n",
      "Iteration 201, loss = 3.17870784\n",
      "Iteration 202, loss = 3.17843027\n",
      "Iteration 203, loss = 3.17813509\n",
      "Iteration 204, loss = 3.17762695\n",
      "Iteration 205, loss = 3.17752097\n",
      "Iteration 206, loss = 3.17740738\n",
      "Iteration 207, loss = 3.17722778\n",
      "Iteration 208, loss = 3.17655511\n",
      "Iteration 209, loss = 3.17604897\n",
      "Iteration 210, loss = 3.17562008\n",
      "Iteration 211, loss = 3.17535007\n",
      "Iteration 212, loss = 3.17501338\n",
      "Iteration 213, loss = 3.17457033\n",
      "Iteration 214, loss = 3.17442197\n",
      "Iteration 215, loss = 3.17418207\n",
      "Iteration 216, loss = 3.17378964\n",
      "Iteration 217, loss = 3.17387824\n",
      "Iteration 218, loss = 3.17310294\n",
      "Iteration 219, loss = 3.17303361\n",
      "Iteration 220, loss = 3.17322403\n",
      "Iteration 221, loss = 3.17295216\n",
      "Iteration 222, loss = 3.17196615\n",
      "Iteration 223, loss = 3.17154292\n",
      "Iteration 224, loss = 3.17121221\n",
      "Iteration 225, loss = 3.17114570\n",
      "Iteration 226, loss = 3.17057040\n",
      "Iteration 227, loss = 3.17017248\n",
      "Iteration 228, loss = 3.17007931\n",
      "Iteration 229, loss = 3.16962744\n",
      "Iteration 230, loss = 3.16954630\n",
      "Iteration 231, loss = 3.16920991\n",
      "Iteration 232, loss = 3.16975065\n",
      "Iteration 233, loss = 3.16910344\n",
      "Iteration 234, loss = 3.16781029\n",
      "Iteration 235, loss = 3.16755525\n",
      "Iteration 236, loss = 3.16715482\n",
      "Iteration 237, loss = 3.16627389\n",
      "Iteration 238, loss = 3.16597404\n",
      "Iteration 239, loss = 3.16587065\n",
      "Iteration 240, loss = 3.16574451\n",
      "Iteration 241, loss = 3.16561586\n",
      "Iteration 242, loss = 3.16501385\n",
      "Iteration 243, loss = 3.16448887\n",
      "Iteration 244, loss = 3.16376005\n",
      "Iteration 245, loss = 3.16369763\n",
      "Iteration 246, loss = 3.16347138\n",
      "Iteration 247, loss = 3.16308951\n",
      "Iteration 248, loss = 3.16289239\n",
      "Iteration 249, loss = 3.16241087\n",
      "Iteration 250, loss = 3.16215657\n",
      "Iteration 251, loss = 3.16181991\n",
      "Iteration 252, loss = 3.16134175\n",
      "Iteration 253, loss = 3.16109883\n",
      "Iteration 254, loss = 3.16117864\n",
      "Iteration 255, loss = 3.16013802\n",
      "Iteration 256, loss = 3.15952941\n",
      "Iteration 257, loss = 3.15973353\n",
      "Iteration 258, loss = 3.15933442\n",
      "Iteration 259, loss = 3.15904682\n",
      "Iteration 260, loss = 3.15873069\n",
      "Iteration 261, loss = 3.15834938\n",
      "Iteration 262, loss = 3.15796415\n",
      "Iteration 263, loss = 3.15706745\n",
      "Iteration 264, loss = 3.15659015\n",
      "Iteration 265, loss = 3.15621379\n",
      "Iteration 266, loss = 3.15621445\n",
      "Iteration 267, loss = 3.15580800\n",
      "Iteration 268, loss = 3.15506205\n",
      "Iteration 269, loss = 3.15475381\n",
      "Iteration 270, loss = 3.15463045\n",
      "Iteration 271, loss = 3.15422055\n",
      "Iteration 272, loss = 3.15396111\n",
      "Iteration 273, loss = 3.15369381\n",
      "Iteration 274, loss = 3.15331037\n",
      "Iteration 275, loss = 3.15259518\n",
      "Iteration 276, loss = 3.15263704\n",
      "Iteration 277, loss = 3.15175971\n",
      "Iteration 278, loss = 3.15154962\n",
      "Iteration 279, loss = 3.15135348\n",
      "Iteration 280, loss = 3.15069964\n",
      "Iteration 281, loss = 3.15051778\n",
      "Iteration 282, loss = 3.15000140\n",
      "Iteration 283, loss = 3.14972486\n",
      "Iteration 284, loss = 3.14907096\n",
      "Iteration 285, loss = 3.14897168\n",
      "Iteration 286, loss = 3.14919648\n",
      "Iteration 287, loss = 3.14849570\n",
      "Iteration 288, loss = 3.14819494\n",
      "Iteration 289, loss = 3.14752537\n",
      "Iteration 290, loss = 3.14694341\n",
      "Iteration 291, loss = 3.14654568\n",
      "Iteration 292, loss = 3.14584090\n",
      "Iteration 293, loss = 3.14587798\n",
      "Iteration 294, loss = 3.14545346\n",
      "Iteration 295, loss = 3.14517062\n",
      "Iteration 296, loss = 3.14444632\n",
      "Iteration 297, loss = 3.14443331\n",
      "Iteration 298, loss = 3.14366589\n",
      "Iteration 299, loss = 3.14357669\n",
      "Iteration 300, loss = 3.14366531\n",
      "Iteration 301, loss = 3.14266501\n",
      "Iteration 302, loss = 3.14245719\n",
      "Iteration 303, loss = 3.14202075\n",
      "Iteration 304, loss = 3.14241067\n",
      "Iteration 305, loss = 3.14123008\n",
      "Iteration 306, loss = 3.14067107\n",
      "Iteration 307, loss = 3.14031630\n",
      "Iteration 308, loss = 3.13965787\n",
      "Iteration 309, loss = 3.13954567\n",
      "Iteration 310, loss = 3.13914850\n",
      "Iteration 311, loss = 3.13906244\n",
      "Iteration 312, loss = 3.13848658\n",
      "Iteration 313, loss = 3.13852082\n",
      "Iteration 314, loss = 3.13781448\n",
      "Iteration 315, loss = 3.13729681\n",
      "Iteration 316, loss = 3.13693940\n",
      "Iteration 317, loss = 3.13677826\n",
      "Iteration 318, loss = 3.13667015\n",
      "Iteration 319, loss = 3.13609479\n",
      "Iteration 320, loss = 3.13522928\n",
      "Iteration 321, loss = 3.13483478\n",
      "Iteration 322, loss = 3.13513855\n",
      "Iteration 323, loss = 3.13476425\n",
      "Iteration 324, loss = 3.13356501\n",
      "Iteration 325, loss = 3.13266224\n",
      "Iteration 326, loss = 3.13243010\n",
      "Iteration 327, loss = 3.13159688\n",
      "Iteration 328, loss = 3.13136927\n",
      "Iteration 329, loss = 3.13146653\n",
      "Iteration 330, loss = 3.13081396\n",
      "Iteration 331, loss = 3.13049025\n",
      "Iteration 332, loss = 3.12986593\n",
      "Iteration 333, loss = 3.12967525\n",
      "Iteration 334, loss = 3.12913644\n",
      "Iteration 335, loss = 3.12825034\n",
      "Iteration 336, loss = 3.12776295\n",
      "Iteration 337, loss = 3.12771571\n",
      "Iteration 338, loss = 3.12738161\n",
      "Iteration 339, loss = 3.12702613\n",
      "Iteration 340, loss = 3.12666714\n",
      "Iteration 341, loss = 3.12601647\n",
      "Iteration 342, loss = 3.12561268\n",
      "Iteration 343, loss = 3.12531571\n",
      "Iteration 344, loss = 3.12486288\n",
      "Iteration 345, loss = 3.12398648\n",
      "Iteration 346, loss = 3.12390135\n",
      "Iteration 347, loss = 3.12367426\n",
      "Iteration 348, loss = 3.12330537\n",
      "Iteration 349, loss = 3.12241959\n",
      "Iteration 350, loss = 3.12237578\n",
      "Iteration 351, loss = 3.12161489\n",
      "Iteration 352, loss = 3.12132909\n",
      "Iteration 353, loss = 3.12164588\n",
      "Iteration 354, loss = 3.12098145\n",
      "Iteration 355, loss = 3.12014922\n",
      "Iteration 356, loss = 3.11951603\n",
      "Iteration 357, loss = 3.11994689\n",
      "Iteration 358, loss = 3.11978717\n",
      "Iteration 359, loss = 3.11889560\n",
      "Iteration 360, loss = 3.11756764\n",
      "Iteration 361, loss = 3.11725067\n",
      "Iteration 362, loss = 3.11684056\n",
      "Iteration 363, loss = 3.11646673\n",
      "Iteration 364, loss = 3.11583057\n",
      "Iteration 365, loss = 3.11556961\n",
      "Iteration 366, loss = 3.11535902\n",
      "Iteration 367, loss = 3.11482557\n",
      "Iteration 368, loss = 3.11433968\n",
      "Iteration 369, loss = 3.11406421\n",
      "Iteration 370, loss = 3.11376043\n",
      "Iteration 371, loss = 3.11305361\n",
      "Iteration 372, loss = 3.11214687\n",
      "Iteration 373, loss = 3.11164515\n",
      "Iteration 374, loss = 3.11134731\n",
      "Iteration 375, loss = 3.11138045\n",
      "Iteration 376, loss = 3.11116266\n",
      "Iteration 377, loss = 3.11083046\n",
      "Iteration 378, loss = 3.10973364\n",
      "Iteration 379, loss = 3.10929344\n",
      "Iteration 380, loss = 3.10902784\n",
      "Iteration 381, loss = 3.10872122\n",
      "Iteration 382, loss = 3.10844983\n",
      "Iteration 383, loss = 3.10776528\n",
      "Iteration 384, loss = 3.10727526\n",
      "Iteration 385, loss = 3.10722147\n",
      "Iteration 386, loss = 3.10598464\n",
      "Iteration 387, loss = 3.10563936\n",
      "Iteration 388, loss = 3.10551017\n",
      "Iteration 389, loss = 3.10487435\n",
      "Iteration 390, loss = 3.10425073\n",
      "Iteration 391, loss = 3.10480767\n",
      "Iteration 392, loss = 3.10381074\n",
      "Iteration 393, loss = 3.10298690\n",
      "Iteration 394, loss = 3.10229808\n",
      "Iteration 395, loss = 3.10204042\n",
      "Iteration 396, loss = 3.10173610\n",
      "Iteration 397, loss = 3.10131062\n",
      "Iteration 398, loss = 3.10132750\n",
      "Iteration 399, loss = 3.10074768\n",
      "Iteration 400, loss = 3.10028314\n",
      "Iteration 401, loss = 3.09944151\n",
      "Iteration 402, loss = 3.09901487\n",
      "Iteration 403, loss = 3.09901823\n",
      "Iteration 404, loss = 3.09876505\n",
      "Iteration 405, loss = 3.09882120\n",
      "Iteration 406, loss = 3.09762908\n",
      "Iteration 407, loss = 3.09669207\n",
      "Iteration 408, loss = 3.09634487\n",
      "Iteration 409, loss = 3.09577787\n",
      "Iteration 410, loss = 3.09519864\n",
      "Iteration 411, loss = 3.09518492\n",
      "Iteration 412, loss = 3.09474095\n",
      "Iteration 413, loss = 3.09403881\n",
      "Iteration 414, loss = 3.09363180\n",
      "Iteration 415, loss = 3.09289252\n",
      "Iteration 416, loss = 3.09256400\n",
      "Iteration 417, loss = 3.09188591\n",
      "Iteration 418, loss = 3.09149898\n",
      "Iteration 419, loss = 3.09120387\n",
      "Iteration 420, loss = 3.09079132\n",
      "Iteration 421, loss = 3.09039765\n",
      "Iteration 422, loss = 3.08979865\n",
      "Iteration 423, loss = 3.08942173\n",
      "Iteration 424, loss = 3.08901955\n",
      "Iteration 425, loss = 3.08841369\n",
      "Iteration 426, loss = 3.08778849\n",
      "Iteration 427, loss = 3.08749679\n",
      "Iteration 428, loss = 3.08683867\n",
      "Iteration 429, loss = 3.08649404\n",
      "Iteration 430, loss = 3.08641719\n",
      "Iteration 431, loss = 3.08602470\n",
      "Iteration 432, loss = 3.08513550\n",
      "Iteration 433, loss = 3.08435674\n",
      "Iteration 434, loss = 3.08395850\n",
      "Iteration 435, loss = 3.08317340\n",
      "Iteration 436, loss = 3.08273680\n",
      "Iteration 437, loss = 3.08230043\n",
      "Iteration 438, loss = 3.08192485\n",
      "Iteration 439, loss = 3.08164147\n",
      "Iteration 440, loss = 3.08141743\n",
      "Iteration 441, loss = 3.08066424\n",
      "Iteration 442, loss = 3.07988490\n",
      "Iteration 443, loss = 3.08070243\n",
      "Iteration 444, loss = 3.07972782\n",
      "Iteration 445, loss = 3.07863005\n",
      "Iteration 446, loss = 3.07821368\n",
      "Iteration 447, loss = 3.07795983\n",
      "Iteration 448, loss = 3.07777186\n",
      "Iteration 449, loss = 3.07709107\n",
      "Iteration 450, loss = 3.07653962\n",
      "Iteration 451, loss = 3.07618060\n",
      "Iteration 452, loss = 3.07596483\n",
      "Iteration 453, loss = 3.07556268\n",
      "Iteration 454, loss = 3.07565131\n",
      "Iteration 455, loss = 3.07529833\n",
      "Iteration 456, loss = 3.07417433\n",
      "Iteration 457, loss = 3.07381048\n",
      "Iteration 458, loss = 3.07304158\n",
      "Iteration 459, loss = 3.07262230\n",
      "Iteration 460, loss = 3.07248419\n",
      "Iteration 461, loss = 3.07166163\n",
      "Iteration 462, loss = 3.07117621\n",
      "Iteration 463, loss = 3.07101918\n",
      "Iteration 464, loss = 3.07019932\n",
      "Iteration 465, loss = 3.06978524\n",
      "Iteration 466, loss = 3.06949812\n",
      "Iteration 467, loss = 3.06924886\n",
      "Iteration 468, loss = 3.06877797\n",
      "Iteration 469, loss = 3.06844478\n",
      "Iteration 470, loss = 3.06745509\n",
      "Iteration 471, loss = 3.06711453\n",
      "Iteration 472, loss = 3.06684774\n",
      "Iteration 473, loss = 3.06628070\n",
      "Iteration 474, loss = 3.06596500\n",
      "Iteration 475, loss = 3.06528153\n",
      "Iteration 476, loss = 3.06509080\n",
      "Iteration 477, loss = 3.06453492\n",
      "Iteration 478, loss = 3.06401674\n",
      "Iteration 479, loss = 3.06360740\n",
      "Iteration 480, loss = 3.06307007\n",
      "Iteration 481, loss = 3.06257628\n",
      "Iteration 482, loss = 3.06236501\n",
      "Iteration 483, loss = 3.06216352\n",
      "Iteration 484, loss = 3.06185292\n",
      "Iteration 485, loss = 3.06067408\n",
      "Iteration 486, loss = 3.06079248\n",
      "Iteration 487, loss = 3.06046937\n",
      "Iteration 488, loss = 3.05942164\n",
      "Iteration 489, loss = 3.05900124\n",
      "Iteration 490, loss = 3.05837154\n",
      "Iteration 491, loss = 3.05819010\n",
      "Iteration 492, loss = 3.05834298\n",
      "Iteration 493, loss = 3.05717753\n",
      "Iteration 494, loss = 3.05673461\n",
      "Iteration 495, loss = 3.05638809\n",
      "Iteration 496, loss = 3.05653622\n",
      "Iteration 497, loss = 3.05668434\n",
      "Iteration 498, loss = 3.05596879\n",
      "Iteration 499, loss = 3.05498442\n",
      "Iteration 500, loss = 3.05447027\n",
      "Iteration 501, loss = 3.05441900\n",
      "Iteration 502, loss = 3.05367201\n",
      "Iteration 503, loss = 3.05323023\n",
      "Iteration 504, loss = 3.05280093\n",
      "Iteration 505, loss = 3.05240188\n",
      "Iteration 506, loss = 3.05180441\n",
      "Iteration 507, loss = 3.05164024\n",
      "Iteration 508, loss = 3.05097648\n",
      "Iteration 509, loss = 3.05036235\n",
      "Iteration 510, loss = 3.05070101\n",
      "Iteration 511, loss = 3.05000961\n",
      "Iteration 512, loss = 3.04938320\n",
      "Iteration 513, loss = 3.04910883\n",
      "Iteration 514, loss = 3.04819335\n",
      "Iteration 515, loss = 3.04827605\n",
      "Iteration 516, loss = 3.04751172\n",
      "Iteration 517, loss = 3.04674977\n",
      "Iteration 518, loss = 3.04639217\n",
      "Iteration 519, loss = 3.04654897\n",
      "Iteration 520, loss = 3.04708356\n",
      "Iteration 521, loss = 3.04648250\n",
      "Iteration 522, loss = 3.04532260\n",
      "Iteration 523, loss = 3.04598318\n",
      "Iteration 524, loss = 3.04549577\n",
      "Iteration 525, loss = 3.04457258\n",
      "Iteration 526, loss = 3.04360911\n",
      "Iteration 527, loss = 3.04323394\n",
      "Iteration 528, loss = 3.04256170\n",
      "Iteration 529, loss = 3.04220688\n",
      "Iteration 530, loss = 3.04233849\n",
      "Iteration 531, loss = 3.04186525\n",
      "Iteration 532, loss = 3.04126220\n",
      "Iteration 533, loss = 3.04107469\n",
      "Iteration 534, loss = 3.04085269\n",
      "Iteration 535, loss = 3.04027661\n",
      "Iteration 536, loss = 3.04016817\n",
      "Iteration 537, loss = 3.03958739\n",
      "Iteration 538, loss = 3.03867598\n",
      "Iteration 539, loss = 3.03848821\n",
      "Iteration 540, loss = 3.03797438\n",
      "Iteration 541, loss = 3.03739074\n",
      "Iteration 542, loss = 3.03716133\n",
      "Iteration 543, loss = 3.03683049\n",
      "Iteration 544, loss = 3.03631647\n",
      "Iteration 545, loss = 3.03604113\n",
      "Iteration 546, loss = 3.03569160\n",
      "Iteration 547, loss = 3.03484871\n",
      "Iteration 548, loss = 3.03540084\n",
      "Iteration 549, loss = 3.03459221\n",
      "Iteration 550, loss = 3.03387685\n",
      "Iteration 551, loss = 3.03371105\n",
      "Iteration 552, loss = 3.03328271\n",
      "Iteration 553, loss = 3.03286771\n",
      "Iteration 554, loss = 3.03279850\n",
      "Iteration 555, loss = 3.03179734\n",
      "Iteration 556, loss = 3.03161201\n",
      "Iteration 557, loss = 3.03099042\n",
      "Iteration 558, loss = 3.03092846\n",
      "Iteration 559, loss = 3.03050532\n",
      "Iteration 560, loss = 3.03046340\n",
      "Iteration 561, loss = 3.02954318\n",
      "Iteration 562, loss = 3.02960994\n",
      "Iteration 563, loss = 3.02907069\n",
      "Iteration 564, loss = 3.02871949\n",
      "Iteration 565, loss = 3.02864924\n",
      "Iteration 566, loss = 3.02782769\n",
      "Iteration 567, loss = 3.02740089\n",
      "Iteration 568, loss = 3.02712132\n",
      "Iteration 569, loss = 3.02666118\n",
      "Iteration 570, loss = 3.02608060\n",
      "Iteration 571, loss = 3.02592625\n",
      "Iteration 572, loss = 3.02538784\n",
      "Iteration 573, loss = 3.02516219\n",
      "Iteration 574, loss = 3.02460149\n",
      "Iteration 575, loss = 3.02471377\n",
      "Iteration 576, loss = 3.02351943\n",
      "Iteration 577, loss = 3.02338815\n",
      "Iteration 578, loss = 3.02299953\n",
      "Iteration 579, loss = 3.02242857\n",
      "Iteration 580, loss = 3.02251311\n",
      "Iteration 581, loss = 3.02194812\n",
      "Iteration 582, loss = 3.02112850\n",
      "Iteration 583, loss = 3.02081800\n",
      "Iteration 584, loss = 3.02053060\n",
      "Iteration 585, loss = 3.02040924\n",
      "Iteration 586, loss = 3.01985869\n",
      "Iteration 587, loss = 3.01999717\n",
      "Iteration 588, loss = 3.01981093\n",
      "Iteration 589, loss = 3.01887868\n",
      "Iteration 590, loss = 3.01888396\n",
      "Iteration 591, loss = 3.01901397\n",
      "Iteration 592, loss = 3.01820696\n",
      "Iteration 593, loss = 3.01777385\n",
      "Iteration 594, loss = 3.01708199\n",
      "Iteration 595, loss = 3.01646715\n",
      "Iteration 596, loss = 3.01738599\n",
      "Iteration 597, loss = 3.01754334\n",
      "Iteration 598, loss = 3.01560542\n",
      "Iteration 599, loss = 3.01476677\n",
      "Iteration 600, loss = 3.01449282\n",
      "Iteration 601, loss = 3.01449796\n",
      "Iteration 602, loss = 3.01395300\n",
      "Iteration 603, loss = 3.01369935\n",
      "Iteration 604, loss = 3.01340159\n",
      "Iteration 605, loss = 3.01267670\n",
      "Iteration 606, loss = 3.01295805\n",
      "Iteration 607, loss = 3.01255095\n",
      "Iteration 608, loss = 3.01240900\n",
      "Iteration 609, loss = 3.01162665\n",
      "Iteration 610, loss = 3.01140009\n",
      "Iteration 611, loss = 3.01092791\n",
      "Iteration 612, loss = 3.01058896\n",
      "Iteration 613, loss = 3.01018329\n",
      "Iteration 614, loss = 3.00983965\n",
      "Iteration 615, loss = 3.00918735\n",
      "Iteration 616, loss = 3.00868797\n",
      "Iteration 617, loss = 3.00831178\n",
      "Iteration 618, loss = 3.00756636\n",
      "Iteration 619, loss = 3.00749028\n",
      "Iteration 620, loss = 3.00721624\n",
      "Iteration 621, loss = 3.00692708\n",
      "Iteration 622, loss = 3.00698129\n",
      "Iteration 623, loss = 3.00640419\n",
      "Iteration 624, loss = 3.00603712\n",
      "Iteration 625, loss = 3.00551867\n",
      "Iteration 626, loss = 3.00526162\n",
      "Iteration 627, loss = 3.00494263\n",
      "Iteration 628, loss = 3.00421063\n",
      "Iteration 629, loss = 3.00381293\n",
      "Iteration 630, loss = 3.00316644\n",
      "Iteration 631, loss = 3.00279824\n",
      "Iteration 632, loss = 3.00314395\n",
      "Iteration 633, loss = 3.00278492\n",
      "Iteration 634, loss = 3.00232154\n",
      "Iteration 635, loss = 3.00201694\n",
      "Iteration 636, loss = 3.00185452\n",
      "Iteration 637, loss = 3.00115689\n",
      "Iteration 638, loss = 3.00084421\n",
      "Iteration 639, loss = 3.00028102\n",
      "Iteration 640, loss = 2.99976515\n",
      "Iteration 641, loss = 2.99939887\n",
      "Iteration 642, loss = 2.99936781\n",
      "Iteration 643, loss = 2.99921897\n",
      "Iteration 644, loss = 2.99849408\n",
      "Iteration 645, loss = 2.99882645\n",
      "Iteration 646, loss = 2.99902252\n",
      "Iteration 647, loss = 2.99825785\n",
      "Iteration 648, loss = 2.99727938\n",
      "Iteration 649, loss = 2.99700267\n",
      "Iteration 650, loss = 2.99648821\n",
      "Iteration 651, loss = 2.99606697\n",
      "Iteration 652, loss = 2.99608598\n",
      "Iteration 653, loss = 2.99558151\n",
      "Iteration 654, loss = 2.99521598\n",
      "Iteration 655, loss = 2.99474278\n",
      "Iteration 656, loss = 2.99427904\n",
      "Iteration 657, loss = 2.99392118\n",
      "Iteration 658, loss = 2.99334417\n",
      "Iteration 659, loss = 2.99307390\n",
      "Iteration 660, loss = 2.99311111\n",
      "Iteration 661, loss = 2.99245503\n",
      "Iteration 662, loss = 2.99201548\n",
      "Iteration 663, loss = 2.99207630\n",
      "Iteration 664, loss = 2.99231572\n",
      "Iteration 665, loss = 2.99157469\n",
      "Iteration 666, loss = 2.99079447\n",
      "Iteration 667, loss = 2.99077946\n",
      "Iteration 668, loss = 2.99023808\n",
      "Iteration 669, loss = 2.99054959\n",
      "Iteration 670, loss = 2.99012250\n",
      "Iteration 671, loss = 2.98967039\n",
      "Iteration 672, loss = 2.98941408\n",
      "Iteration 673, loss = 2.98880017\n",
      "Iteration 674, loss = 2.98834800\n",
      "Iteration 675, loss = 2.98793692\n",
      "Iteration 676, loss = 2.98762384\n",
      "Iteration 677, loss = 2.98691538\n",
      "Iteration 678, loss = 2.98680476\n",
      "Iteration 679, loss = 2.98662696\n",
      "Iteration 680, loss = 2.98618792\n",
      "Iteration 681, loss = 2.98557952\n",
      "Iteration 682, loss = 2.98528885\n",
      "Iteration 683, loss = 2.98526977\n",
      "Iteration 684, loss = 2.98505567\n",
      "Iteration 685, loss = 2.98442194\n",
      "Iteration 686, loss = 2.98394636\n",
      "Iteration 687, loss = 2.98378630\n",
      "Iteration 688, loss = 2.98342288\n",
      "Iteration 689, loss = 2.98332133\n",
      "Iteration 690, loss = 2.98302870\n",
      "Iteration 691, loss = 2.98276048\n",
      "Iteration 692, loss = 2.98183870\n",
      "Iteration 693, loss = 2.98192744\n",
      "Iteration 694, loss = 2.98153137\n",
      "Iteration 695, loss = 2.98141118\n",
      "Iteration 696, loss = 2.98069056\n",
      "Iteration 697, loss = 2.98035989\n",
      "Iteration 698, loss = 2.97964888\n",
      "Iteration 699, loss = 2.97998436\n",
      "Iteration 700, loss = 2.97919699\n",
      "Iteration 701, loss = 2.97965457\n",
      "Iteration 702, loss = 2.97974191\n",
      "Iteration 703, loss = 2.97878771\n",
      "Iteration 704, loss = 2.97801470\n",
      "Iteration 705, loss = 2.97788829\n",
      "Iteration 706, loss = 2.97808817\n",
      "Iteration 707, loss = 2.97761385\n",
      "Iteration 708, loss = 2.97692801\n",
      "Iteration 709, loss = 2.97659844\n",
      "Iteration 710, loss = 2.97588685\n",
      "Iteration 711, loss = 2.97607023\n",
      "Iteration 712, loss = 2.97565279\n",
      "Iteration 713, loss = 2.97488015\n",
      "Iteration 714, loss = 2.97507600\n",
      "Iteration 715, loss = 2.97510158\n",
      "Iteration 716, loss = 2.97509580\n",
      "Iteration 717, loss = 2.97387543\n",
      "Iteration 718, loss = 2.97370095\n",
      "Iteration 719, loss = 2.97323886\n",
      "Iteration 720, loss = 2.97323184\n",
      "Iteration 721, loss = 2.97265264\n",
      "Iteration 722, loss = 2.97236521\n",
      "Iteration 723, loss = 2.97195654\n",
      "Iteration 724, loss = 2.97134622\n",
      "Iteration 725, loss = 2.97144854\n",
      "Iteration 726, loss = 2.97089316\n",
      "Iteration 727, loss = 2.97073928\n",
      "Iteration 728, loss = 2.97056421\n",
      "Iteration 729, loss = 2.97032381\n",
      "Iteration 730, loss = 2.96935816\n",
      "Iteration 731, loss = 2.96923854\n",
      "Iteration 732, loss = 2.96935805\n",
      "Iteration 733, loss = 2.96907273\n",
      "Iteration 734, loss = 2.96871986\n",
      "Iteration 735, loss = 2.96853626\n",
      "Iteration 736, loss = 2.96813294\n",
      "Iteration 737, loss = 2.96773869\n",
      "Iteration 738, loss = 2.96741972\n",
      "Iteration 739, loss = 2.96689661\n",
      "Iteration 740, loss = 2.96710688\n",
      "Iteration 741, loss = 2.96597822\n",
      "Iteration 742, loss = 2.96587917\n",
      "Iteration 743, loss = 2.96567319\n",
      "Iteration 744, loss = 2.96561119\n",
      "Iteration 745, loss = 2.96568453\n",
      "Iteration 746, loss = 2.96527064\n",
      "Iteration 747, loss = 2.96462057\n",
      "Iteration 748, loss = 2.96432676\n",
      "Iteration 749, loss = 2.96436857\n",
      "Iteration 750, loss = 2.96403548\n",
      "Iteration 751, loss = 2.96318981\n",
      "Iteration 752, loss = 2.96252163\n",
      "Iteration 753, loss = 2.96230382\n",
      "Iteration 754, loss = 2.96245925\n",
      "Iteration 755, loss = 2.96199365\n",
      "Iteration 756, loss = 2.96218603\n",
      "Iteration 757, loss = 2.96175060\n",
      "Iteration 758, loss = 2.96178917\n",
      "Iteration 759, loss = 2.96121025\n",
      "Iteration 760, loss = 2.96016520\n",
      "Iteration 761, loss = 2.96003815\n",
      "Iteration 762, loss = 2.96003495\n",
      "Iteration 763, loss = 2.95960902\n",
      "Iteration 764, loss = 2.95909918\n",
      "Iteration 765, loss = 2.95853143\n",
      "Iteration 766, loss = 2.95818229\n",
      "Iteration 767, loss = 2.95802441\n",
      "Iteration 768, loss = 2.95765741\n",
      "Iteration 769, loss = 2.95739758\n",
      "Iteration 770, loss = 2.95769714\n",
      "Iteration 771, loss = 2.95668349\n",
      "Iteration 772, loss = 2.95751720\n",
      "Iteration 773, loss = 2.95566568\n",
      "Iteration 774, loss = 2.95652306\n",
      "Iteration 775, loss = 2.95672531\n",
      "Iteration 776, loss = 2.95568235\n",
      "Iteration 777, loss = 2.95489578\n",
      "Iteration 778, loss = 2.95431285\n",
      "Iteration 779, loss = 2.95403478\n",
      "Iteration 780, loss = 2.95393199\n",
      "Iteration 781, loss = 2.95360300\n",
      "Iteration 782, loss = 2.95347367\n",
      "Iteration 783, loss = 2.95268890\n",
      "Iteration 784, loss = 2.95269449\n",
      "Iteration 785, loss = 2.95265614\n",
      "Iteration 786, loss = 2.95222775\n",
      "Iteration 787, loss = 2.95147739\n",
      "Iteration 788, loss = 2.95145437\n",
      "Iteration 789, loss = 2.95116070\n",
      "Iteration 790, loss = 2.95080441\n",
      "Iteration 791, loss = 2.95067230\n",
      "Iteration 792, loss = 2.95030603\n",
      "Iteration 793, loss = 2.95004920\n",
      "Iteration 794, loss = 2.94981860\n",
      "Iteration 795, loss = 2.94916282\n",
      "Iteration 796, loss = 2.94961586\n",
      "Iteration 797, loss = 2.94917881\n",
      "Iteration 798, loss = 2.94933779\n",
      "Iteration 799, loss = 2.94855630\n",
      "Iteration 800, loss = 2.94814811\n",
      "Iteration 801, loss = 2.94806739\n",
      "Iteration 802, loss = 2.94774185\n",
      "Iteration 803, loss = 2.94722180\n",
      "Iteration 804, loss = 2.94709597\n",
      "Iteration 805, loss = 2.94632537\n",
      "Iteration 806, loss = 2.94662226\n",
      "Iteration 807, loss = 2.94643296\n",
      "Iteration 808, loss = 2.94683860\n",
      "Iteration 809, loss = 2.94607862\n",
      "Iteration 810, loss = 2.94611508\n",
      "Iteration 811, loss = 2.94555480\n",
      "Iteration 812, loss = 2.94437336\n",
      "Iteration 813, loss = 2.94418839\n",
      "Iteration 814, loss = 2.94358661\n",
      "Iteration 815, loss = 2.94405287\n",
      "Iteration 816, loss = 2.94422722\n",
      "Iteration 817, loss = 2.94332179\n",
      "Iteration 818, loss = 2.94308588\n",
      "Iteration 819, loss = 2.94250114\n",
      "Iteration 820, loss = 2.94206943\n",
      "Iteration 821, loss = 2.94203215\n",
      "Iteration 822, loss = 2.94190737\n",
      "Iteration 823, loss = 2.94116062\n",
      "Iteration 824, loss = 2.94106505\n",
      "Iteration 825, loss = 2.94077447\n",
      "Iteration 826, loss = 2.94140064\n",
      "Iteration 827, loss = 2.94009073\n",
      "Iteration 828, loss = 2.93975950\n",
      "Iteration 829, loss = 2.93993545\n",
      "Iteration 830, loss = 2.93973008\n",
      "Iteration 831, loss = 2.93941147\n",
      "Iteration 832, loss = 2.93914840\n",
      "Iteration 833, loss = 2.93862436\n",
      "Iteration 834, loss = 2.93840986\n",
      "Iteration 835, loss = 2.93805742\n",
      "Iteration 836, loss = 2.93813174\n",
      "Iteration 837, loss = 2.93839473\n",
      "Iteration 838, loss = 2.93805903\n",
      "Iteration 839, loss = 2.93795276\n",
      "Iteration 840, loss = 2.93745538\n",
      "Iteration 841, loss = 2.93699671\n",
      "Iteration 842, loss = 2.93650461\n",
      "Iteration 843, loss = 2.93629502\n",
      "Iteration 844, loss = 2.93643350\n",
      "Iteration 845, loss = 2.93601247\n",
      "Iteration 846, loss = 2.93563749\n",
      "Iteration 847, loss = 2.93515539\n",
      "Iteration 848, loss = 2.93431088\n",
      "Iteration 849, loss = 2.93444184\n",
      "Iteration 850, loss = 2.93499168\n",
      "Iteration 851, loss = 2.93502643\n",
      "Iteration 852, loss = 2.93388550\n",
      "Iteration 853, loss = 2.93365396\n",
      "Iteration 854, loss = 2.93309261\n",
      "Iteration 855, loss = 2.93345803\n",
      "Iteration 856, loss = 2.93303792\n",
      "Iteration 857, loss = 2.93187410\n",
      "Iteration 858, loss = 2.93165939\n",
      "Iteration 859, loss = 2.93158991\n",
      "Iteration 860, loss = 2.93121367\n",
      "Iteration 861, loss = 2.93064433\n",
      "Iteration 862, loss = 2.93046815\n",
      "Iteration 863, loss = 2.92992113\n",
      "Iteration 864, loss = 2.93061980\n",
      "Iteration 865, loss = 2.92997838\n",
      "Iteration 866, loss = 2.92917173\n",
      "Iteration 867, loss = 2.92923057\n",
      "Iteration 868, loss = 2.92938473\n",
      "Iteration 869, loss = 2.92879595\n",
      "Iteration 870, loss = 2.92808468\n",
      "Iteration 871, loss = 2.92796892\n",
      "Iteration 872, loss = 2.92786811\n",
      "Iteration 873, loss = 2.92740888\n",
      "Iteration 874, loss = 2.92721802\n",
      "Iteration 875, loss = 2.92663135\n",
      "Iteration 876, loss = 2.92669787\n",
      "Iteration 877, loss = 2.92644002\n",
      "Iteration 878, loss = 2.92588686\n",
      "Iteration 879, loss = 2.92596663\n",
      "Iteration 880, loss = 2.92552016\n",
      "Iteration 881, loss = 2.92538793\n",
      "Iteration 882, loss = 2.92515427\n",
      "Iteration 883, loss = 2.92430649\n",
      "Iteration 884, loss = 2.92431329\n",
      "Iteration 885, loss = 2.92412502\n",
      "Iteration 886, loss = 2.92348241\n",
      "Iteration 887, loss = 2.92345150\n",
      "Iteration 888, loss = 2.92358069\n",
      "Iteration 889, loss = 2.92309497\n",
      "Iteration 890, loss = 2.92281472\n",
      "Iteration 891, loss = 2.92228333\n",
      "Iteration 892, loss = 2.92213441\n",
      "Iteration 893, loss = 2.92217929\n",
      "Iteration 894, loss = 2.92188051\n",
      "Iteration 895, loss = 2.92143984\n",
      "Iteration 896, loss = 2.92128403\n",
      "Iteration 897, loss = 2.92070964\n",
      "Iteration 898, loss = 2.92051222\n",
      "Iteration 899, loss = 2.92072959\n",
      "Iteration 900, loss = 2.92106018\n",
      "Iteration 901, loss = 2.92044122\n",
      "Iteration 902, loss = 2.91975827\n",
      "Iteration 903, loss = 2.91937772\n",
      "Iteration 904, loss = 2.91882685\n",
      "Iteration 905, loss = 2.91823002\n",
      "Iteration 906, loss = 2.91827842\n",
      "Iteration 907, loss = 2.91787303\n",
      "Iteration 908, loss = 2.91769253\n",
      "Iteration 909, loss = 2.91740804\n",
      "Iteration 910, loss = 2.91727775\n",
      "Iteration 911, loss = 2.91732311\n",
      "Iteration 912, loss = 2.91729280\n",
      "Iteration 913, loss = 2.91818499\n",
      "Iteration 914, loss = 2.91691957\n",
      "Iteration 915, loss = 2.91612719\n",
      "Iteration 916, loss = 2.91591313\n",
      "Iteration 917, loss = 2.91532719\n",
      "Iteration 918, loss = 2.91519782\n",
      "Iteration 919, loss = 2.91483796\n",
      "Iteration 920, loss = 2.91501489\n",
      "Iteration 921, loss = 2.91527533\n",
      "Iteration 922, loss = 2.91461676\n",
      "Iteration 923, loss = 2.91390437\n",
      "Iteration 924, loss = 2.91379183\n",
      "Iteration 925, loss = 2.91349614\n",
      "Iteration 926, loss = 2.91300860\n",
      "Iteration 927, loss = 2.91289706\n",
      "Iteration 928, loss = 2.91225953\n",
      "Iteration 929, loss = 2.91207556\n",
      "Iteration 930, loss = 2.91220038\n",
      "Iteration 931, loss = 2.91192117\n",
      "Iteration 932, loss = 2.91115728\n",
      "Iteration 933, loss = 2.91123293\n",
      "Iteration 934, loss = 2.91069425\n",
      "Iteration 935, loss = 2.91140608\n",
      "Iteration 936, loss = 2.91080601\n",
      "Iteration 937, loss = 2.91069357\n",
      "Iteration 938, loss = 2.91026408\n",
      "Iteration 939, loss = 2.91039593\n",
      "Iteration 940, loss = 2.91021537\n",
      "Iteration 941, loss = 2.90996303\n",
      "Iteration 942, loss = 2.90916081\n",
      "Iteration 943, loss = 2.90904867\n",
      "Iteration 944, loss = 2.90826334\n",
      "Iteration 945, loss = 2.90823513\n",
      "Iteration 946, loss = 2.90800420\n",
      "Iteration 947, loss = 2.90778806\n",
      "Iteration 948, loss = 2.90769824\n",
      "Iteration 949, loss = 2.90740742\n",
      "Iteration 950, loss = 2.90704477\n",
      "Iteration 951, loss = 2.90637030\n",
      "Iteration 952, loss = 2.90610833\n",
      "Iteration 953, loss = 2.90718886\n",
      "Iteration 954, loss = 2.90679325\n",
      "Iteration 955, loss = 2.90608182\n",
      "Iteration 956, loss = 2.90549866\n",
      "Iteration 957, loss = 2.90548791\n",
      "Iteration 958, loss = 2.90489299\n",
      "Iteration 959, loss = 2.90493930\n",
      "Iteration 960, loss = 2.90483921\n",
      "Iteration 961, loss = 2.90521817\n",
      "Iteration 962, loss = 2.90484337\n",
      "Iteration 963, loss = 2.90403254\n",
      "Iteration 964, loss = 2.90364878\n",
      "Iteration 965, loss = 2.90371417\n",
      "Iteration 966, loss = 2.90314016\n",
      "Iteration 967, loss = 2.90320843\n",
      "Iteration 968, loss = 2.90338929\n",
      "Iteration 969, loss = 2.90299313\n",
      "Iteration 970, loss = 2.90232170\n",
      "Iteration 971, loss = 2.90204070\n",
      "Iteration 972, loss = 2.90260869\n",
      "Iteration 973, loss = 2.90188558\n",
      "Iteration 974, loss = 2.90096445\n",
      "Iteration 975, loss = 2.90083232\n",
      "Iteration 976, loss = 2.90053380\n",
      "Iteration 977, loss = 2.89992638\n",
      "Iteration 978, loss = 2.90003642\n",
      "Iteration 979, loss = 2.90029071\n",
      "Iteration 980, loss = 2.89946678\n",
      "Iteration 981, loss = 2.89881496\n",
      "Iteration 982, loss = 2.89928761\n",
      "Iteration 983, loss = 2.89925853\n",
      "Iteration 984, loss = 2.89816149\n",
      "Iteration 985, loss = 2.89833337\n",
      "Iteration 986, loss = 2.89792048\n",
      "Iteration 987, loss = 2.89765036\n",
      "Iteration 988, loss = 2.89769706\n",
      "Iteration 989, loss = 2.89751365\n",
      "Iteration 990, loss = 2.89701172\n",
      "Iteration 991, loss = 2.89685302\n",
      "Iteration 992, loss = 2.89674921\n",
      "Iteration 993, loss = 2.89632263\n",
      "Iteration 994, loss = 2.89619362\n",
      "Iteration 995, loss = 2.89611172\n",
      "Iteration 996, loss = 2.89612451\n",
      "Iteration 997, loss = 2.89562371\n",
      "Iteration 998, loss = 2.89593385\n",
      "Iteration 999, loss = 2.89521776\n",
      "Iteration 1000, loss = 2.89468270\n",
      "Iteration 1001, loss = 2.89398926\n",
      "Iteration 1002, loss = 2.89454210\n",
      "Iteration 1003, loss = 2.89407916\n",
      "Iteration 1004, loss = 2.89388483\n",
      "Iteration 1005, loss = 2.89434904\n",
      "Iteration 1006, loss = 2.89330008\n",
      "Iteration 1007, loss = 2.89275913\n",
      "Iteration 1008, loss = 2.89320849\n",
      "Iteration 1009, loss = 2.89287688\n",
      "Iteration 1010, loss = 2.89217359\n",
      "Iteration 1011, loss = 2.89248264\n",
      "Iteration 1012, loss = 2.89215094\n",
      "Iteration 1013, loss = 2.89177655\n",
      "Iteration 1014, loss = 2.89122219\n",
      "Iteration 1015, loss = 2.89102682\n",
      "Iteration 1016, loss = 2.89090415\n",
      "Iteration 1017, loss = 2.89072737\n",
      "Iteration 1018, loss = 2.89072538\n",
      "Iteration 1019, loss = 2.88973179\n",
      "Iteration 1020, loss = 2.88951906\n",
      "Iteration 1021, loss = 2.88905990\n",
      "Iteration 1022, loss = 2.88962473\n",
      "Iteration 1023, loss = 2.88939734\n",
      "Iteration 1024, loss = 2.88944120\n",
      "Iteration 1025, loss = 2.89038244\n",
      "Iteration 1026, loss = 2.88932582\n",
      "Iteration 1027, loss = 2.88852835\n",
      "Iteration 1028, loss = 2.88832643\n",
      "Iteration 1029, loss = 2.88768657\n",
      "Iteration 1030, loss = 2.88730980\n",
      "Iteration 1031, loss = 2.88705948\n",
      "Iteration 1032, loss = 2.88701536\n",
      "Iteration 1033, loss = 2.88685226\n",
      "Iteration 1034, loss = 2.88650211\n",
      "Iteration 1035, loss = 2.88634973\n",
      "Iteration 1036, loss = 2.88642381\n",
      "Iteration 1037, loss = 2.88590987\n",
      "Iteration 1038, loss = 2.88623239\n",
      "Iteration 1039, loss = 2.88554375\n",
      "Iteration 1040, loss = 2.88541259\n",
      "Iteration 1041, loss = 2.88519689\n",
      "Iteration 1042, loss = 2.88497109\n",
      "Iteration 1043, loss = 2.88535217\n",
      "Iteration 1044, loss = 2.88455836\n",
      "Iteration 1045, loss = 2.88401889\n",
      "Iteration 1046, loss = 2.88398621\n",
      "Iteration 1047, loss = 2.88384046\n",
      "Iteration 1048, loss = 2.88296237\n",
      "Iteration 1049, loss = 2.88363067\n",
      "Iteration 1050, loss = 2.88338491\n",
      "Iteration 1051, loss = 2.88315903\n",
      "Iteration 1052, loss = 2.88256614\n",
      "Iteration 1053, loss = 2.88218914\n",
      "Iteration 1054, loss = 2.88299007\n",
      "Iteration 1055, loss = 2.88275697\n",
      "Iteration 1056, loss = 2.88200117\n",
      "Iteration 1057, loss = 2.88158193\n",
      "Iteration 1058, loss = 2.88103531\n",
      "Iteration 1059, loss = 2.88100329\n",
      "Iteration 1060, loss = 2.88110182\n",
      "Iteration 1061, loss = 2.88052958\n",
      "Iteration 1062, loss = 2.88041802\n",
      "Iteration 1063, loss = 2.88036911\n",
      "Iteration 1064, loss = 2.88082383\n",
      "Iteration 1065, loss = 2.87988677\n",
      "Iteration 1066, loss = 2.87962550\n",
      "Iteration 1067, loss = 2.87927493\n",
      "Iteration 1068, loss = 2.87904217\n",
      "Iteration 1069, loss = 2.87868308\n",
      "Iteration 1070, loss = 2.87834828\n",
      "Iteration 1071, loss = 2.87767701\n",
      "Iteration 1072, loss = 2.87760391\n",
      "Iteration 1073, loss = 2.87705088\n",
      "Iteration 1074, loss = 2.87803156\n",
      "Iteration 1075, loss = 2.87705648\n",
      "Iteration 1076, loss = 2.87786814\n",
      "Iteration 1077, loss = 2.87722434\n",
      "Iteration 1078, loss = 2.87685576\n",
      "Iteration 1079, loss = 2.87644397\n",
      "Iteration 1080, loss = 2.87606242\n",
      "Iteration 1081, loss = 2.87592924\n",
      "Iteration 1082, loss = 2.87595778\n",
      "Iteration 1083, loss = 2.87618447\n",
      "Iteration 1084, loss = 2.87543140\n",
      "Iteration 1085, loss = 2.87520118\n",
      "Iteration 1086, loss = 2.87461655\n",
      "Iteration 1087, loss = 2.87469361\n",
      "Iteration 1088, loss = 2.87458524\n",
      "Iteration 1089, loss = 2.87453079\n",
      "Iteration 1090, loss = 2.87413825\n",
      "Iteration 1091, loss = 2.87375864\n",
      "Iteration 1092, loss = 2.87332762\n",
      "Iteration 1093, loss = 2.87360083\n",
      "Iteration 1094, loss = 2.87302793\n",
      "Iteration 1095, loss = 2.87307167\n",
      "Iteration 1096, loss = 2.87388494\n",
      "Iteration 1097, loss = 2.87363278\n",
      "Iteration 1098, loss = 2.87247022\n",
      "Iteration 1099, loss = 2.87167904\n",
      "Iteration 1100, loss = 2.87151023\n",
      "Iteration 1101, loss = 2.87145945\n",
      "Iteration 1102, loss = 2.87114234\n",
      "Iteration 1103, loss = 2.87065726\n",
      "Iteration 1104, loss = 2.87062042\n",
      "Iteration 1105, loss = 2.87043054\n",
      "Iteration 1106, loss = 2.86996301\n",
      "Iteration 1107, loss = 2.87016411\n",
      "Iteration 1108, loss = 2.87017859\n",
      "Iteration 1109, loss = 2.86997423\n",
      "Iteration 1110, loss = 2.86997561\n",
      "Iteration 1111, loss = 2.86920540\n",
      "Iteration 1112, loss = 2.86916110\n",
      "Iteration 1113, loss = 2.86943365\n",
      "Iteration 1114, loss = 2.86868736\n",
      "Iteration 1115, loss = 2.86882983\n",
      "Iteration 1116, loss = 2.86833170\n",
      "Iteration 1117, loss = 2.86839853\n",
      "Iteration 1118, loss = 2.86780123\n",
      "Iteration 1119, loss = 2.86732451\n",
      "Iteration 1120, loss = 2.86670211\n",
      "Iteration 1121, loss = 2.86737753\n",
      "Iteration 1122, loss = 2.86731302\n",
      "Iteration 1123, loss = 2.86631874\n",
      "Iteration 1124, loss = 2.86617298\n",
      "Iteration 1125, loss = 2.86611100\n",
      "Iteration 1126, loss = 2.86633387\n",
      "Iteration 1127, loss = 2.86563463\n",
      "Iteration 1128, loss = 2.86552136\n",
      "Iteration 1129, loss = 2.86532067\n",
      "Iteration 1130, loss = 2.86512492\n",
      "Iteration 1131, loss = 2.86453943\n",
      "Iteration 1132, loss = 2.86460994\n",
      "Iteration 1133, loss = 2.86494964\n",
      "Iteration 1134, loss = 2.86463597\n",
      "Iteration 1135, loss = 2.86376793\n",
      "Iteration 1136, loss = 2.86359098\n",
      "Iteration 1137, loss = 2.86333392\n",
      "Iteration 1138, loss = 2.86354604\n",
      "Iteration 1139, loss = 2.86344079\n",
      "Iteration 1140, loss = 2.86320057\n",
      "Iteration 1141, loss = 2.86216172\n",
      "Iteration 1142, loss = 2.86200188\n",
      "Iteration 1143, loss = 2.86188335\n",
      "Iteration 1144, loss = 2.86244341\n",
      "Iteration 1145, loss = 2.86175516\n",
      "Iteration 1146, loss = 2.86196534\n",
      "Iteration 1147, loss = 2.86157830\n",
      "Iteration 1148, loss = 2.86101457\n",
      "Iteration 1149, loss = 2.86163161\n",
      "Iteration 1150, loss = 2.86136702\n",
      "Iteration 1151, loss = 2.86075407\n",
      "Iteration 1152, loss = 2.86096608\n",
      "Iteration 1153, loss = 2.86038314\n",
      "Iteration 1154, loss = 2.85985840\n",
      "Iteration 1155, loss = 2.85963733\n",
      "Iteration 1156, loss = 2.85929507\n",
      "Iteration 1157, loss = 2.85937452\n",
      "Iteration 1158, loss = 2.85924643\n",
      "Iteration 1159, loss = 2.85871791\n",
      "Iteration 1160, loss = 2.85883106\n",
      "Iteration 1161, loss = 2.85840007\n",
      "Iteration 1162, loss = 2.85841767\n",
      "Iteration 1163, loss = 2.85852745\n",
      "Iteration 1164, loss = 2.85838785\n",
      "Iteration 1165, loss = 2.85862376\n",
      "Iteration 1166, loss = 2.85817092\n",
      "Iteration 1167, loss = 2.85740785\n",
      "Iteration 1168, loss = 2.85692916\n",
      "Iteration 1169, loss = 2.85652509\n",
      "Iteration 1170, loss = 2.85632049\n",
      "Iteration 1171, loss = 2.85631655\n",
      "Iteration 1172, loss = 2.85649516\n",
      "Iteration 1173, loss = 2.85591845\n",
      "Iteration 1174, loss = 2.85550914\n",
      "Iteration 1175, loss = 2.85544437\n",
      "Iteration 1176, loss = 2.85558341\n",
      "Iteration 1177, loss = 2.85497043\n",
      "Iteration 1178, loss = 2.85493510\n",
      "Iteration 1179, loss = 2.85449228\n",
      "Iteration 1180, loss = 2.85459549\n",
      "Iteration 1181, loss = 2.85459364\n",
      "Iteration 1182, loss = 2.85388270\n",
      "Iteration 1183, loss = 2.85533864\n",
      "Iteration 1184, loss = 2.85445887\n",
      "Iteration 1185, loss = 2.85347931\n",
      "Iteration 1186, loss = 2.85306179\n",
      "Iteration 1187, loss = 2.85305585\n",
      "Iteration 1188, loss = 2.85303267\n",
      "Iteration 1189, loss = 2.85265557\n",
      "Iteration 1190, loss = 2.85227944\n",
      "Iteration 1191, loss = 2.85179978\n",
      "Iteration 1192, loss = 2.85200420\n",
      "Iteration 1193, loss = 2.85260177\n",
      "Iteration 1194, loss = 2.85219852\n",
      "Iteration 1195, loss = 2.85182245\n",
      "Iteration 1196, loss = 2.85152427\n",
      "Iteration 1197, loss = 2.85071301\n",
      "Iteration 1198, loss = 2.85099481\n",
      "Iteration 1199, loss = 2.85056570\n",
      "Iteration 1200, loss = 2.85077051\n",
      "Iteration 1201, loss = 2.85046145\n",
      "Iteration 1202, loss = 2.85023947\n",
      "Iteration 1203, loss = 2.85006574\n",
      "Iteration 1204, loss = 2.84985874\n",
      "Iteration 1205, loss = 2.84927815\n",
      "Iteration 1206, loss = 2.84950207\n",
      "Iteration 1207, loss = 2.84938303\n",
      "Iteration 1208, loss = 2.84882782\n",
      "Iteration 1209, loss = 2.84852649\n",
      "Iteration 1210, loss = 2.84886714\n",
      "Iteration 1211, loss = 2.84885913\n",
      "Iteration 1212, loss = 2.84856540\n",
      "Iteration 1213, loss = 2.84768197\n",
      "Iteration 1214, loss = 2.84760244\n",
      "Iteration 1215, loss = 2.84771526\n",
      "Iteration 1216, loss = 2.84751043\n",
      "Iteration 1217, loss = 2.84712092\n",
      "Iteration 1218, loss = 2.84719418\n",
      "Iteration 1219, loss = 2.84718908\n",
      "Iteration 1220, loss = 2.84632505\n",
      "Iteration 1221, loss = 2.84637239\n",
      "Iteration 1222, loss = 2.84609307\n",
      "Iteration 1223, loss = 2.84535261\n",
      "Iteration 1224, loss = 2.84534046\n",
      "Iteration 1225, loss = 2.84547223\n",
      "Iteration 1226, loss = 2.84528286\n",
      "Iteration 1227, loss = 2.84482121\n",
      "Iteration 1228, loss = 2.84425378\n",
      "Iteration 1229, loss = 2.84431596\n",
      "Iteration 1230, loss = 2.84498946\n",
      "Iteration 1231, loss = 2.84486105\n",
      "Iteration 1232, loss = 2.84382688\n",
      "Iteration 1233, loss = 2.84365487\n",
      "Iteration 1234, loss = 2.84331877\n",
      "Iteration 1235, loss = 2.84420559\n",
      "Iteration 1236, loss = 2.84414419\n",
      "Iteration 1237, loss = 2.84336794\n",
      "Iteration 1238, loss = 2.84299902\n",
      "Iteration 1239, loss = 2.84277859\n",
      "Iteration 1240, loss = 2.84252153\n",
      "Iteration 1241, loss = 2.84235151\n",
      "Iteration 1242, loss = 2.84208960\n",
      "Iteration 1243, loss = 2.84201514\n",
      "Iteration 1244, loss = 2.84193954\n",
      "Iteration 1245, loss = 2.84178227\n",
      "Iteration 1246, loss = 2.84103511\n",
      "Iteration 1247, loss = 2.84089830\n",
      "Iteration 1248, loss = 2.84073215\n",
      "Iteration 1249, loss = 2.84048742\n",
      "Iteration 1250, loss = 2.84047456\n",
      "Iteration 1251, loss = 2.84010813\n",
      "Iteration 1252, loss = 2.84029468\n",
      "Iteration 1253, loss = 2.83963484\n",
      "Iteration 1254, loss = 2.83925901\n",
      "Iteration 1255, loss = 2.83889913\n",
      "Iteration 1256, loss = 2.83912433\n",
      "Iteration 1257, loss = 2.83898133\n",
      "Iteration 1258, loss = 2.83864298\n",
      "Iteration 1259, loss = 2.83847446\n",
      "Iteration 1260, loss = 2.83826405\n",
      "Iteration 1261, loss = 2.83813227\n",
      "Iteration 1262, loss = 2.83827444\n",
      "Iteration 1263, loss = 2.83850634\n",
      "Iteration 1264, loss = 2.83790243\n",
      "Iteration 1265, loss = 2.83774450\n",
      "Iteration 1266, loss = 2.83695959\n",
      "Iteration 1267, loss = 2.83713489\n",
      "Iteration 1268, loss = 2.83721637\n",
      "Iteration 1269, loss = 2.83714768\n",
      "Iteration 1270, loss = 2.83682740\n",
      "Iteration 1271, loss = 2.83591628\n",
      "Iteration 1272, loss = 2.83656137\n",
      "Iteration 1273, loss = 2.83655449\n",
      "Iteration 1274, loss = 2.83559979\n",
      "Iteration 1275, loss = 2.83548951\n",
      "Iteration 1276, loss = 2.83574169\n",
      "Iteration 1277, loss = 2.83537251\n",
      "Iteration 1278, loss = 2.83523580\n",
      "Iteration 1279, loss = 2.83481802\n",
      "Iteration 1280, loss = 2.83445454\n",
      "Iteration 1281, loss = 2.83455140\n",
      "Iteration 1282, loss = 2.83503075\n",
      "Iteration 1283, loss = 2.83538167\n",
      "Iteration 1284, loss = 2.83517724\n",
      "Iteration 1285, loss = 2.83413864\n",
      "Iteration 1286, loss = 2.83343489\n",
      "Iteration 1287, loss = 2.83344040\n",
      "Iteration 1288, loss = 2.83294863\n",
      "Iteration 1289, loss = 2.83257826\n",
      "Iteration 1290, loss = 2.83324252\n",
      "Iteration 1291, loss = 2.83241703\n",
      "Iteration 1292, loss = 2.83238789\n",
      "Iteration 1293, loss = 2.83186888\n",
      "Iteration 1294, loss = 2.83196547\n",
      "Iteration 1295, loss = 2.83155415\n",
      "Iteration 1296, loss = 2.83131157\n",
      "Iteration 1297, loss = 2.83124541\n",
      "Iteration 1298, loss = 2.83099545\n",
      "Iteration 1299, loss = 2.83140859\n",
      "Iteration 1300, loss = 2.83115743\n",
      "Iteration 1301, loss = 2.83066109\n",
      "Iteration 1302, loss = 2.83037646\n",
      "Iteration 1303, loss = 2.83001939\n",
      "Iteration 1304, loss = 2.83003180\n",
      "Iteration 1305, loss = 2.83025007\n",
      "Iteration 1306, loss = 2.82990108\n",
      "Iteration 1307, loss = 2.83028495\n",
      "Iteration 1308, loss = 2.82991574\n",
      "Iteration 1309, loss = 2.82930655\n",
      "Iteration 1310, loss = 2.82934829\n",
      "Iteration 1311, loss = 2.82847429\n",
      "Iteration 1312, loss = 2.82842696\n",
      "Iteration 1313, loss = 2.82812059\n",
      "Iteration 1314, loss = 2.82822782\n",
      "Iteration 1315, loss = 2.82790338\n",
      "Iteration 1316, loss = 2.82791700\n",
      "Iteration 1317, loss = 2.82802108\n",
      "Iteration 1318, loss = 2.82770398\n",
      "Iteration 1319, loss = 2.82707835\n",
      "Iteration 1320, loss = 2.82804866\n",
      "Iteration 1321, loss = 2.82788026\n",
      "Iteration 1322, loss = 2.82700641\n",
      "Iteration 1323, loss = 2.82631140\n",
      "Iteration 1324, loss = 2.82608078\n",
      "Iteration 1325, loss = 2.82570665\n",
      "Iteration 1326, loss = 2.82557280\n",
      "Iteration 1327, loss = 2.82575900\n",
      "Iteration 1328, loss = 2.82579198\n",
      "Iteration 1329, loss = 2.82595351\n",
      "Iteration 1330, loss = 2.82521070\n",
      "Iteration 1331, loss = 2.82535183\n",
      "Iteration 1332, loss = 2.82550298\n",
      "Iteration 1333, loss = 2.82450837\n",
      "Iteration 1334, loss = 2.82494954\n",
      "Iteration 1335, loss = 2.82432037\n",
      "Iteration 1336, loss = 2.82435342\n",
      "Iteration 1337, loss = 2.82404280\n",
      "Iteration 1338, loss = 2.82425252\n",
      "Iteration 1339, loss = 2.82367617\n",
      "Iteration 1340, loss = 2.82305453\n",
      "Iteration 1341, loss = 2.82258219\n",
      "Iteration 1342, loss = 2.82270413\n",
      "Iteration 1343, loss = 2.82224461\n",
      "Iteration 1344, loss = 2.82228556\n",
      "Iteration 1345, loss = 2.82249664\n",
      "Iteration 1346, loss = 2.82287706\n",
      "Iteration 1347, loss = 2.82231997\n",
      "Iteration 1348, loss = 2.82150411\n",
      "Iteration 1349, loss = 2.82199017\n",
      "Iteration 1350, loss = 2.82206133\n",
      "Iteration 1351, loss = 2.82149925\n",
      "Iteration 1352, loss = 2.82106319\n",
      "Iteration 1353, loss = 2.82161823\n",
      "Iteration 1354, loss = 2.82165253\n",
      "Iteration 1355, loss = 2.82102361\n",
      "Iteration 1356, loss = 2.82066850\n",
      "Iteration 1357, loss = 2.82020341\n",
      "Iteration 1358, loss = 2.82084497\n",
      "Iteration 1359, loss = 2.82037531\n",
      "Iteration 1360, loss = 2.81997428\n",
      "Iteration 1361, loss = 2.81926362\n",
      "Iteration 1362, loss = 2.81916446\n",
      "Iteration 1363, loss = 2.81896218\n",
      "Iteration 1364, loss = 2.81979066\n",
      "Iteration 1365, loss = 2.81976881\n",
      "Iteration 1366, loss = 2.81900633\n",
      "Iteration 1367, loss = 2.81823487\n",
      "Iteration 1368, loss = 2.81781708\n",
      "Iteration 1369, loss = 2.81789701\n",
      "Iteration 1370, loss = 2.81826065\n",
      "Iteration 1371, loss = 2.81736066\n",
      "Iteration 1372, loss = 2.81771063\n",
      "Iteration 1373, loss = 2.81758554\n",
      "Iteration 1374, loss = 2.81738875\n",
      "Iteration 1375, loss = 2.81738169\n",
      "Iteration 1376, loss = 2.81713527\n",
      "Iteration 1377, loss = 2.81680123\n",
      "Iteration 1378, loss = 2.81659177\n",
      "Iteration 1379, loss = 2.81584261\n",
      "Iteration 1380, loss = 2.81558329\n",
      "Iteration 1381, loss = 2.81543543\n",
      "Iteration 1382, loss = 2.81646807\n",
      "Iteration 1383, loss = 2.81568726\n",
      "Iteration 1384, loss = 2.81564858\n",
      "Iteration 1385, loss = 2.81582808\n",
      "Iteration 1386, loss = 2.81506053\n",
      "Iteration 1387, loss = 2.81483290\n",
      "Iteration 1388, loss = 2.81429754\n",
      "Iteration 1389, loss = 2.81445400\n",
      "Iteration 1390, loss = 2.81479514\n",
      "Iteration 1391, loss = 2.81486542\n",
      "Iteration 1392, loss = 2.81430507\n",
      "Iteration 1393, loss = 2.81336484\n",
      "Iteration 1394, loss = 2.81371206\n",
      "Iteration 1395, loss = 2.81379462\n",
      "Iteration 1396, loss = 2.81333190\n",
      "Iteration 1397, loss = 2.81323933\n",
      "Iteration 1398, loss = 2.81271515\n",
      "Iteration 1399, loss = 2.81302838\n",
      "Iteration 1400, loss = 2.81304780\n",
      "Iteration 1401, loss = 2.81277449\n",
      "Iteration 1402, loss = 2.81289318\n",
      "Iteration 1403, loss = 2.81233725\n",
      "Iteration 1404, loss = 2.81247454\n",
      "Iteration 1405, loss = 2.81179839\n",
      "Iteration 1406, loss = 2.81153414\n",
      "Iteration 1407, loss = 2.81134474\n",
      "Iteration 1408, loss = 2.81125952\n",
      "Iteration 1409, loss = 2.81076932\n",
      "Iteration 1410, loss = 2.81209227\n",
      "Iteration 1411, loss = 2.81108979\n",
      "Iteration 1412, loss = 2.81120057\n",
      "Iteration 1413, loss = 2.81118002\n",
      "Iteration 1414, loss = 2.81076172\n",
      "Iteration 1415, loss = 2.81034287\n",
      "Iteration 1416, loss = 2.80995062\n",
      "Iteration 1417, loss = 2.80935022\n",
      "Iteration 1418, loss = 2.80899305\n",
      "Iteration 1419, loss = 2.80904041\n",
      "Iteration 1420, loss = 2.80906915\n",
      "Iteration 1421, loss = 2.80902000\n",
      "Iteration 1422, loss = 2.80898537\n",
      "Iteration 1423, loss = 2.80882704\n",
      "Iteration 1424, loss = 2.80808675\n",
      "Iteration 1425, loss = 2.80886048\n",
      "Iteration 1426, loss = 2.80837219\n",
      "Iteration 1427, loss = 2.80848079\n",
      "Iteration 1428, loss = 2.80789604\n",
      "Iteration 1429, loss = 2.80808860\n",
      "Iteration 1430, loss = 2.80747397\n",
      "Iteration 1431, loss = 2.80742742\n",
      "Iteration 1432, loss = 2.80733256\n",
      "Iteration 1433, loss = 2.80756480\n",
      "Iteration 1434, loss = 2.80686257\n",
      "Iteration 1435, loss = 2.80667116\n",
      "Iteration 1436, loss = 2.80699241\n",
      "Iteration 1437, loss = 2.80635657\n",
      "Iteration 1438, loss = 2.80575303\n",
      "Iteration 1439, loss = 2.80592888\n",
      "Iteration 1440, loss = 2.80575377\n",
      "Iteration 1441, loss = 2.80522888\n",
      "Iteration 1442, loss = 2.80649683\n",
      "Iteration 1443, loss = 2.80586750\n",
      "Iteration 1444, loss = 2.80565823\n",
      "Iteration 1445, loss = 2.80548785\n",
      "Iteration 1446, loss = 2.80579319\n",
      "Iteration 1447, loss = 2.80477915\n",
      "Iteration 1448, loss = 2.80472048\n",
      "Iteration 1449, loss = 2.80395361\n",
      "Iteration 1450, loss = 2.80360969\n",
      "Iteration 1451, loss = 2.80367626\n",
      "Iteration 1452, loss = 2.80380145\n",
      "Iteration 1453, loss = 2.80553977\n",
      "Iteration 1454, loss = 2.80435737\n",
      "Iteration 1455, loss = 2.80384809\n",
      "Iteration 1456, loss = 2.80369085\n",
      "Iteration 1457, loss = 2.80325718\n",
      "Iteration 1458, loss = 2.80359725\n",
      "Iteration 1459, loss = 2.80328210\n",
      "Iteration 1460, loss = 2.80252232\n",
      "Iteration 1461, loss = 2.80220768\n",
      "Iteration 1462, loss = 2.80276965\n",
      "Iteration 1463, loss = 2.80216355\n",
      "Iteration 1464, loss = 2.80233145\n",
      "Iteration 1465, loss = 2.80165508\n",
      "Iteration 1466, loss = 2.80186135\n",
      "Iteration 1467, loss = 2.80212324\n",
      "Iteration 1468, loss = 2.80195955\n",
      "Iteration 1469, loss = 2.80131847\n",
      "Iteration 1470, loss = 2.80020946\n",
      "Iteration 1471, loss = 2.80038789\n",
      "Iteration 1472, loss = 2.80050609\n",
      "Iteration 1473, loss = 2.80060510\n",
      "Iteration 1474, loss = 2.79996485\n",
      "Iteration 1475, loss = 2.79946288\n",
      "Iteration 1476, loss = 2.79989344\n",
      "Iteration 1477, loss = 2.79977373\n",
      "Iteration 1478, loss = 2.79968277\n",
      "Iteration 1479, loss = 2.79978629\n",
      "Iteration 1480, loss = 2.79914825\n",
      "Iteration 1481, loss = 2.79889650\n",
      "Iteration 1482, loss = 2.79902505\n",
      "Iteration 1483, loss = 2.79858519\n",
      "Iteration 1484, loss = 2.79809760\n",
      "Iteration 1485, loss = 2.79810956\n",
      "Iteration 1486, loss = 2.79838632\n",
      "Iteration 1487, loss = 2.79819333\n",
      "Iteration 1488, loss = 2.79767828\n",
      "Iteration 1489, loss = 2.79784961\n",
      "Iteration 1490, loss = 2.79787924\n",
      "Iteration 1491, loss = 2.79741042\n",
      "Iteration 1492, loss = 2.79725862\n",
      "Iteration 1493, loss = 2.79706806\n",
      "Iteration 1494, loss = 2.79714666\n",
      "Iteration 1495, loss = 2.79735479\n",
      "Iteration 1496, loss = 2.79787146\n",
      "Iteration 1497, loss = 2.79670235\n",
      "Iteration 1498, loss = 2.79615426\n",
      "Iteration 1499, loss = 2.79602438\n",
      "Iteration 1500, loss = 2.79558764\n",
      "Iteration 1501, loss = 2.79574088\n",
      "Iteration 1502, loss = 2.79579954\n",
      "Iteration 1503, loss = 2.79594643\n",
      "Iteration 1504, loss = 2.79574906\n",
      "Iteration 1505, loss = 2.79546458\n",
      "Iteration 1506, loss = 2.79549507\n",
      "Iteration 1507, loss = 2.79536693\n",
      "Iteration 1508, loss = 2.79557352\n",
      "Iteration 1509, loss = 2.79620047\n",
      "Iteration 1510, loss = 2.79486082\n",
      "Iteration 1511, loss = 2.79435851\n",
      "Iteration 1512, loss = 2.79446923\n",
      "Iteration 1513, loss = 2.79396147\n",
      "Iteration 1514, loss = 2.79381221\n",
      "Iteration 1515, loss = 2.79369292\n",
      "Iteration 1516, loss = 2.79425085\n",
      "Iteration 1517, loss = 2.79335563\n",
      "Iteration 1518, loss = 2.79328173\n",
      "Iteration 1519, loss = 2.79313302\n",
      "Iteration 1520, loss = 2.79279644\n",
      "Iteration 1521, loss = 2.79277596\n",
      "Iteration 1522, loss = 2.79274015\n",
      "Iteration 1523, loss = 2.79208268\n",
      "Iteration 1524, loss = 2.79212082\n",
      "Iteration 1525, loss = 2.79162426\n",
      "Iteration 1526, loss = 2.79276239\n",
      "Iteration 1527, loss = 2.79236740\n",
      "Iteration 1528, loss = 2.79209893\n",
      "Iteration 1529, loss = 2.79187163\n",
      "Iteration 1530, loss = 2.79069355\n",
      "Iteration 1531, loss = 2.79077290\n",
      "Iteration 1532, loss = 2.79161949\n",
      "Iteration 1533, loss = 2.79085823\n",
      "Iteration 1534, loss = 2.79015853\n",
      "Iteration 1535, loss = 2.79094730\n",
      "Iteration 1536, loss = 2.79101694\n",
      "Iteration 1537, loss = 2.79053566\n",
      "Iteration 1538, loss = 2.78996018\n",
      "Iteration 1539, loss = 2.78960988\n",
      "Iteration 1540, loss = 2.79051238\n",
      "Iteration 1541, loss = 2.78997571\n",
      "Iteration 1542, loss = 2.78921300\n",
      "Iteration 1543, loss = 2.78966174\n",
      "Iteration 1544, loss = 2.79006414\n",
      "Iteration 1545, loss = 2.79010482\n",
      "Iteration 1546, loss = 2.78902923\n",
      "Iteration 1547, loss = 2.78856263\n",
      "Iteration 1548, loss = 2.78803271\n",
      "Iteration 1549, loss = 2.78836291\n",
      "Iteration 1550, loss = 2.78805680\n",
      "Iteration 1551, loss = 2.78806198\n",
      "Iteration 1552, loss = 2.78738785\n",
      "Iteration 1553, loss = 2.78811612\n",
      "Iteration 1554, loss = 2.78832665\n",
      "Iteration 1555, loss = 2.78771609\n",
      "Iteration 1556, loss = 2.78771550\n",
      "Iteration 1557, loss = 2.78700961\n",
      "Iteration 1558, loss = 2.78805992\n",
      "Iteration 1559, loss = 2.78705766\n",
      "Iteration 1560, loss = 2.78720683\n",
      "Iteration 1561, loss = 2.78773532\n",
      "Iteration 1562, loss = 2.78745136\n",
      "Iteration 1563, loss = 2.78603224\n",
      "Iteration 1564, loss = 2.78587112\n",
      "Iteration 1565, loss = 2.78614438\n",
      "Iteration 1566, loss = 2.78626431\n",
      "Iteration 1567, loss = 2.78545864\n",
      "Iteration 1568, loss = 2.78542349\n",
      "Iteration 1569, loss = 2.78528431\n",
      "Iteration 1570, loss = 2.78526448\n",
      "Iteration 1571, loss = 2.78484024\n",
      "Iteration 1572, loss = 2.78453465\n",
      "Iteration 1573, loss = 2.78463154\n",
      "Iteration 1574, loss = 2.78493896\n",
      "Iteration 1575, loss = 2.78491719\n",
      "Iteration 1576, loss = 2.78447239\n",
      "Iteration 1577, loss = 2.78468704\n",
      "Iteration 1578, loss = 2.78491820\n",
      "Iteration 1579, loss = 2.78480442\n",
      "Iteration 1580, loss = 2.78333049\n",
      "Iteration 1581, loss = 2.78309938\n",
      "Iteration 1582, loss = 2.78304725\n",
      "Iteration 1583, loss = 2.78339941\n",
      "Iteration 1584, loss = 2.78348457\n",
      "Iteration 1585, loss = 2.78282199\n",
      "Iteration 1586, loss = 2.78278194\n",
      "Iteration 1587, loss = 2.78230021\n",
      "Iteration 1588, loss = 2.78211861\n",
      "Iteration 1589, loss = 2.78220007\n",
      "Iteration 1590, loss = 2.78222841\n",
      "Iteration 1591, loss = 2.78181726\n",
      "Iteration 1592, loss = 2.78176294\n",
      "Iteration 1593, loss = 2.78175331\n",
      "Iteration 1594, loss = 2.78246499\n",
      "Iteration 1595, loss = 2.78251116\n",
      "Iteration 1596, loss = 2.78175811\n",
      "Iteration 1597, loss = 2.78056625\n",
      "Iteration 1598, loss = 2.78058165\n",
      "Iteration 1599, loss = 2.78051187\n",
      "Iteration 1600, loss = 2.78034760\n",
      "Iteration 1601, loss = 2.78119296\n",
      "Iteration 1602, loss = 2.78089415\n",
      "Iteration 1603, loss = 2.78023687\n",
      "Iteration 1604, loss = 2.78037412\n",
      "Iteration 1605, loss = 2.78050083\n",
      "Iteration 1606, loss = 2.78004733\n",
      "Iteration 1607, loss = 2.77969429\n",
      "Iteration 1608, loss = 2.77923533\n",
      "Iteration 1609, loss = 2.77939845\n",
      "Iteration 1610, loss = 2.77943898\n",
      "Iteration 1611, loss = 2.77883324\n",
      "Iteration 1612, loss = 2.77923069\n",
      "Iteration 1613, loss = 2.77922675\n",
      "Iteration 1614, loss = 2.77825547\n",
      "Iteration 1615, loss = 2.77855241\n",
      "Iteration 1616, loss = 2.77811036\n",
      "Iteration 1617, loss = 2.77894670\n",
      "Iteration 1618, loss = 2.78005417\n",
      "Iteration 1619, loss = 2.77841665\n",
      "Iteration 1620, loss = 2.77764322\n",
      "Iteration 1621, loss = 2.77787803\n",
      "Iteration 1622, loss = 2.77759155\n",
      "Iteration 1623, loss = 2.77753189\n",
      "Iteration 1624, loss = 2.77729820\n",
      "Iteration 1625, loss = 2.77778744\n",
      "Iteration 1626, loss = 2.77658373\n",
      "Iteration 1627, loss = 2.77707416\n",
      "Iteration 1628, loss = 2.77654462\n",
      "Iteration 1629, loss = 2.77658074\n",
      "Iteration 1630, loss = 2.77647691\n",
      "Iteration 1631, loss = 2.77695483\n",
      "Iteration 1632, loss = 2.77626774\n",
      "Iteration 1633, loss = 2.77634854\n",
      "Iteration 1634, loss = 2.77603447\n",
      "Iteration 1635, loss = 2.77560297\n",
      "Iteration 1636, loss = 2.77529630\n",
      "Iteration 1637, loss = 2.77522400\n",
      "Iteration 1638, loss = 2.77661236\n",
      "Iteration 1639, loss = 2.77581991\n",
      "Iteration 1640, loss = 2.77560985\n",
      "Iteration 1641, loss = 2.77534599\n",
      "Iteration 1642, loss = 2.77503857\n",
      "Iteration 1643, loss = 2.77446072\n",
      "Iteration 1644, loss = 2.77442473\n",
      "Iteration 1645, loss = 2.77406651\n",
      "Iteration 1646, loss = 2.77393100\n",
      "Iteration 1647, loss = 2.77414568\n",
      "Iteration 1648, loss = 2.77425322\n",
      "Iteration 1649, loss = 2.77354050\n",
      "Iteration 1650, loss = 2.77373244\n",
      "Iteration 1651, loss = 2.77369646\n",
      "Iteration 1652, loss = 2.77359721\n",
      "Iteration 1653, loss = 2.77327342\n",
      "Iteration 1654, loss = 2.77285112\n",
      "Iteration 1655, loss = 2.77287898\n",
      "Iteration 1656, loss = 2.77278484\n",
      "Iteration 1657, loss = 2.77244858\n",
      "Iteration 1658, loss = 2.77243975\n",
      "Iteration 1659, loss = 2.77274658\n",
      "Iteration 1660, loss = 2.77248484\n",
      "Iteration 1661, loss = 2.77197174\n",
      "Iteration 1662, loss = 2.77188894\n",
      "Iteration 1663, loss = 2.77172417\n",
      "Iteration 1664, loss = 2.77142670\n",
      "Iteration 1665, loss = 2.77158233\n",
      "Iteration 1666, loss = 2.77216056\n",
      "Iteration 1667, loss = 2.77154456\n",
      "Iteration 1668, loss = 2.77154075\n",
      "Iteration 1669, loss = 2.77140819\n",
      "Iteration 1670, loss = 2.77145876\n",
      "Iteration 1671, loss = 2.77102810\n",
      "Iteration 1672, loss = 2.77072257\n",
      "Iteration 1673, loss = 2.77037361\n",
      "Iteration 1674, loss = 2.77020095\n",
      "Iteration 1675, loss = 2.77009295\n",
      "Iteration 1676, loss = 2.77037743\n",
      "Iteration 1677, loss = 2.77082781\n",
      "Iteration 1678, loss = 2.76991638\n",
      "Iteration 1679, loss = 2.76929846\n",
      "Iteration 1680, loss = 2.76891783\n",
      "Iteration 1681, loss = 2.76932000\n",
      "Iteration 1682, loss = 2.76886443\n",
      "Iteration 1683, loss = 2.76896270\n",
      "Iteration 1684, loss = 2.76921313\n",
      "Iteration 1685, loss = 2.76929525\n",
      "Iteration 1686, loss = 2.76874280\n",
      "Iteration 1687, loss = 2.76869530\n",
      "Iteration 1688, loss = 2.76826989\n",
      "Iteration 1689, loss = 2.76832009\n",
      "Iteration 1690, loss = 2.76803131\n",
      "Iteration 1691, loss = 2.76792402\n",
      "Iteration 1692, loss = 2.76814201\n",
      "Iteration 1693, loss = 2.76805932\n",
      "Iteration 1694, loss = 2.76746002\n",
      "Iteration 1695, loss = 2.76736007\n",
      "Iteration 1696, loss = 2.76777252\n",
      "Iteration 1697, loss = 2.76754437\n",
      "Iteration 1698, loss = 2.76726967\n",
      "Iteration 1699, loss = 2.76652778\n",
      "Iteration 1700, loss = 2.76661349\n",
      "Iteration 1701, loss = 2.76655573\n",
      "Iteration 1702, loss = 2.76675464\n",
      "Iteration 1703, loss = 2.76656169\n",
      "Iteration 1704, loss = 2.76606924\n",
      "Iteration 1705, loss = 2.76681663\n",
      "Iteration 1706, loss = 2.76649732\n",
      "Iteration 1707, loss = 2.76590457\n",
      "Iteration 1708, loss = 2.76660530\n",
      "Iteration 1709, loss = 2.76662724\n",
      "Iteration 1710, loss = 2.76604148\n",
      "Iteration 1711, loss = 2.76556402\n",
      "Iteration 1712, loss = 2.76605681\n",
      "Iteration 1713, loss = 2.76602672\n",
      "Iteration 1714, loss = 2.76635006\n",
      "Iteration 1715, loss = 2.76543354\n",
      "Iteration 1716, loss = 2.76447240\n",
      "Iteration 1717, loss = 2.76555271\n",
      "Iteration 1718, loss = 2.76484009\n",
      "Iteration 1719, loss = 2.76382141\n",
      "Iteration 1720, loss = 2.76649501\n",
      "Iteration 1721, loss = 2.76516696\n",
      "Iteration 1722, loss = 2.76420326\n",
      "Iteration 1723, loss = 2.76429377\n",
      "Iteration 1724, loss = 2.76370559\n",
      "Iteration 1725, loss = 2.76413976\n",
      "Iteration 1726, loss = 2.76425769\n",
      "Iteration 1727, loss = 2.76306229\n",
      "Iteration 1728, loss = 2.76349164\n",
      "Iteration 1729, loss = 2.76298915\n",
      "Iteration 1730, loss = 2.76406868\n",
      "Iteration 1731, loss = 2.76316363\n",
      "Iteration 1732, loss = 2.76327615\n",
      "Iteration 1733, loss = 2.76223059\n",
      "Iteration 1734, loss = 2.76237182\n",
      "Iteration 1735, loss = 2.76261797\n",
      "Iteration 1736, loss = 2.76227625\n",
      "Iteration 1737, loss = 2.76184400\n",
      "Iteration 1738, loss = 2.76158852\n",
      "Iteration 1739, loss = 2.76174921\n",
      "Iteration 1740, loss = 2.76171604\n",
      "Iteration 1741, loss = 2.76142787\n",
      "Iteration 1742, loss = 2.76171623\n",
      "Iteration 1743, loss = 2.76140906\n",
      "Iteration 1744, loss = 2.76109377\n",
      "Iteration 1745, loss = 2.76137370\n",
      "Iteration 1746, loss = 2.76088498\n",
      "Iteration 1747, loss = 2.76090522\n",
      "Iteration 1748, loss = 2.76083977\n",
      "Iteration 1749, loss = 2.76059730\n",
      "Iteration 1750, loss = 2.75992678\n",
      "Iteration 1751, loss = 2.76086905\n",
      "Iteration 1752, loss = 2.76094518\n",
      "Iteration 1753, loss = 2.76009296\n",
      "Iteration 1754, loss = 2.76043385\n",
      "Iteration 1755, loss = 2.75987622\n",
      "Iteration 1756, loss = 2.75981010\n",
      "Iteration 1757, loss = 2.76002431\n",
      "Iteration 1758, loss = 2.76073133\n",
      "Iteration 1759, loss = 2.76035436\n",
      "Iteration 1760, loss = 2.75837057\n",
      "Iteration 1761, loss = 2.76046826\n",
      "Iteration 1762, loss = 2.75992415\n",
      "Iteration 1763, loss = 2.75884993\n",
      "Iteration 1764, loss = 2.75910069\n",
      "Iteration 1765, loss = 2.75933624\n",
      "Iteration 1766, loss = 2.75874926\n",
      "Iteration 1767, loss = 2.75819170\n",
      "Iteration 1768, loss = 2.75779345\n",
      "Iteration 1769, loss = 2.75788557\n",
      "Iteration 1770, loss = 2.75763625\n",
      "Iteration 1771, loss = 2.75748797\n",
      "Iteration 1772, loss = 2.75730253\n",
      "Iteration 1773, loss = 2.75780160\n",
      "Iteration 1774, loss = 2.75803071\n",
      "Iteration 1775, loss = 2.75784659\n",
      "Iteration 1776, loss = 2.75743845\n",
      "Iteration 1777, loss = 2.75743340\n",
      "Iteration 1778, loss = 2.75686846\n",
      "Iteration 1779, loss = 2.75685847\n",
      "Iteration 1780, loss = 2.75723380\n",
      "Iteration 1781, loss = 2.75682638\n",
      "Iteration 1782, loss = 2.75663940\n",
      "Iteration 1783, loss = 2.75718914\n",
      "Iteration 1784, loss = 2.75645295\n",
      "Iteration 1785, loss = 2.75638098\n",
      "Iteration 1786, loss = 2.75585445\n",
      "Iteration 1787, loss = 2.75562990\n",
      "Iteration 1788, loss = 2.75501982\n",
      "Iteration 1789, loss = 2.75540996\n",
      "Iteration 1790, loss = 2.75609344\n",
      "Iteration 1791, loss = 2.75598273\n",
      "Iteration 1792, loss = 2.75509539\n",
      "Iteration 1793, loss = 2.75446068\n",
      "Iteration 1794, loss = 2.75443259\n",
      "Iteration 1795, loss = 2.75469795\n",
      "Iteration 1796, loss = 2.75473950\n",
      "Iteration 1797, loss = 2.75527749\n",
      "Iteration 1798, loss = 2.75433675\n",
      "Iteration 1799, loss = 2.75454553\n",
      "Iteration 1800, loss = 2.75445317\n",
      "Iteration 1801, loss = 2.75412952\n",
      "Iteration 1802, loss = 2.75380981\n",
      "Iteration 1803, loss = 2.75362523\n",
      "Iteration 1804, loss = 2.75349994\n",
      "Iteration 1805, loss = 2.75384005\n",
      "Iteration 1806, loss = 2.75338234\n",
      "Iteration 1807, loss = 2.75412284\n",
      "Iteration 1808, loss = 2.75394637\n",
      "Iteration 1809, loss = 2.75273221\n",
      "Iteration 1810, loss = 2.75383631\n",
      "Iteration 1811, loss = 2.75466200\n",
      "Iteration 1812, loss = 2.75323979\n",
      "Iteration 1813, loss = 2.75220833\n",
      "Iteration 1814, loss = 2.75242618\n",
      "Iteration 1815, loss = 2.75178124\n",
      "Iteration 1816, loss = 2.75175913\n",
      "Iteration 1817, loss = 2.75175992\n",
      "Iteration 1818, loss = 2.75217430\n",
      "Iteration 1819, loss = 2.75209638\n",
      "Iteration 1820, loss = 2.75175141\n",
      "Iteration 1821, loss = 2.75204355\n",
      "Iteration 1822, loss = 2.75203568\n",
      "Iteration 1823, loss = 2.75126056\n",
      "Iteration 1824, loss = 2.75088577\n",
      "Iteration 1825, loss = 2.75098971\n",
      "Iteration 1826, loss = 2.75119352\n",
      "Iteration 1827, loss = 2.75049747\n",
      "Iteration 1828, loss = 2.75012564\n",
      "Iteration 1829, loss = 2.75109536\n",
      "Iteration 1830, loss = 2.75201574\n",
      "Iteration 1831, loss = 2.75062936\n",
      "Iteration 1832, loss = 2.75015710\n",
      "Iteration 1833, loss = 2.74999109\n",
      "Iteration 1834, loss = 2.74962117\n",
      "Iteration 1835, loss = 2.74994853\n",
      "Iteration 1836, loss = 2.74970878\n",
      "Iteration 1837, loss = 2.75058824\n",
      "Iteration 1838, loss = 2.74994185\n",
      "Iteration 1839, loss = 2.74988002\n",
      "Iteration 1840, loss = 2.74957897\n",
      "Iteration 1841, loss = 2.75017797\n",
      "Iteration 1842, loss = 2.74974727\n",
      "Iteration 1843, loss = 2.74889888\n",
      "Iteration 1844, loss = 2.74922675\n",
      "Iteration 1845, loss = 2.74851706\n",
      "Iteration 1846, loss = 2.74865708\n",
      "Iteration 1847, loss = 2.74888769\n",
      "Iteration 1848, loss = 2.74795138\n",
      "Iteration 1849, loss = 2.74757260\n",
      "Iteration 1850, loss = 2.74825251\n",
      "Iteration 1851, loss = 2.74790984\n",
      "Iteration 1852, loss = 2.74741769\n",
      "Iteration 1853, loss = 2.74744060\n",
      "Iteration 1854, loss = 2.74732463\n",
      "Iteration 1855, loss = 2.74657597\n",
      "Iteration 1856, loss = 2.74691735\n",
      "Iteration 1857, loss = 2.74742277\n",
      "Iteration 1858, loss = 2.74714271\n",
      "Iteration 1859, loss = 2.74771067\n",
      "Iteration 1860, loss = 2.74706562\n",
      "Iteration 1861, loss = 2.74656202\n",
      "Iteration 1862, loss = 2.74623908\n",
      "Iteration 1863, loss = 2.74621440\n",
      "Iteration 1864, loss = 2.74597781\n",
      "Iteration 1865, loss = 2.74573962\n",
      "Iteration 1866, loss = 2.74571631\n",
      "Iteration 1867, loss = 2.74598187\n",
      "Iteration 1868, loss = 2.74606557\n",
      "Iteration 1869, loss = 2.74563741\n",
      "Iteration 1870, loss = 2.74642675\n",
      "Iteration 1871, loss = 2.74578949\n",
      "Iteration 1872, loss = 2.74530030\n",
      "Iteration 1873, loss = 2.74534254\n",
      "Iteration 1874, loss = 2.74571650\n",
      "Iteration 1875, loss = 2.74609197\n",
      "Iteration 1876, loss = 2.74600754\n",
      "Iteration 1877, loss = 2.74524554\n",
      "Iteration 1878, loss = 2.74530114\n",
      "Iteration 1879, loss = 2.74474508\n",
      "Iteration 1880, loss = 2.74463797\n",
      "Iteration 1881, loss = 2.74422937\n",
      "Iteration 1882, loss = 2.74496613\n",
      "Iteration 1883, loss = 2.74525937\n",
      "Iteration 1884, loss = 2.74420738\n",
      "Iteration 1885, loss = 2.74393795\n",
      "Iteration 1886, loss = 2.74385512\n",
      "Iteration 1887, loss = 2.74354994\n",
      "Iteration 1888, loss = 2.74460331\n",
      "Iteration 1889, loss = 2.74310703\n",
      "Iteration 1890, loss = 2.74353290\n",
      "Iteration 1891, loss = 2.74436174\n",
      "Iteration 1892, loss = 2.74335741\n",
      "Iteration 1893, loss = 2.74318961\n",
      "Iteration 1894, loss = 2.74345399\n",
      "Iteration 1895, loss = 2.74220297\n",
      "Iteration 1896, loss = 2.74245925\n",
      "Iteration 1897, loss = 2.74336799\n",
      "Iteration 1898, loss = 2.74171336\n",
      "Iteration 1899, loss = 2.74248598\n",
      "Iteration 1900, loss = 2.74264392\n",
      "Iteration 1901, loss = 2.74196041\n",
      "Iteration 1902, loss = 2.74221241\n",
      "Iteration 1903, loss = 2.74179502\n",
      "Iteration 1904, loss = 2.74135245\n",
      "Iteration 1905, loss = 2.74096891\n",
      "Iteration 1906, loss = 2.74148246\n",
      "Iteration 1907, loss = 2.74125777\n",
      "Iteration 1908, loss = 2.74102591\n",
      "Iteration 1909, loss = 2.74073390\n",
      "Iteration 1910, loss = 2.74041378\n",
      "Iteration 1911, loss = 2.74050406\n",
      "Iteration 1912, loss = 2.74085783\n",
      "Iteration 1913, loss = 2.74053869\n",
      "Iteration 1914, loss = 2.74022065\n",
      "Iteration 1915, loss = 2.74057505\n",
      "Iteration 1916, loss = 2.73999261\n",
      "Iteration 1917, loss = 2.74060796\n",
      "Iteration 1918, loss = 2.73957048\n",
      "Iteration 1919, loss = 2.74151581\n",
      "Iteration 1920, loss = 2.74082668\n",
      "Iteration 1921, loss = 2.73949557\n",
      "Iteration 1922, loss = 2.74013275\n",
      "Iteration 1923, loss = 2.74000362\n",
      "Iteration 1924, loss = 2.73946902\n",
      "Iteration 1925, loss = 2.73949462\n",
      "Iteration 1926, loss = 2.73917293\n",
      "Iteration 1927, loss = 2.73957178\n",
      "Iteration 1928, loss = 2.73948802\n",
      "Iteration 1929, loss = 2.73921237\n",
      "Iteration 1930, loss = 2.73898809\n",
      "Iteration 1931, loss = 2.73850494\n",
      "Iteration 1932, loss = 2.73867305\n",
      "Iteration 1933, loss = 2.73809091\n",
      "Iteration 1934, loss = 2.73837322\n",
      "Iteration 1935, loss = 2.73811916\n",
      "Iteration 1936, loss = 2.73746477\n",
      "Iteration 1937, loss = 2.73815248\n",
      "Iteration 1938, loss = 2.73819529\n",
      "Iteration 1939, loss = 2.73815805\n",
      "Iteration 1940, loss = 2.73789115\n",
      "Iteration 1941, loss = 2.73748795\n",
      "Iteration 1942, loss = 2.73733467\n",
      "Iteration 1943, loss = 2.73745711\n",
      "Iteration 1944, loss = 2.73676365\n",
      "Iteration 1945, loss = 2.73632969\n",
      "Iteration 1946, loss = 2.73654508\n",
      "Iteration 1947, loss = 2.73645644\n",
      "Iteration 1948, loss = 2.73632289\n",
      "Iteration 1949, loss = 2.73645639\n",
      "Iteration 1950, loss = 2.73700640\n",
      "Iteration 1951, loss = 2.73614478\n",
      "Iteration 1952, loss = 2.73652769\n",
      "Iteration 1953, loss = 2.73657634\n",
      "Iteration 1954, loss = 2.73594714\n",
      "Iteration 1955, loss = 2.73617374\n",
      "Iteration 1956, loss = 2.73639358\n",
      "Iteration 1957, loss = 2.73599316\n",
      "Iteration 1958, loss = 2.73597117\n",
      "Iteration 1959, loss = 2.73538928\n",
      "Iteration 1960, loss = 2.73631330\n",
      "Iteration 1961, loss = 2.73511448\n",
      "Iteration 1962, loss = 2.73514889\n",
      "Iteration 1963, loss = 2.73497341\n",
      "Iteration 1964, loss = 2.73461048\n",
      "Iteration 1965, loss = 2.73488410\n",
      "Iteration 1966, loss = 2.73478914\n",
      "Iteration 1967, loss = 2.73449987\n",
      "Iteration 1968, loss = 2.73491347\n",
      "Iteration 1969, loss = 2.73457732\n",
      "Iteration 1970, loss = 2.73474012\n",
      "Iteration 1971, loss = 2.73413278\n",
      "Iteration 1972, loss = 2.73394149\n",
      "Iteration 1973, loss = 2.73391310\n",
      "Iteration 1974, loss = 2.73369997\n",
      "Iteration 1975, loss = 2.73353256\n",
      "Iteration 1976, loss = 2.73506559\n",
      "Iteration 1977, loss = 2.73437731\n",
      "Iteration 1978, loss = 2.73344949\n",
      "Iteration 1979, loss = 2.73413584\n",
      "Iteration 1980, loss = 2.73275542\n",
      "Iteration 1981, loss = 2.73258341\n",
      "Iteration 1982, loss = 2.73375688\n",
      "Iteration 1983, loss = 2.73266783\n",
      "Iteration 1984, loss = 2.73232653\n",
      "Iteration 1985, loss = 2.73259432\n",
      "Iteration 1986, loss = 2.73263947\n",
      "Iteration 1987, loss = 2.73244160\n",
      "Iteration 1988, loss = 2.73203170\n",
      "Iteration 1989, loss = 2.73204673\n",
      "Iteration 1990, loss = 2.73229656\n",
      "Iteration 1991, loss = 2.73220653\n",
      "Iteration 1992, loss = 2.73153343\n",
      "Iteration 1993, loss = 2.73243973\n",
      "Iteration 1994, loss = 2.73193131\n",
      "Iteration 1995, loss = 2.73212048\n",
      "Iteration 1996, loss = 2.73229178\n",
      "Iteration 1997, loss = 2.73153585\n",
      "Iteration 1998, loss = 2.73073775\n",
      "Iteration 1999, loss = 2.73145630\n",
      "Iteration 2000, loss = 2.73180000\n",
      "Iteration 2001, loss = 2.73137945\n",
      "Iteration 2002, loss = 2.73116334\n",
      "Iteration 2003, loss = 2.73086577\n",
      "Iteration 2004, loss = 2.73116601\n",
      "Iteration 2005, loss = 2.73032672\n",
      "Iteration 2006, loss = 2.73060492\n",
      "Iteration 2007, loss = 2.73052881\n",
      "Iteration 2008, loss = 2.73059377\n",
      "Iteration 2009, loss = 2.73110466\n",
      "Iteration 2010, loss = 2.72990873\n",
      "Iteration 2011, loss = 2.73038160\n",
      "Iteration 2012, loss = 2.72981927\n",
      "Iteration 2013, loss = 2.72987771\n",
      "Iteration 2014, loss = 2.72957300\n",
      "Iteration 2015, loss = 2.72934078\n",
      "Iteration 2016, loss = 2.72901745\n",
      "Iteration 2017, loss = 2.72882598\n",
      "Iteration 2018, loss = 2.72912639\n",
      "Iteration 2019, loss = 2.72877253\n",
      "Iteration 2020, loss = 2.72862982\n",
      "Iteration 2021, loss = 2.72855934\n",
      "Iteration 2022, loss = 2.72915481\n",
      "Iteration 2023, loss = 2.72913104\n",
      "Iteration 2024, loss = 2.72879979\n",
      "Iteration 2025, loss = 2.72788260\n",
      "Iteration 2026, loss = 2.72744186\n",
      "Iteration 2027, loss = 2.72791049\n",
      "Iteration 2028, loss = 2.72781927\n",
      "Iteration 2029, loss = 2.72869027\n",
      "Iteration 2030, loss = 2.72766311\n",
      "Iteration 2031, loss = 2.72867716\n",
      "Iteration 2032, loss = 2.72855786\n",
      "Iteration 2033, loss = 2.72829300\n",
      "Iteration 2034, loss = 2.72748692\n",
      "Iteration 2035, loss = 2.72691025\n",
      "Iteration 2036, loss = 2.72679103\n",
      "Iteration 2037, loss = 2.72700877\n",
      "Iteration 2038, loss = 2.72704066\n",
      "Iteration 2039, loss = 2.72704939\n",
      "Iteration 2040, loss = 2.72749117\n",
      "Iteration 2041, loss = 2.72656194\n",
      "Iteration 2042, loss = 2.72662077\n",
      "Iteration 2043, loss = 2.72665396\n",
      "Iteration 2044, loss = 2.72641281\n",
      "Iteration 2045, loss = 2.72576109\n",
      "Iteration 2046, loss = 2.72607117\n",
      "Iteration 2047, loss = 2.72591446\n",
      "Iteration 2048, loss = 2.72567587\n",
      "Iteration 2049, loss = 2.72622886\n",
      "Iteration 2050, loss = 2.72624842\n",
      "Iteration 2051, loss = 2.72523080\n",
      "Iteration 2052, loss = 2.72561740\n",
      "Iteration 2053, loss = 2.72574297\n",
      "Iteration 2054, loss = 2.72622476\n",
      "Iteration 2055, loss = 2.72498976\n",
      "Iteration 2056, loss = 2.72609729\n",
      "Iteration 2057, loss = 2.72502388\n",
      "Iteration 2058, loss = 2.72438156\n",
      "Iteration 2059, loss = 2.72491887\n",
      "Iteration 2060, loss = 2.72534505\n",
      "Iteration 2061, loss = 2.72525894\n",
      "Iteration 2062, loss = 2.72391584\n",
      "Iteration 2063, loss = 2.72414164\n",
      "Iteration 2064, loss = 2.72425867\n",
      "Iteration 2065, loss = 2.72428417\n",
      "Iteration 2066, loss = 2.72399010\n",
      "Iteration 2067, loss = 2.72346994\n",
      "Iteration 2068, loss = 2.72301016\n",
      "Iteration 2069, loss = 2.72421916\n",
      "Iteration 2070, loss = 2.72374245\n",
      "Iteration 2071, loss = 2.72371515\n",
      "Iteration 2072, loss = 2.72326757\n",
      "Iteration 2073, loss = 2.72318802\n",
      "Iteration 2074, loss = 2.72312616\n",
      "Iteration 2075, loss = 2.72302168\n",
      "Iteration 2076, loss = 2.72303431\n",
      "Iteration 2077, loss = 2.72287717\n",
      "Iteration 2078, loss = 2.72221233\n",
      "Iteration 2079, loss = 2.72302720\n",
      "Iteration 2080, loss = 2.72431264\n",
      "Iteration 2081, loss = 2.72337008\n",
      "Iteration 2082, loss = 2.72205481\n",
      "Iteration 2083, loss = 2.72171610\n",
      "Iteration 2084, loss = 2.72152564\n",
      "Iteration 2085, loss = 2.72230264\n",
      "Iteration 2086, loss = 2.72206779\n",
      "Iteration 2087, loss = 2.72168869\n",
      "Iteration 2088, loss = 2.72166329\n",
      "Iteration 2089, loss = 2.72201490\n",
      "Iteration 2090, loss = 2.72167099\n",
      "Iteration 2091, loss = 2.72157423\n",
      "Iteration 2092, loss = 2.72133509\n",
      "Iteration 2093, loss = 2.72094271\n",
      "Iteration 2094, loss = 2.72082594\n",
      "Iteration 2095, loss = 2.72076825\n",
      "Iteration 2096, loss = 2.72077938\n",
      "Iteration 2097, loss = 2.72132599\n",
      "Iteration 2098, loss = 2.72107209\n",
      "Iteration 2099, loss = 2.72012441\n",
      "Iteration 2100, loss = 2.71977264\n",
      "Iteration 2101, loss = 2.72024661\n",
      "Iteration 2102, loss = 2.72025981\n",
      "Iteration 2103, loss = 2.72030751\n",
      "Iteration 2104, loss = 2.71946112\n",
      "Iteration 2105, loss = 2.71991079\n",
      "Iteration 2106, loss = 2.72084904\n",
      "Iteration 2107, loss = 2.72064372\n",
      "Iteration 2108, loss = 2.71977147\n",
      "Iteration 2109, loss = 2.71955722\n",
      "Iteration 2110, loss = 2.71995140\n",
      "Iteration 2111, loss = 2.72085130\n",
      "Iteration 2112, loss = 2.72000546\n",
      "Iteration 2113, loss = 2.71962969\n",
      "Iteration 2114, loss = 2.71969700\n",
      "Iteration 2115, loss = 2.71872407\n",
      "Iteration 2116, loss = 2.71927869\n",
      "Iteration 2117, loss = 2.71927243\n",
      "Iteration 2118, loss = 2.72013706\n",
      "Iteration 2119, loss = 2.71821969\n",
      "Iteration 2120, loss = 2.71970511\n",
      "Iteration 2121, loss = 2.71927865\n",
      "Iteration 2122, loss = 2.71835290\n",
      "Iteration 2123, loss = 2.71907143\n",
      "Iteration 2124, loss = 2.71944705\n",
      "Iteration 2125, loss = 2.71833440\n",
      "Iteration 2126, loss = 2.71828524\n",
      "Iteration 2127, loss = 2.71803889\n",
      "Iteration 2128, loss = 2.71790800\n",
      "Iteration 2129, loss = 2.71693370\n",
      "Iteration 2130, loss = 2.71832128\n",
      "Iteration 2131, loss = 2.71806484\n",
      "Iteration 2132, loss = 2.71728129\n",
      "Iteration 2133, loss = 2.71760264\n",
      "Iteration 2134, loss = 2.71736026\n",
      "Iteration 2135, loss = 2.71830464\n",
      "Iteration 2136, loss = 2.71719077\n",
      "Iteration 2137, loss = 2.71607208\n",
      "Iteration 2138, loss = 2.71648596\n",
      "Iteration 2139, loss = 2.71675295\n",
      "Iteration 2140, loss = 2.71681072\n",
      "Iteration 2141, loss = 2.71659879\n",
      "Iteration 2142, loss = 2.71571330\n",
      "Iteration 2143, loss = 2.71594080\n",
      "Iteration 2144, loss = 2.71731612\n",
      "Iteration 2145, loss = 2.71604987\n",
      "Iteration 2146, loss = 2.71626335\n",
      "Iteration 2147, loss = 2.71544546\n",
      "Iteration 2148, loss = 2.71676467\n",
      "Iteration 2149, loss = 2.71686144\n",
      "Iteration 2150, loss = 2.71567673\n",
      "Iteration 2151, loss = 2.71557407\n",
      "Iteration 2152, loss = 2.71564175\n",
      "Iteration 2153, loss = 2.71560864\n",
      "Iteration 2154, loss = 2.71568028\n",
      "Iteration 2155, loss = 2.71596344\n",
      "Iteration 2156, loss = 2.71483912\n",
      "Iteration 2157, loss = 2.71500740\n",
      "Iteration 2158, loss = 2.71514552\n",
      "Iteration 2159, loss = 2.71546086\n",
      "Iteration 2160, loss = 2.71572632\n",
      "Iteration 2161, loss = 2.71519426\n",
      "Iteration 2162, loss = 2.71538587\n",
      "Iteration 2163, loss = 2.71450133\n",
      "Iteration 2164, loss = 2.71490603\n",
      "Iteration 2165, loss = 2.71538794\n",
      "Iteration 2166, loss = 2.71516930\n",
      "Iteration 2167, loss = 2.71535128\n",
      "Iteration 2168, loss = 2.71434324\n",
      "Iteration 2169, loss = 2.71409211\n",
      "Iteration 2170, loss = 2.71417947\n",
      "Iteration 2171, loss = 2.71546337\n",
      "Iteration 2172, loss = 2.71378672\n",
      "Iteration 2173, loss = 2.71357161\n",
      "Iteration 2174, loss = 2.71296919\n",
      "Iteration 2175, loss = 2.71306685\n",
      "Iteration 2176, loss = 2.71273034\n",
      "Iteration 2177, loss = 2.71357818\n",
      "Iteration 2178, loss = 2.71228794\n",
      "Iteration 2179, loss = 2.71239618\n",
      "Iteration 2180, loss = 2.71624758\n",
      "Iteration 2181, loss = 2.71497760\n",
      "Iteration 2182, loss = 2.71266587\n",
      "Iteration 2183, loss = 2.71253449\n",
      "Iteration 2184, loss = 2.71251045\n",
      "Iteration 2185, loss = 2.71465008\n",
      "Iteration 2186, loss = 2.71237535\n",
      "Iteration 2187, loss = 2.71188250\n",
      "Iteration 2188, loss = 2.71230525\n",
      "Iteration 2189, loss = 2.71247796\n",
      "Iteration 2190, loss = 2.71249311\n",
      "Iteration 2191, loss = 2.71172628\n",
      "Iteration 2192, loss = 2.71157212\n",
      "Iteration 2193, loss = 2.71148430\n",
      "Iteration 2194, loss = 2.71124440\n",
      "Iteration 2195, loss = 2.71239289\n",
      "Iteration 2196, loss = 2.71322103\n",
      "Iteration 2197, loss = 2.71290888\n",
      "Iteration 2198, loss = 2.71187298\n",
      "Iteration 2199, loss = 2.71157819\n",
      "Iteration 2200, loss = 2.71171280\n",
      "Iteration 2201, loss = 2.71058467\n",
      "Iteration 2202, loss = 2.71059282\n",
      "Iteration 2203, loss = 2.71036185\n",
      "Iteration 2204, loss = 2.70933504\n",
      "Iteration 2205, loss = 2.71006830\n",
      "Iteration 2206, loss = 2.70988679\n",
      "Iteration 2207, loss = 2.70939336\n",
      "Iteration 2208, loss = 2.71053901\n",
      "Iteration 2209, loss = 2.71053345\n",
      "Iteration 2210, loss = 2.71027307\n",
      "Iteration 2211, loss = 2.70905023\n",
      "Iteration 2212, loss = 2.70948932\n",
      "Iteration 2213, loss = 2.71001266\n",
      "Iteration 2214, loss = 2.70996980\n",
      "Iteration 2215, loss = 2.70900571\n",
      "Iteration 2216, loss = 2.70923578\n",
      "Iteration 2217, loss = 2.70940458\n",
      "Iteration 2218, loss = 2.70932269\n",
      "Iteration 2219, loss = 2.71018738\n",
      "Iteration 2220, loss = 2.70962460\n",
      "Iteration 2221, loss = 2.71000992\n",
      "Iteration 2222, loss = 2.70889925\n",
      "Iteration 2223, loss = 2.70866310\n",
      "Iteration 2224, loss = 2.70793191\n",
      "Iteration 2225, loss = 2.70876348\n",
      "Iteration 2226, loss = 2.70832922\n",
      "Iteration 2227, loss = 2.70815155\n",
      "Iteration 2228, loss = 2.70813860\n",
      "Iteration 2229, loss = 2.70831088\n",
      "Iteration 2230, loss = 2.70748004\n",
      "Iteration 2231, loss = 2.70871186\n",
      "Iteration 2232, loss = 2.70815917\n",
      "Iteration 2233, loss = 2.70835645\n",
      "Iteration 2234, loss = 2.70839703\n",
      "Iteration 2235, loss = 2.70786236\n",
      "Iteration 2236, loss = 2.70724502\n",
      "Iteration 2237, loss = 2.70738883\n",
      "Iteration 2238, loss = 2.70734893\n",
      "Iteration 2239, loss = 2.70691431\n",
      "Iteration 2240, loss = 2.70736428\n",
      "Iteration 2241, loss = 2.70722151\n",
      "Iteration 2242, loss = 2.70698663\n",
      "Iteration 2243, loss = 2.70693781\n",
      "Iteration 2244, loss = 2.70734597\n",
      "Iteration 2245, loss = 2.70670293\n",
      "Iteration 2246, loss = 2.70668175\n",
      "Iteration 2247, loss = 2.70591746\n",
      "Iteration 2248, loss = 2.70565487\n",
      "Iteration 2249, loss = 2.70634111\n",
      "Iteration 2250, loss = 2.70591776\n",
      "Iteration 2251, loss = 2.70646542\n",
      "Iteration 2252, loss = 2.70598450\n",
      "Iteration 2253, loss = 2.70634273\n",
      "Iteration 2254, loss = 2.70635218\n",
      "Iteration 2255, loss = 2.70593522\n",
      "Iteration 2256, loss = 2.70678897\n",
      "Iteration 2257, loss = 2.70730318\n",
      "Iteration 2258, loss = 2.70698099\n",
      "Iteration 2259, loss = 2.70636660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.95362821\n",
      "Iteration 2, loss = 3.36850562\n",
      "Iteration 3, loss = 3.27975000\n",
      "Iteration 4, loss = 3.29330610\n",
      "Iteration 5, loss = 3.25637438\n",
      "Iteration 6, loss = 3.26373256\n",
      "Iteration 7, loss = 3.25764429\n",
      "Iteration 8, loss = 3.25245598\n",
      "Iteration 9, loss = 3.24618026\n",
      "Iteration 10, loss = 3.24945920\n",
      "Iteration 11, loss = 3.25404141\n",
      "Iteration 12, loss = 3.25242351\n",
      "Iteration 13, loss = 3.24971045\n",
      "Iteration 14, loss = 3.25002905\n",
      "Iteration 15, loss = 3.24952107\n",
      "Iteration 16, loss = 3.25301387\n",
      "Iteration 17, loss = 3.24636854\n",
      "Iteration 18, loss = 3.24989856\n",
      "Iteration 19, loss = 3.25456207\n",
      "Iteration 20, loss = 3.25112150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.95197394\n",
      "Iteration 2, loss = 3.37294780\n",
      "Iteration 3, loss = 3.28572982\n",
      "Iteration 4, loss = 3.30308513\n",
      "Iteration 5, loss = 3.25554435\n",
      "Iteration 6, loss = 3.25622465\n",
      "Iteration 7, loss = 3.25744124\n",
      "Iteration 8, loss = 3.25950202\n",
      "Iteration 9, loss = 3.25791204\n",
      "Iteration 10, loss = 3.25412456\n",
      "Iteration 11, loss = 3.25935990\n",
      "Iteration 12, loss = 3.24994290\n",
      "Iteration 13, loss = 3.25110825\n",
      "Iteration 14, loss = 3.25626547\n",
      "Iteration 15, loss = 3.25057706\n",
      "Iteration 16, loss = 3.25862836\n",
      "Iteration 17, loss = 3.25700259\n",
      "Iteration 18, loss = 3.25454074\n",
      "Iteration 19, loss = 3.25367303\n",
      "Iteration 20, loss = 3.25304841\n",
      "Iteration 21, loss = 3.25436269\n",
      "Iteration 22, loss = 3.25465982\n",
      "Iteration 23, loss = 3.25457761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.95027352\n",
      "Iteration 2, loss = 3.37163806\n",
      "Iteration 3, loss = 3.27655258\n",
      "Iteration 4, loss = 3.29537830\n",
      "Iteration 5, loss = 3.26083265\n",
      "Iteration 6, loss = 3.25186661\n",
      "Iteration 7, loss = 3.24608318\n",
      "Iteration 8, loss = 3.24972333\n",
      "Iteration 9, loss = 3.24622503\n",
      "Iteration 10, loss = 3.24854808\n",
      "Iteration 11, loss = 3.25119237\n",
      "Iteration 12, loss = 3.24345502\n",
      "Iteration 13, loss = 3.24623100\n",
      "Iteration 14, loss = 3.25163371\n",
      "Iteration 15, loss = 3.24682146\n",
      "Iteration 16, loss = 3.24882085\n",
      "Iteration 17, loss = 3.25059505\n",
      "Iteration 18, loss = 3.24489027\n",
      "Iteration 19, loss = 3.25180974\n",
      "Iteration 20, loss = 3.24895690\n",
      "Iteration 21, loss = 3.25143139\n",
      "Iteration 22, loss = 3.24945825\n",
      "Iteration 23, loss = 3.24838324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.93327642\n",
      "Iteration 2, loss = 3.37497421\n",
      "Iteration 3, loss = 3.27970553\n",
      "Iteration 4, loss = 3.29853784\n",
      "Iteration 5, loss = 3.25872865\n",
      "Iteration 6, loss = 3.25522063\n",
      "Iteration 7, loss = 3.25237662\n",
      "Iteration 8, loss = 3.25167130\n",
      "Iteration 9, loss = 3.24794191\n",
      "Iteration 10, loss = 3.24940317\n",
      "Iteration 11, loss = 3.24319806\n",
      "Iteration 12, loss = 3.24698272\n",
      "Iteration 13, loss = 3.24597961\n",
      "Iteration 14, loss = 3.24427510\n",
      "Iteration 15, loss = 3.24532125\n",
      "Iteration 16, loss = 3.24267492\n",
      "Iteration 17, loss = 3.24484802\n",
      "Iteration 18, loss = 3.24515043\n",
      "Iteration 19, loss = 3.25138396\n",
      "Iteration 20, loss = 3.24852442\n",
      "Iteration 21, loss = 3.24810387\n",
      "Iteration 22, loss = 3.24573342\n",
      "Iteration 23, loss = 3.24437036\n",
      "Iteration 24, loss = 3.24228411\n",
      "Iteration 25, loss = 3.24657672\n",
      "Iteration 26, loss = 3.25090591\n",
      "Iteration 27, loss = 3.24605537\n",
      "Iteration 28, loss = 3.24516080\n",
      "Iteration 29, loss = 3.24724252\n",
      "Iteration 30, loss = 3.23786638\n",
      "Iteration 31, loss = 3.24429442\n",
      "Iteration 32, loss = 3.25009583\n",
      "Iteration 33, loss = 3.24489289\n",
      "Iteration 34, loss = 3.24462435\n",
      "Iteration 35, loss = 3.24965108\n",
      "Iteration 36, loss = 3.24586150\n",
      "Iteration 37, loss = 3.24944678\n",
      "Iteration 38, loss = 3.24227034\n",
      "Iteration 39, loss = 3.24616188\n",
      "Iteration 40, loss = 3.24959490\n",
      "Iteration 41, loss = 3.24512561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.95299887\n",
      "Iteration 2, loss = 3.35178095\n",
      "Iteration 3, loss = 3.28084956\n",
      "Iteration 4, loss = 3.31005009\n",
      "Iteration 5, loss = 3.26604153\n",
      "Iteration 6, loss = 3.26307785\n",
      "Iteration 7, loss = 3.25697412\n",
      "Iteration 8, loss = 3.25801003\n",
      "Iteration 9, loss = 3.25667004\n",
      "Iteration 10, loss = 3.25301023\n",
      "Iteration 11, loss = 3.24956851\n",
      "Iteration 12, loss = 3.25057820\n",
      "Iteration 13, loss = 3.25445978\n",
      "Iteration 14, loss = 3.25118280\n",
      "Iteration 15, loss = 3.25148191\n",
      "Iteration 16, loss = 3.24903121\n",
      "Iteration 17, loss = 3.24958192\n",
      "Iteration 18, loss = 3.25166702\n",
      "Iteration 19, loss = 3.25381265\n",
      "Iteration 20, loss = 3.25007236\n",
      "Iteration 21, loss = 3.25267544\n",
      "Iteration 22, loss = 3.25546586\n",
      "Iteration 23, loss = 3.25117794\n",
      "Iteration 24, loss = 3.25160728\n",
      "Iteration 25, loss = 3.25086116\n",
      "Iteration 26, loss = 3.25295499\n",
      "Iteration 27, loss = 3.25438893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15103528\n",
      "Iteration 2, loss = 4.02257888\n",
      "Iteration 3, loss = 3.90434946\n",
      "Iteration 4, loss = 3.79240130\n",
      "Iteration 5, loss = 3.68577869\n",
      "Iteration 6, loss = 3.58362290\n",
      "Iteration 7, loss = 3.49296908\n",
      "Iteration 8, loss = 3.41206746\n",
      "Iteration 9, loss = 3.34471487\n",
      "Iteration 10, loss = 3.29639478\n",
      "Iteration 11, loss = 3.26281598\n",
      "Iteration 12, loss = 3.24515022\n",
      "Iteration 13, loss = 3.23846994\n",
      "Iteration 14, loss = 3.23601761\n",
      "Iteration 15, loss = 3.23454863\n",
      "Iteration 16, loss = 3.23307019\n",
      "Iteration 17, loss = 3.23194410\n",
      "Iteration 18, loss = 3.23009719\n",
      "Iteration 19, loss = 3.22894544\n",
      "Iteration 20, loss = 3.22833599\n",
      "Iteration 21, loss = 3.22799224\n",
      "Iteration 22, loss = 3.22715856\n",
      "Iteration 23, loss = 3.22641316\n",
      "Iteration 24, loss = 3.22603988\n",
      "Iteration 25, loss = 3.22568712\n",
      "Iteration 26, loss = 3.22479863\n",
      "Iteration 27, loss = 3.22482047\n",
      "Iteration 28, loss = 3.22477738\n",
      "Iteration 29, loss = 3.22410033\n",
      "Iteration 30, loss = 3.22298089\n",
      "Iteration 31, loss = 3.22204281\n",
      "Iteration 32, loss = 3.22182628\n",
      "Iteration 33, loss = 3.22216395\n",
      "Iteration 34, loss = 3.22130095\n",
      "Iteration 35, loss = 3.22078667\n",
      "Iteration 36, loss = 3.21998716\n",
      "Iteration 37, loss = 3.21989393\n",
      "Iteration 38, loss = 3.21968664\n",
      "Iteration 39, loss = 3.21910540\n",
      "Iteration 40, loss = 3.21872361\n",
      "Iteration 41, loss = 3.21789395\n",
      "Iteration 42, loss = 3.21811624\n",
      "Iteration 43, loss = 3.21794796\n",
      "Iteration 44, loss = 3.21751704\n",
      "Iteration 45, loss = 3.21671491\n",
      "Iteration 46, loss = 3.21678712\n",
      "Iteration 47, loss = 3.21710614\n",
      "Iteration 48, loss = 3.21681741\n",
      "Iteration 49, loss = 3.21623847\n",
      "Iteration 50, loss = 3.21566399\n",
      "Iteration 51, loss = 3.21559553\n",
      "Iteration 52, loss = 3.21470681\n",
      "Iteration 53, loss = 3.21477212\n",
      "Iteration 54, loss = 3.21432507\n",
      "Iteration 55, loss = 3.21382550\n",
      "Iteration 56, loss = 3.21382811\n",
      "Iteration 57, loss = 3.21373141\n",
      "Iteration 58, loss = 3.21367939\n",
      "Iteration 59, loss = 3.21303930\n",
      "Iteration 60, loss = 3.21256030\n",
      "Iteration 61, loss = 3.21271515\n",
      "Iteration 62, loss = 3.21303545\n",
      "Iteration 63, loss = 3.21358895\n",
      "Iteration 64, loss = 3.21312943\n",
      "Iteration 65, loss = 3.21216835\n",
      "Iteration 66, loss = 3.21146096\n",
      "Iteration 67, loss = 3.21165057\n",
      "Iteration 68, loss = 3.21136982\n",
      "Iteration 69, loss = 3.21112567\n",
      "Iteration 70, loss = 3.21062240\n",
      "Iteration 71, loss = 3.21017482\n",
      "Iteration 72, loss = 3.20965717\n",
      "Iteration 73, loss = 3.20983678\n",
      "Iteration 74, loss = 3.20915894\n",
      "Iteration 75, loss = 3.20969680\n",
      "Iteration 76, loss = 3.20904616\n",
      "Iteration 77, loss = 3.20923399\n",
      "Iteration 78, loss = 3.20931835\n",
      "Iteration 79, loss = 3.20908871\n",
      "Iteration 80, loss = 3.20917115\n",
      "Iteration 81, loss = 3.20913810\n",
      "Iteration 82, loss = 3.20872101\n",
      "Iteration 83, loss = 3.20850885\n",
      "Iteration 84, loss = 3.20877669\n",
      "Iteration 85, loss = 3.20822382\n",
      "Iteration 86, loss = 3.20776096\n",
      "Iteration 87, loss = 3.20763991\n",
      "Iteration 88, loss = 3.20726297\n",
      "Iteration 89, loss = 3.20700399\n",
      "Iteration 90, loss = 3.20706038\n",
      "Iteration 91, loss = 3.20733731\n",
      "Iteration 92, loss = 3.20673115\n",
      "Iteration 93, loss = 3.20688989\n",
      "Iteration 94, loss = 3.20728003\n",
      "Iteration 95, loss = 3.20663434\n",
      "Iteration 96, loss = 3.20563188\n",
      "Iteration 97, loss = 3.20596310\n",
      "Iteration 98, loss = 3.20557513\n",
      "Iteration 99, loss = 3.20591864\n",
      "Iteration 100, loss = 3.20523442\n",
      "Iteration 101, loss = 3.20473883\n",
      "Iteration 102, loss = 3.20509735\n",
      "Iteration 103, loss = 3.20486300\n",
      "Iteration 104, loss = 3.20459552\n",
      "Iteration 105, loss = 3.20456722\n",
      "Iteration 106, loss = 3.20446256\n",
      "Iteration 107, loss = 3.20466514\n",
      "Iteration 108, loss = 3.20437743\n",
      "Iteration 109, loss = 3.20394919\n",
      "Iteration 110, loss = 3.20394375\n",
      "Iteration 111, loss = 3.20385644\n",
      "Iteration 112, loss = 3.20359569\n",
      "Iteration 113, loss = 3.20385507\n",
      "Iteration 114, loss = 3.20335919\n",
      "Iteration 115, loss = 3.20273648\n",
      "Iteration 116, loss = 3.20311550\n",
      "Iteration 117, loss = 3.20358782\n",
      "Iteration 118, loss = 3.20318766\n",
      "Iteration 119, loss = 3.20203696\n",
      "Iteration 120, loss = 3.20156036\n",
      "Iteration 121, loss = 3.20196021\n",
      "Iteration 122, loss = 3.20140236\n",
      "Iteration 123, loss = 3.20161876\n",
      "Iteration 124, loss = 3.20089182\n",
      "Iteration 125, loss = 3.20123020\n",
      "Iteration 126, loss = 3.20184077\n",
      "Iteration 127, loss = 3.20140629\n",
      "Iteration 128, loss = 3.20098674\n",
      "Iteration 129, loss = 3.20129773\n",
      "Iteration 130, loss = 3.20110405\n",
      "Iteration 131, loss = 3.20073814\n",
      "Iteration 132, loss = 3.20043271\n",
      "Iteration 133, loss = 3.20026984\n",
      "Iteration 134, loss = 3.20025493\n",
      "Iteration 135, loss = 3.19986781\n",
      "Iteration 136, loss = 3.19921734\n",
      "Iteration 137, loss = 3.19961266\n",
      "Iteration 138, loss = 3.19944501\n",
      "Iteration 139, loss = 3.19942424\n",
      "Iteration 140, loss = 3.19922954\n",
      "Iteration 141, loss = 3.19897115\n",
      "Iteration 142, loss = 3.19893681\n",
      "Iteration 143, loss = 3.19939539\n",
      "Iteration 144, loss = 3.19931675\n",
      "Iteration 145, loss = 3.19863371\n",
      "Iteration 146, loss = 3.19810528\n",
      "Iteration 147, loss = 3.19841861\n",
      "Iteration 148, loss = 3.19797836\n",
      "Iteration 149, loss = 3.19820516\n",
      "Iteration 150, loss = 3.19794289\n",
      "Iteration 151, loss = 3.19784266\n",
      "Iteration 152, loss = 3.19763878\n",
      "Iteration 153, loss = 3.19676737\n",
      "Iteration 154, loss = 3.19696819\n",
      "Iteration 155, loss = 3.19671493\n",
      "Iteration 156, loss = 3.19785238\n",
      "Iteration 157, loss = 3.19739446\n",
      "Iteration 158, loss = 3.19706651\n",
      "Iteration 159, loss = 3.19652970\n",
      "Iteration 160, loss = 3.19621204\n",
      "Iteration 161, loss = 3.19592299\n",
      "Iteration 162, loss = 3.19552522\n",
      "Iteration 163, loss = 3.19547826\n",
      "Iteration 164, loss = 3.19545253\n",
      "Iteration 165, loss = 3.19547269\n",
      "Iteration 166, loss = 3.19502101\n",
      "Iteration 167, loss = 3.19512050\n",
      "Iteration 168, loss = 3.19544000\n",
      "Iteration 169, loss = 3.19529305\n",
      "Iteration 170, loss = 3.19526828\n",
      "Iteration 171, loss = 3.19539070\n",
      "Iteration 172, loss = 3.19539748\n",
      "Iteration 173, loss = 3.19500146\n",
      "Iteration 174, loss = 3.19431047\n",
      "Iteration 175, loss = 3.19423879\n",
      "Iteration 176, loss = 3.19405489\n",
      "Iteration 177, loss = 3.19335772\n",
      "Iteration 178, loss = 3.19374899\n",
      "Iteration 179, loss = 3.19354508\n",
      "Iteration 180, loss = 3.19333660\n",
      "Iteration 181, loss = 3.19370306\n",
      "Iteration 182, loss = 3.19363476\n",
      "Iteration 183, loss = 3.19304147\n",
      "Iteration 184, loss = 3.19288292\n",
      "Iteration 185, loss = 3.19225794\n",
      "Iteration 186, loss = 3.19252502\n",
      "Iteration 187, loss = 3.19225984\n",
      "Iteration 188, loss = 3.19180515\n",
      "Iteration 189, loss = 3.19176921\n",
      "Iteration 190, loss = 3.19178217\n",
      "Iteration 191, loss = 3.19210162\n",
      "Iteration 192, loss = 3.19173523\n",
      "Iteration 193, loss = 3.19093412\n",
      "Iteration 194, loss = 3.19145803\n",
      "Iteration 195, loss = 3.19087139\n",
      "Iteration 196, loss = 3.19043445\n",
      "Iteration 197, loss = 3.18985631\n",
      "Iteration 198, loss = 3.19002116\n",
      "Iteration 199, loss = 3.19010891\n",
      "Iteration 200, loss = 3.19039275\n",
      "Iteration 201, loss = 3.18982139\n",
      "Iteration 202, loss = 3.18964725\n",
      "Iteration 203, loss = 3.18983413\n",
      "Iteration 204, loss = 3.18939059\n",
      "Iteration 205, loss = 3.18872476\n",
      "Iteration 206, loss = 3.18949032\n",
      "Iteration 207, loss = 3.19010621\n",
      "Iteration 208, loss = 3.18951318\n",
      "Iteration 209, loss = 3.18831237\n",
      "Iteration 210, loss = 3.18787013\n",
      "Iteration 211, loss = 3.18847821\n",
      "Iteration 212, loss = 3.18867773\n",
      "Iteration 213, loss = 3.18846199\n",
      "Iteration 214, loss = 3.18822663\n",
      "Iteration 215, loss = 3.18825816\n",
      "Iteration 216, loss = 3.18742223\n",
      "Iteration 217, loss = 3.18732617\n",
      "Iteration 218, loss = 3.18707322\n",
      "Iteration 219, loss = 3.18718798\n",
      "Iteration 220, loss = 3.18643374\n",
      "Iteration 221, loss = 3.18708432\n",
      "Iteration 222, loss = 3.18676534\n",
      "Iteration 223, loss = 3.18790156\n",
      "Iteration 224, loss = 3.18676129\n",
      "Iteration 225, loss = 3.18523936\n",
      "Iteration 226, loss = 3.18541011\n",
      "Iteration 227, loss = 3.18568771\n",
      "Iteration 228, loss = 3.18501607\n",
      "Iteration 229, loss = 3.18469543\n",
      "Iteration 230, loss = 3.18525921\n",
      "Iteration 231, loss = 3.18501461\n",
      "Iteration 232, loss = 3.18422115\n",
      "Iteration 233, loss = 3.18378003\n",
      "Iteration 234, loss = 3.18374270\n",
      "Iteration 235, loss = 3.18377957\n",
      "Iteration 236, loss = 3.18425657\n",
      "Iteration 237, loss = 3.18395685\n",
      "Iteration 238, loss = 3.18406175\n",
      "Iteration 239, loss = 3.18329637\n",
      "Iteration 240, loss = 3.18334301\n",
      "Iteration 241, loss = 3.18370653\n",
      "Iteration 242, loss = 3.18337716\n",
      "Iteration 243, loss = 3.18313509\n",
      "Iteration 244, loss = 3.18272965\n",
      "Iteration 245, loss = 3.18248168\n",
      "Iteration 246, loss = 3.18198920\n",
      "Iteration 247, loss = 3.18131788\n",
      "Iteration 248, loss = 3.18204328\n",
      "Iteration 249, loss = 3.18246588\n",
      "Iteration 250, loss = 3.18167747\n",
      "Iteration 251, loss = 3.18172353\n",
      "Iteration 252, loss = 3.18199505\n",
      "Iteration 253, loss = 3.18175641\n",
      "Iteration 254, loss = 3.18125453\n",
      "Iteration 255, loss = 3.18080894\n",
      "Iteration 256, loss = 3.18047082\n",
      "Iteration 257, loss = 3.18098670\n",
      "Iteration 258, loss = 3.18026491\n",
      "Iteration 259, loss = 3.18028983\n",
      "Iteration 260, loss = 3.17990113\n",
      "Iteration 261, loss = 3.17971267\n",
      "Iteration 262, loss = 3.17964113\n",
      "Iteration 263, loss = 3.18033755\n",
      "Iteration 264, loss = 3.17931681\n",
      "Iteration 265, loss = 3.17909776\n",
      "Iteration 266, loss = 3.18009845\n",
      "Iteration 267, loss = 3.17965564\n",
      "Iteration 268, loss = 3.17945140\n",
      "Iteration 269, loss = 3.17830289\n",
      "Iteration 270, loss = 3.17903103\n",
      "Iteration 271, loss = 3.17891828\n",
      "Iteration 272, loss = 3.17807941\n",
      "Iteration 273, loss = 3.17750605\n",
      "Iteration 274, loss = 3.17790965\n",
      "Iteration 275, loss = 3.17768147\n",
      "Iteration 276, loss = 3.17835051\n",
      "Iteration 277, loss = 3.17856911\n",
      "Iteration 278, loss = 3.17763251\n",
      "Iteration 279, loss = 3.17651795\n",
      "Iteration 280, loss = 3.17722339\n",
      "Iteration 281, loss = 3.17669400\n",
      "Iteration 282, loss = 3.17637617\n",
      "Iteration 283, loss = 3.17622000\n",
      "Iteration 284, loss = 3.17640408\n",
      "Iteration 285, loss = 3.17618572\n",
      "Iteration 286, loss = 3.17610288\n",
      "Iteration 287, loss = 3.17560102\n",
      "Iteration 288, loss = 3.17532335\n",
      "Iteration 289, loss = 3.17513174\n",
      "Iteration 290, loss = 3.17507698\n",
      "Iteration 291, loss = 3.17476843\n",
      "Iteration 292, loss = 3.17544564\n",
      "Iteration 293, loss = 3.17529092\n",
      "Iteration 294, loss = 3.17507322\n",
      "Iteration 295, loss = 3.17482543\n",
      "Iteration 296, loss = 3.17457172\n",
      "Iteration 297, loss = 3.17450418\n",
      "Iteration 298, loss = 3.17480913\n",
      "Iteration 299, loss = 3.17487339\n",
      "Iteration 300, loss = 3.17462966\n",
      "Iteration 301, loss = 3.17412698\n",
      "Iteration 302, loss = 3.17386704\n",
      "Iteration 303, loss = 3.17358079\n",
      "Iteration 304, loss = 3.17387660\n",
      "Iteration 305, loss = 3.17306051\n",
      "Iteration 306, loss = 3.17287971\n",
      "Iteration 307, loss = 3.17277898\n",
      "Iteration 308, loss = 3.17311403\n",
      "Iteration 309, loss = 3.17231820\n",
      "Iteration 310, loss = 3.17163276\n",
      "Iteration 311, loss = 3.17132678\n",
      "Iteration 312, loss = 3.17130915\n",
      "Iteration 313, loss = 3.17141965\n",
      "Iteration 314, loss = 3.17108482\n",
      "Iteration 315, loss = 3.17125143\n",
      "Iteration 316, loss = 3.17151524\n",
      "Iteration 317, loss = 3.17136255\n",
      "Iteration 318, loss = 3.17075304\n",
      "Iteration 319, loss = 3.17082774\n",
      "Iteration 320, loss = 3.17000294\n",
      "Iteration 321, loss = 3.17029491\n",
      "Iteration 322, loss = 3.17118904\n",
      "Iteration 323, loss = 3.17095188\n",
      "Iteration 324, loss = 3.17004847\n",
      "Iteration 325, loss = 3.17034931\n",
      "Iteration 326, loss = 3.17012152\n",
      "Iteration 327, loss = 3.16978364\n",
      "Iteration 328, loss = 3.17045052\n",
      "Iteration 329, loss = 3.17067770\n",
      "Iteration 330, loss = 3.16968004\n",
      "Iteration 331, loss = 3.16894835\n",
      "Iteration 332, loss = 3.16903449\n",
      "Iteration 333, loss = 3.16888763\n",
      "Iteration 334, loss = 3.16854036\n",
      "Iteration 335, loss = 3.16807940\n",
      "Iteration 336, loss = 3.16786163\n",
      "Iteration 337, loss = 3.16788095\n",
      "Iteration 338, loss = 3.16742279\n",
      "Iteration 339, loss = 3.16724089\n",
      "Iteration 340, loss = 3.16738436\n",
      "Iteration 341, loss = 3.16729036\n",
      "Iteration 342, loss = 3.16619326\n",
      "Iteration 343, loss = 3.16637279\n",
      "Iteration 344, loss = 3.16679528\n",
      "Iteration 345, loss = 3.16700728\n",
      "Iteration 346, loss = 3.16717029\n",
      "Iteration 347, loss = 3.16674197\n",
      "Iteration 348, loss = 3.16640863\n",
      "Iteration 349, loss = 3.16635945\n",
      "Iteration 350, loss = 3.16626442\n",
      "Iteration 351, loss = 3.16618315\n",
      "Iteration 352, loss = 3.16549114\n",
      "Iteration 353, loss = 3.16530914\n",
      "Iteration 354, loss = 3.16504313\n",
      "Iteration 355, loss = 3.16568959\n",
      "Iteration 356, loss = 3.16544583\n",
      "Iteration 357, loss = 3.16532132\n",
      "Iteration 358, loss = 3.16484704\n",
      "Iteration 359, loss = 3.16522113\n",
      "Iteration 360, loss = 3.16506355\n",
      "Iteration 361, loss = 3.16556070\n",
      "Iteration 362, loss = 3.16515748\n",
      "Iteration 363, loss = 3.16323733\n",
      "Iteration 364, loss = 3.16331179\n",
      "Iteration 365, loss = 3.16342797\n",
      "Iteration 366, loss = 3.16329488\n",
      "Iteration 367, loss = 3.16331566\n",
      "Iteration 368, loss = 3.16319227\n",
      "Iteration 369, loss = 3.16281664\n",
      "Iteration 370, loss = 3.16230655\n",
      "Iteration 371, loss = 3.16241174\n",
      "Iteration 372, loss = 3.16261649\n",
      "Iteration 373, loss = 3.16209254\n",
      "Iteration 374, loss = 3.16155623\n",
      "Iteration 375, loss = 3.16203959\n",
      "Iteration 376, loss = 3.16227875\n",
      "Iteration 377, loss = 3.16244538\n",
      "Iteration 378, loss = 3.16179334\n",
      "Iteration 379, loss = 3.16206050\n",
      "Iteration 380, loss = 3.16112309\n",
      "Iteration 381, loss = 3.16124922\n",
      "Iteration 382, loss = 3.16134974\n",
      "Iteration 383, loss = 3.16100847\n",
      "Iteration 384, loss = 3.16118494\n",
      "Iteration 385, loss = 3.16101401\n",
      "Iteration 386, loss = 3.16023319\n",
      "Iteration 387, loss = 3.16025786\n",
      "Iteration 388, loss = 3.16094032\n",
      "Iteration 389, loss = 3.16041449\n",
      "Iteration 390, loss = 3.16052757\n",
      "Iteration 391, loss = 3.16023881\n",
      "Iteration 392, loss = 3.16087351\n",
      "Iteration 393, loss = 3.16037809\n",
      "Iteration 394, loss = 3.15902362\n",
      "Iteration 395, loss = 3.15925711\n",
      "Iteration 396, loss = 3.15912493\n",
      "Iteration 397, loss = 3.15919646\n",
      "Iteration 398, loss = 3.15882596\n",
      "Iteration 399, loss = 3.15877877\n",
      "Iteration 400, loss = 3.15838223\n",
      "Iteration 401, loss = 3.15865537\n",
      "Iteration 402, loss = 3.15900560\n",
      "Iteration 403, loss = 3.15853195\n",
      "Iteration 404, loss = 3.15755494\n",
      "Iteration 405, loss = 3.15758201\n",
      "Iteration 406, loss = 3.15744197\n",
      "Iteration 407, loss = 3.15730940\n",
      "Iteration 408, loss = 3.15791990\n",
      "Iteration 409, loss = 3.15883244\n",
      "Iteration 410, loss = 3.15833614\n",
      "Iteration 411, loss = 3.15742142\n",
      "Iteration 412, loss = 3.15734963\n",
      "Iteration 413, loss = 3.15676377\n",
      "Iteration 414, loss = 3.15619551\n",
      "Iteration 415, loss = 3.15644172\n",
      "Iteration 416, loss = 3.15671448\n",
      "Iteration 417, loss = 3.15726602\n",
      "Iteration 418, loss = 3.15631229\n",
      "Iteration 419, loss = 3.15608319\n",
      "Iteration 420, loss = 3.15539425\n",
      "Iteration 421, loss = 3.15518139\n",
      "Iteration 422, loss = 3.15502929\n",
      "Iteration 423, loss = 3.15438443\n",
      "Iteration 424, loss = 3.15496279\n",
      "Iteration 425, loss = 3.15493926\n",
      "Iteration 426, loss = 3.15360056\n",
      "Iteration 427, loss = 3.15429278\n",
      "Iteration 428, loss = 3.15507735\n",
      "Iteration 429, loss = 3.15365448\n",
      "Iteration 430, loss = 3.15363800\n",
      "Iteration 431, loss = 3.15425676\n",
      "Iteration 432, loss = 3.15348452\n",
      "Iteration 433, loss = 3.15345317\n",
      "Iteration 434, loss = 3.15346271\n",
      "Iteration 435, loss = 3.15272547\n",
      "Iteration 436, loss = 3.15304916\n",
      "Iteration 437, loss = 3.15297825\n",
      "Iteration 438, loss = 3.15279699\n",
      "Iteration 439, loss = 3.15260079\n",
      "Iteration 440, loss = 3.15231430\n",
      "Iteration 441, loss = 3.15211151\n",
      "Iteration 442, loss = 3.15311403\n",
      "Iteration 443, loss = 3.15275210\n",
      "Iteration 444, loss = 3.15314088\n",
      "Iteration 445, loss = 3.15282138\n",
      "Iteration 446, loss = 3.15199187\n",
      "Iteration 447, loss = 3.15150471\n",
      "Iteration 448, loss = 3.15147989\n",
      "Iteration 449, loss = 3.15136335\n",
      "Iteration 450, loss = 3.15120073\n",
      "Iteration 451, loss = 3.15101777\n",
      "Iteration 452, loss = 3.15107925\n",
      "Iteration 453, loss = 3.15072911\n",
      "Iteration 454, loss = 3.15021492\n",
      "Iteration 455, loss = 3.14958974\n",
      "Iteration 456, loss = 3.14969543\n",
      "Iteration 457, loss = 3.15002667\n",
      "Iteration 458, loss = 3.15021912\n",
      "Iteration 459, loss = 3.14992712\n",
      "Iteration 460, loss = 3.14950658\n",
      "Iteration 461, loss = 3.15051136\n",
      "Iteration 462, loss = 3.14968806\n",
      "Iteration 463, loss = 3.14922554\n",
      "Iteration 464, loss = 3.14876138\n",
      "Iteration 465, loss = 3.14917787\n",
      "Iteration 466, loss = 3.14919838\n",
      "Iteration 467, loss = 3.14906789\n",
      "Iteration 468, loss = 3.14917669\n",
      "Iteration 469, loss = 3.14941814\n",
      "Iteration 470, loss = 3.14808714\n",
      "Iteration 471, loss = 3.14827812\n",
      "Iteration 472, loss = 3.14829869\n",
      "Iteration 473, loss = 3.14914378\n",
      "Iteration 474, loss = 3.14838843\n",
      "Iteration 475, loss = 3.14751799\n",
      "Iteration 476, loss = 3.14802960\n",
      "Iteration 477, loss = 3.14749722\n",
      "Iteration 478, loss = 3.14697376\n",
      "Iteration 479, loss = 3.14779644\n",
      "Iteration 480, loss = 3.14746268\n",
      "Iteration 481, loss = 3.14602204\n",
      "Iteration 482, loss = 3.14635375\n",
      "Iteration 483, loss = 3.14653125\n",
      "Iteration 484, loss = 3.14651233\n",
      "Iteration 485, loss = 3.14658472\n",
      "Iteration 486, loss = 3.14638679\n",
      "Iteration 487, loss = 3.14646921\n",
      "Iteration 488, loss = 3.14595994\n",
      "Iteration 489, loss = 3.14635963\n",
      "Iteration 490, loss = 3.14560044\n",
      "Iteration 491, loss = 3.14591597\n",
      "Iteration 492, loss = 3.14594317\n",
      "Iteration 493, loss = 3.14604577\n",
      "Iteration 494, loss = 3.14511074\n",
      "Iteration 495, loss = 3.14604055\n",
      "Iteration 496, loss = 3.14482822\n",
      "Iteration 497, loss = 3.14458719\n",
      "Iteration 498, loss = 3.14450510\n",
      "Iteration 499, loss = 3.14470689\n",
      "Iteration 500, loss = 3.14515733\n",
      "Iteration 501, loss = 3.14547641\n",
      "Iteration 502, loss = 3.14556015\n",
      "Iteration 503, loss = 3.14519483\n",
      "Iteration 504, loss = 3.14471078\n",
      "Iteration 505, loss = 3.14436104\n",
      "Iteration 506, loss = 3.14424928\n",
      "Iteration 507, loss = 3.14454247\n",
      "Iteration 508, loss = 3.14422919\n",
      "Iteration 509, loss = 3.14418673\n",
      "Iteration 510, loss = 3.14399736\n",
      "Iteration 511, loss = 3.14440640\n",
      "Iteration 512, loss = 3.14445862\n",
      "Iteration 513, loss = 3.14229969\n",
      "Iteration 514, loss = 3.14353570\n",
      "Iteration 515, loss = 3.14374298\n",
      "Iteration 516, loss = 3.14315699\n",
      "Iteration 517, loss = 3.14284655\n",
      "Iteration 518, loss = 3.14245434\n",
      "Iteration 519, loss = 3.14225629\n",
      "Iteration 520, loss = 3.14207251\n",
      "Iteration 521, loss = 3.14227516\n",
      "Iteration 522, loss = 3.14272125\n",
      "Iteration 523, loss = 3.14275549\n",
      "Iteration 524, loss = 3.14223287\n",
      "Iteration 525, loss = 3.14272649\n",
      "Iteration 526, loss = 3.14138986\n",
      "Iteration 527, loss = 3.14057650\n",
      "Iteration 528, loss = 3.14152632\n",
      "Iteration 529, loss = 3.14101113\n",
      "Iteration 530, loss = 3.14080292\n",
      "Iteration 531, loss = 3.14146028\n",
      "Iteration 532, loss = 3.14190717\n",
      "Iteration 533, loss = 3.14029111\n",
      "Iteration 534, loss = 3.14056939\n",
      "Iteration 535, loss = 3.14084276\n",
      "Iteration 536, loss = 3.14006276\n",
      "Iteration 537, loss = 3.14047778\n",
      "Iteration 538, loss = 3.14074240\n",
      "Iteration 539, loss = 3.14035231\n",
      "Iteration 540, loss = 3.14007260\n",
      "Iteration 541, loss = 3.13996583\n",
      "Iteration 542, loss = 3.14081537\n",
      "Iteration 543, loss = 3.14043264\n",
      "Iteration 544, loss = 3.14038252\n",
      "Iteration 545, loss = 3.13977083\n",
      "Iteration 546, loss = 3.13971626\n",
      "Iteration 547, loss = 3.13938091\n",
      "Iteration 548, loss = 3.13984236\n",
      "Iteration 549, loss = 3.13920609\n",
      "Iteration 550, loss = 3.13954018\n",
      "Iteration 551, loss = 3.13927472\n",
      "Iteration 552, loss = 3.13945998\n",
      "Iteration 553, loss = 3.13938781\n",
      "Iteration 554, loss = 3.13948360\n",
      "Iteration 555, loss = 3.13888475\n",
      "Iteration 556, loss = 3.13850962\n",
      "Iteration 557, loss = 3.13781193\n",
      "Iteration 558, loss = 3.13830211\n",
      "Iteration 559, loss = 3.13818379\n",
      "Iteration 560, loss = 3.13759331\n",
      "Iteration 561, loss = 3.13721029\n",
      "Iteration 562, loss = 3.13785433\n",
      "Iteration 563, loss = 3.13747940\n",
      "Iteration 564, loss = 3.13693107\n",
      "Iteration 565, loss = 3.13700605\n",
      "Iteration 566, loss = 3.13649739\n",
      "Iteration 567, loss = 3.13737058\n",
      "Iteration 568, loss = 3.13616883\n",
      "Iteration 569, loss = 3.13620286\n",
      "Iteration 570, loss = 3.13675019\n",
      "Iteration 571, loss = 3.13637182\n",
      "Iteration 572, loss = 3.13616197\n",
      "Iteration 573, loss = 3.13616375\n",
      "Iteration 574, loss = 3.13622939\n",
      "Iteration 575, loss = 3.13618320\n",
      "Iteration 576, loss = 3.13552297\n",
      "Iteration 577, loss = 3.13531911\n",
      "Iteration 578, loss = 3.13546317\n",
      "Iteration 579, loss = 3.13546541\n",
      "Iteration 580, loss = 3.13486425\n",
      "Iteration 581, loss = 3.13551998\n",
      "Iteration 582, loss = 3.13504676\n",
      "Iteration 583, loss = 3.13460263\n",
      "Iteration 584, loss = 3.13524564\n",
      "Iteration 585, loss = 3.13585106\n",
      "Iteration 586, loss = 3.13567631\n",
      "Iteration 587, loss = 3.13507897\n",
      "Iteration 588, loss = 3.13487579\n",
      "Iteration 589, loss = 3.13444015\n",
      "Iteration 590, loss = 3.13471562\n",
      "Iteration 591, loss = 3.13550675\n",
      "Iteration 592, loss = 3.13495502\n",
      "Iteration 593, loss = 3.13458612\n",
      "Iteration 594, loss = 3.13488102\n",
      "Iteration 595, loss = 3.13436090\n",
      "Iteration 596, loss = 3.13546542\n",
      "Iteration 597, loss = 3.13520476\n",
      "Iteration 598, loss = 3.13531987\n",
      "Iteration 599, loss = 3.13453336\n",
      "Iteration 600, loss = 3.13384412\n",
      "Iteration 601, loss = 3.13351181\n",
      "Iteration 602, loss = 3.13287025\n",
      "Iteration 603, loss = 3.13509810\n",
      "Iteration 604, loss = 3.13428367\n",
      "Iteration 605, loss = 3.13412526\n",
      "Iteration 606, loss = 3.13351283\n",
      "Iteration 607, loss = 3.13333034\n",
      "Iteration 608, loss = 3.13370954\n",
      "Iteration 609, loss = 3.13393046\n",
      "Iteration 610, loss = 3.13367501\n",
      "Iteration 611, loss = 3.13325222\n",
      "Iteration 612, loss = 3.13334984\n",
      "Iteration 613, loss = 3.13284505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15229928\n",
      "Iteration 2, loss = 4.02440868\n",
      "Iteration 3, loss = 3.90522395\n",
      "Iteration 4, loss = 3.79368106\n",
      "Iteration 5, loss = 3.68445126\n",
      "Iteration 6, loss = 3.58363345\n",
      "Iteration 7, loss = 3.49164474\n",
      "Iteration 8, loss = 3.41277854\n",
      "Iteration 9, loss = 3.34374720\n",
      "Iteration 10, loss = 3.29452178\n",
      "Iteration 11, loss = 3.26422359\n",
      "Iteration 12, loss = 3.24959617\n",
      "Iteration 13, loss = 3.24288476\n",
      "Iteration 14, loss = 3.23982415\n",
      "Iteration 15, loss = 3.23835892\n",
      "Iteration 16, loss = 3.23705561\n",
      "Iteration 17, loss = 3.23528935\n",
      "Iteration 18, loss = 3.23378662\n",
      "Iteration 19, loss = 3.23357556\n",
      "Iteration 20, loss = 3.23205691\n",
      "Iteration 21, loss = 3.23179565\n",
      "Iteration 22, loss = 3.23186511\n",
      "Iteration 23, loss = 3.23132161\n",
      "Iteration 24, loss = 3.23048922\n",
      "Iteration 25, loss = 3.23043989\n",
      "Iteration 26, loss = 3.22975335\n",
      "Iteration 27, loss = 3.22863243\n",
      "Iteration 28, loss = 3.22842807\n",
      "Iteration 29, loss = 3.22780513\n",
      "Iteration 30, loss = 3.22785084\n",
      "Iteration 31, loss = 3.22704171\n",
      "Iteration 32, loss = 3.22675312\n",
      "Iteration 33, loss = 3.22701647\n",
      "Iteration 34, loss = 3.22657162\n",
      "Iteration 35, loss = 3.22605295\n",
      "Iteration 36, loss = 3.22515141\n",
      "Iteration 37, loss = 3.22421080\n",
      "Iteration 38, loss = 3.22373722\n",
      "Iteration 39, loss = 3.22320066\n",
      "Iteration 40, loss = 3.22311942\n",
      "Iteration 41, loss = 3.22275286\n",
      "Iteration 42, loss = 3.22228497\n",
      "Iteration 43, loss = 3.22184804\n",
      "Iteration 44, loss = 3.22166466\n",
      "Iteration 45, loss = 3.22177704\n",
      "Iteration 46, loss = 3.22090316\n",
      "Iteration 47, loss = 3.22078183\n",
      "Iteration 48, loss = 3.22071042\n",
      "Iteration 49, loss = 3.22048567\n",
      "Iteration 50, loss = 3.22004722\n",
      "Iteration 51, loss = 3.22014597\n",
      "Iteration 52, loss = 3.21970372\n",
      "Iteration 53, loss = 3.21980730\n",
      "Iteration 54, loss = 3.21917208\n",
      "Iteration 55, loss = 3.21922746\n",
      "Iteration 56, loss = 3.21873689\n",
      "Iteration 57, loss = 3.21772471\n",
      "Iteration 58, loss = 3.21735645\n",
      "Iteration 59, loss = 3.21711267\n",
      "Iteration 60, loss = 3.21661471\n",
      "Iteration 61, loss = 3.21633442\n",
      "Iteration 62, loss = 3.21618402\n",
      "Iteration 63, loss = 3.21620234\n",
      "Iteration 64, loss = 3.21561419\n",
      "Iteration 65, loss = 3.21606618\n",
      "Iteration 66, loss = 3.21508843\n",
      "Iteration 67, loss = 3.21464878\n",
      "Iteration 68, loss = 3.21439601\n",
      "Iteration 69, loss = 3.21418356\n",
      "Iteration 70, loss = 3.21448380\n",
      "Iteration 71, loss = 3.21409153\n",
      "Iteration 72, loss = 3.21425285\n",
      "Iteration 73, loss = 3.21378461\n",
      "Iteration 74, loss = 3.21449018\n",
      "Iteration 75, loss = 3.21360199\n",
      "Iteration 76, loss = 3.21313249\n",
      "Iteration 77, loss = 3.21263948\n",
      "Iteration 78, loss = 3.21294986\n",
      "Iteration 79, loss = 3.21291483\n",
      "Iteration 80, loss = 3.21227120\n",
      "Iteration 81, loss = 3.21206431\n",
      "Iteration 82, loss = 3.21198137\n",
      "Iteration 83, loss = 3.21177961\n",
      "Iteration 84, loss = 3.21134149\n",
      "Iteration 85, loss = 3.21107552\n",
      "Iteration 86, loss = 3.21082933\n",
      "Iteration 87, loss = 3.21084226\n",
      "Iteration 88, loss = 3.21057809\n",
      "Iteration 89, loss = 3.21015907\n",
      "Iteration 90, loss = 3.21008913\n",
      "Iteration 91, loss = 3.20967418\n",
      "Iteration 92, loss = 3.20990607\n",
      "Iteration 93, loss = 3.21032376\n",
      "Iteration 94, loss = 3.21039089\n",
      "Iteration 95, loss = 3.20949493\n",
      "Iteration 96, loss = 3.20913829\n",
      "Iteration 97, loss = 3.20874667\n",
      "Iteration 98, loss = 3.20813925\n",
      "Iteration 99, loss = 3.20851281\n",
      "Iteration 100, loss = 3.20856700\n",
      "Iteration 101, loss = 3.20793351\n",
      "Iteration 102, loss = 3.20764093\n",
      "Iteration 103, loss = 3.20734491\n",
      "Iteration 104, loss = 3.20681538\n",
      "Iteration 105, loss = 3.20691974\n",
      "Iteration 106, loss = 3.20713263\n",
      "Iteration 107, loss = 3.20646897\n",
      "Iteration 108, loss = 3.20634604\n",
      "Iteration 109, loss = 3.20648607\n",
      "Iteration 110, loss = 3.20626480\n",
      "Iteration 111, loss = 3.20665752\n",
      "Iteration 112, loss = 3.20568958\n",
      "Iteration 113, loss = 3.20519204\n",
      "Iteration 114, loss = 3.20554716\n",
      "Iteration 115, loss = 3.20527719\n",
      "Iteration 116, loss = 3.20509285\n",
      "Iteration 117, loss = 3.20504799\n",
      "Iteration 118, loss = 3.20519783\n",
      "Iteration 119, loss = 3.20499890\n",
      "Iteration 120, loss = 3.20494934\n",
      "Iteration 121, loss = 3.20418190\n",
      "Iteration 122, loss = 3.20405876\n",
      "Iteration 123, loss = 3.20379946\n",
      "Iteration 124, loss = 3.20451225\n",
      "Iteration 125, loss = 3.20458604\n",
      "Iteration 126, loss = 3.20374591\n",
      "Iteration 127, loss = 3.20294167\n",
      "Iteration 128, loss = 3.20326412\n",
      "Iteration 129, loss = 3.20351031\n",
      "Iteration 130, loss = 3.20250729\n",
      "Iteration 131, loss = 3.20229923\n",
      "Iteration 132, loss = 3.20251370\n",
      "Iteration 133, loss = 3.20210968\n",
      "Iteration 134, loss = 3.20194129\n",
      "Iteration 135, loss = 3.20111804\n",
      "Iteration 136, loss = 3.20138338\n",
      "Iteration 137, loss = 3.20194770\n",
      "Iteration 138, loss = 3.20148788\n",
      "Iteration 139, loss = 3.20182823\n",
      "Iteration 140, loss = 3.20138338\n",
      "Iteration 141, loss = 3.20069074\n",
      "Iteration 142, loss = 3.20040256\n",
      "Iteration 143, loss = 3.20032689\n",
      "Iteration 144, loss = 3.20032010\n",
      "Iteration 145, loss = 3.20010774\n",
      "Iteration 146, loss = 3.20016959\n",
      "Iteration 147, loss = 3.19945216\n",
      "Iteration 148, loss = 3.19919013\n",
      "Iteration 149, loss = 3.19932008\n",
      "Iteration 150, loss = 3.19922157\n",
      "Iteration 151, loss = 3.19911036\n",
      "Iteration 152, loss = 3.19921813\n",
      "Iteration 153, loss = 3.19864239\n",
      "Iteration 154, loss = 3.19861143\n",
      "Iteration 155, loss = 3.19856121\n",
      "Iteration 156, loss = 3.19819945\n",
      "Iteration 157, loss = 3.19836883\n",
      "Iteration 158, loss = 3.19816420\n",
      "Iteration 159, loss = 3.19816780\n",
      "Iteration 160, loss = 3.19787064\n",
      "Iteration 161, loss = 3.19753952\n",
      "Iteration 162, loss = 3.19723449\n",
      "Iteration 163, loss = 3.19677114\n",
      "Iteration 164, loss = 3.19615556\n",
      "Iteration 165, loss = 3.19620144\n",
      "Iteration 166, loss = 3.19657859\n",
      "Iteration 167, loss = 3.19607099\n",
      "Iteration 168, loss = 3.19573227\n",
      "Iteration 169, loss = 3.19521248\n",
      "Iteration 170, loss = 3.19513685\n",
      "Iteration 171, loss = 3.19554239\n",
      "Iteration 172, loss = 3.19596062\n",
      "Iteration 173, loss = 3.19498709\n",
      "Iteration 174, loss = 3.19588645\n",
      "Iteration 175, loss = 3.19512717\n",
      "Iteration 176, loss = 3.19501263\n",
      "Iteration 177, loss = 3.19491990\n",
      "Iteration 178, loss = 3.19395939\n",
      "Iteration 179, loss = 3.19349155\n",
      "Iteration 180, loss = 3.19379383\n",
      "Iteration 181, loss = 3.19327052\n",
      "Iteration 182, loss = 3.19256598\n",
      "Iteration 183, loss = 3.19255354\n",
      "Iteration 184, loss = 3.19239673\n",
      "Iteration 185, loss = 3.19242454\n",
      "Iteration 186, loss = 3.19232356\n",
      "Iteration 187, loss = 3.19188235\n",
      "Iteration 188, loss = 3.19152066\n",
      "Iteration 189, loss = 3.19174790\n",
      "Iteration 190, loss = 3.19215535\n",
      "Iteration 191, loss = 3.19237638\n",
      "Iteration 192, loss = 3.19094680\n",
      "Iteration 193, loss = 3.19225175\n",
      "Iteration 194, loss = 3.19229199\n",
      "Iteration 195, loss = 3.19145630\n",
      "Iteration 196, loss = 3.19092452\n",
      "Iteration 197, loss = 3.19076941\n",
      "Iteration 198, loss = 3.18983986\n",
      "Iteration 199, loss = 3.19017922\n",
      "Iteration 200, loss = 3.19012683\n",
      "Iteration 201, loss = 3.19057795\n",
      "Iteration 202, loss = 3.19037008\n",
      "Iteration 203, loss = 3.19073137\n",
      "Iteration 204, loss = 3.18953980\n",
      "Iteration 205, loss = 3.18914541\n",
      "Iteration 206, loss = 3.18905409\n",
      "Iteration 207, loss = 3.18922960\n",
      "Iteration 208, loss = 3.18923748\n",
      "Iteration 209, loss = 3.18844542\n",
      "Iteration 210, loss = 3.18811444\n",
      "Iteration 211, loss = 3.18808965\n",
      "Iteration 212, loss = 3.18934647\n",
      "Iteration 213, loss = 3.18849273\n",
      "Iteration 214, loss = 3.18811728\n",
      "Iteration 215, loss = 3.18811523\n",
      "Iteration 216, loss = 3.18880788\n",
      "Iteration 217, loss = 3.18818965\n",
      "Iteration 218, loss = 3.18787646\n",
      "Iteration 219, loss = 3.18738925\n",
      "Iteration 220, loss = 3.18678821\n",
      "Iteration 221, loss = 3.18601799\n",
      "Iteration 222, loss = 3.18575409\n",
      "Iteration 223, loss = 3.18621724\n",
      "Iteration 224, loss = 3.18576934\n",
      "Iteration 225, loss = 3.18615715\n",
      "Iteration 226, loss = 3.18611774\n",
      "Iteration 227, loss = 3.18601232\n",
      "Iteration 228, loss = 3.18543025\n",
      "Iteration 229, loss = 3.18668486\n",
      "Iteration 230, loss = 3.18563374\n",
      "Iteration 231, loss = 3.18538025\n",
      "Iteration 232, loss = 3.18559071\n",
      "Iteration 233, loss = 3.18431574\n",
      "Iteration 234, loss = 3.18433015\n",
      "Iteration 235, loss = 3.18439025\n",
      "Iteration 236, loss = 3.18443482\n",
      "Iteration 237, loss = 3.18401902\n",
      "Iteration 238, loss = 3.18391378\n",
      "Iteration 239, loss = 3.18385204\n",
      "Iteration 240, loss = 3.18360760\n",
      "Iteration 241, loss = 3.18349736\n",
      "Iteration 242, loss = 3.18325010\n",
      "Iteration 243, loss = 3.18333461\n",
      "Iteration 244, loss = 3.18270774\n",
      "Iteration 245, loss = 3.18278379\n",
      "Iteration 246, loss = 3.18252110\n",
      "Iteration 247, loss = 3.18253238\n",
      "Iteration 248, loss = 3.18246988\n",
      "Iteration 249, loss = 3.18244499\n",
      "Iteration 250, loss = 3.18202998\n",
      "Iteration 251, loss = 3.18203395\n",
      "Iteration 252, loss = 3.18101216\n",
      "Iteration 253, loss = 3.18089834\n",
      "Iteration 254, loss = 3.18085830\n",
      "Iteration 255, loss = 3.18079211\n",
      "Iteration 256, loss = 3.18116450\n",
      "Iteration 257, loss = 3.18081262\n",
      "Iteration 258, loss = 3.18014302\n",
      "Iteration 259, loss = 3.17968408\n",
      "Iteration 260, loss = 3.18022243\n",
      "Iteration 261, loss = 3.17944617\n",
      "Iteration 262, loss = 3.17928797\n",
      "Iteration 263, loss = 3.18018739\n",
      "Iteration 264, loss = 3.18003291\n",
      "Iteration 265, loss = 3.17957051\n",
      "Iteration 266, loss = 3.18000850\n",
      "Iteration 267, loss = 3.17902483\n",
      "Iteration 268, loss = 3.17862955\n",
      "Iteration 269, loss = 3.18011811\n",
      "Iteration 270, loss = 3.17997362\n",
      "Iteration 271, loss = 3.17811807\n",
      "Iteration 272, loss = 3.17777569\n",
      "Iteration 273, loss = 3.17784585\n",
      "Iteration 274, loss = 3.17828893\n",
      "Iteration 275, loss = 3.17781325\n",
      "Iteration 276, loss = 3.17791088\n",
      "Iteration 277, loss = 3.17752819\n",
      "Iteration 278, loss = 3.17691539\n",
      "Iteration 279, loss = 3.17686221\n",
      "Iteration 280, loss = 3.17681760\n",
      "Iteration 281, loss = 3.17751089\n",
      "Iteration 282, loss = 3.17697489\n",
      "Iteration 283, loss = 3.17668112\n",
      "Iteration 284, loss = 3.17635283\n",
      "Iteration 285, loss = 3.17607100\n",
      "Iteration 286, loss = 3.17591166\n",
      "Iteration 287, loss = 3.17548169\n",
      "Iteration 288, loss = 3.17575308\n",
      "Iteration 289, loss = 3.17529649\n",
      "Iteration 290, loss = 3.17565601\n",
      "Iteration 291, loss = 3.17531478\n",
      "Iteration 292, loss = 3.17504037\n",
      "Iteration 293, loss = 3.17437338\n",
      "Iteration 294, loss = 3.17447570\n",
      "Iteration 295, loss = 3.17488936\n",
      "Iteration 296, loss = 3.17491766\n",
      "Iteration 297, loss = 3.17484036\n",
      "Iteration 298, loss = 3.17434682\n",
      "Iteration 299, loss = 3.17438346\n",
      "Iteration 300, loss = 3.17416653\n",
      "Iteration 301, loss = 3.17383019\n",
      "Iteration 302, loss = 3.17387141\n",
      "Iteration 303, loss = 3.17322544\n",
      "Iteration 304, loss = 3.17316833\n",
      "Iteration 305, loss = 3.17281954\n",
      "Iteration 306, loss = 3.17235060\n",
      "Iteration 307, loss = 3.17267748\n",
      "Iteration 308, loss = 3.17209585\n",
      "Iteration 309, loss = 3.17175285\n",
      "Iteration 310, loss = 3.17218371\n",
      "Iteration 311, loss = 3.17272959\n",
      "Iteration 312, loss = 3.17216152\n",
      "Iteration 313, loss = 3.17177787\n",
      "Iteration 314, loss = 3.17162291\n",
      "Iteration 315, loss = 3.17120372\n",
      "Iteration 316, loss = 3.17062321\n",
      "Iteration 317, loss = 3.17036103\n",
      "Iteration 318, loss = 3.17038223\n",
      "Iteration 319, loss = 3.17010922\n",
      "Iteration 320, loss = 3.17011912\n",
      "Iteration 321, loss = 3.16971045\n",
      "Iteration 322, loss = 3.16952693\n",
      "Iteration 323, loss = 3.16948758\n",
      "Iteration 324, loss = 3.16974288\n",
      "Iteration 325, loss = 3.16913028\n",
      "Iteration 326, loss = 3.16868196\n",
      "Iteration 327, loss = 3.16844423\n",
      "Iteration 328, loss = 3.16836600\n",
      "Iteration 329, loss = 3.16915207\n",
      "Iteration 330, loss = 3.16964671\n",
      "Iteration 331, loss = 3.16951655\n",
      "Iteration 332, loss = 3.16918468\n",
      "Iteration 333, loss = 3.16913420\n",
      "Iteration 334, loss = 3.16821260\n",
      "Iteration 335, loss = 3.16765809\n",
      "Iteration 336, loss = 3.16775016\n",
      "Iteration 337, loss = 3.16761049\n",
      "Iteration 338, loss = 3.16729169\n",
      "Iteration 339, loss = 3.16709362\n",
      "Iteration 340, loss = 3.16654893\n",
      "Iteration 341, loss = 3.16632620\n",
      "Iteration 342, loss = 3.16614526\n",
      "Iteration 343, loss = 3.16725954\n",
      "Iteration 344, loss = 3.16675418\n",
      "Iteration 345, loss = 3.16610912\n",
      "Iteration 346, loss = 3.16576649\n",
      "Iteration 347, loss = 3.16548519\n",
      "Iteration 348, loss = 3.16528108\n",
      "Iteration 349, loss = 3.16552936\n",
      "Iteration 350, loss = 3.16526550\n",
      "Iteration 351, loss = 3.16535176\n",
      "Iteration 352, loss = 3.16518990\n",
      "Iteration 353, loss = 3.16465915\n",
      "Iteration 354, loss = 3.16432522\n",
      "Iteration 355, loss = 3.16422353\n",
      "Iteration 356, loss = 3.16441965\n",
      "Iteration 357, loss = 3.16390589\n",
      "Iteration 358, loss = 3.16312634\n",
      "Iteration 359, loss = 3.16314435\n",
      "Iteration 360, loss = 3.16335196\n",
      "Iteration 361, loss = 3.16283967\n",
      "Iteration 362, loss = 3.16322970\n",
      "Iteration 363, loss = 3.16284847\n",
      "Iteration 364, loss = 3.16283030\n",
      "Iteration 365, loss = 3.16244025\n",
      "Iteration 366, loss = 3.16196679\n",
      "Iteration 367, loss = 3.16203482\n",
      "Iteration 368, loss = 3.16258775\n",
      "Iteration 369, loss = 3.16190515\n",
      "Iteration 370, loss = 3.16192240\n",
      "Iteration 371, loss = 3.16255190\n",
      "Iteration 372, loss = 3.16175984\n",
      "Iteration 373, loss = 3.16178654\n",
      "Iteration 374, loss = 3.16106836\n",
      "Iteration 375, loss = 3.16077191\n",
      "Iteration 376, loss = 3.16226491\n",
      "Iteration 377, loss = 3.16054374\n",
      "Iteration 378, loss = 3.16031398\n",
      "Iteration 379, loss = 3.16079413\n",
      "Iteration 380, loss = 3.16069923\n",
      "Iteration 381, loss = 3.16015701\n",
      "Iteration 382, loss = 3.16044737\n",
      "Iteration 383, loss = 3.15886880\n",
      "Iteration 384, loss = 3.15920674\n",
      "Iteration 385, loss = 3.15892689\n",
      "Iteration 386, loss = 3.15936687\n",
      "Iteration 387, loss = 3.15964968\n",
      "Iteration 388, loss = 3.15933617\n",
      "Iteration 389, loss = 3.15863335\n",
      "Iteration 390, loss = 3.15825807\n",
      "Iteration 391, loss = 3.15908173\n",
      "Iteration 392, loss = 3.15850732\n",
      "Iteration 393, loss = 3.15743105\n",
      "Iteration 394, loss = 3.15770568\n",
      "Iteration 395, loss = 3.15746065\n",
      "Iteration 396, loss = 3.15759486\n",
      "Iteration 397, loss = 3.15788423\n",
      "Iteration 398, loss = 3.15858007\n",
      "Iteration 399, loss = 3.15712165\n",
      "Iteration 400, loss = 3.15740754\n",
      "Iteration 401, loss = 3.15766515\n",
      "Iteration 402, loss = 3.15706170\n",
      "Iteration 403, loss = 3.15596585\n",
      "Iteration 404, loss = 3.15660545\n",
      "Iteration 405, loss = 3.15649410\n",
      "Iteration 406, loss = 3.15584996\n",
      "Iteration 407, loss = 3.15537429\n",
      "Iteration 408, loss = 3.15751775\n",
      "Iteration 409, loss = 3.15609639\n",
      "Iteration 410, loss = 3.15478839\n",
      "Iteration 411, loss = 3.15490778\n",
      "Iteration 412, loss = 3.15500409\n",
      "Iteration 413, loss = 3.15472402\n",
      "Iteration 414, loss = 3.15408314\n",
      "Iteration 415, loss = 3.15361811\n",
      "Iteration 416, loss = 3.15379254\n",
      "Iteration 417, loss = 3.15482782\n",
      "Iteration 418, loss = 3.15415692\n",
      "Iteration 419, loss = 3.15378235\n",
      "Iteration 420, loss = 3.15344064\n",
      "Iteration 421, loss = 3.15274323\n",
      "Iteration 422, loss = 3.15203373\n",
      "Iteration 423, loss = 3.15249795\n",
      "Iteration 424, loss = 3.15189266\n",
      "Iteration 425, loss = 3.15153968\n",
      "Iteration 426, loss = 3.15205272\n",
      "Iteration 427, loss = 3.15166860\n",
      "Iteration 428, loss = 3.15131163\n",
      "Iteration 429, loss = 3.15115214\n",
      "Iteration 430, loss = 3.15131216\n",
      "Iteration 431, loss = 3.15143753\n",
      "Iteration 432, loss = 3.15252577\n",
      "Iteration 433, loss = 3.15151312\n",
      "Iteration 434, loss = 3.15096769\n",
      "Iteration 435, loss = 3.15046269\n",
      "Iteration 436, loss = 3.15034271\n",
      "Iteration 437, loss = 3.15069677\n",
      "Iteration 438, loss = 3.15022391\n",
      "Iteration 439, loss = 3.15040284\n",
      "Iteration 440, loss = 3.15012047\n",
      "Iteration 441, loss = 3.14967547\n",
      "Iteration 442, loss = 3.14941530\n",
      "Iteration 443, loss = 3.15071166\n",
      "Iteration 444, loss = 3.14986246\n",
      "Iteration 445, loss = 3.15072525\n",
      "Iteration 446, loss = 3.15264467\n",
      "Iteration 447, loss = 3.14989774\n",
      "Iteration 448, loss = 3.14866641\n",
      "Iteration 449, loss = 3.14831856\n",
      "Iteration 450, loss = 3.14828457\n",
      "Iteration 451, loss = 3.15086062\n",
      "Iteration 452, loss = 3.14945632\n",
      "Iteration 453, loss = 3.14714798\n",
      "Iteration 454, loss = 3.14746282\n",
      "Iteration 455, loss = 3.14704693\n",
      "Iteration 456, loss = 3.14647594\n",
      "Iteration 457, loss = 3.14690177\n",
      "Iteration 458, loss = 3.14625829\n",
      "Iteration 459, loss = 3.14693523\n",
      "Iteration 460, loss = 3.14658275\n",
      "Iteration 461, loss = 3.14665479\n",
      "Iteration 462, loss = 3.14650497\n",
      "Iteration 463, loss = 3.14614581\n",
      "Iteration 464, loss = 3.14675915\n",
      "Iteration 465, loss = 3.14753012\n",
      "Iteration 466, loss = 3.14600992\n",
      "Iteration 467, loss = 3.14634783\n",
      "Iteration 468, loss = 3.14568967\n",
      "Iteration 469, loss = 3.14478663\n",
      "Iteration 470, loss = 3.14458428\n",
      "Iteration 471, loss = 3.14479053\n",
      "Iteration 472, loss = 3.14480088\n",
      "Iteration 473, loss = 3.14412895\n",
      "Iteration 474, loss = 3.14452738\n",
      "Iteration 475, loss = 3.14400229\n",
      "Iteration 476, loss = 3.14352874\n",
      "Iteration 477, loss = 3.14532574\n",
      "Iteration 478, loss = 3.14475264\n",
      "Iteration 479, loss = 3.14324694\n",
      "Iteration 480, loss = 3.14409456\n",
      "Iteration 481, loss = 3.14335644\n",
      "Iteration 482, loss = 3.14323268\n",
      "Iteration 483, loss = 3.14359358\n",
      "Iteration 484, loss = 3.14232287\n",
      "Iteration 485, loss = 3.14195670\n",
      "Iteration 486, loss = 3.14262426\n",
      "Iteration 487, loss = 3.14174931\n",
      "Iteration 488, loss = 3.14213910\n",
      "Iteration 489, loss = 3.14171025\n",
      "Iteration 490, loss = 3.14159700\n",
      "Iteration 491, loss = 3.14139590\n",
      "Iteration 492, loss = 3.14130961\n",
      "Iteration 493, loss = 3.14110210\n",
      "Iteration 494, loss = 3.14192126\n",
      "Iteration 495, loss = 3.14199337\n",
      "Iteration 496, loss = 3.14061202\n",
      "Iteration 497, loss = 3.14119828\n",
      "Iteration 498, loss = 3.14083222\n",
      "Iteration 499, loss = 3.13961174\n",
      "Iteration 500, loss = 3.14042048\n",
      "Iteration 501, loss = 3.14050405\n",
      "Iteration 502, loss = 3.14061449\n",
      "Iteration 503, loss = 3.14014981\n",
      "Iteration 504, loss = 3.13917180\n",
      "Iteration 505, loss = 3.13956435\n",
      "Iteration 506, loss = 3.14018845\n",
      "Iteration 507, loss = 3.13934670\n",
      "Iteration 508, loss = 3.13966217\n",
      "Iteration 509, loss = 3.13974003\n",
      "Iteration 510, loss = 3.13968801\n",
      "Iteration 511, loss = 3.13896008\n",
      "Iteration 512, loss = 3.13882193\n",
      "Iteration 513, loss = 3.13852344\n",
      "Iteration 514, loss = 3.13810404\n",
      "Iteration 515, loss = 3.13857187\n",
      "Iteration 516, loss = 3.13768703\n",
      "Iteration 517, loss = 3.13786230\n",
      "Iteration 518, loss = 3.13766272\n",
      "Iteration 519, loss = 3.13750179\n",
      "Iteration 520, loss = 3.13776549\n",
      "Iteration 521, loss = 3.13777226\n",
      "Iteration 522, loss = 3.13743094\n",
      "Iteration 523, loss = 3.13722870\n",
      "Iteration 524, loss = 3.13740094\n",
      "Iteration 525, loss = 3.13887986\n",
      "Iteration 526, loss = 3.13715711\n",
      "Iteration 527, loss = 3.13701513\n",
      "Iteration 528, loss = 3.13646903\n",
      "Iteration 529, loss = 3.13865788\n",
      "Iteration 530, loss = 3.13733889\n",
      "Iteration 531, loss = 3.13682200\n",
      "Iteration 532, loss = 3.13713985\n",
      "Iteration 533, loss = 3.13580459\n",
      "Iteration 534, loss = 3.13608977\n",
      "Iteration 535, loss = 3.13511173\n",
      "Iteration 536, loss = 3.13503327\n",
      "Iteration 537, loss = 3.13525255\n",
      "Iteration 538, loss = 3.13523866\n",
      "Iteration 539, loss = 3.13575637\n",
      "Iteration 540, loss = 3.13677881\n",
      "Iteration 541, loss = 3.13658023\n",
      "Iteration 542, loss = 3.13502508\n",
      "Iteration 543, loss = 3.13471940\n",
      "Iteration 544, loss = 3.13477694\n",
      "Iteration 545, loss = 3.13360287\n",
      "Iteration 546, loss = 3.13377890\n",
      "Iteration 547, loss = 3.13331894\n",
      "Iteration 548, loss = 3.13321234\n",
      "Iteration 549, loss = 3.13295045\n",
      "Iteration 550, loss = 3.13233340\n",
      "Iteration 551, loss = 3.13270372\n",
      "Iteration 552, loss = 3.13345271\n",
      "Iteration 553, loss = 3.13353517\n",
      "Iteration 554, loss = 3.13223877\n",
      "Iteration 555, loss = 3.13245798\n",
      "Iteration 556, loss = 3.13296721\n",
      "Iteration 557, loss = 3.13230987\n",
      "Iteration 558, loss = 3.13186098\n",
      "Iteration 559, loss = 3.13178059\n",
      "Iteration 560, loss = 3.13217968\n",
      "Iteration 561, loss = 3.13101279\n",
      "Iteration 562, loss = 3.13165658\n",
      "Iteration 563, loss = 3.13256773\n",
      "Iteration 564, loss = 3.13235483\n",
      "Iteration 565, loss = 3.13179786\n",
      "Iteration 566, loss = 3.13167146\n",
      "Iteration 567, loss = 3.13117770\n",
      "Iteration 568, loss = 3.13139035\n",
      "Iteration 569, loss = 3.13066406\n",
      "Iteration 570, loss = 3.13099779\n",
      "Iteration 571, loss = 3.13056516\n",
      "Iteration 572, loss = 3.13237081\n",
      "Iteration 573, loss = 3.13112021\n",
      "Iteration 574, loss = 3.13178834\n",
      "Iteration 575, loss = 3.13090758\n",
      "Iteration 576, loss = 3.13033735\n",
      "Iteration 577, loss = 3.12988389\n",
      "Iteration 578, loss = 3.12989161\n",
      "Iteration 579, loss = 3.12894569\n",
      "Iteration 580, loss = 3.12863479\n",
      "Iteration 581, loss = 3.12860689\n",
      "Iteration 582, loss = 3.12841472\n",
      "Iteration 583, loss = 3.12791406\n",
      "Iteration 584, loss = 3.12816881\n",
      "Iteration 585, loss = 3.12846238\n",
      "Iteration 586, loss = 3.12899733\n",
      "Iteration 587, loss = 3.12858969\n",
      "Iteration 588, loss = 3.12717940\n",
      "Iteration 589, loss = 3.12883345\n",
      "Iteration 590, loss = 3.12812655\n",
      "Iteration 591, loss = 3.12730667\n",
      "Iteration 592, loss = 3.12668479\n",
      "Iteration 593, loss = 3.12723013\n",
      "Iteration 594, loss = 3.12727248\n",
      "Iteration 595, loss = 3.12710837\n",
      "Iteration 596, loss = 3.12750161\n",
      "Iteration 597, loss = 3.12942290\n",
      "Iteration 598, loss = 3.12714232\n",
      "Iteration 599, loss = 3.12714407\n",
      "Iteration 600, loss = 3.12822818\n",
      "Iteration 601, loss = 3.12672484\n",
      "Iteration 602, loss = 3.12701714\n",
      "Iteration 603, loss = 3.12650503\n",
      "Iteration 604, loss = 3.12581338\n",
      "Iteration 605, loss = 3.12581187\n",
      "Iteration 606, loss = 3.12567695\n",
      "Iteration 607, loss = 3.12558889\n",
      "Iteration 608, loss = 3.12562690\n",
      "Iteration 609, loss = 3.12526708\n",
      "Iteration 610, loss = 3.12568827\n",
      "Iteration 611, loss = 3.12535361\n",
      "Iteration 612, loss = 3.12511667\n",
      "Iteration 613, loss = 3.12501385\n",
      "Iteration 614, loss = 3.12587671\n",
      "Iteration 615, loss = 3.12547939\n",
      "Iteration 616, loss = 3.12475921\n",
      "Iteration 617, loss = 3.12477678\n",
      "Iteration 618, loss = 3.12472151\n",
      "Iteration 619, loss = 3.12466254\n",
      "Iteration 620, loss = 3.12447197\n",
      "Iteration 621, loss = 3.12394297\n",
      "Iteration 622, loss = 3.12356174\n",
      "Iteration 623, loss = 3.12358063\n",
      "Iteration 624, loss = 3.12291794\n",
      "Iteration 625, loss = 3.12344732\n",
      "Iteration 626, loss = 3.12308833\n",
      "Iteration 627, loss = 3.12345367\n",
      "Iteration 628, loss = 3.12401691\n",
      "Iteration 629, loss = 3.12325491\n",
      "Iteration 630, loss = 3.12326271\n",
      "Iteration 631, loss = 3.12294011\n",
      "Iteration 632, loss = 3.12268240\n",
      "Iteration 633, loss = 3.12297127\n",
      "Iteration 634, loss = 3.12235757\n",
      "Iteration 635, loss = 3.12434800\n",
      "Iteration 636, loss = 3.12275753\n",
      "Iteration 637, loss = 3.12232088\n",
      "Iteration 638, loss = 3.12419788\n",
      "Iteration 639, loss = 3.12265972\n",
      "Iteration 640, loss = 3.12207005\n",
      "Iteration 641, loss = 3.12176733\n",
      "Iteration 642, loss = 3.12195810\n",
      "Iteration 643, loss = 3.12161555\n",
      "Iteration 644, loss = 3.12153406\n",
      "Iteration 645, loss = 3.12135568\n",
      "Iteration 646, loss = 3.12082179\n",
      "Iteration 647, loss = 3.12053154\n",
      "Iteration 648, loss = 3.12071544\n",
      "Iteration 649, loss = 3.12105126\n",
      "Iteration 650, loss = 3.12030762\n",
      "Iteration 651, loss = 3.11993325\n",
      "Iteration 652, loss = 3.12034674\n",
      "Iteration 653, loss = 3.12049656\n",
      "Iteration 654, loss = 3.12000474\n",
      "Iteration 655, loss = 3.12052404\n",
      "Iteration 656, loss = 3.12024201\n",
      "Iteration 657, loss = 3.11984325\n",
      "Iteration 658, loss = 3.11950095\n",
      "Iteration 659, loss = 3.12011521\n",
      "Iteration 660, loss = 3.12064535\n",
      "Iteration 661, loss = 3.11983849\n",
      "Iteration 662, loss = 3.11995221\n",
      "Iteration 663, loss = 3.11921442\n",
      "Iteration 664, loss = 3.11904058\n",
      "Iteration 665, loss = 3.11921692\n",
      "Iteration 666, loss = 3.11881596\n",
      "Iteration 667, loss = 3.11957236\n",
      "Iteration 668, loss = 3.11850542\n",
      "Iteration 669, loss = 3.11921922\n",
      "Iteration 670, loss = 3.11922748\n",
      "Iteration 671, loss = 3.11849155\n",
      "Iteration 672, loss = 3.11846238\n",
      "Iteration 673, loss = 3.11836438\n",
      "Iteration 674, loss = 3.11836990\n",
      "Iteration 675, loss = 3.11881446\n",
      "Iteration 676, loss = 3.11783033\n",
      "Iteration 677, loss = 3.11745559\n",
      "Iteration 678, loss = 3.11716708\n",
      "Iteration 679, loss = 3.11749294\n",
      "Iteration 680, loss = 3.11733642\n",
      "Iteration 681, loss = 3.11687759\n",
      "Iteration 682, loss = 3.11709417\n",
      "Iteration 683, loss = 3.11733764\n",
      "Iteration 684, loss = 3.11782454\n",
      "Iteration 685, loss = 3.11741182\n",
      "Iteration 686, loss = 3.11729995\n",
      "Iteration 687, loss = 3.11692598\n",
      "Iteration 688, loss = 3.11714315\n",
      "Iteration 689, loss = 3.11692016\n",
      "Iteration 690, loss = 3.11624277\n",
      "Iteration 691, loss = 3.11602063\n",
      "Iteration 692, loss = 3.11555999\n",
      "Iteration 693, loss = 3.11603642\n",
      "Iteration 694, loss = 3.11626216\n",
      "Iteration 695, loss = 3.11656725\n",
      "Iteration 696, loss = 3.11659373\n",
      "Iteration 697, loss = 3.11538089\n",
      "Iteration 698, loss = 3.11525089\n",
      "Iteration 699, loss = 3.11531593\n",
      "Iteration 700, loss = 3.11543554\n",
      "Iteration 701, loss = 3.11477315\n",
      "Iteration 702, loss = 3.11508653\n",
      "Iteration 703, loss = 3.11470930\n",
      "Iteration 704, loss = 3.11436828\n",
      "Iteration 705, loss = 3.11462753\n",
      "Iteration 706, loss = 3.11436687\n",
      "Iteration 707, loss = 3.11453533\n",
      "Iteration 708, loss = 3.11423744\n",
      "Iteration 709, loss = 3.11485704\n",
      "Iteration 710, loss = 3.11444486\n",
      "Iteration 711, loss = 3.11457437\n",
      "Iteration 712, loss = 3.11441172\n",
      "Iteration 713, loss = 3.11405224\n",
      "Iteration 714, loss = 3.11411868\n",
      "Iteration 715, loss = 3.11386173\n",
      "Iteration 716, loss = 3.11352931\n",
      "Iteration 717, loss = 3.11300477\n",
      "Iteration 718, loss = 3.11303775\n",
      "Iteration 719, loss = 3.11355672\n",
      "Iteration 720, loss = 3.11365514\n",
      "Iteration 721, loss = 3.11381616\n",
      "Iteration 722, loss = 3.11254723\n",
      "Iteration 723, loss = 3.11338131\n",
      "Iteration 724, loss = 3.11276043\n",
      "Iteration 725, loss = 3.11261115\n",
      "Iteration 726, loss = 3.11214737\n",
      "Iteration 727, loss = 3.11204670\n",
      "Iteration 728, loss = 3.11315500\n",
      "Iteration 729, loss = 3.11369040\n",
      "Iteration 730, loss = 3.11291405\n",
      "Iteration 731, loss = 3.11194203\n",
      "Iteration 732, loss = 3.11177151\n",
      "Iteration 733, loss = 3.11164649\n",
      "Iteration 734, loss = 3.11213231\n",
      "Iteration 735, loss = 3.11197391\n",
      "Iteration 736, loss = 3.11195726\n",
      "Iteration 737, loss = 3.11173949\n",
      "Iteration 738, loss = 3.11172110\n",
      "Iteration 739, loss = 3.11165355\n",
      "Iteration 740, loss = 3.11180906\n",
      "Iteration 741, loss = 3.11261275\n",
      "Iteration 742, loss = 3.11091292\n",
      "Iteration 743, loss = 3.11062886\n",
      "Iteration 744, loss = 3.11157493\n",
      "Iteration 745, loss = 3.11189876\n",
      "Iteration 746, loss = 3.11128523\n",
      "Iteration 747, loss = 3.11072870\n",
      "Iteration 748, loss = 3.11030005\n",
      "Iteration 749, loss = 3.11151398\n",
      "Iteration 750, loss = 3.11045780\n",
      "Iteration 751, loss = 3.10967345\n",
      "Iteration 752, loss = 3.10938681\n",
      "Iteration 753, loss = 3.10956761\n",
      "Iteration 754, loss = 3.10945587\n",
      "Iteration 755, loss = 3.10929348\n",
      "Iteration 756, loss = 3.11047731\n",
      "Iteration 757, loss = 3.10998738\n",
      "Iteration 758, loss = 3.11009227\n",
      "Iteration 759, loss = 3.10984401\n",
      "Iteration 760, loss = 3.10932381\n",
      "Iteration 761, loss = 3.11023476\n",
      "Iteration 762, loss = 3.11004118\n",
      "Iteration 763, loss = 3.10944256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15057018\n",
      "Iteration 2, loss = 4.02321899\n",
      "Iteration 3, loss = 3.90411302\n",
      "Iteration 4, loss = 3.79273701\n",
      "Iteration 5, loss = 3.68550569\n",
      "Iteration 6, loss = 3.58547165\n",
      "Iteration 7, loss = 3.49107195\n",
      "Iteration 8, loss = 3.41217705\n",
      "Iteration 9, loss = 3.34430804\n",
      "Iteration 10, loss = 3.29232493\n",
      "Iteration 11, loss = 3.25941251\n",
      "Iteration 12, loss = 3.24232245\n",
      "Iteration 13, loss = 3.23821163\n",
      "Iteration 14, loss = 3.23585487\n",
      "Iteration 15, loss = 3.23455906\n",
      "Iteration 16, loss = 3.23306909\n",
      "Iteration 17, loss = 3.23129175\n",
      "Iteration 18, loss = 3.23014648\n",
      "Iteration 19, loss = 3.23002363\n",
      "Iteration 20, loss = 3.22885233\n",
      "Iteration 21, loss = 3.22854397\n",
      "Iteration 22, loss = 3.22787685\n",
      "Iteration 23, loss = 3.22795890\n",
      "Iteration 24, loss = 3.22697608\n",
      "Iteration 25, loss = 3.22631430\n",
      "Iteration 26, loss = 3.22572369\n",
      "Iteration 27, loss = 3.22468562\n",
      "Iteration 28, loss = 3.22456021\n",
      "Iteration 29, loss = 3.22460095\n",
      "Iteration 30, loss = 3.22419803\n",
      "Iteration 31, loss = 3.22402050\n",
      "Iteration 32, loss = 3.22367543\n",
      "Iteration 33, loss = 3.22296925\n",
      "Iteration 34, loss = 3.22283494\n",
      "Iteration 35, loss = 3.22233862\n",
      "Iteration 36, loss = 3.22265470\n",
      "Iteration 37, loss = 3.22212215\n",
      "Iteration 38, loss = 3.22150995\n",
      "Iteration 39, loss = 3.22080635\n",
      "Iteration 40, loss = 3.22020356\n",
      "Iteration 41, loss = 3.21997390\n",
      "Iteration 42, loss = 3.21971568\n",
      "Iteration 43, loss = 3.21933697\n",
      "Iteration 44, loss = 3.21901793\n",
      "Iteration 45, loss = 3.21866612\n",
      "Iteration 46, loss = 3.21883207\n",
      "Iteration 47, loss = 3.21832245\n",
      "Iteration 48, loss = 3.21851736\n",
      "Iteration 49, loss = 3.21791909\n",
      "Iteration 50, loss = 3.21719087\n",
      "Iteration 51, loss = 3.21690480\n",
      "Iteration 52, loss = 3.21660314\n",
      "Iteration 53, loss = 3.21660906\n",
      "Iteration 54, loss = 3.21654398\n",
      "Iteration 55, loss = 3.21637281\n",
      "Iteration 56, loss = 3.21684184\n",
      "Iteration 57, loss = 3.21655502\n",
      "Iteration 58, loss = 3.21571289\n",
      "Iteration 59, loss = 3.21489875\n",
      "Iteration 60, loss = 3.21515137\n",
      "Iteration 61, loss = 3.21513416\n",
      "Iteration 62, loss = 3.21441279\n",
      "Iteration 63, loss = 3.21406923\n",
      "Iteration 64, loss = 3.21347078\n",
      "Iteration 65, loss = 3.21325216\n",
      "Iteration 66, loss = 3.21235618\n",
      "Iteration 67, loss = 3.21237153\n",
      "Iteration 68, loss = 3.21228580\n",
      "Iteration 69, loss = 3.21141254\n",
      "Iteration 70, loss = 3.21158652\n",
      "Iteration 71, loss = 3.21145338\n",
      "Iteration 72, loss = 3.21047906\n",
      "Iteration 73, loss = 3.21081618\n",
      "Iteration 74, loss = 3.21041313\n",
      "Iteration 75, loss = 3.21020652\n",
      "Iteration 76, loss = 3.21037312\n",
      "Iteration 77, loss = 3.20991292\n",
      "Iteration 78, loss = 3.20999537\n",
      "Iteration 79, loss = 3.20960465\n",
      "Iteration 80, loss = 3.20924618\n",
      "Iteration 81, loss = 3.20915558\n",
      "Iteration 82, loss = 3.20909116\n",
      "Iteration 83, loss = 3.20928510\n",
      "Iteration 84, loss = 3.20863214\n",
      "Iteration 85, loss = 3.20863694\n",
      "Iteration 86, loss = 3.20924221\n",
      "Iteration 87, loss = 3.20856217\n",
      "Iteration 88, loss = 3.20841398\n",
      "Iteration 89, loss = 3.20801368\n",
      "Iteration 90, loss = 3.20829843\n",
      "Iteration 91, loss = 3.20771833\n",
      "Iteration 92, loss = 3.20720184\n",
      "Iteration 93, loss = 3.20653912\n",
      "Iteration 94, loss = 3.20648502\n",
      "Iteration 95, loss = 3.20648379\n",
      "Iteration 96, loss = 3.20618242\n",
      "Iteration 97, loss = 3.20579073\n",
      "Iteration 98, loss = 3.20556165\n",
      "Iteration 99, loss = 3.20574714\n",
      "Iteration 100, loss = 3.20564692\n",
      "Iteration 101, loss = 3.20575759\n",
      "Iteration 102, loss = 3.20584035\n",
      "Iteration 103, loss = 3.20524777\n",
      "Iteration 104, loss = 3.20428638\n",
      "Iteration 105, loss = 3.20400655\n",
      "Iteration 106, loss = 3.20387272\n",
      "Iteration 107, loss = 3.20406656\n",
      "Iteration 108, loss = 3.20366210\n",
      "Iteration 109, loss = 3.20298646\n",
      "Iteration 110, loss = 3.20293056\n",
      "Iteration 111, loss = 3.20329142\n",
      "Iteration 112, loss = 3.20239241\n",
      "Iteration 113, loss = 3.20224967\n",
      "Iteration 114, loss = 3.20220957\n",
      "Iteration 115, loss = 3.20160488\n",
      "Iteration 116, loss = 3.20137512\n",
      "Iteration 117, loss = 3.20125324\n",
      "Iteration 118, loss = 3.20165812\n",
      "Iteration 119, loss = 3.20058385\n",
      "Iteration 120, loss = 3.20065885\n",
      "Iteration 121, loss = 3.20048845\n",
      "Iteration 122, loss = 3.20034014\n",
      "Iteration 123, loss = 3.20035440\n",
      "Iteration 124, loss = 3.20021207\n",
      "Iteration 125, loss = 3.20003520\n",
      "Iteration 126, loss = 3.20037936\n",
      "Iteration 127, loss = 3.19965965\n",
      "Iteration 128, loss = 3.19989618\n",
      "Iteration 129, loss = 3.20011298\n",
      "Iteration 130, loss = 3.20007785\n",
      "Iteration 131, loss = 3.19912164\n",
      "Iteration 132, loss = 3.19813280\n",
      "Iteration 133, loss = 3.19818595\n",
      "Iteration 134, loss = 3.19824179\n",
      "Iteration 135, loss = 3.19705183\n",
      "Iteration 136, loss = 3.19791402\n",
      "Iteration 137, loss = 3.19707088\n",
      "Iteration 138, loss = 3.19699368\n",
      "Iteration 139, loss = 3.19684176\n",
      "Iteration 140, loss = 3.19677024\n",
      "Iteration 141, loss = 3.19655519\n",
      "Iteration 142, loss = 3.19619117\n",
      "Iteration 143, loss = 3.19618477\n",
      "Iteration 144, loss = 3.19662956\n",
      "Iteration 145, loss = 3.19567297\n",
      "Iteration 146, loss = 3.19519630\n",
      "Iteration 147, loss = 3.19490576\n",
      "Iteration 148, loss = 3.19497683\n",
      "Iteration 149, loss = 3.19506990\n",
      "Iteration 150, loss = 3.19427849\n",
      "Iteration 151, loss = 3.19372607\n",
      "Iteration 152, loss = 3.19419588\n",
      "Iteration 153, loss = 3.19411809\n",
      "Iteration 154, loss = 3.19375982\n",
      "Iteration 155, loss = 3.19476924\n",
      "Iteration 156, loss = 3.19340381\n",
      "Iteration 157, loss = 3.19293496\n",
      "Iteration 158, loss = 3.19249276\n",
      "Iteration 159, loss = 3.19256818\n",
      "Iteration 160, loss = 3.19236519\n",
      "Iteration 161, loss = 3.19195458\n",
      "Iteration 162, loss = 3.19185577\n",
      "Iteration 163, loss = 3.19129568\n",
      "Iteration 164, loss = 3.19099483\n",
      "Iteration 165, loss = 3.19165570\n",
      "Iteration 166, loss = 3.19166333\n",
      "Iteration 167, loss = 3.19094470\n",
      "Iteration 168, loss = 3.19029378\n",
      "Iteration 169, loss = 3.19100562\n",
      "Iteration 170, loss = 3.19052585\n",
      "Iteration 171, loss = 3.18968628\n",
      "Iteration 172, loss = 3.18978430\n",
      "Iteration 173, loss = 3.18960678\n",
      "Iteration 174, loss = 3.18953227\n",
      "Iteration 175, loss = 3.18936691\n",
      "Iteration 176, loss = 3.18917005\n",
      "Iteration 177, loss = 3.18917491\n",
      "Iteration 178, loss = 3.18908714\n",
      "Iteration 179, loss = 3.18900182\n",
      "Iteration 180, loss = 3.18866259\n",
      "Iteration 181, loss = 3.18808704\n",
      "Iteration 182, loss = 3.18826632\n",
      "Iteration 183, loss = 3.18791944\n",
      "Iteration 184, loss = 3.18791895\n",
      "Iteration 185, loss = 3.18693514\n",
      "Iteration 186, loss = 3.18721589\n",
      "Iteration 187, loss = 3.18720852\n",
      "Iteration 188, loss = 3.18641969\n",
      "Iteration 189, loss = 3.18572313\n",
      "Iteration 190, loss = 3.18569595\n",
      "Iteration 191, loss = 3.18636445\n",
      "Iteration 192, loss = 3.18619115\n",
      "Iteration 193, loss = 3.18624394\n",
      "Iteration 194, loss = 3.18519918\n",
      "Iteration 195, loss = 3.18534956\n",
      "Iteration 196, loss = 3.18522462\n",
      "Iteration 197, loss = 3.18457699\n",
      "Iteration 198, loss = 3.18477934\n",
      "Iteration 199, loss = 3.18442484\n",
      "Iteration 200, loss = 3.18396883\n",
      "Iteration 201, loss = 3.18404616\n",
      "Iteration 202, loss = 3.18323494\n",
      "Iteration 203, loss = 3.18286124\n",
      "Iteration 204, loss = 3.18255115\n",
      "Iteration 205, loss = 3.18252594\n",
      "Iteration 206, loss = 3.18238526\n",
      "Iteration 207, loss = 3.18282475\n",
      "Iteration 208, loss = 3.18219378\n",
      "Iteration 209, loss = 3.18263270\n",
      "Iteration 210, loss = 3.18194571\n",
      "Iteration 211, loss = 3.18198085\n",
      "Iteration 212, loss = 3.18181808\n",
      "Iteration 213, loss = 3.18102354\n",
      "Iteration 214, loss = 3.18151386\n",
      "Iteration 215, loss = 3.18061711\n",
      "Iteration 216, loss = 3.18057304\n",
      "Iteration 217, loss = 3.18073863\n",
      "Iteration 218, loss = 3.18102176\n",
      "Iteration 219, loss = 3.17960323\n",
      "Iteration 220, loss = 3.17933468\n",
      "Iteration 221, loss = 3.17899074\n",
      "Iteration 222, loss = 3.17929632\n",
      "Iteration 223, loss = 3.17918657\n",
      "Iteration 224, loss = 3.17888284\n",
      "Iteration 225, loss = 3.17882665\n",
      "Iteration 226, loss = 3.17845381\n",
      "Iteration 227, loss = 3.17844609\n",
      "Iteration 228, loss = 3.17817155\n",
      "Iteration 229, loss = 3.17816096\n",
      "Iteration 230, loss = 3.17793839\n",
      "Iteration 231, loss = 3.17786828\n",
      "Iteration 232, loss = 3.17759405\n",
      "Iteration 233, loss = 3.17719039\n",
      "Iteration 234, loss = 3.17667072\n",
      "Iteration 235, loss = 3.17715794\n",
      "Iteration 236, loss = 3.17589401\n",
      "Iteration 237, loss = 3.17562366\n",
      "Iteration 238, loss = 3.17557948\n",
      "Iteration 239, loss = 3.17582566\n",
      "Iteration 240, loss = 3.17556957\n",
      "Iteration 241, loss = 3.17511006\n",
      "Iteration 242, loss = 3.17474172\n",
      "Iteration 243, loss = 3.17482062\n",
      "Iteration 244, loss = 3.17445748\n",
      "Iteration 245, loss = 3.17484545\n",
      "Iteration 246, loss = 3.17466798\n",
      "Iteration 247, loss = 3.17401232\n",
      "Iteration 248, loss = 3.17361482\n",
      "Iteration 249, loss = 3.17384778\n",
      "Iteration 250, loss = 3.17424599\n",
      "Iteration 251, loss = 3.17526231\n",
      "Iteration 252, loss = 3.17332985\n",
      "Iteration 253, loss = 3.17266083\n",
      "Iteration 254, loss = 3.17309186\n",
      "Iteration 255, loss = 3.17297535\n",
      "Iteration 256, loss = 3.17225595\n",
      "Iteration 257, loss = 3.17136142\n",
      "Iteration 258, loss = 3.17171251\n",
      "Iteration 259, loss = 3.17156049\n",
      "Iteration 260, loss = 3.17169265\n",
      "Iteration 261, loss = 3.17130453\n",
      "Iteration 262, loss = 3.17076639\n",
      "Iteration 263, loss = 3.17054686\n",
      "Iteration 264, loss = 3.17041513\n",
      "Iteration 265, loss = 3.17048467\n",
      "Iteration 266, loss = 3.16977576\n",
      "Iteration 267, loss = 3.17014513\n",
      "Iteration 268, loss = 3.17067332\n",
      "Iteration 269, loss = 3.17093462\n",
      "Iteration 270, loss = 3.17027981\n",
      "Iteration 271, loss = 3.17034559\n",
      "Iteration 272, loss = 3.16948822\n",
      "Iteration 273, loss = 3.16861782\n",
      "Iteration 274, loss = 3.16866973\n",
      "Iteration 275, loss = 3.16815303\n",
      "Iteration 276, loss = 3.16769526\n",
      "Iteration 277, loss = 3.16804770\n",
      "Iteration 278, loss = 3.16700732\n",
      "Iteration 279, loss = 3.16778814\n",
      "Iteration 280, loss = 3.16840560\n",
      "Iteration 281, loss = 3.16790511\n",
      "Iteration 282, loss = 3.16744361\n",
      "Iteration 283, loss = 3.16785704\n",
      "Iteration 284, loss = 3.16693594\n",
      "Iteration 285, loss = 3.16681255\n",
      "Iteration 286, loss = 3.16648538\n",
      "Iteration 287, loss = 3.16625614\n",
      "Iteration 288, loss = 3.16564972\n",
      "Iteration 289, loss = 3.16616377\n",
      "Iteration 290, loss = 3.16583920\n",
      "Iteration 291, loss = 3.16537582\n",
      "Iteration 292, loss = 3.16490176\n",
      "Iteration 293, loss = 3.16467277\n",
      "Iteration 294, loss = 3.16468427\n",
      "Iteration 295, loss = 3.16443548\n",
      "Iteration 296, loss = 3.16400766\n",
      "Iteration 297, loss = 3.16375805\n",
      "Iteration 298, loss = 3.16346554\n",
      "Iteration 299, loss = 3.16332890\n",
      "Iteration 300, loss = 3.16337934\n",
      "Iteration 301, loss = 3.16287621\n",
      "Iteration 302, loss = 3.16274928\n",
      "Iteration 303, loss = 3.16190045\n",
      "Iteration 304, loss = 3.16219190\n",
      "Iteration 305, loss = 3.16251654\n",
      "Iteration 306, loss = 3.16207078\n",
      "Iteration 307, loss = 3.16193080\n",
      "Iteration 308, loss = 3.16118248\n",
      "Iteration 309, loss = 3.16175858\n",
      "Iteration 310, loss = 3.16161118\n",
      "Iteration 311, loss = 3.16143667\n",
      "Iteration 312, loss = 3.16108984\n",
      "Iteration 313, loss = 3.16090883\n",
      "Iteration 314, loss = 3.16003906\n",
      "Iteration 315, loss = 3.16025114\n",
      "Iteration 316, loss = 3.15997887\n",
      "Iteration 317, loss = 3.15958080\n",
      "Iteration 318, loss = 3.15968819\n",
      "Iteration 319, loss = 3.15982347\n",
      "Iteration 320, loss = 3.15934412\n",
      "Iteration 321, loss = 3.15956787\n",
      "Iteration 322, loss = 3.15909682\n",
      "Iteration 323, loss = 3.15909935\n",
      "Iteration 324, loss = 3.15916521\n",
      "Iteration 325, loss = 3.15840766\n",
      "Iteration 326, loss = 3.15810185\n",
      "Iteration 327, loss = 3.16046124\n",
      "Iteration 328, loss = 3.15968294\n",
      "Iteration 329, loss = 3.15798862\n",
      "Iteration 330, loss = 3.15820612\n",
      "Iteration 331, loss = 3.15742884\n",
      "Iteration 332, loss = 3.15780519\n",
      "Iteration 333, loss = 3.15729981\n",
      "Iteration 334, loss = 3.15628021\n",
      "Iteration 335, loss = 3.15767902\n",
      "Iteration 336, loss = 3.15737524\n",
      "Iteration 337, loss = 3.15640244\n",
      "Iteration 338, loss = 3.15670012\n",
      "Iteration 339, loss = 3.15622873\n",
      "Iteration 340, loss = 3.15607390\n",
      "Iteration 341, loss = 3.15535050\n",
      "Iteration 342, loss = 3.15534496\n",
      "Iteration 343, loss = 3.15560579\n",
      "Iteration 344, loss = 3.15545749\n",
      "Iteration 345, loss = 3.15499616\n",
      "Iteration 346, loss = 3.15519645\n",
      "Iteration 347, loss = 3.15494917\n",
      "Iteration 348, loss = 3.15421820\n",
      "Iteration 349, loss = 3.15426594\n",
      "Iteration 350, loss = 3.15462759\n",
      "Iteration 351, loss = 3.15375249\n",
      "Iteration 352, loss = 3.15346992\n",
      "Iteration 353, loss = 3.15539601\n",
      "Iteration 354, loss = 3.15400856\n",
      "Iteration 355, loss = 3.15332515\n",
      "Iteration 356, loss = 3.15337157\n",
      "Iteration 357, loss = 3.15314464\n",
      "Iteration 358, loss = 3.15351493\n",
      "Iteration 359, loss = 3.15352870\n",
      "Iteration 360, loss = 3.15304204\n",
      "Iteration 361, loss = 3.15250055\n",
      "Iteration 362, loss = 3.15267546\n",
      "Iteration 363, loss = 3.15313516\n",
      "Iteration 364, loss = 3.15172464\n",
      "Iteration 365, loss = 3.15157740\n",
      "Iteration 366, loss = 3.15159190\n",
      "Iteration 367, loss = 3.15216032\n",
      "Iteration 368, loss = 3.15137891\n",
      "Iteration 369, loss = 3.15117226\n",
      "Iteration 370, loss = 3.15081887\n",
      "Iteration 371, loss = 3.15064086\n",
      "Iteration 372, loss = 3.14971008\n",
      "Iteration 373, loss = 3.15032319\n",
      "Iteration 374, loss = 3.14948232\n",
      "Iteration 375, loss = 3.14979843\n",
      "Iteration 376, loss = 3.14959985\n",
      "Iteration 377, loss = 3.14932356\n",
      "Iteration 378, loss = 3.14894524\n",
      "Iteration 379, loss = 3.14951315\n",
      "Iteration 380, loss = 3.14887578\n",
      "Iteration 381, loss = 3.14789155\n",
      "Iteration 382, loss = 3.14840718\n",
      "Iteration 383, loss = 3.14766080\n",
      "Iteration 384, loss = 3.14757654\n",
      "Iteration 385, loss = 3.14752442\n",
      "Iteration 386, loss = 3.14739899\n",
      "Iteration 387, loss = 3.14846896\n",
      "Iteration 388, loss = 3.14799974\n",
      "Iteration 389, loss = 3.14735877\n",
      "Iteration 390, loss = 3.14672021\n",
      "Iteration 391, loss = 3.14660689\n",
      "Iteration 392, loss = 3.14703549\n",
      "Iteration 393, loss = 3.14709128\n",
      "Iteration 394, loss = 3.14827957\n",
      "Iteration 395, loss = 3.14638412\n",
      "Iteration 396, loss = 3.14613959\n",
      "Iteration 397, loss = 3.14598140\n",
      "Iteration 398, loss = 3.14554610\n",
      "Iteration 399, loss = 3.14569964\n",
      "Iteration 400, loss = 3.14550281\n",
      "Iteration 401, loss = 3.14626991\n",
      "Iteration 402, loss = 3.14562683\n",
      "Iteration 403, loss = 3.14554420\n",
      "Iteration 404, loss = 3.14545213\n",
      "Iteration 405, loss = 3.14455440\n",
      "Iteration 406, loss = 3.14570704\n",
      "Iteration 407, loss = 3.14419943\n",
      "Iteration 408, loss = 3.14414838\n",
      "Iteration 409, loss = 3.14407069\n",
      "Iteration 410, loss = 3.14646538\n",
      "Iteration 411, loss = 3.14447735\n",
      "Iteration 412, loss = 3.14298811\n",
      "Iteration 413, loss = 3.14312534\n",
      "Iteration 414, loss = 3.14339099\n",
      "Iteration 415, loss = 3.14355981\n",
      "Iteration 416, loss = 3.14314032\n",
      "Iteration 417, loss = 3.14453951\n",
      "Iteration 418, loss = 3.14408976\n",
      "Iteration 419, loss = 3.14363893\n",
      "Iteration 420, loss = 3.14294429\n",
      "Iteration 421, loss = 3.14243735\n",
      "Iteration 422, loss = 3.14198873\n",
      "Iteration 423, loss = 3.14137775\n",
      "Iteration 424, loss = 3.14118585\n",
      "Iteration 425, loss = 3.14181908\n",
      "Iteration 426, loss = 3.14117743\n",
      "Iteration 427, loss = 3.14064395\n",
      "Iteration 428, loss = 3.14125205\n",
      "Iteration 429, loss = 3.14042730\n",
      "Iteration 430, loss = 3.14121519\n",
      "Iteration 431, loss = 3.14032250\n",
      "Iteration 432, loss = 3.14056695\n",
      "Iteration 433, loss = 3.14195511\n",
      "Iteration 434, loss = 3.14007671\n",
      "Iteration 435, loss = 3.13928016\n",
      "Iteration 436, loss = 3.13939400\n",
      "Iteration 437, loss = 3.13980553\n",
      "Iteration 438, loss = 3.13878541\n",
      "Iteration 439, loss = 3.14025177\n",
      "Iteration 440, loss = 3.13945412\n",
      "Iteration 441, loss = 3.13890008\n",
      "Iteration 442, loss = 3.13987436\n",
      "Iteration 443, loss = 3.13959705\n",
      "Iteration 444, loss = 3.13874641\n",
      "Iteration 445, loss = 3.13907971\n",
      "Iteration 446, loss = 3.13816376\n",
      "Iteration 447, loss = 3.13803703\n",
      "Iteration 448, loss = 3.13740510\n",
      "Iteration 449, loss = 3.13782947\n",
      "Iteration 450, loss = 3.13887261\n",
      "Iteration 451, loss = 3.13812814\n",
      "Iteration 452, loss = 3.13762386\n",
      "Iteration 453, loss = 3.13681371\n",
      "Iteration 454, loss = 3.13726002\n",
      "Iteration 455, loss = 3.13773613\n",
      "Iteration 456, loss = 3.13680074\n",
      "Iteration 457, loss = 3.13774271\n",
      "Iteration 458, loss = 3.13931524\n",
      "Iteration 459, loss = 3.13741074\n",
      "Iteration 460, loss = 3.13644971\n",
      "Iteration 461, loss = 3.13601671\n",
      "Iteration 462, loss = 3.13538160\n",
      "Iteration 463, loss = 3.13631319\n",
      "Iteration 464, loss = 3.13567835\n",
      "Iteration 465, loss = 3.13624274\n",
      "Iteration 466, loss = 3.13604005\n",
      "Iteration 467, loss = 3.13517313\n",
      "Iteration 468, loss = 3.13509445\n",
      "Iteration 469, loss = 3.13487157\n",
      "Iteration 470, loss = 3.13622083\n",
      "Iteration 471, loss = 3.13510949\n",
      "Iteration 472, loss = 3.13505158\n",
      "Iteration 473, loss = 3.13426628\n",
      "Iteration 474, loss = 3.13424133\n",
      "Iteration 475, loss = 3.13385023\n",
      "Iteration 476, loss = 3.13353833\n",
      "Iteration 477, loss = 3.13386791\n",
      "Iteration 478, loss = 3.13344196\n",
      "Iteration 479, loss = 3.13317864\n",
      "Iteration 480, loss = 3.13287300\n",
      "Iteration 481, loss = 3.13239972\n",
      "Iteration 482, loss = 3.13376775\n",
      "Iteration 483, loss = 3.13404375\n",
      "Iteration 484, loss = 3.13369899\n",
      "Iteration 485, loss = 3.13329711\n",
      "Iteration 486, loss = 3.13289931\n",
      "Iteration 487, loss = 3.13311293\n",
      "Iteration 488, loss = 3.13275122\n",
      "Iteration 489, loss = 3.13298571\n",
      "Iteration 490, loss = 3.13252088\n",
      "Iteration 491, loss = 3.13210257\n",
      "Iteration 492, loss = 3.13159494\n",
      "Iteration 493, loss = 3.13169484\n",
      "Iteration 494, loss = 3.13139118\n",
      "Iteration 495, loss = 3.13138972\n",
      "Iteration 496, loss = 3.13167177\n",
      "Iteration 497, loss = 3.13154375\n",
      "Iteration 498, loss = 3.13195873\n",
      "Iteration 499, loss = 3.13158272\n",
      "Iteration 500, loss = 3.13135132\n",
      "Iteration 501, loss = 3.13153717\n",
      "Iteration 502, loss = 3.13031589\n",
      "Iteration 503, loss = 3.13112460\n",
      "Iteration 504, loss = 3.13130318\n",
      "Iteration 505, loss = 3.13012120\n",
      "Iteration 506, loss = 3.12992386\n",
      "Iteration 507, loss = 3.12991873\n",
      "Iteration 508, loss = 3.13024137\n",
      "Iteration 509, loss = 3.13012319\n",
      "Iteration 510, loss = 3.12939253\n",
      "Iteration 511, loss = 3.13057707\n",
      "Iteration 512, loss = 3.12967026\n",
      "Iteration 513, loss = 3.13166331\n",
      "Iteration 514, loss = 3.12911429\n",
      "Iteration 515, loss = 3.12948758\n",
      "Iteration 516, loss = 3.12969666\n",
      "Iteration 517, loss = 3.12861847\n",
      "Iteration 518, loss = 3.12878235\n",
      "Iteration 519, loss = 3.12955750\n",
      "Iteration 520, loss = 3.12829871\n",
      "Iteration 521, loss = 3.12861766\n",
      "Iteration 522, loss = 3.12878354\n",
      "Iteration 523, loss = 3.12808527\n",
      "Iteration 524, loss = 3.12786761\n",
      "Iteration 525, loss = 3.12806646\n",
      "Iteration 526, loss = 3.12872017\n",
      "Iteration 527, loss = 3.12771272\n",
      "Iteration 528, loss = 3.12895259\n",
      "Iteration 529, loss = 3.12907259\n",
      "Iteration 530, loss = 3.12923139\n",
      "Iteration 531, loss = 3.12822456\n",
      "Iteration 532, loss = 3.12810051\n",
      "Iteration 533, loss = 3.12836394\n",
      "Iteration 534, loss = 3.12774382\n",
      "Iteration 535, loss = 3.12721184\n",
      "Iteration 536, loss = 3.12729093\n",
      "Iteration 537, loss = 3.12694115\n",
      "Iteration 538, loss = 3.12632729\n",
      "Iteration 539, loss = 3.12670492\n",
      "Iteration 540, loss = 3.12761501\n",
      "Iteration 541, loss = 3.12705914\n",
      "Iteration 542, loss = 3.12597113\n",
      "Iteration 543, loss = 3.12690507\n",
      "Iteration 544, loss = 3.12574175\n",
      "Iteration 545, loss = 3.12653263\n",
      "Iteration 546, loss = 3.12688903\n",
      "Iteration 547, loss = 3.12429652\n",
      "Iteration 548, loss = 3.12620520\n",
      "Iteration 549, loss = 3.12546607\n",
      "Iteration 550, loss = 3.12690738\n",
      "Iteration 551, loss = 3.12467450\n",
      "Iteration 552, loss = 3.12473405\n",
      "Iteration 553, loss = 3.12464247\n",
      "Iteration 554, loss = 3.12543002\n",
      "Iteration 555, loss = 3.12548196\n",
      "Iteration 556, loss = 3.12495492\n",
      "Iteration 557, loss = 3.12449580\n",
      "Iteration 558, loss = 3.12404648\n",
      "Iteration 559, loss = 3.12389234\n",
      "Iteration 560, loss = 3.12353729\n",
      "Iteration 561, loss = 3.12363707\n",
      "Iteration 562, loss = 3.12405947\n",
      "Iteration 563, loss = 3.12365482\n",
      "Iteration 564, loss = 3.12384862\n",
      "Iteration 565, loss = 3.12365088\n",
      "Iteration 566, loss = 3.12357720\n",
      "Iteration 567, loss = 3.12282320\n",
      "Iteration 568, loss = 3.12342758\n",
      "Iteration 569, loss = 3.12284546\n",
      "Iteration 570, loss = 3.12305390\n",
      "Iteration 571, loss = 3.12283315\n",
      "Iteration 572, loss = 3.12308012\n",
      "Iteration 573, loss = 3.12317007\n",
      "Iteration 574, loss = 3.12292738\n",
      "Iteration 575, loss = 3.12257978\n",
      "Iteration 576, loss = 3.12334755\n",
      "Iteration 577, loss = 3.12274400\n",
      "Iteration 578, loss = 3.12239874\n",
      "Iteration 579, loss = 3.12259314\n",
      "Iteration 580, loss = 3.12431173\n",
      "Iteration 581, loss = 3.12231964\n",
      "Iteration 582, loss = 3.12212837\n",
      "Iteration 583, loss = 3.12156650\n",
      "Iteration 584, loss = 3.12154967\n",
      "Iteration 585, loss = 3.12232672\n",
      "Iteration 586, loss = 3.12304578\n",
      "Iteration 587, loss = 3.12166109\n",
      "Iteration 588, loss = 3.12117501\n",
      "Iteration 589, loss = 3.12138909\n",
      "Iteration 590, loss = 3.12098807\n",
      "Iteration 591, loss = 3.12063852\n",
      "Iteration 592, loss = 3.12076166\n",
      "Iteration 593, loss = 3.12144976\n",
      "Iteration 594, loss = 3.12163033\n",
      "Iteration 595, loss = 3.12041513\n",
      "Iteration 596, loss = 3.12122500\n",
      "Iteration 597, loss = 3.12175765\n",
      "Iteration 598, loss = 3.12056156\n",
      "Iteration 599, loss = 3.12053625\n",
      "Iteration 600, loss = 3.12101939\n",
      "Iteration 601, loss = 3.12020829\n",
      "Iteration 602, loss = 3.12011815\n",
      "Iteration 603, loss = 3.12032383\n",
      "Iteration 604, loss = 3.12035370\n",
      "Iteration 605, loss = 3.12006854\n",
      "Iteration 606, loss = 3.11960282\n",
      "Iteration 607, loss = 3.11935231\n",
      "Iteration 608, loss = 3.12015951\n",
      "Iteration 609, loss = 3.12000091\n",
      "Iteration 610, loss = 3.11961784\n",
      "Iteration 611, loss = 3.11919347\n",
      "Iteration 612, loss = 3.11876045\n",
      "Iteration 613, loss = 3.11858706\n",
      "Iteration 614, loss = 3.11815409\n",
      "Iteration 615, loss = 3.11952131\n",
      "Iteration 616, loss = 3.11904459\n",
      "Iteration 617, loss = 3.11867268\n",
      "Iteration 618, loss = 3.11846373\n",
      "Iteration 619, loss = 3.11831906\n",
      "Iteration 620, loss = 3.11831911\n",
      "Iteration 621, loss = 3.11787960\n",
      "Iteration 622, loss = 3.11755586\n",
      "Iteration 623, loss = 3.11794828\n",
      "Iteration 624, loss = 3.11902113\n",
      "Iteration 625, loss = 3.11906541\n",
      "Iteration 626, loss = 3.11819154\n",
      "Iteration 627, loss = 3.11699832\n",
      "Iteration 628, loss = 3.12068211\n",
      "Iteration 629, loss = 3.12012566\n",
      "Iteration 630, loss = 3.11699900\n",
      "Iteration 631, loss = 3.11913520\n",
      "Iteration 632, loss = 3.11873123\n",
      "Iteration 633, loss = 3.11728347\n",
      "Iteration 634, loss = 3.11844118\n",
      "Iteration 635, loss = 3.11735861\n",
      "Iteration 636, loss = 3.11666517\n",
      "Iteration 637, loss = 3.11616615\n",
      "Iteration 638, loss = 3.11604026\n",
      "Iteration 639, loss = 3.11661040\n",
      "Iteration 640, loss = 3.11709238\n",
      "Iteration 641, loss = 3.11752985\n",
      "Iteration 642, loss = 3.11595193\n",
      "Iteration 643, loss = 3.11692000\n",
      "Iteration 644, loss = 3.11745944\n",
      "Iteration 645, loss = 3.11680266\n",
      "Iteration 646, loss = 3.11572234\n",
      "Iteration 647, loss = 3.11692106\n",
      "Iteration 648, loss = 3.11638273\n",
      "Iteration 649, loss = 3.11550251\n",
      "Iteration 650, loss = 3.11515673\n",
      "Iteration 651, loss = 3.11541864\n",
      "Iteration 652, loss = 3.11442315\n",
      "Iteration 653, loss = 3.11452671\n",
      "Iteration 654, loss = 3.11458514\n",
      "Iteration 655, loss = 3.11434540\n",
      "Iteration 656, loss = 3.11426500\n",
      "Iteration 657, loss = 3.11482317\n",
      "Iteration 658, loss = 3.11430903\n",
      "Iteration 659, loss = 3.11402301\n",
      "Iteration 660, loss = 3.11394041\n",
      "Iteration 661, loss = 3.11450153\n",
      "Iteration 662, loss = 3.11468855\n",
      "Iteration 663, loss = 3.11416591\n",
      "Iteration 664, loss = 3.11394281\n",
      "Iteration 665, loss = 3.11405831\n",
      "Iteration 666, loss = 3.11454316\n",
      "Iteration 667, loss = 3.11480707\n",
      "Iteration 668, loss = 3.11376999\n",
      "Iteration 669, loss = 3.11331443\n",
      "Iteration 670, loss = 3.11381470\n",
      "Iteration 671, loss = 3.11369443\n",
      "Iteration 672, loss = 3.11401726\n",
      "Iteration 673, loss = 3.11324437\n",
      "Iteration 674, loss = 3.11296967\n",
      "Iteration 675, loss = 3.11311261\n",
      "Iteration 676, loss = 3.11340042\n",
      "Iteration 677, loss = 3.11315551\n",
      "Iteration 678, loss = 3.11293316\n",
      "Iteration 679, loss = 3.11313869\n",
      "Iteration 680, loss = 3.11481770\n",
      "Iteration 681, loss = 3.11495833\n",
      "Iteration 682, loss = 3.11279882\n",
      "Iteration 683, loss = 3.11285385\n",
      "Iteration 684, loss = 3.11292908\n",
      "Iteration 685, loss = 3.11257869\n",
      "Iteration 686, loss = 3.11248943\n",
      "Iteration 687, loss = 3.11229151\n",
      "Iteration 688, loss = 3.11288767\n",
      "Iteration 689, loss = 3.11234947\n",
      "Iteration 690, loss = 3.11222927\n",
      "Iteration 691, loss = 3.11187670\n",
      "Iteration 692, loss = 3.11268858\n",
      "Iteration 693, loss = 3.11304102\n",
      "Iteration 694, loss = 3.11206843\n",
      "Iteration 695, loss = 3.11243061\n",
      "Iteration 696, loss = 3.11192814\n",
      "Iteration 697, loss = 3.11216741\n",
      "Iteration 698, loss = 3.11205888\n",
      "Iteration 699, loss = 3.11158540\n",
      "Iteration 700, loss = 3.11240155\n",
      "Iteration 701, loss = 3.11199796\n",
      "Iteration 702, loss = 3.11166123\n",
      "Iteration 703, loss = 3.11245140\n",
      "Iteration 704, loss = 3.11152379\n",
      "Iteration 705, loss = 3.11156796\n",
      "Iteration 706, loss = 3.11133328\n",
      "Iteration 707, loss = 3.11172336\n",
      "Iteration 708, loss = 3.11104295\n",
      "Iteration 709, loss = 3.11136439\n",
      "Iteration 710, loss = 3.11222818\n",
      "Iteration 711, loss = 3.11128418\n",
      "Iteration 712, loss = 3.11185884\n",
      "Iteration 713, loss = 3.11111373\n",
      "Iteration 714, loss = 3.11015013\n",
      "Iteration 715, loss = 3.11047720\n",
      "Iteration 716, loss = 3.11039794\n",
      "Iteration 717, loss = 3.10969439\n",
      "Iteration 718, loss = 3.11076363\n",
      "Iteration 719, loss = 3.11004717\n",
      "Iteration 720, loss = 3.10960569\n",
      "Iteration 721, loss = 3.11014763\n",
      "Iteration 722, loss = 3.10914460\n",
      "Iteration 723, loss = 3.10884269\n",
      "Iteration 724, loss = 3.10925193\n",
      "Iteration 725, loss = 3.10910439\n",
      "Iteration 726, loss = 3.11050412\n",
      "Iteration 727, loss = 3.11015799\n",
      "Iteration 728, loss = 3.10947786\n",
      "Iteration 729, loss = 3.10970382\n",
      "Iteration 730, loss = 3.11010402\n",
      "Iteration 731, loss = 3.10952618\n",
      "Iteration 732, loss = 3.10947746\n",
      "Iteration 733, loss = 3.10908314\n",
      "Iteration 734, loss = 3.10921832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15138187\n",
      "Iteration 2, loss = 4.02127284\n",
      "Iteration 3, loss = 3.90242155\n",
      "Iteration 4, loss = 3.78846226\n",
      "Iteration 5, loss = 3.67917764\n",
      "Iteration 6, loss = 3.57516171\n",
      "Iteration 7, loss = 3.48202228\n",
      "Iteration 8, loss = 3.40036254\n",
      "Iteration 9, loss = 3.33338593\n",
      "Iteration 10, loss = 3.28578782\n",
      "Iteration 11, loss = 3.25657418\n",
      "Iteration 12, loss = 3.24063288\n",
      "Iteration 13, loss = 3.23509827\n",
      "Iteration 14, loss = 3.23230660\n",
      "Iteration 15, loss = 3.23098253\n",
      "Iteration 16, loss = 3.22919594\n",
      "Iteration 17, loss = 3.22771867\n",
      "Iteration 18, loss = 3.22600578\n",
      "Iteration 19, loss = 3.22561351\n",
      "Iteration 20, loss = 3.22505637\n",
      "Iteration 21, loss = 3.22492881\n",
      "Iteration 22, loss = 3.22415977\n",
      "Iteration 23, loss = 3.22384519\n",
      "Iteration 24, loss = 3.22294835\n",
      "Iteration 25, loss = 3.22300036\n",
      "Iteration 26, loss = 3.22192672\n",
      "Iteration 27, loss = 3.22208991\n",
      "Iteration 28, loss = 3.22155517\n",
      "Iteration 29, loss = 3.22217426\n",
      "Iteration 30, loss = 3.22147459\n",
      "Iteration 31, loss = 3.22102205\n",
      "Iteration 32, loss = 3.22107087\n",
      "Iteration 33, loss = 3.22050837\n",
      "Iteration 34, loss = 3.22012407\n",
      "Iteration 35, loss = 3.21994011\n",
      "Iteration 36, loss = 3.21920099\n",
      "Iteration 37, loss = 3.21856412\n",
      "Iteration 38, loss = 3.21804162\n",
      "Iteration 39, loss = 3.21771484\n",
      "Iteration 40, loss = 3.21750289\n",
      "Iteration 41, loss = 3.21744978\n",
      "Iteration 42, loss = 3.21709968\n",
      "Iteration 43, loss = 3.21656496\n",
      "Iteration 44, loss = 3.21648487\n",
      "Iteration 45, loss = 3.21620405\n",
      "Iteration 46, loss = 3.21594483\n",
      "Iteration 47, loss = 3.21565058\n",
      "Iteration 48, loss = 3.21559185\n",
      "Iteration 49, loss = 3.21507416\n",
      "Iteration 50, loss = 3.21512124\n",
      "Iteration 51, loss = 3.21523364\n",
      "Iteration 52, loss = 3.21476481\n",
      "Iteration 53, loss = 3.21406169\n",
      "Iteration 54, loss = 3.21396567\n",
      "Iteration 55, loss = 3.21431828\n",
      "Iteration 56, loss = 3.21377940\n",
      "Iteration 57, loss = 3.21360660\n",
      "Iteration 58, loss = 3.21317362\n",
      "Iteration 59, loss = 3.21311924\n",
      "Iteration 60, loss = 3.21283645\n",
      "Iteration 61, loss = 3.21275199\n",
      "Iteration 62, loss = 3.21193128\n",
      "Iteration 63, loss = 3.21166799\n",
      "Iteration 64, loss = 3.21137068\n",
      "Iteration 65, loss = 3.21156871\n",
      "Iteration 66, loss = 3.21074841\n",
      "Iteration 67, loss = 3.21092518\n",
      "Iteration 68, loss = 3.21138209\n",
      "Iteration 69, loss = 3.21119725\n",
      "Iteration 70, loss = 3.21149177\n",
      "Iteration 71, loss = 3.21078032\n",
      "Iteration 72, loss = 3.21005933\n",
      "Iteration 73, loss = 3.20975219\n",
      "Iteration 74, loss = 3.20925870\n",
      "Iteration 75, loss = 3.20824568\n",
      "Iteration 76, loss = 3.20798251\n",
      "Iteration 77, loss = 3.20763447\n",
      "Iteration 78, loss = 3.20802493\n",
      "Iteration 79, loss = 3.20749020\n",
      "Iteration 80, loss = 3.20713910\n",
      "Iteration 81, loss = 3.20706625\n",
      "Iteration 82, loss = 3.20704041\n",
      "Iteration 83, loss = 3.20694805\n",
      "Iteration 84, loss = 3.20663308\n",
      "Iteration 85, loss = 3.20646367\n",
      "Iteration 86, loss = 3.20690110\n",
      "Iteration 87, loss = 3.20624153\n",
      "Iteration 88, loss = 3.20614138\n",
      "Iteration 89, loss = 3.20636774\n",
      "Iteration 90, loss = 3.20667828\n",
      "Iteration 91, loss = 3.20594206\n",
      "Iteration 92, loss = 3.20533460\n",
      "Iteration 93, loss = 3.20458058\n",
      "Iteration 94, loss = 3.20421678\n",
      "Iteration 95, loss = 3.20366580\n",
      "Iteration 96, loss = 3.20383100\n",
      "Iteration 97, loss = 3.20338709\n",
      "Iteration 98, loss = 3.20328932\n",
      "Iteration 99, loss = 3.20387417\n",
      "Iteration 100, loss = 3.20370797\n",
      "Iteration 101, loss = 3.20308401\n",
      "Iteration 102, loss = 3.20236243\n",
      "Iteration 103, loss = 3.20232033\n",
      "Iteration 104, loss = 3.20245291\n",
      "Iteration 105, loss = 3.20206963\n",
      "Iteration 106, loss = 3.20207220\n",
      "Iteration 107, loss = 3.20125050\n",
      "Iteration 108, loss = 3.20140534\n",
      "Iteration 109, loss = 3.20083116\n",
      "Iteration 110, loss = 3.20035741\n",
      "Iteration 111, loss = 3.20027518\n",
      "Iteration 112, loss = 3.20001543\n",
      "Iteration 113, loss = 3.20008387\n",
      "Iteration 114, loss = 3.19994571\n",
      "Iteration 115, loss = 3.20031313\n",
      "Iteration 116, loss = 3.19997375\n",
      "Iteration 117, loss = 3.20008373\n",
      "Iteration 118, loss = 3.19958961\n",
      "Iteration 119, loss = 3.19904074\n",
      "Iteration 120, loss = 3.19895493\n",
      "Iteration 121, loss = 3.19891651\n",
      "Iteration 122, loss = 3.19960398\n",
      "Iteration 123, loss = 3.19883449\n",
      "Iteration 124, loss = 3.19876017\n",
      "Iteration 125, loss = 3.19763752\n",
      "Iteration 126, loss = 3.19787293\n",
      "Iteration 127, loss = 3.19756030\n",
      "Iteration 128, loss = 3.19673174\n",
      "Iteration 129, loss = 3.19653466\n",
      "Iteration 130, loss = 3.19656437\n",
      "Iteration 131, loss = 3.19657123\n",
      "Iteration 132, loss = 3.19660604\n",
      "Iteration 133, loss = 3.19651347\n",
      "Iteration 134, loss = 3.19719972\n",
      "Iteration 135, loss = 3.19619471\n",
      "Iteration 136, loss = 3.19595136\n",
      "Iteration 137, loss = 3.19572971\n",
      "Iteration 138, loss = 3.19528238\n",
      "Iteration 139, loss = 3.19530495\n",
      "Iteration 140, loss = 3.19464451\n",
      "Iteration 141, loss = 3.19411247\n",
      "Iteration 142, loss = 3.19416714\n",
      "Iteration 143, loss = 3.19406961\n",
      "Iteration 144, loss = 3.19380490\n",
      "Iteration 145, loss = 3.19318110\n",
      "Iteration 146, loss = 3.19341216\n",
      "Iteration 147, loss = 3.19341517\n",
      "Iteration 148, loss = 3.19324445\n",
      "Iteration 149, loss = 3.19231954\n",
      "Iteration 150, loss = 3.19221301\n",
      "Iteration 151, loss = 3.19225205\n",
      "Iteration 152, loss = 3.19197786\n",
      "Iteration 153, loss = 3.19185660\n",
      "Iteration 154, loss = 3.19260505\n",
      "Iteration 155, loss = 3.19195048\n",
      "Iteration 156, loss = 3.19189879\n",
      "Iteration 157, loss = 3.19168368\n",
      "Iteration 158, loss = 3.19142921\n",
      "Iteration 159, loss = 3.19121624\n",
      "Iteration 160, loss = 3.19117252\n",
      "Iteration 161, loss = 3.19072690\n",
      "Iteration 162, loss = 3.19063094\n",
      "Iteration 163, loss = 3.19042406\n",
      "Iteration 164, loss = 3.19002563\n",
      "Iteration 165, loss = 3.18966477\n",
      "Iteration 166, loss = 3.18945283\n",
      "Iteration 167, loss = 3.18968173\n",
      "Iteration 168, loss = 3.18912280\n",
      "Iteration 169, loss = 3.18903691\n",
      "Iteration 170, loss = 3.18904108\n",
      "Iteration 171, loss = 3.18859803\n",
      "Iteration 172, loss = 3.18828849\n",
      "Iteration 173, loss = 3.18845549\n",
      "Iteration 174, loss = 3.18796428\n",
      "Iteration 175, loss = 3.18748855\n",
      "Iteration 176, loss = 3.18820467\n",
      "Iteration 177, loss = 3.18771015\n",
      "Iteration 178, loss = 3.18760517\n",
      "Iteration 179, loss = 3.18705107\n",
      "Iteration 180, loss = 3.18673525\n",
      "Iteration 181, loss = 3.18715184\n",
      "Iteration 182, loss = 3.18665406\n",
      "Iteration 183, loss = 3.18629205\n",
      "Iteration 184, loss = 3.18701328\n",
      "Iteration 185, loss = 3.18743238\n",
      "Iteration 186, loss = 3.18617726\n",
      "Iteration 187, loss = 3.18607587\n",
      "Iteration 188, loss = 3.18797097\n",
      "Iteration 189, loss = 3.18584639\n",
      "Iteration 190, loss = 3.18593835\n",
      "Iteration 191, loss = 3.18621904\n",
      "Iteration 192, loss = 3.18553802\n",
      "Iteration 193, loss = 3.18549921\n",
      "Iteration 194, loss = 3.18472412\n",
      "Iteration 195, loss = 3.18421077\n",
      "Iteration 196, loss = 3.18439908\n",
      "Iteration 197, loss = 3.18403731\n",
      "Iteration 198, loss = 3.18368278\n",
      "Iteration 199, loss = 3.18362098\n",
      "Iteration 200, loss = 3.18307724\n",
      "Iteration 201, loss = 3.18468497\n",
      "Iteration 202, loss = 3.18460525\n",
      "Iteration 203, loss = 3.18408961\n",
      "Iteration 204, loss = 3.18401095\n",
      "Iteration 205, loss = 3.18347823\n",
      "Iteration 206, loss = 3.18236013\n",
      "Iteration 207, loss = 3.18327262\n",
      "Iteration 208, loss = 3.18257059\n",
      "Iteration 209, loss = 3.18212599\n",
      "Iteration 210, loss = 3.18183171\n",
      "Iteration 211, loss = 3.18135275\n",
      "Iteration 212, loss = 3.18074738\n",
      "Iteration 213, loss = 3.18137703\n",
      "Iteration 214, loss = 3.18118417\n",
      "Iteration 215, loss = 3.18132392\n",
      "Iteration 216, loss = 3.18056101\n",
      "Iteration 217, loss = 3.18034679\n",
      "Iteration 218, loss = 3.18018933\n",
      "Iteration 219, loss = 3.17989239\n",
      "Iteration 220, loss = 3.18015765\n",
      "Iteration 221, loss = 3.17976024\n",
      "Iteration 222, loss = 3.17991687\n",
      "Iteration 223, loss = 3.17936269\n",
      "Iteration 224, loss = 3.17969762\n",
      "Iteration 225, loss = 3.17938355\n",
      "Iteration 226, loss = 3.17929732\n",
      "Iteration 227, loss = 3.17961031\n",
      "Iteration 228, loss = 3.17927308\n",
      "Iteration 229, loss = 3.17909869\n",
      "Iteration 230, loss = 3.17861991\n",
      "Iteration 231, loss = 3.17878370\n",
      "Iteration 232, loss = 3.17858878\n",
      "Iteration 233, loss = 3.17810871\n",
      "Iteration 234, loss = 3.17833140\n",
      "Iteration 235, loss = 3.17731493\n",
      "Iteration 236, loss = 3.17761752\n",
      "Iteration 237, loss = 3.17762501\n",
      "Iteration 238, loss = 3.17810288\n",
      "Iteration 239, loss = 3.17787003\n",
      "Iteration 240, loss = 3.17810345\n",
      "Iteration 241, loss = 3.17713325\n",
      "Iteration 242, loss = 3.17644263\n",
      "Iteration 243, loss = 3.17725661\n",
      "Iteration 244, loss = 3.17864178\n",
      "Iteration 245, loss = 3.17810852\n",
      "Iteration 246, loss = 3.17667942\n",
      "Iteration 247, loss = 3.17650714\n",
      "Iteration 248, loss = 3.17634670\n",
      "Iteration 249, loss = 3.17586524\n",
      "Iteration 250, loss = 3.17584414\n",
      "Iteration 251, loss = 3.17545951\n",
      "Iteration 252, loss = 3.17605284\n",
      "Iteration 253, loss = 3.17588089\n",
      "Iteration 254, loss = 3.17524170\n",
      "Iteration 255, loss = 3.17464639\n",
      "Iteration 256, loss = 3.17493727\n",
      "Iteration 257, loss = 3.17468890\n",
      "Iteration 258, loss = 3.17375502\n",
      "Iteration 259, loss = 3.17395362\n",
      "Iteration 260, loss = 3.17448146\n",
      "Iteration 261, loss = 3.17399480\n",
      "Iteration 262, loss = 3.17410413\n",
      "Iteration 263, loss = 3.17390135\n",
      "Iteration 264, loss = 3.17309729\n",
      "Iteration 265, loss = 3.17254461\n",
      "Iteration 266, loss = 3.17308344\n",
      "Iteration 267, loss = 3.17253703\n",
      "Iteration 268, loss = 3.17289741\n",
      "Iteration 269, loss = 3.17252262\n",
      "Iteration 270, loss = 3.17351971\n",
      "Iteration 271, loss = 3.17323492\n",
      "Iteration 272, loss = 3.17214187\n",
      "Iteration 273, loss = 3.17188742\n",
      "Iteration 274, loss = 3.17228121\n",
      "Iteration 275, loss = 3.17179365\n",
      "Iteration 276, loss = 3.17297724\n",
      "Iteration 277, loss = 3.17180895\n",
      "Iteration 278, loss = 3.17129935\n",
      "Iteration 279, loss = 3.17182317\n",
      "Iteration 280, loss = 3.17219463\n",
      "Iteration 281, loss = 3.17171688\n",
      "Iteration 282, loss = 3.17086317\n",
      "Iteration 283, loss = 3.17046210\n",
      "Iteration 284, loss = 3.17010595\n",
      "Iteration 285, loss = 3.17035513\n",
      "Iteration 286, loss = 3.17029433\n",
      "Iteration 287, loss = 3.17017576\n",
      "Iteration 288, loss = 3.16979521\n",
      "Iteration 289, loss = 3.17007631\n",
      "Iteration 290, loss = 3.16961125\n",
      "Iteration 291, loss = 3.16937979\n",
      "Iteration 292, loss = 3.16973464\n",
      "Iteration 293, loss = 3.16952592\n",
      "Iteration 294, loss = 3.16941287\n",
      "Iteration 295, loss = 3.16847355\n",
      "Iteration 296, loss = 3.16894705\n",
      "Iteration 297, loss = 3.17009401\n",
      "Iteration 298, loss = 3.16921986\n",
      "Iteration 299, loss = 3.16996435\n",
      "Iteration 300, loss = 3.16993022\n",
      "Iteration 301, loss = 3.16837402\n",
      "Iteration 302, loss = 3.17023654\n",
      "Iteration 303, loss = 3.16831366\n",
      "Iteration 304, loss = 3.16873283\n",
      "Iteration 305, loss = 3.17062249\n",
      "Iteration 306, loss = 3.16848361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.15113972\n",
      "Iteration 2, loss = 4.02371370\n",
      "Iteration 3, loss = 3.90247091\n",
      "Iteration 4, loss = 3.78940783\n",
      "Iteration 5, loss = 3.68196393\n",
      "Iteration 6, loss = 3.58024031\n",
      "Iteration 7, loss = 3.48550732\n",
      "Iteration 8, loss = 3.40324734\n",
      "Iteration 9, loss = 3.33725746\n",
      "Iteration 10, loss = 3.29104096\n",
      "Iteration 11, loss = 3.26050785\n",
      "Iteration 12, loss = 3.24635563\n",
      "Iteration 13, loss = 3.24016291\n",
      "Iteration 14, loss = 3.23871706\n",
      "Iteration 15, loss = 3.23735608\n",
      "Iteration 16, loss = 3.23544616\n",
      "Iteration 17, loss = 3.23458151\n",
      "Iteration 18, loss = 3.23323821\n",
      "Iteration 19, loss = 3.23259476\n",
      "Iteration 20, loss = 3.23184457\n",
      "Iteration 21, loss = 3.23123779\n",
      "Iteration 22, loss = 3.23062278\n",
      "Iteration 23, loss = 3.23019371\n",
      "Iteration 24, loss = 3.22975161\n",
      "Iteration 25, loss = 3.22910071\n",
      "Iteration 26, loss = 3.22850149\n",
      "Iteration 27, loss = 3.22791822\n",
      "Iteration 28, loss = 3.22733252\n",
      "Iteration 29, loss = 3.22703678\n",
      "Iteration 30, loss = 3.22629225\n",
      "Iteration 31, loss = 3.22649238\n",
      "Iteration 32, loss = 3.22620085\n",
      "Iteration 33, loss = 3.22567698\n",
      "Iteration 34, loss = 3.22550495\n",
      "Iteration 35, loss = 3.22544641\n",
      "Iteration 36, loss = 3.22539771\n",
      "Iteration 37, loss = 3.22482089\n",
      "Iteration 38, loss = 3.22428916\n",
      "Iteration 39, loss = 3.22436841\n",
      "Iteration 40, loss = 3.22461991\n",
      "Iteration 41, loss = 3.22498175\n",
      "Iteration 42, loss = 3.22393258\n",
      "Iteration 43, loss = 3.22353618\n",
      "Iteration 44, loss = 3.22385489\n",
      "Iteration 45, loss = 3.22329720\n",
      "Iteration 46, loss = 3.22237515\n",
      "Iteration 47, loss = 3.22244230\n",
      "Iteration 48, loss = 3.22157516\n",
      "Iteration 49, loss = 3.22219556\n",
      "Iteration 50, loss = 3.22145113\n",
      "Iteration 51, loss = 3.22153661\n",
      "Iteration 52, loss = 3.22152288\n",
      "Iteration 53, loss = 3.22144322\n",
      "Iteration 54, loss = 3.22149448\n",
      "Iteration 55, loss = 3.22133362\n",
      "Iteration 56, loss = 3.22085755\n",
      "Iteration 57, loss = 3.22004059\n",
      "Iteration 58, loss = 3.21921189\n",
      "Iteration 59, loss = 3.21934185\n",
      "Iteration 60, loss = 3.21921764\n",
      "Iteration 61, loss = 3.21872638\n",
      "Iteration 62, loss = 3.21860872\n",
      "Iteration 63, loss = 3.21865623\n",
      "Iteration 64, loss = 3.21775963\n",
      "Iteration 65, loss = 3.21881440\n",
      "Iteration 66, loss = 3.21894373\n",
      "Iteration 67, loss = 3.21845137\n",
      "Iteration 68, loss = 3.21806130\n",
      "Iteration 69, loss = 3.21660102\n",
      "Iteration 70, loss = 3.21652307\n",
      "Iteration 71, loss = 3.21722550\n",
      "Iteration 72, loss = 3.21721665\n",
      "Iteration 73, loss = 3.21590495\n",
      "Iteration 74, loss = 3.21672216\n",
      "Iteration 75, loss = 3.21694824\n",
      "Iteration 76, loss = 3.21629777\n",
      "Iteration 77, loss = 3.21648690\n",
      "Iteration 78, loss = 3.21627292\n",
      "Iteration 79, loss = 3.21609344\n",
      "Iteration 80, loss = 3.21546335\n",
      "Iteration 81, loss = 3.21430774\n",
      "Iteration 82, loss = 3.21453127\n",
      "Iteration 83, loss = 3.21428854\n",
      "Iteration 84, loss = 3.21403426\n",
      "Iteration 85, loss = 3.21409747\n",
      "Iteration 86, loss = 3.21382163\n",
      "Iteration 87, loss = 3.21375608\n",
      "Iteration 88, loss = 3.21375001\n",
      "Iteration 89, loss = 3.21422070\n",
      "Iteration 90, loss = 3.21380264\n",
      "Iteration 91, loss = 3.21341256\n",
      "Iteration 92, loss = 3.21337248\n",
      "Iteration 93, loss = 3.21315346\n",
      "Iteration 94, loss = 3.21286442\n",
      "Iteration 95, loss = 3.21249146\n",
      "Iteration 96, loss = 3.21225943\n",
      "Iteration 97, loss = 3.21236813\n",
      "Iteration 98, loss = 3.21215680\n",
      "Iteration 99, loss = 3.21173029\n",
      "Iteration 100, loss = 3.21131148\n",
      "Iteration 101, loss = 3.21116228\n",
      "Iteration 102, loss = 3.21071043\n",
      "Iteration 103, loss = 3.21056683\n",
      "Iteration 104, loss = 3.21051510\n",
      "Iteration 105, loss = 3.21033612\n",
      "Iteration 106, loss = 3.21012883\n",
      "Iteration 107, loss = 3.21071956\n",
      "Iteration 108, loss = 3.21102508\n",
      "Iteration 109, loss = 3.21041607\n",
      "Iteration 110, loss = 3.21024123\n",
      "Iteration 111, loss = 3.21042667\n",
      "Iteration 112, loss = 3.21061496\n",
      "Iteration 113, loss = 3.21044325\n",
      "Iteration 114, loss = 3.20951042\n",
      "Iteration 115, loss = 3.20927738\n",
      "Iteration 116, loss = 3.20910522\n",
      "Iteration 117, loss = 3.20880298\n",
      "Iteration 118, loss = 3.20841055\n",
      "Iteration 119, loss = 3.20809995\n",
      "Iteration 120, loss = 3.20739728\n",
      "Iteration 121, loss = 3.20804326\n",
      "Iteration 122, loss = 3.20741362\n",
      "Iteration 123, loss = 3.20758798\n",
      "Iteration 124, loss = 3.20693196\n",
      "Iteration 125, loss = 3.20669685\n",
      "Iteration 126, loss = 3.20660400\n",
      "Iteration 127, loss = 3.20701468\n",
      "Iteration 128, loss = 3.20726661\n",
      "Iteration 129, loss = 3.20627617\n",
      "Iteration 130, loss = 3.20603998\n",
      "Iteration 131, loss = 3.20609172\n",
      "Iteration 132, loss = 3.20541633\n",
      "Iteration 133, loss = 3.20556866\n",
      "Iteration 134, loss = 3.20532408\n",
      "Iteration 135, loss = 3.20516924\n",
      "Iteration 136, loss = 3.20476504\n",
      "Iteration 137, loss = 3.20462558\n",
      "Iteration 138, loss = 3.20437244\n",
      "Iteration 139, loss = 3.20400940\n",
      "Iteration 140, loss = 3.20441746\n",
      "Iteration 141, loss = 3.20405744\n",
      "Iteration 142, loss = 3.20399441\n",
      "Iteration 143, loss = 3.20361186\n",
      "Iteration 144, loss = 3.20312159\n",
      "Iteration 145, loss = 3.20286760\n",
      "Iteration 146, loss = 3.20331811\n",
      "Iteration 147, loss = 3.20210234\n",
      "Iteration 148, loss = 3.20216524\n",
      "Iteration 149, loss = 3.20189816\n",
      "Iteration 150, loss = 3.20172623\n",
      "Iteration 151, loss = 3.20139188\n",
      "Iteration 152, loss = 3.20156653\n",
      "Iteration 153, loss = 3.20165068\n",
      "Iteration 154, loss = 3.20156897\n",
      "Iteration 155, loss = 3.20082669\n",
      "Iteration 156, loss = 3.20086375\n",
      "Iteration 157, loss = 3.20054857\n",
      "Iteration 158, loss = 3.20026452\n",
      "Iteration 159, loss = 3.20017991\n",
      "Iteration 160, loss = 3.20060340\n",
      "Iteration 161, loss = 3.20002800\n",
      "Iteration 162, loss = 3.20030405\n",
      "Iteration 163, loss = 3.19985785\n",
      "Iteration 164, loss = 3.20012064\n",
      "Iteration 165, loss = 3.19950718\n",
      "Iteration 166, loss = 3.19924331\n",
      "Iteration 167, loss = 3.19921676\n",
      "Iteration 168, loss = 3.19890075\n",
      "Iteration 169, loss = 3.19826630\n",
      "Iteration 170, loss = 3.19877350\n",
      "Iteration 171, loss = 3.19803569\n",
      "Iteration 172, loss = 3.19792225\n",
      "Iteration 173, loss = 3.19746131\n",
      "Iteration 174, loss = 3.19748849\n",
      "Iteration 175, loss = 3.19721740\n",
      "Iteration 176, loss = 3.19697603\n",
      "Iteration 177, loss = 3.19643997\n",
      "Iteration 178, loss = 3.19628329\n",
      "Iteration 179, loss = 3.19662040\n",
      "Iteration 180, loss = 3.19689104\n",
      "Iteration 181, loss = 3.19664341\n",
      "Iteration 182, loss = 3.19628786\n",
      "Iteration 183, loss = 3.19607692\n",
      "Iteration 184, loss = 3.19587385\n",
      "Iteration 185, loss = 3.19576912\n",
      "Iteration 186, loss = 3.19594196\n",
      "Iteration 187, loss = 3.19589794\n",
      "Iteration 188, loss = 3.19554611\n",
      "Iteration 189, loss = 3.19486152\n",
      "Iteration 190, loss = 3.19462424\n",
      "Iteration 191, loss = 3.19453764\n",
      "Iteration 192, loss = 3.19425996\n",
      "Iteration 193, loss = 3.19406343\n",
      "Iteration 194, loss = 3.19447808\n",
      "Iteration 195, loss = 3.19378247\n",
      "Iteration 196, loss = 3.19421707\n",
      "Iteration 197, loss = 3.19316092\n",
      "Iteration 198, loss = 3.19344045\n",
      "Iteration 199, loss = 3.19298867\n",
      "Iteration 200, loss = 3.19235114\n",
      "Iteration 201, loss = 3.19211124\n",
      "Iteration 202, loss = 3.19218396\n",
      "Iteration 203, loss = 3.19200361\n",
      "Iteration 204, loss = 3.19226342\n",
      "Iteration 205, loss = 3.19190059\n",
      "Iteration 206, loss = 3.19192057\n",
      "Iteration 207, loss = 3.19138539\n",
      "Iteration 208, loss = 3.19115353\n",
      "Iteration 209, loss = 3.19108483\n",
      "Iteration 210, loss = 3.19072000\n",
      "Iteration 211, loss = 3.19038813\n",
      "Iteration 212, loss = 3.18996536\n",
      "Iteration 213, loss = 3.19034340\n",
      "Iteration 214, loss = 3.19079730\n",
      "Iteration 215, loss = 3.18983508\n",
      "Iteration 216, loss = 3.18962471\n",
      "Iteration 217, loss = 3.18913009\n",
      "Iteration 218, loss = 3.18915158\n",
      "Iteration 219, loss = 3.18868347\n",
      "Iteration 220, loss = 3.18849139\n",
      "Iteration 221, loss = 3.18885824\n",
      "Iteration 222, loss = 3.18937332\n",
      "Iteration 223, loss = 3.18842233\n",
      "Iteration 224, loss = 3.18822047\n",
      "Iteration 225, loss = 3.18778262\n",
      "Iteration 226, loss = 3.18770338\n",
      "Iteration 227, loss = 3.18754612\n",
      "Iteration 228, loss = 3.18683050\n",
      "Iteration 229, loss = 3.18709522\n",
      "Iteration 230, loss = 3.18672927\n",
      "Iteration 231, loss = 3.18684228\n",
      "Iteration 232, loss = 3.18664089\n",
      "Iteration 233, loss = 3.18648025\n",
      "Iteration 234, loss = 3.18628287\n",
      "Iteration 235, loss = 3.18547305\n",
      "Iteration 236, loss = 3.18635808\n",
      "Iteration 237, loss = 3.18626781\n",
      "Iteration 238, loss = 3.18552538\n",
      "Iteration 239, loss = 3.18519452\n",
      "Iteration 240, loss = 3.18501265\n",
      "Iteration 241, loss = 3.18461034\n",
      "Iteration 242, loss = 3.18460772\n",
      "Iteration 243, loss = 3.18462119\n",
      "Iteration 244, loss = 3.18458757\n",
      "Iteration 245, loss = 3.18432321\n",
      "Iteration 246, loss = 3.18407898\n",
      "Iteration 247, loss = 3.18373822\n",
      "Iteration 248, loss = 3.18354479\n",
      "Iteration 249, loss = 3.18313195\n",
      "Iteration 250, loss = 3.18340505\n",
      "Iteration 251, loss = 3.18325522\n",
      "Iteration 252, loss = 3.18329332\n",
      "Iteration 253, loss = 3.18210907\n",
      "Iteration 254, loss = 3.18242265\n",
      "Iteration 255, loss = 3.18236605\n",
      "Iteration 256, loss = 3.18200399\n",
      "Iteration 257, loss = 3.18198652\n",
      "Iteration 258, loss = 3.18180206\n",
      "Iteration 259, loss = 3.18130169\n",
      "Iteration 260, loss = 3.18109665\n",
      "Iteration 261, loss = 3.18070114\n",
      "Iteration 262, loss = 3.18052122\n",
      "Iteration 263, loss = 3.18108048\n",
      "Iteration 264, loss = 3.18160799\n",
      "Iteration 265, loss = 3.18176215\n",
      "Iteration 266, loss = 3.18025767\n",
      "Iteration 267, loss = 3.18033064\n",
      "Iteration 268, loss = 3.18002585\n",
      "Iteration 269, loss = 3.17946639\n",
      "Iteration 270, loss = 3.17913748\n",
      "Iteration 271, loss = 3.17948150\n",
      "Iteration 272, loss = 3.17894403\n",
      "Iteration 273, loss = 3.17912146\n",
      "Iteration 274, loss = 3.17879668\n",
      "Iteration 275, loss = 3.17909755\n",
      "Iteration 276, loss = 3.17866400\n",
      "Iteration 277, loss = 3.17885337\n",
      "Iteration 278, loss = 3.17862324\n",
      "Iteration 279, loss = 3.17799588\n",
      "Iteration 280, loss = 3.17779511\n",
      "Iteration 281, loss = 3.17752579\n",
      "Iteration 282, loss = 3.17727585\n",
      "Iteration 283, loss = 3.17726950\n",
      "Iteration 284, loss = 3.17644722\n",
      "Iteration 285, loss = 3.17650046\n",
      "Iteration 286, loss = 3.17575960\n",
      "Iteration 287, loss = 3.17526754\n",
      "Iteration 288, loss = 3.17510190\n",
      "Iteration 289, loss = 3.17572495\n",
      "Iteration 290, loss = 3.17613604\n",
      "Iteration 291, loss = 3.17578990\n",
      "Iteration 292, loss = 3.17489040\n",
      "Iteration 293, loss = 3.17493729\n",
      "Iteration 294, loss = 3.17477566\n",
      "Iteration 295, loss = 3.17484243\n",
      "Iteration 296, loss = 3.17391848\n",
      "Iteration 297, loss = 3.17374951\n",
      "Iteration 298, loss = 3.17374159\n",
      "Iteration 299, loss = 3.17308232\n",
      "Iteration 300, loss = 3.17299229\n",
      "Iteration 301, loss = 3.17295391\n",
      "Iteration 302, loss = 3.17261939\n",
      "Iteration 303, loss = 3.17277604\n",
      "Iteration 304, loss = 3.17203528\n",
      "Iteration 305, loss = 3.17231241\n",
      "Iteration 306, loss = 3.17199206\n",
      "Iteration 307, loss = 3.17213177\n",
      "Iteration 308, loss = 3.17194561\n",
      "Iteration 309, loss = 3.17174736\n",
      "Iteration 310, loss = 3.17157454\n",
      "Iteration 311, loss = 3.17211016\n",
      "Iteration 312, loss = 3.17097962\n",
      "Iteration 313, loss = 3.17095185\n",
      "Iteration 314, loss = 3.17064046\n",
      "Iteration 315, loss = 3.17136985\n",
      "Iteration 316, loss = 3.17081604\n",
      "Iteration 317, loss = 3.17048257\n",
      "Iteration 318, loss = 3.17107904\n",
      "Iteration 319, loss = 3.17062954\n",
      "Iteration 320, loss = 3.16953314\n",
      "Iteration 321, loss = 3.16933743\n",
      "Iteration 322, loss = 3.16933404\n",
      "Iteration 323, loss = 3.16936040\n",
      "Iteration 324, loss = 3.16961914\n",
      "Iteration 325, loss = 3.16970549\n",
      "Iteration 326, loss = 3.17033577\n",
      "Iteration 327, loss = 3.16975297\n",
      "Iteration 328, loss = 3.16852296\n",
      "Iteration 329, loss = 3.16921772\n",
      "Iteration 330, loss = 3.16854701\n",
      "Iteration 331, loss = 3.16850227\n",
      "Iteration 332, loss = 3.16792271\n",
      "Iteration 333, loss = 3.16795764\n",
      "Iteration 334, loss = 3.16731153\n",
      "Iteration 335, loss = 3.16793110\n",
      "Iteration 336, loss = 3.16737330\n",
      "Iteration 337, loss = 3.16729464\n",
      "Iteration 338, loss = 3.16696828\n",
      "Iteration 339, loss = 3.16717296\n",
      "Iteration 340, loss = 3.16643487\n",
      "Iteration 341, loss = 3.16654132\n",
      "Iteration 342, loss = 3.16560102\n",
      "Iteration 343, loss = 3.16549664\n",
      "Iteration 344, loss = 3.16517558\n",
      "Iteration 345, loss = 3.16504023\n",
      "Iteration 346, loss = 3.16545552\n",
      "Iteration 347, loss = 3.16493438\n",
      "Iteration 348, loss = 3.16474778\n",
      "Iteration 349, loss = 3.16485631\n",
      "Iteration 350, loss = 3.16436684\n",
      "Iteration 351, loss = 3.16396594\n",
      "Iteration 352, loss = 3.16407111\n",
      "Iteration 353, loss = 3.16427684\n",
      "Iteration 354, loss = 3.16399007\n",
      "Iteration 355, loss = 3.16390801\n",
      "Iteration 356, loss = 3.16350537\n",
      "Iteration 357, loss = 3.16294281\n",
      "Iteration 358, loss = 3.16248310\n",
      "Iteration 359, loss = 3.16272057\n",
      "Iteration 360, loss = 3.16321820\n",
      "Iteration 361, loss = 3.16370760\n",
      "Iteration 362, loss = 3.16213394\n",
      "Iteration 363, loss = 3.16198341\n",
      "Iteration 364, loss = 3.16157481\n",
      "Iteration 365, loss = 3.16107856\n",
      "Iteration 366, loss = 3.16159229\n",
      "Iteration 367, loss = 3.16112783\n",
      "Iteration 368, loss = 3.16102038\n",
      "Iteration 369, loss = 3.16067460\n",
      "Iteration 370, loss = 3.16073020\n",
      "Iteration 371, loss = 3.16014099\n",
      "Iteration 372, loss = 3.16030528\n",
      "Iteration 373, loss = 3.16016257\n",
      "Iteration 374, loss = 3.16003343\n",
      "Iteration 375, loss = 3.15959784\n",
      "Iteration 376, loss = 3.15978203\n",
      "Iteration 377, loss = 3.15983457\n",
      "Iteration 378, loss = 3.15898885\n",
      "Iteration 379, loss = 3.15877753\n",
      "Iteration 380, loss = 3.15853495\n",
      "Iteration 381, loss = 3.15807848\n",
      "Iteration 382, loss = 3.15913103\n",
      "Iteration 383, loss = 3.15890091\n",
      "Iteration 384, loss = 3.15876491\n",
      "Iteration 385, loss = 3.15763063\n",
      "Iteration 386, loss = 3.15755471\n",
      "Iteration 387, loss = 3.15727757\n",
      "Iteration 388, loss = 3.15710841\n",
      "Iteration 389, loss = 3.15665646\n",
      "Iteration 390, loss = 3.15645226\n",
      "Iteration 391, loss = 3.15677943\n",
      "Iteration 392, loss = 3.15590365\n",
      "Iteration 393, loss = 3.15622680\n",
      "Iteration 394, loss = 3.15609178\n",
      "Iteration 395, loss = 3.15552795\n",
      "Iteration 396, loss = 3.15603906\n",
      "Iteration 397, loss = 3.15608276\n",
      "Iteration 398, loss = 3.15566404\n",
      "Iteration 399, loss = 3.15497920\n",
      "Iteration 400, loss = 3.15569633\n",
      "Iteration 401, loss = 3.15520198\n",
      "Iteration 402, loss = 3.15546830\n",
      "Iteration 403, loss = 3.15472333\n",
      "Iteration 404, loss = 3.15476696\n",
      "Iteration 405, loss = 3.15413373\n",
      "Iteration 406, loss = 3.15373365\n",
      "Iteration 407, loss = 3.15393954\n",
      "Iteration 408, loss = 3.15340923\n",
      "Iteration 409, loss = 3.15346225\n",
      "Iteration 410, loss = 3.15433068\n",
      "Iteration 411, loss = 3.15372773\n",
      "Iteration 412, loss = 3.15250177\n",
      "Iteration 413, loss = 3.15241565\n",
      "Iteration 414, loss = 3.15302784\n",
      "Iteration 415, loss = 3.15297005\n",
      "Iteration 416, loss = 3.15215505\n",
      "Iteration 417, loss = 3.15217845\n",
      "Iteration 418, loss = 3.15190402\n",
      "Iteration 419, loss = 3.15153449\n",
      "Iteration 420, loss = 3.15109753\n",
      "Iteration 421, loss = 3.15081319\n",
      "Iteration 422, loss = 3.15163562\n",
      "Iteration 423, loss = 3.15127267\n",
      "Iteration 424, loss = 3.15231525\n",
      "Iteration 425, loss = 3.15159508\n",
      "Iteration 426, loss = 3.15133588\n",
      "Iteration 427, loss = 3.15031825\n",
      "Iteration 428, loss = 3.15077353\n",
      "Iteration 429, loss = 3.15033970\n",
      "Iteration 430, loss = 3.15065734\n",
      "Iteration 431, loss = 3.15029619\n",
      "Iteration 432, loss = 3.15027347\n",
      "Iteration 433, loss = 3.14964564\n",
      "Iteration 434, loss = 3.14860470\n",
      "Iteration 435, loss = 3.14878010\n",
      "Iteration 436, loss = 3.14862910\n",
      "Iteration 437, loss = 3.14870340\n",
      "Iteration 438, loss = 3.14856807\n",
      "Iteration 439, loss = 3.14856393\n",
      "Iteration 440, loss = 3.14810018\n",
      "Iteration 441, loss = 3.14841038\n",
      "Iteration 442, loss = 3.14817632\n",
      "Iteration 443, loss = 3.14764311\n",
      "Iteration 444, loss = 3.14784362\n",
      "Iteration 445, loss = 3.14765316\n",
      "Iteration 446, loss = 3.14679795\n",
      "Iteration 447, loss = 3.14675141\n",
      "Iteration 448, loss = 3.14683968\n",
      "Iteration 449, loss = 3.14718098\n",
      "Iteration 450, loss = 3.14635175\n",
      "Iteration 451, loss = 3.14670867\n",
      "Iteration 452, loss = 3.14662820\n",
      "Iteration 453, loss = 3.14579764\n",
      "Iteration 454, loss = 3.14559739\n",
      "Iteration 455, loss = 3.14590598\n",
      "Iteration 456, loss = 3.14606298\n",
      "Iteration 457, loss = 3.14581477\n",
      "Iteration 458, loss = 3.14586863\n",
      "Iteration 459, loss = 3.14533656\n",
      "Iteration 460, loss = 3.14507123\n",
      "Iteration 461, loss = 3.14484299\n",
      "Iteration 462, loss = 3.14442329\n",
      "Iteration 463, loss = 3.14468183\n",
      "Iteration 464, loss = 3.14432701\n",
      "Iteration 465, loss = 3.14443230\n",
      "Iteration 466, loss = 3.14542273\n",
      "Iteration 467, loss = 3.14541444\n",
      "Iteration 468, loss = 3.14418398\n",
      "Iteration 469, loss = 3.14384604\n",
      "Iteration 470, loss = 3.14287908\n",
      "Iteration 471, loss = 3.14256696\n",
      "Iteration 472, loss = 3.14331088\n",
      "Iteration 473, loss = 3.14288495\n",
      "Iteration 474, loss = 3.14246899\n",
      "Iteration 475, loss = 3.14190956\n",
      "Iteration 476, loss = 3.14221706\n",
      "Iteration 477, loss = 3.14238152\n",
      "Iteration 478, loss = 3.14258504\n",
      "Iteration 479, loss = 3.14252430\n",
      "Iteration 480, loss = 3.14173343\n",
      "Iteration 481, loss = 3.14092036\n",
      "Iteration 482, loss = 3.14085544\n",
      "Iteration 483, loss = 3.14161457\n",
      "Iteration 484, loss = 3.14126718\n",
      "Iteration 485, loss = 3.14117103\n",
      "Iteration 486, loss = 3.14097627\n",
      "Iteration 487, loss = 3.14107119\n",
      "Iteration 488, loss = 3.14086173\n",
      "Iteration 489, loss = 3.14088334\n",
      "Iteration 490, loss = 3.14045550\n",
      "Iteration 491, loss = 3.14071546\n",
      "Iteration 492, loss = 3.14039933\n",
      "Iteration 493, loss = 3.14069497\n",
      "Iteration 494, loss = 3.14097089\n",
      "Iteration 495, loss = 3.13970986\n",
      "Iteration 496, loss = 3.13924521\n",
      "Iteration 497, loss = 3.13928679\n",
      "Iteration 498, loss = 3.13845923\n",
      "Iteration 499, loss = 3.13853323\n",
      "Iteration 500, loss = 3.13941464\n",
      "Iteration 501, loss = 3.13887841\n",
      "Iteration 502, loss = 3.13997513\n",
      "Iteration 503, loss = 3.14027309\n",
      "Iteration 504, loss = 3.13833190\n",
      "Iteration 505, loss = 3.13916311\n",
      "Iteration 506, loss = 3.13954735\n",
      "Iteration 507, loss = 3.13827657\n",
      "Iteration 508, loss = 3.13789357\n",
      "Iteration 509, loss = 3.13778171\n",
      "Iteration 510, loss = 3.13704214\n",
      "Iteration 511, loss = 3.13708550\n",
      "Iteration 512, loss = 3.13828957\n",
      "Iteration 513, loss = 3.13831170\n",
      "Iteration 514, loss = 3.13742910\n",
      "Iteration 515, loss = 3.13679857\n",
      "Iteration 516, loss = 3.13664501\n",
      "Iteration 517, loss = 3.13657142\n",
      "Iteration 518, loss = 3.13735599\n",
      "Iteration 519, loss = 3.13642581\n",
      "Iteration 520, loss = 3.13705670\n",
      "Iteration 521, loss = 3.13683746\n",
      "Iteration 522, loss = 3.13586685\n",
      "Iteration 523, loss = 3.13626304\n",
      "Iteration 524, loss = 3.13578023\n",
      "Iteration 525, loss = 3.13553059\n",
      "Iteration 526, loss = 3.13539109\n",
      "Iteration 527, loss = 3.13592377\n",
      "Iteration 528, loss = 3.13565655\n",
      "Iteration 529, loss = 3.13581607\n",
      "Iteration 530, loss = 3.13510273\n",
      "Iteration 531, loss = 3.13529466\n",
      "Iteration 532, loss = 3.13482046\n",
      "Iteration 533, loss = 3.13427584\n",
      "Iteration 534, loss = 3.13452447\n",
      "Iteration 535, loss = 3.13428574\n",
      "Iteration 536, loss = 3.13341051\n",
      "Iteration 537, loss = 3.13486990\n",
      "Iteration 538, loss = 3.13496651\n",
      "Iteration 539, loss = 3.13442028\n",
      "Iteration 540, loss = 3.13391581\n",
      "Iteration 541, loss = 3.13327343\n",
      "Iteration 542, loss = 3.13342747\n",
      "Iteration 543, loss = 3.13339444\n",
      "Iteration 544, loss = 3.13284068\n",
      "Iteration 545, loss = 3.13347851\n",
      "Iteration 546, loss = 3.13275837\n",
      "Iteration 547, loss = 3.13362737\n",
      "Iteration 548, loss = 3.13233313\n",
      "Iteration 549, loss = 3.13169324\n",
      "Iteration 550, loss = 3.13210694\n",
      "Iteration 551, loss = 3.13226738\n",
      "Iteration 552, loss = 3.13218392\n",
      "Iteration 553, loss = 3.13253543\n",
      "Iteration 554, loss = 3.13217111\n",
      "Iteration 555, loss = 3.13176808\n",
      "Iteration 556, loss = 3.13199230\n",
      "Iteration 557, loss = 3.13140501\n",
      "Iteration 558, loss = 3.13172055\n",
      "Iteration 559, loss = 3.13192261\n",
      "Iteration 560, loss = 3.13127900\n",
      "Iteration 561, loss = 3.13154240\n",
      "Iteration 562, loss = 3.13074747\n",
      "Iteration 563, loss = 3.13094841\n",
      "Iteration 564, loss = 3.13097763\n",
      "Iteration 565, loss = 3.13071700\n",
      "Iteration 566, loss = 3.13042677\n",
      "Iteration 567, loss = 3.12991421\n",
      "Iteration 568, loss = 3.12967675\n",
      "Iteration 569, loss = 3.12953428\n",
      "Iteration 570, loss = 3.12910894\n",
      "Iteration 571, loss = 3.12889950\n",
      "Iteration 572, loss = 3.12880106\n",
      "Iteration 573, loss = 3.12893446\n",
      "Iteration 574, loss = 3.12895387\n",
      "Iteration 575, loss = 3.12806324\n",
      "Iteration 576, loss = 3.12840185\n",
      "Iteration 577, loss = 3.12880086\n",
      "Iteration 578, loss = 3.12876227\n",
      "Iteration 579, loss = 3.12928476\n",
      "Iteration 580, loss = 3.13003398\n",
      "Iteration 581, loss = 3.12893583\n",
      "Iteration 582, loss = 3.12848520\n",
      "Iteration 583, loss = 3.12806899\n",
      "Iteration 584, loss = 3.12854403\n",
      "Iteration 585, loss = 3.12809759\n",
      "Iteration 586, loss = 3.12799857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.12092086\n",
      "Iteration 2, loss = 3.92263848\n",
      "Iteration 3, loss = 3.74339570\n",
      "Iteration 4, loss = 3.57734263\n",
      "Iteration 5, loss = 3.43443718\n",
      "Iteration 6, loss = 3.32560530\n",
      "Iteration 7, loss = 3.26138429\n",
      "Iteration 8, loss = 3.23726227\n",
      "Iteration 9, loss = 3.23966909\n",
      "Iteration 10, loss = 3.23942195\n",
      "Iteration 11, loss = 3.23575719\n",
      "Iteration 12, loss = 3.22981413\n",
      "Iteration 13, loss = 3.22866457\n",
      "Iteration 14, loss = 3.22848830\n",
      "Iteration 15, loss = 3.22799949\n",
      "Iteration 16, loss = 3.22638954\n",
      "Iteration 17, loss = 3.22543821\n",
      "Iteration 18, loss = 3.22490983\n",
      "Iteration 19, loss = 3.22222019\n",
      "Iteration 20, loss = 3.22136344\n",
      "Iteration 21, loss = 3.22166551\n",
      "Iteration 22, loss = 3.22087241\n",
      "Iteration 23, loss = 3.22007129\n",
      "Iteration 24, loss = 3.21992114\n",
      "Iteration 25, loss = 3.21933160\n",
      "Iteration 26, loss = 3.21796059\n",
      "Iteration 27, loss = 3.21709664\n",
      "Iteration 28, loss = 3.21755381\n",
      "Iteration 29, loss = 3.21727392\n",
      "Iteration 30, loss = 3.21612333\n",
      "Iteration 31, loss = 3.21518115\n",
      "Iteration 32, loss = 3.21467745\n",
      "Iteration 33, loss = 3.21441016\n",
      "Iteration 34, loss = 3.21428927\n",
      "Iteration 35, loss = 3.21331798\n",
      "Iteration 36, loss = 3.21335514\n",
      "Iteration 37, loss = 3.21288817\n",
      "Iteration 38, loss = 3.21250679\n",
      "Iteration 39, loss = 3.21232492\n",
      "Iteration 40, loss = 3.21184571\n",
      "Iteration 41, loss = 3.21238984\n",
      "Iteration 42, loss = 3.21418981\n",
      "Iteration 43, loss = 3.21442296\n",
      "Iteration 44, loss = 3.21279213\n",
      "Iteration 45, loss = 3.21234730\n",
      "Iteration 46, loss = 3.21180091\n",
      "Iteration 47, loss = 3.21100808\n",
      "Iteration 48, loss = 3.20999136\n",
      "Iteration 49, loss = 3.20943223\n",
      "Iteration 50, loss = 3.20974611\n",
      "Iteration 51, loss = 3.20977236\n",
      "Iteration 52, loss = 3.20954038\n",
      "Iteration 53, loss = 3.20804289\n",
      "Iteration 54, loss = 3.20704446\n",
      "Iteration 55, loss = 3.20823173\n",
      "Iteration 56, loss = 3.20796179\n",
      "Iteration 57, loss = 3.20817219\n",
      "Iteration 58, loss = 3.20696206\n",
      "Iteration 59, loss = 3.20652645\n",
      "Iteration 60, loss = 3.20619709\n",
      "Iteration 61, loss = 3.20513122\n",
      "Iteration 62, loss = 3.20572564\n",
      "Iteration 63, loss = 3.20568076\n",
      "Iteration 64, loss = 3.20566117\n",
      "Iteration 65, loss = 3.20567024\n",
      "Iteration 66, loss = 3.20507274\n",
      "Iteration 67, loss = 3.20470928\n",
      "Iteration 68, loss = 3.20386726\n",
      "Iteration 69, loss = 3.20448026\n",
      "Iteration 70, loss = 3.20341893\n",
      "Iteration 71, loss = 3.20324287\n",
      "Iteration 72, loss = 3.20332372\n",
      "Iteration 73, loss = 3.20246938\n",
      "Iteration 74, loss = 3.20205433\n",
      "Iteration 75, loss = 3.20167213\n",
      "Iteration 76, loss = 3.20281689\n",
      "Iteration 77, loss = 3.20243652\n",
      "Iteration 78, loss = 3.20147646\n",
      "Iteration 79, loss = 3.20177327\n",
      "Iteration 80, loss = 3.20109216\n",
      "Iteration 81, loss = 3.20168859\n",
      "Iteration 82, loss = 3.20062917\n",
      "Iteration 83, loss = 3.19982789\n",
      "Iteration 84, loss = 3.20117081\n",
      "Iteration 85, loss = 3.19990924\n",
      "Iteration 86, loss = 3.20030266\n",
      "Iteration 87, loss = 3.20085767\n",
      "Iteration 88, loss = 3.20094182\n",
      "Iteration 89, loss = 3.20004726\n",
      "Iteration 90, loss = 3.20027229\n",
      "Iteration 91, loss = 3.19851428\n",
      "Iteration 92, loss = 3.19889801\n",
      "Iteration 93, loss = 3.19934361\n",
      "Iteration 94, loss = 3.19911027\n",
      "Iteration 95, loss = 3.19852883\n",
      "Iteration 96, loss = 3.19823364\n",
      "Iteration 97, loss = 3.19741543\n",
      "Iteration 98, loss = 3.19696929\n",
      "Iteration 99, loss = 3.19688167\n",
      "Iteration 100, loss = 3.19712781\n",
      "Iteration 101, loss = 3.19720343\n",
      "Iteration 102, loss = 3.19563719\n",
      "Iteration 103, loss = 3.19578664\n",
      "Iteration 104, loss = 3.19662721\n",
      "Iteration 105, loss = 3.19677425\n",
      "Iteration 106, loss = 3.19705999\n",
      "Iteration 107, loss = 3.19544324\n",
      "Iteration 108, loss = 3.19488167\n",
      "Iteration 109, loss = 3.19437346\n",
      "Iteration 110, loss = 3.19433173\n",
      "Iteration 111, loss = 3.19430319\n",
      "Iteration 112, loss = 3.19331351\n",
      "Iteration 113, loss = 3.19340537\n",
      "Iteration 114, loss = 3.19333606\n",
      "Iteration 115, loss = 3.19369184\n",
      "Iteration 116, loss = 3.19343856\n",
      "Iteration 117, loss = 3.19276382\n",
      "Iteration 118, loss = 3.19360576\n",
      "Iteration 119, loss = 3.19293304\n",
      "Iteration 120, loss = 3.19228371\n",
      "Iteration 121, loss = 3.19123981\n",
      "Iteration 122, loss = 3.19088455\n",
      "Iteration 123, loss = 3.19100183\n",
      "Iteration 124, loss = 3.19081021\n",
      "Iteration 125, loss = 3.19033787\n",
      "Iteration 126, loss = 3.18997886\n",
      "Iteration 127, loss = 3.18917126\n",
      "Iteration 128, loss = 3.19036504\n",
      "Iteration 129, loss = 3.18971438\n",
      "Iteration 130, loss = 3.18881805\n",
      "Iteration 131, loss = 3.18880430\n",
      "Iteration 132, loss = 3.18906875\n",
      "Iteration 133, loss = 3.18882797\n",
      "Iteration 134, loss = 3.18788976\n",
      "Iteration 135, loss = 3.18817369\n",
      "Iteration 136, loss = 3.18785298\n",
      "Iteration 137, loss = 3.18824958\n",
      "Iteration 138, loss = 3.18826256\n",
      "Iteration 139, loss = 3.18733029\n",
      "Iteration 140, loss = 3.18688894\n",
      "Iteration 141, loss = 3.18654227\n",
      "Iteration 142, loss = 3.18594735\n",
      "Iteration 143, loss = 3.18563166\n",
      "Iteration 144, loss = 3.18580731\n",
      "Iteration 145, loss = 3.18663896\n",
      "Iteration 146, loss = 3.18706180\n",
      "Iteration 147, loss = 3.18589826\n",
      "Iteration 148, loss = 3.18485533\n",
      "Iteration 149, loss = 3.18428639\n",
      "Iteration 150, loss = 3.18400223\n",
      "Iteration 151, loss = 3.18322702\n",
      "Iteration 152, loss = 3.18316376\n",
      "Iteration 153, loss = 3.18267500\n",
      "Iteration 154, loss = 3.18326595\n",
      "Iteration 155, loss = 3.18250339\n",
      "Iteration 156, loss = 3.18196470\n",
      "Iteration 157, loss = 3.18167171\n",
      "Iteration 158, loss = 3.18118402\n",
      "Iteration 159, loss = 3.18265056\n",
      "Iteration 160, loss = 3.18130280\n",
      "Iteration 161, loss = 3.18160802\n",
      "Iteration 162, loss = 3.18037297\n",
      "Iteration 163, loss = 3.17989044\n",
      "Iteration 164, loss = 3.18027917\n",
      "Iteration 165, loss = 3.18019966\n",
      "Iteration 166, loss = 3.18052625\n",
      "Iteration 167, loss = 3.17997565\n",
      "Iteration 168, loss = 3.17995586\n",
      "Iteration 169, loss = 3.17875143\n",
      "Iteration 170, loss = 3.17846128\n",
      "Iteration 171, loss = 3.17839959\n",
      "Iteration 172, loss = 3.17762425\n",
      "Iteration 173, loss = 3.17905976\n",
      "Iteration 174, loss = 3.17747518\n",
      "Iteration 175, loss = 3.17777654\n",
      "Iteration 176, loss = 3.17855162\n",
      "Iteration 177, loss = 3.17743687\n",
      "Iteration 178, loss = 3.17662362\n",
      "Iteration 179, loss = 3.17613559\n",
      "Iteration 180, loss = 3.17750796\n",
      "Iteration 181, loss = 3.17608904\n",
      "Iteration 182, loss = 3.17549804\n",
      "Iteration 183, loss = 3.17505811\n",
      "Iteration 184, loss = 3.17591696\n",
      "Iteration 185, loss = 3.17548697\n",
      "Iteration 186, loss = 3.17579387\n",
      "Iteration 187, loss = 3.17420691\n",
      "Iteration 188, loss = 3.17465177\n",
      "Iteration 189, loss = 3.17571238\n",
      "Iteration 190, loss = 3.17443543\n",
      "Iteration 191, loss = 3.17333464\n",
      "Iteration 192, loss = 3.17401551\n",
      "Iteration 193, loss = 3.17355257\n",
      "Iteration 194, loss = 3.17215527\n",
      "Iteration 195, loss = 3.17215359\n",
      "Iteration 196, loss = 3.17296077\n",
      "Iteration 197, loss = 3.17229869\n",
      "Iteration 198, loss = 3.17134479\n",
      "Iteration 199, loss = 3.17091731\n",
      "Iteration 200, loss = 3.17126843\n",
      "Iteration 201, loss = 3.17143985\n",
      "Iteration 202, loss = 3.17090183\n",
      "Iteration 203, loss = 3.17059751\n",
      "Iteration 204, loss = 3.17048285\n",
      "Iteration 205, loss = 3.17030372\n",
      "Iteration 206, loss = 3.16879181\n",
      "Iteration 207, loss = 3.16843024\n",
      "Iteration 208, loss = 3.16896432\n",
      "Iteration 209, loss = 3.16904410\n",
      "Iteration 210, loss = 3.16864500\n",
      "Iteration 211, loss = 3.16947561\n",
      "Iteration 212, loss = 3.16938328\n",
      "Iteration 213, loss = 3.16810979\n",
      "Iteration 214, loss = 3.16773340\n",
      "Iteration 215, loss = 3.16695344\n",
      "Iteration 216, loss = 3.16685038\n",
      "Iteration 217, loss = 3.16582028\n",
      "Iteration 218, loss = 3.16554098\n",
      "Iteration 219, loss = 3.16601844\n",
      "Iteration 220, loss = 3.16477713\n",
      "Iteration 221, loss = 3.16803321\n",
      "Iteration 222, loss = 3.16793676\n",
      "Iteration 223, loss = 3.16491859\n",
      "Iteration 224, loss = 3.16475441\n",
      "Iteration 225, loss = 3.16443658\n",
      "Iteration 226, loss = 3.16474662\n",
      "Iteration 227, loss = 3.16404423\n",
      "Iteration 228, loss = 3.16427057\n",
      "Iteration 229, loss = 3.16339074\n",
      "Iteration 230, loss = 3.16324010\n",
      "Iteration 231, loss = 3.16296283\n",
      "Iteration 232, loss = 3.16364573\n",
      "Iteration 233, loss = 3.16294304\n",
      "Iteration 234, loss = 3.16391636\n",
      "Iteration 235, loss = 3.16235624\n",
      "Iteration 236, loss = 3.16330799\n",
      "Iteration 237, loss = 3.16282188\n",
      "Iteration 238, loss = 3.16165937\n",
      "Iteration 239, loss = 3.16161915\n",
      "Iteration 240, loss = 3.16160832\n",
      "Iteration 241, loss = 3.16058107\n",
      "Iteration 242, loss = 3.15993369\n",
      "Iteration 243, loss = 3.16009401\n",
      "Iteration 244, loss = 3.15959860\n",
      "Iteration 245, loss = 3.16017262\n",
      "Iteration 246, loss = 3.15945886\n",
      "Iteration 247, loss = 3.15914743\n",
      "Iteration 248, loss = 3.15921252\n",
      "Iteration 249, loss = 3.15956411\n",
      "Iteration 250, loss = 3.15802881\n",
      "Iteration 251, loss = 3.15910616\n",
      "Iteration 252, loss = 3.15842598\n",
      "Iteration 253, loss = 3.15736029\n",
      "Iteration 254, loss = 3.15791010\n",
      "Iteration 255, loss = 3.15799173\n",
      "Iteration 256, loss = 3.15811958\n",
      "Iteration 257, loss = 3.15734101\n",
      "Iteration 258, loss = 3.15713718\n",
      "Iteration 259, loss = 3.15654188\n",
      "Iteration 260, loss = 3.15612097\n",
      "Iteration 261, loss = 3.15618998\n",
      "Iteration 262, loss = 3.15558215\n",
      "Iteration 263, loss = 3.15694021\n",
      "Iteration 264, loss = 3.15708210\n",
      "Iteration 265, loss = 3.15593981\n",
      "Iteration 266, loss = 3.15570353\n",
      "Iteration 267, loss = 3.15573524\n",
      "Iteration 268, loss = 3.15566847\n",
      "Iteration 269, loss = 3.15455383\n",
      "Iteration 270, loss = 3.15412226\n",
      "Iteration 271, loss = 3.15370527\n",
      "Iteration 272, loss = 3.15391313\n",
      "Iteration 273, loss = 3.15372085\n",
      "Iteration 274, loss = 3.15432621\n",
      "Iteration 275, loss = 3.15547532\n",
      "Iteration 276, loss = 3.15492260\n",
      "Iteration 277, loss = 3.15407820\n",
      "Iteration 278, loss = 3.15412929\n",
      "Iteration 279, loss = 3.15438465\n",
      "Iteration 280, loss = 3.15354680\n",
      "Iteration 281, loss = 3.15196960\n",
      "Iteration 282, loss = 3.15145565\n",
      "Iteration 283, loss = 3.15253530\n",
      "Iteration 284, loss = 3.15240538\n",
      "Iteration 285, loss = 3.15134323\n",
      "Iteration 286, loss = 3.15124550\n",
      "Iteration 287, loss = 3.15084583\n",
      "Iteration 288, loss = 3.15026201\n",
      "Iteration 289, loss = 3.15095903\n",
      "Iteration 290, loss = 3.15040701\n",
      "Iteration 291, loss = 3.14980573\n",
      "Iteration 292, loss = 3.15140442\n",
      "Iteration 293, loss = 3.14955536\n",
      "Iteration 294, loss = 3.14938638\n",
      "Iteration 295, loss = 3.15227572\n",
      "Iteration 296, loss = 3.14863280\n",
      "Iteration 297, loss = 3.15099365\n",
      "Iteration 298, loss = 3.15149833\n",
      "Iteration 299, loss = 3.14813623\n",
      "Iteration 300, loss = 3.14804720\n",
      "Iteration 301, loss = 3.14808474\n",
      "Iteration 302, loss = 3.14797419\n",
      "Iteration 303, loss = 3.14750050\n",
      "Iteration 304, loss = 3.14858796\n",
      "Iteration 305, loss = 3.14763424\n",
      "Iteration 306, loss = 3.14631705\n",
      "Iteration 307, loss = 3.14570905\n",
      "Iteration 308, loss = 3.14550983\n",
      "Iteration 309, loss = 3.14708898\n",
      "Iteration 310, loss = 3.14619503\n",
      "Iteration 311, loss = 3.14564523\n",
      "Iteration 312, loss = 3.14540320\n",
      "Iteration 313, loss = 3.14451095\n",
      "Iteration 314, loss = 3.14413586\n",
      "Iteration 315, loss = 3.14683593\n",
      "Iteration 316, loss = 3.14483308\n",
      "Iteration 317, loss = 3.14481109\n",
      "Iteration 318, loss = 3.14417066\n",
      "Iteration 319, loss = 3.14437045\n",
      "Iteration 320, loss = 3.14476777\n",
      "Iteration 321, loss = 3.14404869\n",
      "Iteration 322, loss = 3.14315141\n",
      "Iteration 323, loss = 3.14320590\n",
      "Iteration 324, loss = 3.14296515\n",
      "Iteration 325, loss = 3.14266817\n",
      "Iteration 326, loss = 3.14159510\n",
      "Iteration 327, loss = 3.14246726\n",
      "Iteration 328, loss = 3.14235196\n",
      "Iteration 329, loss = 3.14218258\n",
      "Iteration 330, loss = 3.14228162\n",
      "Iteration 331, loss = 3.14168103\n",
      "Iteration 332, loss = 3.14262081\n",
      "Iteration 333, loss = 3.14206503\n",
      "Iteration 334, loss = 3.14181771\n",
      "Iteration 335, loss = 3.14175472\n",
      "Iteration 336, loss = 3.14097836\n",
      "Iteration 337, loss = 3.14031170\n",
      "Iteration 338, loss = 3.13991344\n",
      "Iteration 339, loss = 3.14033438\n",
      "Iteration 340, loss = 3.14055089\n",
      "Iteration 341, loss = 3.14040834\n",
      "Iteration 342, loss = 3.14127646\n",
      "Iteration 343, loss = 3.13950885\n",
      "Iteration 344, loss = 3.13978326\n",
      "Iteration 345, loss = 3.13946748\n",
      "Iteration 346, loss = 3.13988070\n",
      "Iteration 347, loss = 3.14146533\n",
      "Iteration 348, loss = 3.14025635\n",
      "Iteration 349, loss = 3.13991412\n",
      "Iteration 350, loss = 3.13782953\n",
      "Iteration 351, loss = 3.13925463\n",
      "Iteration 352, loss = 3.13879722\n",
      "Iteration 353, loss = 3.13927907\n",
      "Iteration 354, loss = 3.13773455\n",
      "Iteration 355, loss = 3.13900925\n",
      "Iteration 356, loss = 3.14047261\n",
      "Iteration 357, loss = 3.13780698\n",
      "Iteration 358, loss = 3.13713961\n",
      "Iteration 359, loss = 3.13633561\n",
      "Iteration 360, loss = 3.13644642\n",
      "Iteration 361, loss = 3.13739364\n",
      "Iteration 362, loss = 3.13663169\n",
      "Iteration 363, loss = 3.13599210\n",
      "Iteration 364, loss = 3.13639147\n",
      "Iteration 365, loss = 3.13609495\n",
      "Iteration 366, loss = 3.13539637\n",
      "Iteration 367, loss = 3.13711972\n",
      "Iteration 368, loss = 3.13543492\n",
      "Iteration 369, loss = 3.13583434\n",
      "Iteration 370, loss = 3.13437258\n",
      "Iteration 371, loss = 3.13444620\n",
      "Iteration 372, loss = 3.13425816\n",
      "Iteration 373, loss = 3.13448549\n",
      "Iteration 374, loss = 3.13351608\n",
      "Iteration 375, loss = 3.13599968\n",
      "Iteration 376, loss = 3.13465937\n",
      "Iteration 377, loss = 3.13398190\n",
      "Iteration 378, loss = 3.13386100\n",
      "Iteration 379, loss = 3.13589361\n",
      "Iteration 380, loss = 3.13426888\n",
      "Iteration 381, loss = 3.13270521\n",
      "Iteration 382, loss = 3.13421981\n",
      "Iteration 383, loss = 3.13296950\n",
      "Iteration 384, loss = 3.13186118\n",
      "Iteration 385, loss = 3.13310724\n",
      "Iteration 386, loss = 3.13210156\n",
      "Iteration 387, loss = 3.13237416\n",
      "Iteration 388, loss = 3.13294942\n",
      "Iteration 389, loss = 3.13300883\n",
      "Iteration 390, loss = 3.13189084\n",
      "Iteration 391, loss = 3.13188202\n",
      "Iteration 392, loss = 3.13244998\n",
      "Iteration 393, loss = 3.13220410\n",
      "Iteration 394, loss = 3.13191705\n",
      "Iteration 395, loss = 3.13060143\n",
      "Iteration 396, loss = 3.13052595\n",
      "Iteration 397, loss = 3.13025810\n",
      "Iteration 398, loss = 3.13027736\n",
      "Iteration 399, loss = 3.13031915\n",
      "Iteration 400, loss = 3.13142943\n",
      "Iteration 401, loss = 3.13164149\n",
      "Iteration 402, loss = 3.13072902\n",
      "Iteration 403, loss = 3.13015951\n",
      "Iteration 404, loss = 3.12971266\n",
      "Iteration 405, loss = 3.13043165\n",
      "Iteration 406, loss = 3.12964635\n",
      "Iteration 407, loss = 3.12933883\n",
      "Iteration 408, loss = 3.12982780\n",
      "Iteration 409, loss = 3.12828111\n",
      "Iteration 410, loss = 3.13017091\n",
      "Iteration 411, loss = 3.12997140\n",
      "Iteration 412, loss = 3.12921927\n",
      "Iteration 413, loss = 3.12896848\n",
      "Iteration 414, loss = 3.12843418\n",
      "Iteration 415, loss = 3.12837344\n",
      "Iteration 416, loss = 3.12816253\n",
      "Iteration 417, loss = 3.12854166\n",
      "Iteration 418, loss = 3.12898938\n",
      "Iteration 419, loss = 3.12864537\n",
      "Iteration 420, loss = 3.12769452\n",
      "Iteration 421, loss = 3.12824301\n",
      "Iteration 422, loss = 3.12740556\n",
      "Iteration 423, loss = 3.12638350\n",
      "Iteration 424, loss = 3.12691705\n",
      "Iteration 425, loss = 3.12725299\n",
      "Iteration 426, loss = 3.12600695\n",
      "Iteration 427, loss = 3.12582783\n",
      "Iteration 428, loss = 3.12517893\n",
      "Iteration 429, loss = 3.12502715\n",
      "Iteration 430, loss = 3.12687208\n",
      "Iteration 431, loss = 3.12642725\n",
      "Iteration 432, loss = 3.12571106\n",
      "Iteration 433, loss = 3.12512722\n",
      "Iteration 434, loss = 3.12550344\n",
      "Iteration 435, loss = 3.12566722\n",
      "Iteration 436, loss = 3.12490750\n",
      "Iteration 437, loss = 3.12621813\n",
      "Iteration 438, loss = 3.12433600\n",
      "Iteration 439, loss = 3.12428407\n",
      "Iteration 440, loss = 3.12334539\n",
      "Iteration 441, loss = 3.12593692\n",
      "Iteration 442, loss = 3.12688871\n",
      "Iteration 443, loss = 3.12277316\n",
      "Iteration 444, loss = 3.12337053\n",
      "Iteration 445, loss = 3.12363389\n",
      "Iteration 446, loss = 3.12275531\n",
      "Iteration 447, loss = 3.12261043\n",
      "Iteration 448, loss = 3.12291720\n",
      "Iteration 449, loss = 3.12311607\n",
      "Iteration 450, loss = 3.12316923\n",
      "Iteration 451, loss = 3.12260422\n",
      "Iteration 452, loss = 3.12241658\n",
      "Iteration 453, loss = 3.12237793\n",
      "Iteration 454, loss = 3.12212912\n",
      "Iteration 455, loss = 3.12221355\n",
      "Iteration 456, loss = 3.12267485\n",
      "Iteration 457, loss = 3.12191594\n",
      "Iteration 458, loss = 3.12191702\n",
      "Iteration 459, loss = 3.12146259\n",
      "Iteration 460, loss = 3.12258954\n",
      "Iteration 461, loss = 3.12133201\n",
      "Iteration 462, loss = 3.12151695\n",
      "Iteration 463, loss = 3.12123011\n",
      "Iteration 464, loss = 3.12190649\n",
      "Iteration 465, loss = 3.12230409\n",
      "Iteration 466, loss = 3.12127839\n",
      "Iteration 467, loss = 3.12231501\n",
      "Iteration 468, loss = 3.12125980\n",
      "Iteration 469, loss = 3.12215221\n",
      "Iteration 470, loss = 3.12117576\n",
      "Iteration 471, loss = 3.12046639\n",
      "Iteration 472, loss = 3.12055112\n",
      "Iteration 473, loss = 3.12051930\n",
      "Iteration 474, loss = 3.12071344\n",
      "Iteration 475, loss = 3.11932741\n",
      "Iteration 476, loss = 3.11904682\n",
      "Iteration 477, loss = 3.12099222\n",
      "Iteration 478, loss = 3.11994554\n",
      "Iteration 479, loss = 3.12141884\n",
      "Iteration 480, loss = 3.12206536\n",
      "Iteration 481, loss = 3.11937473\n",
      "Iteration 482, loss = 3.12150376\n",
      "Iteration 483, loss = 3.11991968\n",
      "Iteration 484, loss = 3.11905496\n",
      "Iteration 485, loss = 3.12266980\n",
      "Iteration 486, loss = 3.12085188\n",
      "Iteration 487, loss = 3.11930318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.11966432\n",
      "Iteration 2, loss = 3.92353573\n",
      "Iteration 3, loss = 3.74397034\n",
      "Iteration 4, loss = 3.58001423\n",
      "Iteration 5, loss = 3.43837923\n",
      "Iteration 6, loss = 3.32737427\n",
      "Iteration 7, loss = 3.26690971\n",
      "Iteration 8, loss = 3.24441416\n",
      "Iteration 9, loss = 3.24260871\n",
      "Iteration 10, loss = 3.24064028\n",
      "Iteration 11, loss = 3.23816230\n",
      "Iteration 12, loss = 3.23419015\n",
      "Iteration 13, loss = 3.23235887\n",
      "Iteration 14, loss = 3.23253586\n",
      "Iteration 15, loss = 3.23216781\n",
      "Iteration 16, loss = 3.23171125\n",
      "Iteration 17, loss = 3.23019459\n",
      "Iteration 18, loss = 3.22920032\n",
      "Iteration 19, loss = 3.22799004\n",
      "Iteration 20, loss = 3.22810161\n",
      "Iteration 21, loss = 3.22586331\n",
      "Iteration 22, loss = 3.22513049\n",
      "Iteration 23, loss = 3.22506773\n",
      "Iteration 24, loss = 3.22389765\n",
      "Iteration 25, loss = 3.22353208\n",
      "Iteration 26, loss = 3.22336030\n",
      "Iteration 27, loss = 3.22303019\n",
      "Iteration 28, loss = 3.22167909\n",
      "Iteration 29, loss = 3.22156775\n",
      "Iteration 30, loss = 3.22056350\n",
      "Iteration 31, loss = 3.22043302\n",
      "Iteration 32, loss = 3.21968654\n",
      "Iteration 33, loss = 3.21935136\n",
      "Iteration 34, loss = 3.21834752\n",
      "Iteration 35, loss = 3.21806441\n",
      "Iteration 36, loss = 3.21715442\n",
      "Iteration 37, loss = 3.21619959\n",
      "Iteration 38, loss = 3.21562301\n",
      "Iteration 39, loss = 3.21555799\n",
      "Iteration 40, loss = 3.21521121\n",
      "Iteration 41, loss = 3.21493735\n",
      "Iteration 42, loss = 3.21597675\n",
      "Iteration 43, loss = 3.21532754\n",
      "Iteration 44, loss = 3.21398070\n",
      "Iteration 45, loss = 3.21354889\n",
      "Iteration 46, loss = 3.21287850\n",
      "Iteration 47, loss = 3.21276600\n",
      "Iteration 48, loss = 3.21229888\n",
      "Iteration 49, loss = 3.21242594\n",
      "Iteration 50, loss = 3.21192032\n",
      "Iteration 51, loss = 3.21204255\n",
      "Iteration 52, loss = 3.21135739\n",
      "Iteration 53, loss = 3.21031165\n",
      "Iteration 54, loss = 3.21029334\n",
      "Iteration 55, loss = 3.20996485\n",
      "Iteration 56, loss = 3.21016233\n",
      "Iteration 57, loss = 3.21084729\n",
      "Iteration 58, loss = 3.20970225\n",
      "Iteration 59, loss = 3.20869193\n",
      "Iteration 60, loss = 3.20780449\n",
      "Iteration 61, loss = 3.20798747\n",
      "Iteration 62, loss = 3.20813734\n",
      "Iteration 63, loss = 3.20721944\n",
      "Iteration 64, loss = 3.20660755\n",
      "Iteration 65, loss = 3.20641783\n",
      "Iteration 66, loss = 3.20654148\n",
      "Iteration 67, loss = 3.20595115\n",
      "Iteration 68, loss = 3.20607889\n",
      "Iteration 69, loss = 3.20590501\n",
      "Iteration 70, loss = 3.20665494\n",
      "Iteration 71, loss = 3.20501764\n",
      "Iteration 72, loss = 3.20427837\n",
      "Iteration 73, loss = 3.20337011\n",
      "Iteration 74, loss = 3.20380958\n",
      "Iteration 75, loss = 3.20251248\n",
      "Iteration 76, loss = 3.20244398\n",
      "Iteration 77, loss = 3.20267833\n",
      "Iteration 78, loss = 3.20263879\n",
      "Iteration 79, loss = 3.20201778\n",
      "Iteration 80, loss = 3.20083817\n",
      "Iteration 81, loss = 3.20064402\n",
      "Iteration 82, loss = 3.20060951\n",
      "Iteration 83, loss = 3.19949762\n",
      "Iteration 84, loss = 3.20102893\n",
      "Iteration 85, loss = 3.20127010\n",
      "Iteration 86, loss = 3.20031423\n",
      "Iteration 87, loss = 3.20098787\n",
      "Iteration 88, loss = 3.19993293\n",
      "Iteration 89, loss = 3.19930428\n",
      "Iteration 90, loss = 3.19951933\n",
      "Iteration 91, loss = 3.19889133\n",
      "Iteration 92, loss = 3.19863308\n",
      "Iteration 93, loss = 3.19804456\n",
      "Iteration 94, loss = 3.19864541\n",
      "Iteration 95, loss = 3.19789811\n",
      "Iteration 96, loss = 3.19750486\n",
      "Iteration 97, loss = 3.19708913\n",
      "Iteration 98, loss = 3.19590025\n",
      "Iteration 99, loss = 3.19560308\n",
      "Iteration 100, loss = 3.19618808\n",
      "Iteration 101, loss = 3.19610868\n",
      "Iteration 102, loss = 3.19542844\n",
      "Iteration 103, loss = 3.19531267\n",
      "Iteration 104, loss = 3.19482496\n",
      "Iteration 105, loss = 3.19491786\n",
      "Iteration 106, loss = 3.19414721\n",
      "Iteration 107, loss = 3.19385474\n",
      "Iteration 108, loss = 3.19411484\n",
      "Iteration 109, loss = 3.19372974\n",
      "Iteration 110, loss = 3.19245989\n",
      "Iteration 111, loss = 3.19177711\n",
      "Iteration 112, loss = 3.19140855\n",
      "Iteration 113, loss = 3.19233442\n",
      "Iteration 114, loss = 3.19217254\n",
      "Iteration 115, loss = 3.19324264\n",
      "Iteration 116, loss = 3.19348080\n",
      "Iteration 117, loss = 3.19189192\n",
      "Iteration 118, loss = 3.19127653\n",
      "Iteration 119, loss = 3.19018126\n",
      "Iteration 120, loss = 3.19045145\n",
      "Iteration 121, loss = 3.19009137\n",
      "Iteration 122, loss = 3.18829308\n",
      "Iteration 123, loss = 3.18879616\n",
      "Iteration 124, loss = 3.18911446\n",
      "Iteration 125, loss = 3.18878005\n",
      "Iteration 126, loss = 3.18800281\n",
      "Iteration 127, loss = 3.18733487\n",
      "Iteration 128, loss = 3.18628355\n",
      "Iteration 129, loss = 3.18816643\n",
      "Iteration 130, loss = 3.18665724\n",
      "Iteration 131, loss = 3.18611865\n",
      "Iteration 132, loss = 3.18617561\n",
      "Iteration 133, loss = 3.18593182\n",
      "Iteration 134, loss = 3.18630872\n",
      "Iteration 135, loss = 3.18556124\n",
      "Iteration 136, loss = 3.18537154\n",
      "Iteration 137, loss = 3.18467825\n",
      "Iteration 138, loss = 3.18433968\n",
      "Iteration 139, loss = 3.18446692\n",
      "Iteration 140, loss = 3.18390636\n",
      "Iteration 141, loss = 3.18421571\n",
      "Iteration 142, loss = 3.18287704\n",
      "Iteration 143, loss = 3.18249081\n",
      "Iteration 144, loss = 3.18300964\n",
      "Iteration 145, loss = 3.18227045\n",
      "Iteration 146, loss = 3.18172342\n",
      "Iteration 147, loss = 3.18255927\n",
      "Iteration 148, loss = 3.18249981\n",
      "Iteration 149, loss = 3.18191244\n",
      "Iteration 150, loss = 3.18124276\n",
      "Iteration 151, loss = 3.18070909\n",
      "Iteration 152, loss = 3.18107197\n",
      "Iteration 153, loss = 3.18034245\n",
      "Iteration 154, loss = 3.18018167\n",
      "Iteration 155, loss = 3.17943241\n",
      "Iteration 156, loss = 3.17936055\n",
      "Iteration 157, loss = 3.17873408\n",
      "Iteration 158, loss = 3.17877040\n",
      "Iteration 159, loss = 3.17932926\n",
      "Iteration 160, loss = 3.17810094\n",
      "Iteration 161, loss = 3.17809566\n",
      "Iteration 162, loss = 3.17733754\n",
      "Iteration 163, loss = 3.17744402\n",
      "Iteration 164, loss = 3.17720310\n",
      "Iteration 165, loss = 3.17701205\n",
      "Iteration 166, loss = 3.17661855\n",
      "Iteration 167, loss = 3.17634216\n",
      "Iteration 168, loss = 3.17665033\n",
      "Iteration 169, loss = 3.17524308\n",
      "Iteration 170, loss = 3.17497910\n",
      "Iteration 171, loss = 3.17473897\n",
      "Iteration 172, loss = 3.17511206\n",
      "Iteration 173, loss = 3.17537423\n",
      "Iteration 174, loss = 3.17449630\n",
      "Iteration 175, loss = 3.17458045\n",
      "Iteration 176, loss = 3.17432508\n",
      "Iteration 177, loss = 3.17357793\n",
      "Iteration 178, loss = 3.17336837\n",
      "Iteration 179, loss = 3.17568142\n",
      "Iteration 180, loss = 3.17563139\n",
      "Iteration 181, loss = 3.17292116\n",
      "Iteration 182, loss = 3.17351154\n",
      "Iteration 183, loss = 3.17232259\n",
      "Iteration 184, loss = 3.17146963\n",
      "Iteration 185, loss = 3.17141822\n",
      "Iteration 186, loss = 3.17131271\n",
      "Iteration 187, loss = 3.17071236\n",
      "Iteration 188, loss = 3.17041642\n",
      "Iteration 189, loss = 3.17076257\n",
      "Iteration 190, loss = 3.16998705\n",
      "Iteration 191, loss = 3.16952722\n",
      "Iteration 192, loss = 3.16822410\n",
      "Iteration 193, loss = 3.16849758\n",
      "Iteration 194, loss = 3.16883065\n",
      "Iteration 195, loss = 3.16945794\n",
      "Iteration 196, loss = 3.16898216\n",
      "Iteration 197, loss = 3.16788685\n",
      "Iteration 198, loss = 3.16741610\n",
      "Iteration 199, loss = 3.16770987\n",
      "Iteration 200, loss = 3.16701157\n",
      "Iteration 201, loss = 3.16643335\n",
      "Iteration 202, loss = 3.16622365\n",
      "Iteration 203, loss = 3.16645026\n",
      "Iteration 204, loss = 3.16582387\n",
      "Iteration 205, loss = 3.16639580\n",
      "Iteration 206, loss = 3.16533154\n",
      "Iteration 207, loss = 3.16443952\n",
      "Iteration 208, loss = 3.16471527\n",
      "Iteration 209, loss = 3.16519672\n",
      "Iteration 210, loss = 3.16412802\n",
      "Iteration 211, loss = 3.16432945\n",
      "Iteration 212, loss = 3.16435922\n",
      "Iteration 213, loss = 3.16421946\n",
      "Iteration 214, loss = 3.16350590\n",
      "Iteration 215, loss = 3.16293590\n",
      "Iteration 216, loss = 3.16223953\n",
      "Iteration 217, loss = 3.16300924\n",
      "Iteration 218, loss = 3.16144023\n",
      "Iteration 219, loss = 3.16200586\n",
      "Iteration 220, loss = 3.16134933\n",
      "Iteration 221, loss = 3.16129717\n",
      "Iteration 222, loss = 3.16118162\n",
      "Iteration 223, loss = 3.16125545\n",
      "Iteration 224, loss = 3.16178805\n",
      "Iteration 225, loss = 3.16033332\n",
      "Iteration 226, loss = 3.15951820\n",
      "Iteration 227, loss = 3.15886837\n",
      "Iteration 228, loss = 3.15862737\n",
      "Iteration 229, loss = 3.15861873\n",
      "Iteration 230, loss = 3.15858980\n",
      "Iteration 231, loss = 3.15884407\n",
      "Iteration 232, loss = 3.15874260\n",
      "Iteration 233, loss = 3.15860204\n",
      "Iteration 234, loss = 3.15800744\n",
      "Iteration 235, loss = 3.15718852\n",
      "Iteration 236, loss = 3.15730468\n",
      "Iteration 237, loss = 3.15651583\n",
      "Iteration 238, loss = 3.15650970\n",
      "Iteration 239, loss = 3.15604880\n",
      "Iteration 240, loss = 3.15676212\n",
      "Iteration 241, loss = 3.15572920\n",
      "Iteration 242, loss = 3.15640861\n",
      "Iteration 243, loss = 3.15535442\n",
      "Iteration 244, loss = 3.15410690\n",
      "Iteration 245, loss = 3.15503741\n",
      "Iteration 246, loss = 3.15551402\n",
      "Iteration 247, loss = 3.15476256\n",
      "Iteration 248, loss = 3.15409323\n",
      "Iteration 249, loss = 3.15559920\n",
      "Iteration 250, loss = 3.15559590\n",
      "Iteration 251, loss = 3.15446347\n",
      "Iteration 252, loss = 3.15417901\n",
      "Iteration 253, loss = 3.15429655\n",
      "Iteration 254, loss = 3.15530449\n",
      "Iteration 255, loss = 3.15199495\n",
      "Iteration 256, loss = 3.15209735\n",
      "Iteration 257, loss = 3.15154686\n",
      "Iteration 258, loss = 3.15195957\n",
      "Iteration 259, loss = 3.15069115\n",
      "Iteration 260, loss = 3.15147745\n",
      "Iteration 261, loss = 3.14998349\n",
      "Iteration 262, loss = 3.15043999\n",
      "Iteration 263, loss = 3.14977546\n",
      "Iteration 264, loss = 3.15011719\n",
      "Iteration 265, loss = 3.15004829\n",
      "Iteration 266, loss = 3.14822866\n",
      "Iteration 267, loss = 3.15100071\n",
      "Iteration 268, loss = 3.14886291\n",
      "Iteration 269, loss = 3.14976979\n",
      "Iteration 270, loss = 3.14971023\n",
      "Iteration 271, loss = 3.14810490\n",
      "Iteration 272, loss = 3.14810360\n",
      "Iteration 273, loss = 3.14746804\n",
      "Iteration 274, loss = 3.14622612\n",
      "Iteration 275, loss = 3.14742620\n",
      "Iteration 276, loss = 3.14611896\n",
      "Iteration 277, loss = 3.14703762\n",
      "Iteration 278, loss = 3.14678655\n",
      "Iteration 279, loss = 3.14611953\n",
      "Iteration 280, loss = 3.14598552\n",
      "Iteration 281, loss = 3.14517791\n",
      "Iteration 282, loss = 3.14498554\n",
      "Iteration 283, loss = 3.14528200\n",
      "Iteration 284, loss = 3.14484476\n",
      "Iteration 285, loss = 3.14470573\n",
      "Iteration 286, loss = 3.14515408\n",
      "Iteration 287, loss = 3.14457870\n",
      "Iteration 288, loss = 3.14425212\n",
      "Iteration 289, loss = 3.14337155\n",
      "Iteration 290, loss = 3.14451337\n",
      "Iteration 291, loss = 3.14291660\n",
      "Iteration 292, loss = 3.14289753\n",
      "Iteration 293, loss = 3.14276843\n",
      "Iteration 294, loss = 3.14164945\n",
      "Iteration 295, loss = 3.14188421\n",
      "Iteration 296, loss = 3.14119557\n",
      "Iteration 297, loss = 3.14137671\n",
      "Iteration 298, loss = 3.14114865\n",
      "Iteration 299, loss = 3.14154197\n",
      "Iteration 300, loss = 3.14048366\n",
      "Iteration 301, loss = 3.14027085\n",
      "Iteration 302, loss = 3.14006049\n",
      "Iteration 303, loss = 3.13980376\n",
      "Iteration 304, loss = 3.14053503\n",
      "Iteration 305, loss = 3.14117899\n",
      "Iteration 306, loss = 3.13976491\n",
      "Iteration 307, loss = 3.13960748\n",
      "Iteration 308, loss = 3.13872919\n",
      "Iteration 309, loss = 3.13822849\n",
      "Iteration 310, loss = 3.13938230\n",
      "Iteration 311, loss = 3.13804630\n",
      "Iteration 312, loss = 3.13744092\n",
      "Iteration 313, loss = 3.13971058\n",
      "Iteration 314, loss = 3.13738739\n",
      "Iteration 315, loss = 3.14063303\n",
      "Iteration 316, loss = 3.13806757\n",
      "Iteration 317, loss = 3.13712062\n",
      "Iteration 318, loss = 3.13780726\n",
      "Iteration 319, loss = 3.13628737\n",
      "Iteration 320, loss = 3.13685872\n",
      "Iteration 321, loss = 3.13881035\n",
      "Iteration 322, loss = 3.13719178\n",
      "Iteration 323, loss = 3.13521109\n",
      "Iteration 324, loss = 3.13475756\n",
      "Iteration 325, loss = 3.13345364\n",
      "Iteration 326, loss = 3.13417619\n",
      "Iteration 327, loss = 3.13471600\n",
      "Iteration 328, loss = 3.13480409\n",
      "Iteration 329, loss = 3.13434721\n",
      "Iteration 330, loss = 3.13422402\n",
      "Iteration 331, loss = 3.13286972\n",
      "Iteration 332, loss = 3.13374072\n",
      "Iteration 333, loss = 3.13222340\n",
      "Iteration 334, loss = 3.13307361\n",
      "Iteration 335, loss = 3.13322695\n",
      "Iteration 336, loss = 3.13201175\n",
      "Iteration 337, loss = 3.13255273\n",
      "Iteration 338, loss = 3.13187682\n",
      "Iteration 339, loss = 3.13207806\n",
      "Iteration 340, loss = 3.13279454\n",
      "Iteration 341, loss = 3.13168938\n",
      "Iteration 342, loss = 3.13190138\n",
      "Iteration 343, loss = 3.13154557\n",
      "Iteration 344, loss = 3.13112496\n",
      "Iteration 345, loss = 3.13053162\n",
      "Iteration 346, loss = 3.13024144\n",
      "Iteration 347, loss = 3.13024101\n",
      "Iteration 348, loss = 3.12977663\n",
      "Iteration 349, loss = 3.12992701\n",
      "Iteration 350, loss = 3.13068171\n",
      "Iteration 351, loss = 3.13069386\n",
      "Iteration 352, loss = 3.13006801\n",
      "Iteration 353, loss = 3.12907457\n",
      "Iteration 354, loss = 3.12927293\n",
      "Iteration 355, loss = 3.12898909\n",
      "Iteration 356, loss = 3.12863351\n",
      "Iteration 357, loss = 3.12947708\n",
      "Iteration 358, loss = 3.12831981\n",
      "Iteration 359, loss = 3.12721516\n",
      "Iteration 360, loss = 3.12767893\n",
      "Iteration 361, loss = 3.12720999\n",
      "Iteration 362, loss = 3.12838547\n",
      "Iteration 363, loss = 3.12758590\n",
      "Iteration 364, loss = 3.12700777\n",
      "Iteration 365, loss = 3.12735435\n",
      "Iteration 366, loss = 3.12853818\n",
      "Iteration 367, loss = 3.12915602\n",
      "Iteration 368, loss = 3.12888223\n",
      "Iteration 369, loss = 3.12785392\n",
      "Iteration 370, loss = 3.12599875\n",
      "Iteration 371, loss = 3.12565028\n",
      "Iteration 372, loss = 3.12558787\n",
      "Iteration 373, loss = 3.12467936\n",
      "Iteration 374, loss = 3.12552103\n",
      "Iteration 375, loss = 3.12508764\n",
      "Iteration 376, loss = 3.12630023\n",
      "Iteration 377, loss = 3.12445181\n",
      "Iteration 378, loss = 3.12409241\n",
      "Iteration 379, loss = 3.12546956\n",
      "Iteration 380, loss = 3.12596535\n",
      "Iteration 381, loss = 3.12464611\n",
      "Iteration 382, loss = 3.12417252\n",
      "Iteration 383, loss = 3.12365487\n",
      "Iteration 384, loss = 3.12278864\n",
      "Iteration 385, loss = 3.12259446\n",
      "Iteration 386, loss = 3.12258981\n",
      "Iteration 387, loss = 3.12254307\n",
      "Iteration 388, loss = 3.12302189\n",
      "Iteration 389, loss = 3.12271533\n",
      "Iteration 390, loss = 3.12146760\n",
      "Iteration 391, loss = 3.12170804\n",
      "Iteration 392, loss = 3.12105605\n",
      "Iteration 393, loss = 3.12094860\n",
      "Iteration 394, loss = 3.12035175\n",
      "Iteration 395, loss = 3.12032497\n",
      "Iteration 396, loss = 3.12184161\n",
      "Iteration 397, loss = 3.12169607\n",
      "Iteration 398, loss = 3.12314922\n",
      "Iteration 399, loss = 3.12365039\n",
      "Iteration 400, loss = 3.12153622\n",
      "Iteration 401, loss = 3.12034771\n",
      "Iteration 402, loss = 3.12025994\n",
      "Iteration 403, loss = 3.11973770\n",
      "Iteration 404, loss = 3.11998352\n",
      "Iteration 405, loss = 3.12197781\n",
      "Iteration 406, loss = 3.12247301\n",
      "Iteration 407, loss = 3.11938337\n",
      "Iteration 408, loss = 3.12116273\n",
      "Iteration 409, loss = 3.11861974\n",
      "Iteration 410, loss = 3.11885153\n",
      "Iteration 411, loss = 3.12085429\n",
      "Iteration 412, loss = 3.12101556\n",
      "Iteration 413, loss = 3.11856946\n",
      "Iteration 414, loss = 3.12135837\n",
      "Iteration 415, loss = 3.12361255\n",
      "Iteration 416, loss = 3.11815217\n",
      "Iteration 417, loss = 3.12059896\n",
      "Iteration 418, loss = 3.11864833\n",
      "Iteration 419, loss = 3.11747271\n",
      "Iteration 420, loss = 3.11714044\n",
      "Iteration 421, loss = 3.11673760\n",
      "Iteration 422, loss = 3.11600596\n",
      "Iteration 423, loss = 3.11605074\n",
      "Iteration 424, loss = 3.11546299\n",
      "Iteration 425, loss = 3.11617931\n",
      "Iteration 426, loss = 3.11527811\n",
      "Iteration 427, loss = 3.11634187\n",
      "Iteration 428, loss = 3.11565499\n",
      "Iteration 429, loss = 3.11587990\n",
      "Iteration 430, loss = 3.11549393\n",
      "Iteration 431, loss = 3.11508382\n",
      "Iteration 432, loss = 3.11745833\n",
      "Iteration 433, loss = 3.11568142\n",
      "Iteration 434, loss = 3.11580117\n",
      "Iteration 435, loss = 3.11511296\n",
      "Iteration 436, loss = 3.11434739\n",
      "Iteration 437, loss = 3.11562184\n",
      "Iteration 438, loss = 3.11472820\n",
      "Iteration 439, loss = 3.11616104\n",
      "Iteration 440, loss = 3.11530775\n",
      "Iteration 441, loss = 3.11401382\n",
      "Iteration 442, loss = 3.11331997\n",
      "Iteration 443, loss = 3.11439977\n",
      "Iteration 444, loss = 3.11472231\n",
      "Iteration 445, loss = 3.11475441\n",
      "Iteration 446, loss = 3.11254101\n",
      "Iteration 447, loss = 3.11245867\n",
      "Iteration 448, loss = 3.11293764\n",
      "Iteration 449, loss = 3.11261448\n",
      "Iteration 450, loss = 3.11194976\n",
      "Iteration 451, loss = 3.11329328\n",
      "Iteration 452, loss = 3.11257865\n",
      "Iteration 453, loss = 3.11174775\n",
      "Iteration 454, loss = 3.11179907\n",
      "Iteration 455, loss = 3.11198993\n",
      "Iteration 456, loss = 3.11236475\n",
      "Iteration 457, loss = 3.11277726\n",
      "Iteration 458, loss = 3.11187739\n",
      "Iteration 459, loss = 3.11269078\n",
      "Iteration 460, loss = 3.11225531\n",
      "Iteration 461, loss = 3.11117125\n",
      "Iteration 462, loss = 3.11198175\n",
      "Iteration 463, loss = 3.11163825\n",
      "Iteration 464, loss = 3.11151015\n",
      "Iteration 465, loss = 3.11177713\n",
      "Iteration 466, loss = 3.11086434\n",
      "Iteration 467, loss = 3.11185471\n",
      "Iteration 468, loss = 3.11050170\n",
      "Iteration 469, loss = 3.11039315\n",
      "Iteration 470, loss = 3.11025256\n",
      "Iteration 471, loss = 3.10920928\n",
      "Iteration 472, loss = 3.10950026\n",
      "Iteration 473, loss = 3.10981454\n",
      "Iteration 474, loss = 3.11167883\n",
      "Iteration 475, loss = 3.11051866\n",
      "Iteration 476, loss = 3.10947199\n",
      "Iteration 477, loss = 3.11029681\n",
      "Iteration 478, loss = 3.11151472\n",
      "Iteration 479, loss = 3.11013908\n",
      "Iteration 480, loss = 3.10857251\n",
      "Iteration 481, loss = 3.10894472\n",
      "Iteration 482, loss = 3.10789848\n",
      "Iteration 483, loss = 3.10761913\n",
      "Iteration 484, loss = 3.10751732\n",
      "Iteration 485, loss = 3.10837001\n",
      "Iteration 486, loss = 3.10809524\n",
      "Iteration 487, loss = 3.10722984\n",
      "Iteration 488, loss = 3.10768815\n",
      "Iteration 489, loss = 3.10862670\n",
      "Iteration 490, loss = 3.10806566\n",
      "Iteration 491, loss = 3.10756671\n",
      "Iteration 492, loss = 3.10862832\n",
      "Iteration 493, loss = 3.10846470\n",
      "Iteration 494, loss = 3.10901018\n",
      "Iteration 495, loss = 3.10787235\n",
      "Iteration 496, loss = 3.10790852\n",
      "Iteration 497, loss = 3.10724043\n",
      "Iteration 498, loss = 3.10907194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.12123917\n",
      "Iteration 2, loss = 3.92377013\n",
      "Iteration 3, loss = 3.74388846\n",
      "Iteration 4, loss = 3.57944289\n",
      "Iteration 5, loss = 3.44097638\n",
      "Iteration 6, loss = 3.32976282\n",
      "Iteration 7, loss = 3.26984571\n",
      "Iteration 8, loss = 3.24053077\n",
      "Iteration 9, loss = 3.23589526\n",
      "Iteration 10, loss = 3.23688805\n",
      "Iteration 11, loss = 3.23508373\n",
      "Iteration 12, loss = 3.23324189\n",
      "Iteration 13, loss = 3.23025031\n",
      "Iteration 14, loss = 3.22831062\n",
      "Iteration 15, loss = 3.22691032\n",
      "Iteration 16, loss = 3.22754431\n",
      "Iteration 17, loss = 3.22674178\n",
      "Iteration 18, loss = 3.22546704\n",
      "Iteration 19, loss = 3.22518756\n",
      "Iteration 20, loss = 3.22380890\n",
      "Iteration 21, loss = 3.22390350\n",
      "Iteration 22, loss = 3.22353076\n",
      "Iteration 23, loss = 3.22248966\n",
      "Iteration 24, loss = 3.22264827\n",
      "Iteration 25, loss = 3.22144812\n",
      "Iteration 26, loss = 3.22051711\n",
      "Iteration 27, loss = 3.22039451\n",
      "Iteration 28, loss = 3.21951911\n",
      "Iteration 29, loss = 3.21993971\n",
      "Iteration 30, loss = 3.21996168\n",
      "Iteration 31, loss = 3.21920438\n",
      "Iteration 32, loss = 3.21836685\n",
      "Iteration 33, loss = 3.21832407\n",
      "Iteration 34, loss = 3.21622651\n",
      "Iteration 35, loss = 3.21569621\n",
      "Iteration 36, loss = 3.21589395\n",
      "Iteration 37, loss = 3.21734144\n",
      "Iteration 38, loss = 3.21676694\n",
      "Iteration 39, loss = 3.21617010\n",
      "Iteration 40, loss = 3.21422630\n",
      "Iteration 41, loss = 3.21529841\n",
      "Iteration 42, loss = 3.21517783\n",
      "Iteration 43, loss = 3.21330313\n",
      "Iteration 44, loss = 3.21373709\n",
      "Iteration 45, loss = 3.21294554\n",
      "Iteration 46, loss = 3.21226174\n",
      "Iteration 47, loss = 3.21155836\n",
      "Iteration 48, loss = 3.21122709\n",
      "Iteration 49, loss = 3.21138738\n",
      "Iteration 50, loss = 3.21116639\n",
      "Iteration 51, loss = 3.21128806\n",
      "Iteration 52, loss = 3.20998608\n",
      "Iteration 53, loss = 3.20913238\n",
      "Iteration 54, loss = 3.20976388\n",
      "Iteration 55, loss = 3.20869357\n",
      "Iteration 56, loss = 3.20924866\n",
      "Iteration 57, loss = 3.20835587\n",
      "Iteration 58, loss = 3.20911948\n",
      "Iteration 59, loss = 3.20782504\n",
      "Iteration 60, loss = 3.20814428\n",
      "Iteration 61, loss = 3.20719735\n",
      "Iteration 62, loss = 3.20653154\n",
      "Iteration 63, loss = 3.20613862\n",
      "Iteration 64, loss = 3.20446865\n",
      "Iteration 65, loss = 3.20467560\n",
      "Iteration 66, loss = 3.20524977\n",
      "Iteration 67, loss = 3.20467402\n",
      "Iteration 68, loss = 3.20475625\n",
      "Iteration 69, loss = 3.20421769\n",
      "Iteration 70, loss = 3.20397017\n",
      "Iteration 71, loss = 3.20299305\n",
      "Iteration 72, loss = 3.20327812\n",
      "Iteration 73, loss = 3.20227560\n",
      "Iteration 74, loss = 3.20173444\n",
      "Iteration 75, loss = 3.20151999\n",
      "Iteration 76, loss = 3.20079428\n",
      "Iteration 77, loss = 3.20008858\n",
      "Iteration 78, loss = 3.20069677\n",
      "Iteration 79, loss = 3.20011195\n",
      "Iteration 80, loss = 3.20024252\n",
      "Iteration 81, loss = 3.20005355\n",
      "Iteration 82, loss = 3.19938697\n",
      "Iteration 83, loss = 3.20002702\n",
      "Iteration 84, loss = 3.20012767\n",
      "Iteration 85, loss = 3.20048355\n",
      "Iteration 86, loss = 3.20027580\n",
      "Iteration 87, loss = 3.19821521\n",
      "Iteration 88, loss = 3.19811995\n",
      "Iteration 89, loss = 3.19728282\n",
      "Iteration 90, loss = 3.19752854\n",
      "Iteration 91, loss = 3.19727539\n",
      "Iteration 92, loss = 3.19598583\n",
      "Iteration 93, loss = 3.19645190\n",
      "Iteration 94, loss = 3.19637341\n",
      "Iteration 95, loss = 3.19561888\n",
      "Iteration 96, loss = 3.19769724\n",
      "Iteration 97, loss = 3.19703892\n",
      "Iteration 98, loss = 3.19496486\n",
      "Iteration 99, loss = 3.19428468\n",
      "Iteration 100, loss = 3.19361426\n",
      "Iteration 101, loss = 3.19326983\n",
      "Iteration 102, loss = 3.19262242\n",
      "Iteration 103, loss = 3.19217945\n",
      "Iteration 104, loss = 3.19252727\n",
      "Iteration 105, loss = 3.19182346\n",
      "Iteration 106, loss = 3.19138878\n",
      "Iteration 107, loss = 3.19129620\n",
      "Iteration 108, loss = 3.19126865\n",
      "Iteration 109, loss = 3.19022553\n",
      "Iteration 110, loss = 3.19052369\n",
      "Iteration 111, loss = 3.18984724\n",
      "Iteration 112, loss = 3.18927928\n",
      "Iteration 113, loss = 3.19017245\n",
      "Iteration 114, loss = 3.18890891\n",
      "Iteration 115, loss = 3.18769388\n",
      "Iteration 116, loss = 3.18912101\n",
      "Iteration 117, loss = 3.18719909\n",
      "Iteration 118, loss = 3.18717268\n",
      "Iteration 119, loss = 3.18679176\n",
      "Iteration 120, loss = 3.18644320\n",
      "Iteration 121, loss = 3.18696972\n",
      "Iteration 122, loss = 3.18603814\n",
      "Iteration 123, loss = 3.18552602\n",
      "Iteration 124, loss = 3.18535941\n",
      "Iteration 125, loss = 3.18541311\n",
      "Iteration 126, loss = 3.18458227\n",
      "Iteration 127, loss = 3.18400023\n",
      "Iteration 128, loss = 3.18409248\n",
      "Iteration 129, loss = 3.18374930\n",
      "Iteration 130, loss = 3.18409272\n",
      "Iteration 131, loss = 3.18330302\n",
      "Iteration 132, loss = 3.18249134\n",
      "Iteration 133, loss = 3.18264284\n",
      "Iteration 134, loss = 3.18294985\n",
      "Iteration 135, loss = 3.18203376\n",
      "Iteration 136, loss = 3.18147780\n",
      "Iteration 137, loss = 3.18117966\n",
      "Iteration 138, loss = 3.18103890\n",
      "Iteration 139, loss = 3.18032884\n",
      "Iteration 140, loss = 3.18055430\n",
      "Iteration 141, loss = 3.18088812\n",
      "Iteration 142, loss = 3.18057858\n",
      "Iteration 143, loss = 3.17960661\n",
      "Iteration 144, loss = 3.17969799\n",
      "Iteration 145, loss = 3.17829892\n",
      "Iteration 146, loss = 3.17782673\n",
      "Iteration 147, loss = 3.17762026\n",
      "Iteration 148, loss = 3.17778175\n",
      "Iteration 149, loss = 3.17705424\n",
      "Iteration 150, loss = 3.17639707\n",
      "Iteration 151, loss = 3.17553338\n",
      "Iteration 152, loss = 3.17546340\n",
      "Iteration 153, loss = 3.17566355\n",
      "Iteration 154, loss = 3.17571933\n",
      "Iteration 155, loss = 3.17456640\n",
      "Iteration 156, loss = 3.17399622\n",
      "Iteration 157, loss = 3.17440930\n",
      "Iteration 158, loss = 3.17486093\n",
      "Iteration 159, loss = 3.17391270\n",
      "Iteration 160, loss = 3.17360827\n",
      "Iteration 161, loss = 3.17342626\n",
      "Iteration 162, loss = 3.17323843\n",
      "Iteration 163, loss = 3.17324277\n",
      "Iteration 164, loss = 3.17222538\n",
      "Iteration 165, loss = 3.17174294\n",
      "Iteration 166, loss = 3.17317523\n",
      "Iteration 167, loss = 3.17218123\n",
      "Iteration 168, loss = 3.17102999\n",
      "Iteration 169, loss = 3.17047465\n",
      "Iteration 170, loss = 3.17079826\n",
      "Iteration 171, loss = 3.17005174\n",
      "Iteration 172, loss = 3.16995773\n",
      "Iteration 173, loss = 3.17065471\n",
      "Iteration 174, loss = 3.17040744\n",
      "Iteration 175, loss = 3.17075762\n",
      "Iteration 176, loss = 3.17139732\n",
      "Iteration 177, loss = 3.17050650\n",
      "Iteration 178, loss = 3.16788943\n",
      "Iteration 179, loss = 3.16818766\n",
      "Iteration 180, loss = 3.16894560\n",
      "Iteration 181, loss = 3.16860581\n",
      "Iteration 182, loss = 3.16806984\n",
      "Iteration 183, loss = 3.16630255\n",
      "Iteration 184, loss = 3.16714366\n",
      "Iteration 185, loss = 3.16655107\n",
      "Iteration 186, loss = 3.16551549\n",
      "Iteration 187, loss = 3.16517115\n",
      "Iteration 188, loss = 3.16560767\n",
      "Iteration 189, loss = 3.16486693\n",
      "Iteration 190, loss = 3.16542948\n",
      "Iteration 191, loss = 3.16468450\n",
      "Iteration 192, loss = 3.16492055\n",
      "Iteration 193, loss = 3.16407040\n",
      "Iteration 194, loss = 3.16394157\n",
      "Iteration 195, loss = 3.16315909\n",
      "Iteration 196, loss = 3.16266044\n",
      "Iteration 197, loss = 3.16254749\n",
      "Iteration 198, loss = 3.16243328\n",
      "Iteration 199, loss = 3.16205161\n",
      "Iteration 200, loss = 3.16147380\n",
      "Iteration 201, loss = 3.16141827\n",
      "Iteration 202, loss = 3.16131690\n",
      "Iteration 203, loss = 3.16097215\n",
      "Iteration 204, loss = 3.16235772\n",
      "Iteration 205, loss = 3.16185953\n",
      "Iteration 206, loss = 3.16040721\n",
      "Iteration 207, loss = 3.16049886\n",
      "Iteration 208, loss = 3.15964972\n",
      "Iteration 209, loss = 3.15900662\n",
      "Iteration 210, loss = 3.15902421\n",
      "Iteration 211, loss = 3.15859506\n",
      "Iteration 212, loss = 3.15814357\n",
      "Iteration 213, loss = 3.15793952\n",
      "Iteration 214, loss = 3.15835750\n",
      "Iteration 215, loss = 3.15830570\n",
      "Iteration 216, loss = 3.15686388\n",
      "Iteration 217, loss = 3.15562764\n",
      "Iteration 218, loss = 3.15621527\n",
      "Iteration 219, loss = 3.15632662\n",
      "Iteration 220, loss = 3.15713182\n",
      "Iteration 221, loss = 3.15545790\n",
      "Iteration 222, loss = 3.15631266\n",
      "Iteration 223, loss = 3.15681194\n",
      "Iteration 224, loss = 3.15630743\n",
      "Iteration 225, loss = 3.15454237\n",
      "Iteration 226, loss = 3.15365928\n",
      "Iteration 227, loss = 3.15405423\n",
      "Iteration 228, loss = 3.15377376\n",
      "Iteration 229, loss = 3.15470191\n",
      "Iteration 230, loss = 3.15470756\n",
      "Iteration 231, loss = 3.15346767\n",
      "Iteration 232, loss = 3.15351894\n",
      "Iteration 233, loss = 3.15290858\n",
      "Iteration 234, loss = 3.15453814\n",
      "Iteration 235, loss = 3.15304750\n",
      "Iteration 236, loss = 3.15273311\n",
      "Iteration 237, loss = 3.15251698\n",
      "Iteration 238, loss = 3.15166397\n",
      "Iteration 239, loss = 3.15129630\n",
      "Iteration 240, loss = 3.15206822\n",
      "Iteration 241, loss = 3.15093982\n",
      "Iteration 242, loss = 3.15077142\n",
      "Iteration 243, loss = 3.15059039\n",
      "Iteration 244, loss = 3.15136970\n",
      "Iteration 245, loss = 3.15025432\n",
      "Iteration 246, loss = 3.14954170\n",
      "Iteration 247, loss = 3.14871350\n",
      "Iteration 248, loss = 3.14850665\n",
      "Iteration 249, loss = 3.14852012\n",
      "Iteration 250, loss = 3.14832641\n",
      "Iteration 251, loss = 3.14797354\n",
      "Iteration 252, loss = 3.14863480\n",
      "Iteration 253, loss = 3.14714399\n",
      "Iteration 254, loss = 3.14733956\n",
      "Iteration 255, loss = 3.14719258\n",
      "Iteration 256, loss = 3.14853056\n",
      "Iteration 257, loss = 3.14820062\n",
      "Iteration 258, loss = 3.14716434\n",
      "Iteration 259, loss = 3.14724926\n",
      "Iteration 260, loss = 3.14578255\n",
      "Iteration 261, loss = 3.14726875\n",
      "Iteration 262, loss = 3.14658677\n",
      "Iteration 263, loss = 3.14537629\n",
      "Iteration 264, loss = 3.14691178\n",
      "Iteration 265, loss = 3.14480166\n",
      "Iteration 266, loss = 3.14472617\n",
      "Iteration 267, loss = 3.14359408\n",
      "Iteration 268, loss = 3.14629660\n",
      "Iteration 269, loss = 3.14441244\n",
      "Iteration 270, loss = 3.14503938\n",
      "Iteration 271, loss = 3.14477227\n",
      "Iteration 272, loss = 3.14446115\n",
      "Iteration 273, loss = 3.14550018\n",
      "Iteration 274, loss = 3.14360423\n",
      "Iteration 275, loss = 3.14285856\n",
      "Iteration 276, loss = 3.14294616\n",
      "Iteration 277, loss = 3.14213337\n",
      "Iteration 278, loss = 3.14134873\n",
      "Iteration 279, loss = 3.14236535\n",
      "Iteration 280, loss = 3.14102740\n",
      "Iteration 281, loss = 3.14072769\n",
      "Iteration 282, loss = 3.14085085\n",
      "Iteration 283, loss = 3.14038538\n",
      "Iteration 284, loss = 3.14114045\n",
      "Iteration 285, loss = 3.14176603\n",
      "Iteration 286, loss = 3.14127885\n",
      "Iteration 287, loss = 3.14147431\n",
      "Iteration 288, loss = 3.14160225\n",
      "Iteration 289, loss = 3.14028097\n",
      "Iteration 290, loss = 3.13994835\n",
      "Iteration 291, loss = 3.13979793\n",
      "Iteration 292, loss = 3.13934559\n",
      "Iteration 293, loss = 3.13962458\n",
      "Iteration 294, loss = 3.13964762\n",
      "Iteration 295, loss = 3.13896095\n",
      "Iteration 296, loss = 3.13854241\n",
      "Iteration 297, loss = 3.13693487\n",
      "Iteration 298, loss = 3.13839785\n",
      "Iteration 299, loss = 3.13784827\n",
      "Iteration 300, loss = 3.13822775\n",
      "Iteration 301, loss = 3.13791848\n",
      "Iteration 302, loss = 3.13639678\n",
      "Iteration 303, loss = 3.13788831\n",
      "Iteration 304, loss = 3.13670096\n",
      "Iteration 305, loss = 3.13894459\n",
      "Iteration 306, loss = 3.13533772\n",
      "Iteration 307, loss = 3.13756436\n",
      "Iteration 308, loss = 3.13582682\n",
      "Iteration 309, loss = 3.13513175\n",
      "Iteration 310, loss = 3.13546909\n",
      "Iteration 311, loss = 3.13505508\n",
      "Iteration 312, loss = 3.13491798\n",
      "Iteration 313, loss = 3.13433306\n",
      "Iteration 314, loss = 3.13421477\n",
      "Iteration 315, loss = 3.13465719\n",
      "Iteration 316, loss = 3.13345562\n",
      "Iteration 317, loss = 3.13372908\n",
      "Iteration 318, loss = 3.13345546\n",
      "Iteration 319, loss = 3.13390427\n",
      "Iteration 320, loss = 3.13319818\n",
      "Iteration 321, loss = 3.13236102\n",
      "Iteration 322, loss = 3.13203732\n",
      "Iteration 323, loss = 3.13348521\n",
      "Iteration 324, loss = 3.13294939\n",
      "Iteration 325, loss = 3.13443515\n",
      "Iteration 326, loss = 3.13181723\n",
      "Iteration 327, loss = 3.13323808\n",
      "Iteration 328, loss = 3.13079794\n",
      "Iteration 329, loss = 3.13232039\n",
      "Iteration 330, loss = 3.13339088\n",
      "Iteration 331, loss = 3.13080548\n",
      "Iteration 332, loss = 3.13208344\n",
      "Iteration 333, loss = 3.13095660\n",
      "Iteration 334, loss = 3.13031043\n",
      "Iteration 335, loss = 3.12960721\n",
      "Iteration 336, loss = 3.12922033\n",
      "Iteration 337, loss = 3.12877950\n",
      "Iteration 338, loss = 3.12899338\n",
      "Iteration 339, loss = 3.12934635\n",
      "Iteration 340, loss = 3.12865207\n",
      "Iteration 341, loss = 3.12802218\n",
      "Iteration 342, loss = 3.12820649\n",
      "Iteration 343, loss = 3.12785535\n",
      "Iteration 344, loss = 3.12848213\n",
      "Iteration 345, loss = 3.12758988\n",
      "Iteration 346, loss = 3.12747857\n",
      "Iteration 347, loss = 3.12784821\n",
      "Iteration 348, loss = 3.12732854\n",
      "Iteration 349, loss = 3.12705352\n",
      "Iteration 350, loss = 3.12728495\n",
      "Iteration 351, loss = 3.12647996\n",
      "Iteration 352, loss = 3.12636898\n",
      "Iteration 353, loss = 3.12807615\n",
      "Iteration 354, loss = 3.12598819\n",
      "Iteration 355, loss = 3.12640425\n",
      "Iteration 356, loss = 3.12605525\n",
      "Iteration 357, loss = 3.12574405\n",
      "Iteration 358, loss = 3.12447210\n",
      "Iteration 359, loss = 3.12588936\n",
      "Iteration 360, loss = 3.12568287\n",
      "Iteration 361, loss = 3.12440063\n",
      "Iteration 362, loss = 3.12476920\n",
      "Iteration 363, loss = 3.12465217\n",
      "Iteration 364, loss = 3.12348655\n",
      "Iteration 365, loss = 3.12521354\n",
      "Iteration 366, loss = 3.12314501\n",
      "Iteration 367, loss = 3.12383722\n",
      "Iteration 368, loss = 3.12589024\n",
      "Iteration 369, loss = 3.12315387\n",
      "Iteration 370, loss = 3.12302246\n",
      "Iteration 371, loss = 3.12352160\n",
      "Iteration 372, loss = 3.12200033\n",
      "Iteration 373, loss = 3.12339049\n",
      "Iteration 374, loss = 3.12243342\n",
      "Iteration 375, loss = 3.12298485\n",
      "Iteration 376, loss = 3.12150778\n",
      "Iteration 377, loss = 3.12110951\n",
      "Iteration 378, loss = 3.12198572\n",
      "Iteration 379, loss = 3.12204581\n",
      "Iteration 380, loss = 3.12309942\n",
      "Iteration 381, loss = 3.12184111\n",
      "Iteration 382, loss = 3.12100380\n",
      "Iteration 383, loss = 3.12094807\n",
      "Iteration 384, loss = 3.12018217\n",
      "Iteration 385, loss = 3.12068618\n",
      "Iteration 386, loss = 3.12022102\n",
      "Iteration 387, loss = 3.11930493\n",
      "Iteration 388, loss = 3.11982895\n",
      "Iteration 389, loss = 3.12003498\n",
      "Iteration 390, loss = 3.11895269\n",
      "Iteration 391, loss = 3.11902951\n",
      "Iteration 392, loss = 3.12061523\n",
      "Iteration 393, loss = 3.11969687\n",
      "Iteration 394, loss = 3.12009023\n",
      "Iteration 395, loss = 3.11881322\n",
      "Iteration 396, loss = 3.11754350\n",
      "Iteration 397, loss = 3.11823589\n",
      "Iteration 398, loss = 3.11776242\n",
      "Iteration 399, loss = 3.11715911\n",
      "Iteration 400, loss = 3.11859257\n",
      "Iteration 401, loss = 3.11742298\n",
      "Iteration 402, loss = 3.11723966\n",
      "Iteration 403, loss = 3.11854923\n",
      "Iteration 404, loss = 3.11671840\n",
      "Iteration 405, loss = 3.11657422\n",
      "Iteration 406, loss = 3.11642035\n",
      "Iteration 407, loss = 3.11637721\n",
      "Iteration 408, loss = 3.11638211\n",
      "Iteration 409, loss = 3.11786341\n",
      "Iteration 410, loss = 3.11602467\n",
      "Iteration 411, loss = 3.11608306\n",
      "Iteration 412, loss = 3.11505252\n",
      "Iteration 413, loss = 3.11442120\n",
      "Iteration 414, loss = 3.11417422\n",
      "Iteration 415, loss = 3.11513891\n",
      "Iteration 416, loss = 3.11628713\n",
      "Iteration 417, loss = 3.11476729\n",
      "Iteration 418, loss = 3.11473537\n",
      "Iteration 419, loss = 3.11428608\n",
      "Iteration 420, loss = 3.11351330\n",
      "Iteration 421, loss = 3.11331061\n",
      "Iteration 422, loss = 3.11263472\n",
      "Iteration 423, loss = 3.11294494\n",
      "Iteration 424, loss = 3.11383594\n",
      "Iteration 425, loss = 3.11354124\n",
      "Iteration 426, loss = 3.11519125\n",
      "Iteration 427, loss = 3.11394656\n",
      "Iteration 428, loss = 3.11353271\n",
      "Iteration 429, loss = 3.11470253\n",
      "Iteration 430, loss = 3.11250622\n",
      "Iteration 431, loss = 3.11464925\n",
      "Iteration 432, loss = 3.11610643\n",
      "Iteration 433, loss = 3.11166317\n",
      "Iteration 434, loss = 3.11114830\n",
      "Iteration 435, loss = 3.11138295\n",
      "Iteration 436, loss = 3.11072769\n",
      "Iteration 437, loss = 3.11341966\n",
      "Iteration 438, loss = 3.11248939\n",
      "Iteration 439, loss = 3.11030485\n",
      "Iteration 440, loss = 3.11177384\n",
      "Iteration 441, loss = 3.11202696\n",
      "Iteration 442, loss = 3.11044090\n",
      "Iteration 443, loss = 3.11193967\n",
      "Iteration 444, loss = 3.11247525\n",
      "Iteration 445, loss = 3.11032486\n",
      "Iteration 446, loss = 3.11034999\n",
      "Iteration 447, loss = 3.10893992\n",
      "Iteration 448, loss = 3.10899539\n",
      "Iteration 449, loss = 3.10906751\n",
      "Iteration 450, loss = 3.10954691\n",
      "Iteration 451, loss = 3.10998487\n",
      "Iteration 452, loss = 3.10975533\n",
      "Iteration 453, loss = 3.10947251\n",
      "Iteration 454, loss = 3.10901543\n",
      "Iteration 455, loss = 3.10926473\n",
      "Iteration 456, loss = 3.10904783\n",
      "Iteration 457, loss = 3.10971074\n",
      "Iteration 458, loss = 3.10881801\n",
      "Iteration 459, loss = 3.10869839\n",
      "Iteration 460, loss = 3.10865325\n",
      "Iteration 461, loss = 3.10736503\n",
      "Iteration 462, loss = 3.10764701\n",
      "Iteration 463, loss = 3.10843531\n",
      "Iteration 464, loss = 3.10841004\n",
      "Iteration 465, loss = 3.10682041\n",
      "Iteration 466, loss = 3.10700225\n",
      "Iteration 467, loss = 3.10718459\n",
      "Iteration 468, loss = 3.10771173\n",
      "Iteration 469, loss = 3.10851191\n",
      "Iteration 470, loss = 3.10799315\n",
      "Iteration 471, loss = 3.10841517\n",
      "Iteration 472, loss = 3.10811110\n",
      "Iteration 473, loss = 3.10792023\n",
      "Iteration 474, loss = 3.10721841\n",
      "Iteration 475, loss = 3.10750278\n",
      "Iteration 476, loss = 3.10627516\n",
      "Iteration 477, loss = 3.10552068\n",
      "Iteration 478, loss = 3.10518221\n",
      "Iteration 479, loss = 3.10592255\n",
      "Iteration 480, loss = 3.10680661\n",
      "Iteration 481, loss = 3.10520627\n",
      "Iteration 482, loss = 3.10528137\n",
      "Iteration 483, loss = 3.10592224\n",
      "Iteration 484, loss = 3.10479694\n",
      "Iteration 485, loss = 3.10611510\n",
      "Iteration 486, loss = 3.10373403\n",
      "Iteration 487, loss = 3.10502679\n",
      "Iteration 488, loss = 3.10438665\n",
      "Iteration 489, loss = 3.10367612\n",
      "Iteration 490, loss = 3.10385968\n",
      "Iteration 491, loss = 3.10435440\n",
      "Iteration 492, loss = 3.10338063\n",
      "Iteration 493, loss = 3.10448103\n",
      "Iteration 494, loss = 3.10413860\n",
      "Iteration 495, loss = 3.10437194\n",
      "Iteration 496, loss = 3.10576193\n",
      "Iteration 497, loss = 3.10416652\n",
      "Iteration 498, loss = 3.10319844\n",
      "Iteration 499, loss = 3.10321198\n",
      "Iteration 500, loss = 3.10302251\n",
      "Iteration 501, loss = 3.10388908\n",
      "Iteration 502, loss = 3.10368241\n",
      "Iteration 503, loss = 3.10438688\n",
      "Iteration 504, loss = 3.10247460\n",
      "Iteration 505, loss = 3.10224216\n",
      "Iteration 506, loss = 3.10257182\n",
      "Iteration 507, loss = 3.10232239\n",
      "Iteration 508, loss = 3.10257332\n",
      "Iteration 509, loss = 3.10247218\n",
      "Iteration 510, loss = 3.10296323\n",
      "Iteration 511, loss = 3.10208907\n",
      "Iteration 512, loss = 3.10225866\n",
      "Iteration 513, loss = 3.10081590\n",
      "Iteration 514, loss = 3.10151846\n",
      "Iteration 515, loss = 3.10202073\n",
      "Iteration 516, loss = 3.10142493\n",
      "Iteration 517, loss = 3.10082422\n",
      "Iteration 518, loss = 3.10040664\n",
      "Iteration 519, loss = 3.10113066\n",
      "Iteration 520, loss = 3.10050904\n",
      "Iteration 521, loss = 3.10182024\n",
      "Iteration 522, loss = 3.10102426\n",
      "Iteration 523, loss = 3.10002846\n",
      "Iteration 524, loss = 3.10018401\n",
      "Iteration 525, loss = 3.10103663\n",
      "Iteration 526, loss = 3.09983016\n",
      "Iteration 527, loss = 3.10132768\n",
      "Iteration 528, loss = 3.10049813\n",
      "Iteration 529, loss = 3.10170812\n",
      "Iteration 530, loss = 3.10018197\n",
      "Iteration 531, loss = 3.10018537\n",
      "Iteration 532, loss = 3.09933731\n",
      "Iteration 533, loss = 3.09977182\n",
      "Iteration 534, loss = 3.09973894\n",
      "Iteration 535, loss = 3.09984174\n",
      "Iteration 536, loss = 3.09969810\n",
      "Iteration 537, loss = 3.09972794\n",
      "Iteration 538, loss = 3.10028810\n",
      "Iteration 539, loss = 3.10021110\n",
      "Iteration 540, loss = 3.09983916\n",
      "Iteration 541, loss = 3.10120494\n",
      "Iteration 542, loss = 3.10017059\n",
      "Iteration 543, loss = 3.10001635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.11981730\n",
      "Iteration 2, loss = 3.91821048\n",
      "Iteration 3, loss = 3.74084212\n",
      "Iteration 4, loss = 3.57654812\n",
      "Iteration 5, loss = 3.43622923\n",
      "Iteration 6, loss = 3.32862528\n",
      "Iteration 7, loss = 3.26310113\n",
      "Iteration 8, loss = 3.23661869\n",
      "Iteration 9, loss = 3.23752852\n",
      "Iteration 10, loss = 3.23832592\n",
      "Iteration 11, loss = 3.23296949\n",
      "Iteration 12, loss = 3.22793849\n",
      "Iteration 13, loss = 3.22532313\n",
      "Iteration 14, loss = 3.22582637\n",
      "Iteration 15, loss = 3.22546807\n",
      "Iteration 16, loss = 3.22383730\n",
      "Iteration 17, loss = 3.22336694\n",
      "Iteration 18, loss = 3.22325034\n",
      "Iteration 19, loss = 3.22162214\n",
      "Iteration 20, loss = 3.22101855\n",
      "Iteration 21, loss = 3.22071876\n",
      "Iteration 22, loss = 3.21996841\n",
      "Iteration 23, loss = 3.21914909\n",
      "Iteration 24, loss = 3.21858679\n",
      "Iteration 25, loss = 3.21803196\n",
      "Iteration 26, loss = 3.21740801\n",
      "Iteration 27, loss = 3.21715080\n",
      "Iteration 28, loss = 3.21657249\n",
      "Iteration 29, loss = 3.21565028\n",
      "Iteration 30, loss = 3.21638519\n",
      "Iteration 31, loss = 3.21593116\n",
      "Iteration 32, loss = 3.21522866\n",
      "Iteration 33, loss = 3.21441469\n",
      "Iteration 34, loss = 3.21409672\n",
      "Iteration 35, loss = 3.21334035\n",
      "Iteration 36, loss = 3.21380117\n",
      "Iteration 37, loss = 3.21330907\n",
      "Iteration 38, loss = 3.21215968\n",
      "Iteration 39, loss = 3.21250936\n",
      "Iteration 40, loss = 3.21173902\n",
      "Iteration 41, loss = 3.21132973\n",
      "Iteration 42, loss = 3.21147666\n",
      "Iteration 43, loss = 3.21027005\n",
      "Iteration 44, loss = 3.20953018\n",
      "Iteration 45, loss = 3.20892535\n",
      "Iteration 46, loss = 3.20888445\n",
      "Iteration 47, loss = 3.20853324\n",
      "Iteration 48, loss = 3.20835957\n",
      "Iteration 49, loss = 3.20778954\n",
      "Iteration 50, loss = 3.20749668\n",
      "Iteration 51, loss = 3.20709629\n",
      "Iteration 52, loss = 3.20757927\n",
      "Iteration 53, loss = 3.20679654\n",
      "Iteration 54, loss = 3.20609950\n",
      "Iteration 55, loss = 3.20559698\n",
      "Iteration 56, loss = 3.20548413\n",
      "Iteration 57, loss = 3.20359513\n",
      "Iteration 58, loss = 3.20383611\n",
      "Iteration 59, loss = 3.20295054\n",
      "Iteration 60, loss = 3.20143363\n",
      "Iteration 61, loss = 3.20184011\n",
      "Iteration 62, loss = 3.20220949\n",
      "Iteration 63, loss = 3.20162773\n",
      "Iteration 64, loss = 3.20069536\n",
      "Iteration 65, loss = 3.20089438\n",
      "Iteration 66, loss = 3.20071958\n",
      "Iteration 67, loss = 3.20117718\n",
      "Iteration 68, loss = 3.20040515\n",
      "Iteration 69, loss = 3.19915154\n",
      "Iteration 70, loss = 3.19941749\n",
      "Iteration 71, loss = 3.19795376\n",
      "Iteration 72, loss = 3.19874236\n",
      "Iteration 73, loss = 3.19936002\n",
      "Iteration 74, loss = 3.19854432\n",
      "Iteration 75, loss = 3.19651656\n",
      "Iteration 76, loss = 3.19659978\n",
      "Iteration 77, loss = 3.19659711\n",
      "Iteration 78, loss = 3.19676935\n",
      "Iteration 79, loss = 3.19602367\n",
      "Iteration 80, loss = 3.19517746\n",
      "Iteration 81, loss = 3.19436745\n",
      "Iteration 82, loss = 3.19399314\n",
      "Iteration 83, loss = 3.19410133\n",
      "Iteration 84, loss = 3.19370728\n",
      "Iteration 85, loss = 3.19399464\n",
      "Iteration 86, loss = 3.19363364\n",
      "Iteration 87, loss = 3.19320306\n",
      "Iteration 88, loss = 3.19311158\n",
      "Iteration 89, loss = 3.19224923\n",
      "Iteration 90, loss = 3.19252782\n",
      "Iteration 91, loss = 3.19156521\n",
      "Iteration 92, loss = 3.19123571\n",
      "Iteration 93, loss = 3.19048800\n",
      "Iteration 94, loss = 3.18999006\n",
      "Iteration 95, loss = 3.18954468\n",
      "Iteration 96, loss = 3.18998035\n",
      "Iteration 97, loss = 3.19013639\n",
      "Iteration 98, loss = 3.18970831\n",
      "Iteration 99, loss = 3.18997720\n",
      "Iteration 100, loss = 3.18896655\n",
      "Iteration 101, loss = 3.18877237\n",
      "Iteration 102, loss = 3.18783039\n",
      "Iteration 103, loss = 3.18850901\n",
      "Iteration 104, loss = 3.18825971\n",
      "Iteration 105, loss = 3.18773553\n",
      "Iteration 106, loss = 3.18803178\n",
      "Iteration 107, loss = 3.18661640\n",
      "Iteration 108, loss = 3.18584599\n",
      "Iteration 109, loss = 3.18686647\n",
      "Iteration 110, loss = 3.18685229\n",
      "Iteration 111, loss = 3.18562675\n",
      "Iteration 112, loss = 3.18474025\n",
      "Iteration 113, loss = 3.18473825\n",
      "Iteration 114, loss = 3.18374170\n",
      "Iteration 115, loss = 3.18326282\n",
      "Iteration 116, loss = 3.18330641\n",
      "Iteration 117, loss = 3.18327962\n",
      "Iteration 118, loss = 3.18261777\n",
      "Iteration 119, loss = 3.18322587\n",
      "Iteration 120, loss = 3.18372569\n",
      "Iteration 121, loss = 3.18312305\n",
      "Iteration 122, loss = 3.18174726\n",
      "Iteration 123, loss = 3.18202193\n",
      "Iteration 124, loss = 3.18218777\n",
      "Iteration 125, loss = 3.18187183\n",
      "Iteration 126, loss = 3.18195451\n",
      "Iteration 127, loss = 3.18271650\n",
      "Iteration 128, loss = 3.18282438\n",
      "Iteration 129, loss = 3.18023994\n",
      "Iteration 130, loss = 3.18003643\n",
      "Iteration 131, loss = 3.18007295\n",
      "Iteration 132, loss = 3.17969288\n",
      "Iteration 133, loss = 3.17931093\n",
      "Iteration 134, loss = 3.17990587\n",
      "Iteration 135, loss = 3.18029469\n",
      "Iteration 136, loss = 3.17926480\n",
      "Iteration 137, loss = 3.17867694\n",
      "Iteration 138, loss = 3.17791564\n",
      "Iteration 139, loss = 3.17695786\n",
      "Iteration 140, loss = 3.17681097\n",
      "Iteration 141, loss = 3.17704548\n",
      "Iteration 142, loss = 3.17795343\n",
      "Iteration 143, loss = 3.17706005\n",
      "Iteration 144, loss = 3.17699371\n",
      "Iteration 145, loss = 3.17621208\n",
      "Iteration 146, loss = 3.17561074\n",
      "Iteration 147, loss = 3.17642808\n",
      "Iteration 148, loss = 3.17552037\n",
      "Iteration 149, loss = 3.17608312\n",
      "Iteration 150, loss = 3.17517149\n",
      "Iteration 151, loss = 3.17476872\n",
      "Iteration 152, loss = 3.17442113\n",
      "Iteration 153, loss = 3.17374636\n",
      "Iteration 154, loss = 3.17407873\n",
      "Iteration 155, loss = 3.17432854\n",
      "Iteration 156, loss = 3.17312961\n",
      "Iteration 157, loss = 3.17323103\n",
      "Iteration 158, loss = 3.17223190\n",
      "Iteration 159, loss = 3.17289153\n",
      "Iteration 160, loss = 3.17322958\n",
      "Iteration 161, loss = 3.17364022\n",
      "Iteration 162, loss = 3.17361456\n",
      "Iteration 163, loss = 3.17245631\n",
      "Iteration 164, loss = 3.17246342\n",
      "Iteration 165, loss = 3.17050440\n",
      "Iteration 166, loss = 3.17106266\n",
      "Iteration 167, loss = 3.17172743\n",
      "Iteration 168, loss = 3.17151592\n",
      "Iteration 169, loss = 3.17003107\n",
      "Iteration 170, loss = 3.17142729\n",
      "Iteration 171, loss = 3.17036302\n",
      "Iteration 172, loss = 3.17018675\n",
      "Iteration 173, loss = 3.16927843\n",
      "Iteration 174, loss = 3.16876479\n",
      "Iteration 175, loss = 3.16938347\n",
      "Iteration 176, loss = 3.16951803\n",
      "Iteration 177, loss = 3.16867964\n",
      "Iteration 178, loss = 3.16805721\n",
      "Iteration 179, loss = 3.16823321\n",
      "Iteration 180, loss = 3.16758819\n",
      "Iteration 181, loss = 3.16775068\n",
      "Iteration 182, loss = 3.16875365\n",
      "Iteration 183, loss = 3.16717280\n",
      "Iteration 184, loss = 3.16816395\n",
      "Iteration 185, loss = 3.16801828\n",
      "Iteration 186, loss = 3.16723968\n",
      "Iteration 187, loss = 3.16763458\n",
      "Iteration 188, loss = 3.16887775\n",
      "Iteration 189, loss = 3.16832463\n",
      "Iteration 190, loss = 3.16651826\n",
      "Iteration 191, loss = 3.16630478\n",
      "Iteration 192, loss = 3.16555133\n",
      "Iteration 193, loss = 3.16441434\n",
      "Iteration 194, loss = 3.16451483\n",
      "Iteration 195, loss = 3.16427557\n",
      "Iteration 196, loss = 3.16450180\n",
      "Iteration 197, loss = 3.16400444\n",
      "Iteration 198, loss = 3.16376036\n",
      "Iteration 199, loss = 3.16415450\n",
      "Iteration 200, loss = 3.16494624\n",
      "Iteration 201, loss = 3.16465512\n",
      "Iteration 202, loss = 3.16403121\n",
      "Iteration 203, loss = 3.16417320\n",
      "Iteration 204, loss = 3.16279670\n",
      "Iteration 205, loss = 3.16341951\n",
      "Iteration 206, loss = 3.16276944\n",
      "Iteration 207, loss = 3.16230856\n",
      "Iteration 208, loss = 3.16227136\n",
      "Iteration 209, loss = 3.16202181\n",
      "Iteration 210, loss = 3.16194848\n",
      "Iteration 211, loss = 3.16192003\n",
      "Iteration 212, loss = 3.16210508\n",
      "Iteration 213, loss = 3.16127648\n",
      "Iteration 214, loss = 3.16192842\n",
      "Iteration 215, loss = 3.16215526\n",
      "Iteration 216, loss = 3.16106780\n",
      "Iteration 217, loss = 3.16144815\n",
      "Iteration 218, loss = 3.16088373\n",
      "Iteration 219, loss = 3.15953066\n",
      "Iteration 220, loss = 3.16016370\n",
      "Iteration 221, loss = 3.16000791\n",
      "Iteration 222, loss = 3.16039568\n",
      "Iteration 223, loss = 3.15944260\n",
      "Iteration 224, loss = 3.15902316\n",
      "Iteration 225, loss = 3.15939677\n",
      "Iteration 226, loss = 3.15801124\n",
      "Iteration 227, loss = 3.15886694\n",
      "Iteration 228, loss = 3.15859551\n",
      "Iteration 229, loss = 3.15806943\n",
      "Iteration 230, loss = 3.16108844\n",
      "Iteration 231, loss = 3.16020533\n",
      "Iteration 232, loss = 3.15765101\n",
      "Iteration 233, loss = 3.15710388\n",
      "Iteration 234, loss = 3.15721392\n",
      "Iteration 235, loss = 3.15787086\n",
      "Iteration 236, loss = 3.15709081\n",
      "Iteration 237, loss = 3.15678907\n",
      "Iteration 238, loss = 3.15764071\n",
      "Iteration 239, loss = 3.15644204\n",
      "Iteration 240, loss = 3.15651705\n",
      "Iteration 241, loss = 3.15673094\n",
      "Iteration 242, loss = 3.15549467\n",
      "Iteration 243, loss = 3.15543239\n",
      "Iteration 244, loss = 3.15522411\n",
      "Iteration 245, loss = 3.15490276\n",
      "Iteration 246, loss = 3.15330126\n",
      "Iteration 247, loss = 3.15691962\n",
      "Iteration 248, loss = 3.15475974\n",
      "Iteration 249, loss = 3.15362292\n",
      "Iteration 250, loss = 3.15561109\n",
      "Iteration 251, loss = 3.15405649\n",
      "Iteration 252, loss = 3.15362777\n",
      "Iteration 253, loss = 3.15303633\n",
      "Iteration 254, loss = 3.15372655\n",
      "Iteration 255, loss = 3.15350685\n",
      "Iteration 256, loss = 3.15293164\n",
      "Iteration 257, loss = 3.15239013\n",
      "Iteration 258, loss = 3.15213464\n",
      "Iteration 259, loss = 3.15132566\n",
      "Iteration 260, loss = 3.15089113\n",
      "Iteration 261, loss = 3.15041901\n",
      "Iteration 262, loss = 3.15045421\n",
      "Iteration 263, loss = 3.15117777\n",
      "Iteration 264, loss = 3.15142685\n",
      "Iteration 265, loss = 3.15257304\n",
      "Iteration 266, loss = 3.15274156\n",
      "Iteration 267, loss = 3.15079217\n",
      "Iteration 268, loss = 3.15023667\n",
      "Iteration 269, loss = 3.14918141\n",
      "Iteration 270, loss = 3.14930175\n",
      "Iteration 271, loss = 3.14890439\n",
      "Iteration 272, loss = 3.14874197\n",
      "Iteration 273, loss = 3.14916642\n",
      "Iteration 274, loss = 3.14930728\n",
      "Iteration 275, loss = 3.14874443\n",
      "Iteration 276, loss = 3.14917808\n",
      "Iteration 277, loss = 3.14846159\n",
      "Iteration 278, loss = 3.14906140\n",
      "Iteration 279, loss = 3.14769194\n",
      "Iteration 280, loss = 3.14767536\n",
      "Iteration 281, loss = 3.14743279\n",
      "Iteration 282, loss = 3.14760532\n",
      "Iteration 283, loss = 3.14708877\n",
      "Iteration 284, loss = 3.14762369\n",
      "Iteration 285, loss = 3.14699743\n",
      "Iteration 286, loss = 3.14718034\n",
      "Iteration 287, loss = 3.14594088\n",
      "Iteration 288, loss = 3.14589871\n",
      "Iteration 289, loss = 3.14598510\n",
      "Iteration 290, loss = 3.14749830\n",
      "Iteration 291, loss = 3.14658058\n",
      "Iteration 292, loss = 3.14564664\n",
      "Iteration 293, loss = 3.14514728\n",
      "Iteration 294, loss = 3.14573265\n",
      "Iteration 295, loss = 3.14592545\n",
      "Iteration 296, loss = 3.14576230\n",
      "Iteration 297, loss = 3.14615634\n",
      "Iteration 298, loss = 3.14488866\n",
      "Iteration 299, loss = 3.14421080\n",
      "Iteration 300, loss = 3.14454997\n",
      "Iteration 301, loss = 3.14409532\n",
      "Iteration 302, loss = 3.14387947\n",
      "Iteration 303, loss = 3.14346736\n",
      "Iteration 304, loss = 3.14333833\n",
      "Iteration 305, loss = 3.14316621\n",
      "Iteration 306, loss = 3.14348725\n",
      "Iteration 307, loss = 3.14334743\n",
      "Iteration 308, loss = 3.14306673\n",
      "Iteration 309, loss = 3.14306006\n",
      "Iteration 310, loss = 3.14332586\n",
      "Iteration 311, loss = 3.14328386\n",
      "Iteration 312, loss = 3.14274867\n",
      "Iteration 313, loss = 3.14187105\n",
      "Iteration 314, loss = 3.14241255\n",
      "Iteration 315, loss = 3.14260649\n",
      "Iteration 316, loss = 3.14117377\n",
      "Iteration 317, loss = 3.14115138\n",
      "Iteration 318, loss = 3.14083710\n",
      "Iteration 319, loss = 3.14006760\n",
      "Iteration 320, loss = 3.14012899\n",
      "Iteration 321, loss = 3.14144131\n",
      "Iteration 322, loss = 3.14175285\n",
      "Iteration 323, loss = 3.14082920\n",
      "Iteration 324, loss = 3.14087813\n",
      "Iteration 325, loss = 3.13961446\n",
      "Iteration 326, loss = 3.14042206\n",
      "Iteration 327, loss = 3.14067097\n",
      "Iteration 328, loss = 3.14026444\n",
      "Iteration 329, loss = 3.13915984\n",
      "Iteration 330, loss = 3.13889653\n",
      "Iteration 331, loss = 3.13894974\n",
      "Iteration 332, loss = 3.14083524\n",
      "Iteration 333, loss = 3.14053746\n",
      "Iteration 334, loss = 3.13988638\n",
      "Iteration 335, loss = 3.13926005\n",
      "Iteration 336, loss = 3.13868506\n",
      "Iteration 337, loss = 3.13854819\n",
      "Iteration 338, loss = 3.13807611\n",
      "Iteration 339, loss = 3.13759030\n",
      "Iteration 340, loss = 3.13784080\n",
      "Iteration 341, loss = 3.13680977\n",
      "Iteration 342, loss = 3.13641012\n",
      "Iteration 343, loss = 3.13687489\n",
      "Iteration 344, loss = 3.13792400\n",
      "Iteration 345, loss = 3.13629182\n",
      "Iteration 346, loss = 3.13562259\n",
      "Iteration 347, loss = 3.13664282\n",
      "Iteration 348, loss = 3.13634390\n",
      "Iteration 349, loss = 3.13573770\n",
      "Iteration 350, loss = 3.13678748\n",
      "Iteration 351, loss = 3.13741296\n",
      "Iteration 352, loss = 3.13570715\n",
      "Iteration 353, loss = 3.13853976\n",
      "Iteration 354, loss = 3.13659361\n",
      "Iteration 355, loss = 3.13565654\n",
      "Iteration 356, loss = 3.13515492\n",
      "Iteration 357, loss = 3.13500157\n",
      "Iteration 358, loss = 3.13415583\n",
      "Iteration 359, loss = 3.13443897\n",
      "Iteration 360, loss = 3.13463456\n",
      "Iteration 361, loss = 3.13412381\n",
      "Iteration 362, loss = 3.13425679\n",
      "Iteration 363, loss = 3.13385232\n",
      "Iteration 364, loss = 3.13511900\n",
      "Iteration 365, loss = 3.13411881\n",
      "Iteration 366, loss = 3.13342845\n",
      "Iteration 367, loss = 3.13505431\n",
      "Iteration 368, loss = 3.13170160\n",
      "Iteration 369, loss = 3.13841679\n",
      "Iteration 370, loss = 3.13443683\n",
      "Iteration 371, loss = 3.13364303\n",
      "Iteration 372, loss = 3.13304043\n",
      "Iteration 373, loss = 3.13231218\n",
      "Iteration 374, loss = 3.13267003\n",
      "Iteration 375, loss = 3.13294770\n",
      "Iteration 376, loss = 3.13211053\n",
      "Iteration 377, loss = 3.13291334\n",
      "Iteration 378, loss = 3.13287337\n",
      "Iteration 379, loss = 3.13212703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.12062020\n",
      "Iteration 2, loss = 3.92097597\n",
      "Iteration 3, loss = 3.74120937\n",
      "Iteration 4, loss = 3.57889767\n",
      "Iteration 5, loss = 3.43951247\n",
      "Iteration 6, loss = 3.32861403\n",
      "Iteration 7, loss = 3.26389484\n",
      "Iteration 8, loss = 3.24293567\n",
      "Iteration 9, loss = 3.24318174\n",
      "Iteration 10, loss = 3.23902600\n",
      "Iteration 11, loss = 3.23381228\n",
      "Iteration 12, loss = 3.23321914\n",
      "Iteration 13, loss = 3.23267604\n",
      "Iteration 14, loss = 3.23183325\n",
      "Iteration 15, loss = 3.23123855\n",
      "Iteration 16, loss = 3.22981659\n",
      "Iteration 17, loss = 3.22927064\n",
      "Iteration 18, loss = 3.22874627\n",
      "Iteration 19, loss = 3.22807614\n",
      "Iteration 20, loss = 3.22726770\n",
      "Iteration 21, loss = 3.22661319\n",
      "Iteration 22, loss = 3.22634029\n",
      "Iteration 23, loss = 3.22579399\n",
      "Iteration 24, loss = 3.22568525\n",
      "Iteration 25, loss = 3.22569532\n",
      "Iteration 26, loss = 3.22479741\n",
      "Iteration 27, loss = 3.22396465\n",
      "Iteration 28, loss = 3.22383756\n",
      "Iteration 29, loss = 3.22367499\n",
      "Iteration 30, loss = 3.22331313\n",
      "Iteration 31, loss = 3.22309289\n",
      "Iteration 32, loss = 3.22308582\n",
      "Iteration 33, loss = 3.22260721\n",
      "Iteration 34, loss = 3.22119760\n",
      "Iteration 35, loss = 3.22136111\n",
      "Iteration 36, loss = 3.22099853\n",
      "Iteration 37, loss = 3.22022369\n",
      "Iteration 38, loss = 3.22003523\n",
      "Iteration 39, loss = 3.21884789\n",
      "Iteration 40, loss = 3.21912151\n",
      "Iteration 41, loss = 3.21849947\n",
      "Iteration 42, loss = 3.21820032\n",
      "Iteration 43, loss = 3.21752820\n",
      "Iteration 44, loss = 3.21806237\n",
      "Iteration 45, loss = 3.21779922\n",
      "Iteration 46, loss = 3.21759036\n",
      "Iteration 47, loss = 3.21697297\n",
      "Iteration 48, loss = 3.21664106\n",
      "Iteration 49, loss = 3.21654843\n",
      "Iteration 50, loss = 3.21581823\n",
      "Iteration 51, loss = 3.21566918\n",
      "Iteration 52, loss = 3.21564194\n",
      "Iteration 53, loss = 3.21471353\n",
      "Iteration 54, loss = 3.21493283\n",
      "Iteration 55, loss = 3.21428592\n",
      "Iteration 56, loss = 3.21430522\n",
      "Iteration 57, loss = 3.21460677\n",
      "Iteration 58, loss = 3.21404355\n",
      "Iteration 59, loss = 3.21437665\n",
      "Iteration 60, loss = 3.21372338\n",
      "Iteration 61, loss = 3.21299817\n",
      "Iteration 62, loss = 3.21311105\n",
      "Iteration 63, loss = 3.21305347\n",
      "Iteration 64, loss = 3.21291962\n",
      "Iteration 65, loss = 3.21185789\n",
      "Iteration 66, loss = 3.21099298\n",
      "Iteration 67, loss = 3.21084744\n",
      "Iteration 68, loss = 3.21057792\n",
      "Iteration 69, loss = 3.21059371\n",
      "Iteration 70, loss = 3.21032569\n",
      "Iteration 71, loss = 3.21064617\n",
      "Iteration 72, loss = 3.20965241\n",
      "Iteration 73, loss = 3.20811477\n",
      "Iteration 74, loss = 3.20923913\n",
      "Iteration 75, loss = 3.20821988\n",
      "Iteration 76, loss = 3.20745731\n",
      "Iteration 77, loss = 3.20701228\n",
      "Iteration 78, loss = 3.20777513\n",
      "Iteration 79, loss = 3.20795376\n",
      "Iteration 80, loss = 3.20633291\n",
      "Iteration 81, loss = 3.20648954\n",
      "Iteration 82, loss = 3.20652807\n",
      "Iteration 83, loss = 3.20678588\n",
      "Iteration 84, loss = 3.20656424\n",
      "Iteration 85, loss = 3.20585542\n",
      "Iteration 86, loss = 3.20537302\n",
      "Iteration 87, loss = 3.20589806\n",
      "Iteration 88, loss = 3.20448250\n",
      "Iteration 89, loss = 3.20433596\n",
      "Iteration 90, loss = 3.20458637\n",
      "Iteration 91, loss = 3.20355621\n",
      "Iteration 92, loss = 3.20242704\n",
      "Iteration 93, loss = 3.20209401\n",
      "Iteration 94, loss = 3.20240216\n",
      "Iteration 95, loss = 3.20209386\n",
      "Iteration 96, loss = 3.20181061\n",
      "Iteration 97, loss = 3.20241396\n",
      "Iteration 98, loss = 3.20188161\n",
      "Iteration 99, loss = 3.20076101\n",
      "Iteration 100, loss = 3.20097958\n",
      "Iteration 101, loss = 3.20220407\n",
      "Iteration 102, loss = 3.20068947\n",
      "Iteration 103, loss = 3.20061339\n",
      "Iteration 104, loss = 3.20042868\n",
      "Iteration 105, loss = 3.19950665\n",
      "Iteration 106, loss = 3.19906083\n",
      "Iteration 107, loss = 3.19921165\n",
      "Iteration 108, loss = 3.19854001\n",
      "Iteration 109, loss = 3.19864442\n",
      "Iteration 110, loss = 3.19865889\n",
      "Iteration 111, loss = 3.19767952\n",
      "Iteration 112, loss = 3.19751469\n",
      "Iteration 113, loss = 3.19718111\n",
      "Iteration 114, loss = 3.19724004\n",
      "Iteration 115, loss = 3.19750642\n",
      "Iteration 116, loss = 3.19669328\n",
      "Iteration 117, loss = 3.19645157\n",
      "Iteration 118, loss = 3.19637084\n",
      "Iteration 119, loss = 3.19566854\n",
      "Iteration 120, loss = 3.19534203\n",
      "Iteration 121, loss = 3.19513139\n",
      "Iteration 122, loss = 3.19454541\n",
      "Iteration 123, loss = 3.19453628\n",
      "Iteration 124, loss = 3.19441610\n",
      "Iteration 125, loss = 3.19389346\n",
      "Iteration 126, loss = 3.19297217\n",
      "Iteration 127, loss = 3.19246588\n",
      "Iteration 128, loss = 3.19180769\n",
      "Iteration 129, loss = 3.19322446\n",
      "Iteration 130, loss = 3.19379239\n",
      "Iteration 131, loss = 3.19146481\n",
      "Iteration 132, loss = 3.19230670\n",
      "Iteration 133, loss = 3.19118367\n",
      "Iteration 134, loss = 3.19103867\n",
      "Iteration 135, loss = 3.19088407\n",
      "Iteration 136, loss = 3.18929664\n",
      "Iteration 137, loss = 3.18927061\n",
      "Iteration 138, loss = 3.19140549\n",
      "Iteration 139, loss = 3.18937890\n",
      "Iteration 140, loss = 3.18949651\n",
      "Iteration 141, loss = 3.18963955\n",
      "Iteration 142, loss = 3.18767452\n",
      "Iteration 143, loss = 3.18745686\n",
      "Iteration 144, loss = 3.18741735\n",
      "Iteration 145, loss = 3.18744110\n",
      "Iteration 146, loss = 3.18711027\n",
      "Iteration 147, loss = 3.18689211\n",
      "Iteration 148, loss = 3.18612196\n",
      "Iteration 149, loss = 3.18538158\n",
      "Iteration 150, loss = 3.18505602\n",
      "Iteration 151, loss = 3.18564686\n",
      "Iteration 152, loss = 3.18471731\n",
      "Iteration 153, loss = 3.18395622\n",
      "Iteration 154, loss = 3.18398447\n",
      "Iteration 155, loss = 3.18281356\n",
      "Iteration 156, loss = 3.18391454\n",
      "Iteration 157, loss = 3.18262186\n",
      "Iteration 158, loss = 3.18318185\n",
      "Iteration 159, loss = 3.18330501\n",
      "Iteration 160, loss = 3.18231170\n",
      "Iteration 161, loss = 3.18202842\n",
      "Iteration 162, loss = 3.18210668\n",
      "Iteration 163, loss = 3.18088932\n",
      "Iteration 164, loss = 3.18023996\n",
      "Iteration 165, loss = 3.18040190\n",
      "Iteration 166, loss = 3.17997183\n",
      "Iteration 167, loss = 3.17870437\n",
      "Iteration 168, loss = 3.17900570\n",
      "Iteration 169, loss = 3.17940424\n",
      "Iteration 170, loss = 3.17894706\n",
      "Iteration 171, loss = 3.17771547\n",
      "Iteration 172, loss = 3.17810453\n",
      "Iteration 173, loss = 3.17798322\n",
      "Iteration 174, loss = 3.17871970\n",
      "Iteration 175, loss = 3.17672873\n",
      "Iteration 176, loss = 3.17682730\n",
      "Iteration 177, loss = 3.17648889\n",
      "Iteration 178, loss = 3.17501140\n",
      "Iteration 179, loss = 3.17477220\n",
      "Iteration 180, loss = 3.17431427\n",
      "Iteration 181, loss = 3.17393730\n",
      "Iteration 182, loss = 3.17466879\n",
      "Iteration 183, loss = 3.17515061\n",
      "Iteration 184, loss = 3.17436541\n",
      "Iteration 185, loss = 3.17384887\n",
      "Iteration 186, loss = 3.17470877\n",
      "Iteration 187, loss = 3.17428214\n",
      "Iteration 188, loss = 3.17332505\n",
      "Iteration 189, loss = 3.17238009\n",
      "Iteration 190, loss = 3.17093994\n",
      "Iteration 191, loss = 3.17101435\n",
      "Iteration 192, loss = 3.17071244\n",
      "Iteration 193, loss = 3.17143518\n",
      "Iteration 194, loss = 3.17125757\n",
      "Iteration 195, loss = 3.17016265\n",
      "Iteration 196, loss = 3.16988838\n",
      "Iteration 197, loss = 3.17005227\n",
      "Iteration 198, loss = 3.17115143\n",
      "Iteration 199, loss = 3.17036090\n",
      "Iteration 200, loss = 3.17001793\n",
      "Iteration 201, loss = 3.16893572\n",
      "Iteration 202, loss = 3.16897407\n",
      "Iteration 203, loss = 3.16841738\n",
      "Iteration 204, loss = 3.16910091\n",
      "Iteration 205, loss = 3.16735383\n",
      "Iteration 206, loss = 3.16733684\n",
      "Iteration 207, loss = 3.16700782\n",
      "Iteration 208, loss = 3.16560152\n",
      "Iteration 209, loss = 3.16651318\n",
      "Iteration 210, loss = 3.16552427\n",
      "Iteration 211, loss = 3.16564309\n",
      "Iteration 212, loss = 3.16518914\n",
      "Iteration 213, loss = 3.16548106\n",
      "Iteration 214, loss = 3.16666501\n",
      "Iteration 215, loss = 3.16411399\n",
      "Iteration 216, loss = 3.16444504\n",
      "Iteration 217, loss = 3.16472824\n",
      "Iteration 218, loss = 3.16295128\n",
      "Iteration 219, loss = 3.16586995\n",
      "Iteration 220, loss = 3.16392186\n",
      "Iteration 221, loss = 3.16213996\n",
      "Iteration 222, loss = 3.16145779\n",
      "Iteration 223, loss = 3.16151139\n",
      "Iteration 224, loss = 3.16121482\n",
      "Iteration 225, loss = 3.16175506\n",
      "Iteration 226, loss = 3.16057843\n",
      "Iteration 227, loss = 3.16016685\n",
      "Iteration 228, loss = 3.16071493\n",
      "Iteration 229, loss = 3.15956747\n",
      "Iteration 230, loss = 3.15908696\n",
      "Iteration 231, loss = 3.15899322\n",
      "Iteration 232, loss = 3.15959805\n",
      "Iteration 233, loss = 3.16017169\n",
      "Iteration 234, loss = 3.15953955\n",
      "Iteration 235, loss = 3.15842526\n",
      "Iteration 236, loss = 3.15803134\n",
      "Iteration 237, loss = 3.15835058\n",
      "Iteration 238, loss = 3.15768320\n",
      "Iteration 239, loss = 3.15769029\n",
      "Iteration 240, loss = 3.15754338\n",
      "Iteration 241, loss = 3.15578564\n",
      "Iteration 242, loss = 3.15615881\n",
      "Iteration 243, loss = 3.15632674\n",
      "Iteration 244, loss = 3.15588079\n",
      "Iteration 245, loss = 3.15609832\n",
      "Iteration 246, loss = 3.15464264\n",
      "Iteration 247, loss = 3.15389923\n",
      "Iteration 248, loss = 3.15575188\n",
      "Iteration 249, loss = 3.15559860\n",
      "Iteration 250, loss = 3.15454016\n",
      "Iteration 251, loss = 3.15380361\n",
      "Iteration 252, loss = 3.15291173\n",
      "Iteration 253, loss = 3.15304213\n",
      "Iteration 254, loss = 3.15241262\n",
      "Iteration 255, loss = 3.15323823\n",
      "Iteration 256, loss = 3.15282207\n",
      "Iteration 257, loss = 3.15227750\n",
      "Iteration 258, loss = 3.15167032\n",
      "Iteration 259, loss = 3.15099020\n",
      "Iteration 260, loss = 3.15150152\n",
      "Iteration 261, loss = 3.15049252\n",
      "Iteration 262, loss = 3.15044555\n",
      "Iteration 263, loss = 3.15084399\n",
      "Iteration 264, loss = 3.15074237\n",
      "Iteration 265, loss = 3.14927331\n",
      "Iteration 266, loss = 3.15010888\n",
      "Iteration 267, loss = 3.14844776\n",
      "Iteration 268, loss = 3.14972182\n",
      "Iteration 269, loss = 3.14889776\n",
      "Iteration 270, loss = 3.14781968\n",
      "Iteration 271, loss = 3.14857206\n",
      "Iteration 272, loss = 3.14777242\n",
      "Iteration 273, loss = 3.14760618\n",
      "Iteration 274, loss = 3.14661029\n",
      "Iteration 275, loss = 3.14624625\n",
      "Iteration 276, loss = 3.14719564\n",
      "Iteration 277, loss = 3.14706517\n",
      "Iteration 278, loss = 3.14632399\n",
      "Iteration 279, loss = 3.14645564\n",
      "Iteration 280, loss = 3.14632663\n",
      "Iteration 281, loss = 3.14501634\n",
      "Iteration 282, loss = 3.14574988\n",
      "Iteration 283, loss = 3.14380117\n",
      "Iteration 284, loss = 3.14377838\n",
      "Iteration 285, loss = 3.14345413\n",
      "Iteration 286, loss = 3.14369218\n",
      "Iteration 287, loss = 3.14425841\n",
      "Iteration 288, loss = 3.14407785\n",
      "Iteration 289, loss = 3.14481736\n",
      "Iteration 290, loss = 3.14509392\n",
      "Iteration 291, loss = 3.14410230\n",
      "Iteration 292, loss = 3.14380163\n",
      "Iteration 293, loss = 3.14483236\n",
      "Iteration 294, loss = 3.14410703\n",
      "Iteration 295, loss = 3.14138837\n",
      "Iteration 296, loss = 3.14135639\n",
      "Iteration 297, loss = 3.14109977\n",
      "Iteration 298, loss = 3.14164126\n",
      "Iteration 299, loss = 3.14055684\n",
      "Iteration 300, loss = 3.14131144\n",
      "Iteration 301, loss = 3.14228736\n",
      "Iteration 302, loss = 3.14100122\n",
      "Iteration 303, loss = 3.14083590\n",
      "Iteration 304, loss = 3.13970196\n",
      "Iteration 305, loss = 3.13974642\n",
      "Iteration 306, loss = 3.14097438\n",
      "Iteration 307, loss = 3.14083513\n",
      "Iteration 308, loss = 3.13839935\n",
      "Iteration 309, loss = 3.13852837\n",
      "Iteration 310, loss = 3.13866915\n",
      "Iteration 311, loss = 3.13908875\n",
      "Iteration 312, loss = 3.13858872\n",
      "Iteration 313, loss = 3.13829404\n",
      "Iteration 314, loss = 3.13812180\n",
      "Iteration 315, loss = 3.13768886\n",
      "Iteration 316, loss = 3.13711950\n",
      "Iteration 317, loss = 3.13878313\n",
      "Iteration 318, loss = 3.13771034\n",
      "Iteration 319, loss = 3.13756508\n",
      "Iteration 320, loss = 3.13759547\n",
      "Iteration 321, loss = 3.13715645\n",
      "Iteration 322, loss = 3.13912840\n",
      "Iteration 323, loss = 3.13665121\n",
      "Iteration 324, loss = 3.13556863\n",
      "Iteration 325, loss = 3.13585329\n",
      "Iteration 326, loss = 3.13451608\n",
      "Iteration 327, loss = 3.13821905\n",
      "Iteration 328, loss = 3.13773051\n",
      "Iteration 329, loss = 3.13515084\n",
      "Iteration 330, loss = 3.13337070\n",
      "Iteration 331, loss = 3.13408828\n",
      "Iteration 332, loss = 3.13407153\n",
      "Iteration 333, loss = 3.13410592\n",
      "Iteration 334, loss = 3.13314245\n",
      "Iteration 335, loss = 3.13295143\n",
      "Iteration 336, loss = 3.13217370\n",
      "Iteration 337, loss = 3.13352029\n",
      "Iteration 338, loss = 3.13249988\n",
      "Iteration 339, loss = 3.13271456\n",
      "Iteration 340, loss = 3.13234812\n",
      "Iteration 341, loss = 3.13156584\n",
      "Iteration 342, loss = 3.13262696\n",
      "Iteration 343, loss = 3.13144878\n",
      "Iteration 344, loss = 3.13067094\n",
      "Iteration 345, loss = 3.13221873\n",
      "Iteration 346, loss = 3.12882004\n",
      "Iteration 347, loss = 3.12952660\n",
      "Iteration 348, loss = 3.12964776\n",
      "Iteration 349, loss = 3.12990704\n",
      "Iteration 350, loss = 3.13044743\n",
      "Iteration 351, loss = 3.13009055\n",
      "Iteration 352, loss = 3.13038411\n",
      "Iteration 353, loss = 3.12984801\n",
      "Iteration 354, loss = 3.13013827\n",
      "Iteration 355, loss = 3.13055365\n",
      "Iteration 356, loss = 3.12762201\n",
      "Iteration 357, loss = 3.13399527\n",
      "Iteration 358, loss = 3.13051417\n",
      "Iteration 359, loss = 3.12903991\n",
      "Iteration 360, loss = 3.12882591\n",
      "Iteration 361, loss = 3.12853824\n",
      "Iteration 362, loss = 3.12914002\n",
      "Iteration 363, loss = 3.12846205\n",
      "Iteration 364, loss = 3.12845168\n",
      "Iteration 365, loss = 3.12740239\n",
      "Iteration 366, loss = 3.12654210\n",
      "Iteration 367, loss = 3.12604871\n",
      "Iteration 368, loss = 3.12633046\n",
      "Iteration 369, loss = 3.12616963\n",
      "Iteration 370, loss = 3.12764769\n",
      "Iteration 371, loss = 3.12628710\n",
      "Iteration 372, loss = 3.12572881\n",
      "Iteration 373, loss = 3.12489294\n",
      "Iteration 374, loss = 3.12621155\n",
      "Iteration 375, loss = 3.12596669\n",
      "Iteration 376, loss = 3.12551093\n",
      "Iteration 377, loss = 3.12550949\n",
      "Iteration 378, loss = 3.12497234\n",
      "Iteration 379, loss = 3.12466440\n",
      "Iteration 380, loss = 3.12479027\n",
      "Iteration 381, loss = 3.12469035\n",
      "Iteration 382, loss = 3.12435911\n",
      "Iteration 383, loss = 3.12350302\n",
      "Iteration 384, loss = 3.12381684\n",
      "Iteration 385, loss = 3.12417364\n",
      "Iteration 386, loss = 3.12323082\n",
      "Iteration 387, loss = 3.12231382\n",
      "Iteration 388, loss = 3.12259122\n",
      "Iteration 389, loss = 3.12305664\n",
      "Iteration 390, loss = 3.12145995\n",
      "Iteration 391, loss = 3.12288682\n",
      "Iteration 392, loss = 3.12186923\n",
      "Iteration 393, loss = 3.12433736\n",
      "Iteration 394, loss = 3.12376921\n",
      "Iteration 395, loss = 3.12163904\n",
      "Iteration 396, loss = 3.12197530\n",
      "Iteration 397, loss = 3.12213474\n",
      "Iteration 398, loss = 3.12240625\n",
      "Iteration 399, loss = 3.12177623\n",
      "Iteration 400, loss = 3.12077227\n",
      "Iteration 401, loss = 3.12055034\n",
      "Iteration 402, loss = 3.12028940\n",
      "Iteration 403, loss = 3.12047755\n",
      "Iteration 404, loss = 3.12252832\n",
      "Iteration 405, loss = 3.12027767\n",
      "Iteration 406, loss = 3.11935504\n",
      "Iteration 407, loss = 3.11871442\n",
      "Iteration 408, loss = 3.11963886\n",
      "Iteration 409, loss = 3.11974662\n",
      "Iteration 410, loss = 3.11904145\n",
      "Iteration 411, loss = 3.11927439\n",
      "Iteration 412, loss = 3.11861373\n",
      "Iteration 413, loss = 3.11775224\n",
      "Iteration 414, loss = 3.11883583\n",
      "Iteration 415, loss = 3.11857683\n",
      "Iteration 416, loss = 3.11902287\n",
      "Iteration 417, loss = 3.11902038\n",
      "Iteration 418, loss = 3.11832535\n",
      "Iteration 419, loss = 3.11932700\n",
      "Iteration 420, loss = 3.11822382\n",
      "Iteration 421, loss = 3.11762558\n",
      "Iteration 422, loss = 3.11757870\n",
      "Iteration 423, loss = 3.11751397\n",
      "Iteration 424, loss = 3.11799037\n",
      "Iteration 425, loss = 3.11709269\n",
      "Iteration 426, loss = 3.11730924\n",
      "Iteration 427, loss = 3.11733036\n",
      "Iteration 428, loss = 3.11593932\n",
      "Iteration 429, loss = 3.11541826\n",
      "Iteration 430, loss = 3.11784730\n",
      "Iteration 431, loss = 3.11687988\n",
      "Iteration 432, loss = 3.11527074\n",
      "Iteration 433, loss = 3.11697044\n",
      "Iteration 434, loss = 3.11605561\n",
      "Iteration 435, loss = 3.11639242\n",
      "Iteration 436, loss = 3.11556306\n",
      "Iteration 437, loss = 3.11668810\n",
      "Iteration 438, loss = 3.11487919\n",
      "Iteration 439, loss = 3.11516841\n",
      "Iteration 440, loss = 3.11481974\n",
      "Iteration 441, loss = 3.11527036\n",
      "Iteration 442, loss = 3.11526976\n",
      "Iteration 443, loss = 3.11443056\n",
      "Iteration 444, loss = 3.11467902\n",
      "Iteration 445, loss = 3.11405626\n",
      "Iteration 446, loss = 3.11414594\n",
      "Iteration 447, loss = 3.11449120\n",
      "Iteration 448, loss = 3.11421796\n",
      "Iteration 449, loss = 3.11517938\n",
      "Iteration 450, loss = 3.11303961\n",
      "Iteration 451, loss = 3.11231679\n",
      "Iteration 452, loss = 3.11440026\n",
      "Iteration 453, loss = 3.11345323\n",
      "Iteration 454, loss = 3.11453253\n",
      "Iteration 455, loss = 3.11204592\n",
      "Iteration 456, loss = 3.11235531\n",
      "Iteration 457, loss = 3.11192753\n",
      "Iteration 458, loss = 3.11197570\n",
      "Iteration 459, loss = 3.11121795\n",
      "Iteration 460, loss = 3.11095565\n",
      "Iteration 461, loss = 3.11135123\n",
      "Iteration 462, loss = 3.11109477\n",
      "Iteration 463, loss = 3.11131918\n",
      "Iteration 464, loss = 3.11041105\n",
      "Iteration 465, loss = 3.11088466\n",
      "Iteration 466, loss = 3.11165137\n",
      "Iteration 467, loss = 3.11259072\n",
      "Iteration 468, loss = 3.11214945\n",
      "Iteration 469, loss = 3.11276192\n",
      "Iteration 470, loss = 3.11173167\n",
      "Iteration 471, loss = 3.11020271\n",
      "Iteration 472, loss = 3.11347308\n",
      "Iteration 473, loss = 3.11142200\n",
      "Iteration 474, loss = 3.10907244\n",
      "Iteration 475, loss = 3.10919840\n",
      "Iteration 476, loss = 3.10923071\n",
      "Iteration 477, loss = 3.10914955\n",
      "Iteration 478, loss = 3.10889215\n",
      "Iteration 479, loss = 3.10827902\n",
      "Iteration 480, loss = 3.10788287\n",
      "Iteration 481, loss = 3.10923192\n",
      "Iteration 482, loss = 3.10886714\n",
      "Iteration 483, loss = 3.11083124\n",
      "Iteration 484, loss = 3.10872302\n",
      "Iteration 485, loss = 3.10922528\n",
      "Iteration 486, loss = 3.10988183\n",
      "Iteration 487, loss = 3.10881944\n",
      "Iteration 488, loss = 3.10869150\n",
      "Iteration 489, loss = 3.10992987\n",
      "Iteration 490, loss = 3.10908908\n",
      "Iteration 491, loss = 3.10827729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10341791\n",
      "Iteration 2, loss = 3.83240896\n",
      "Iteration 3, loss = 3.62994207\n",
      "Iteration 4, loss = 3.48548352\n",
      "Iteration 5, loss = 3.37677123\n",
      "Iteration 6, loss = 3.30544377\n",
      "Iteration 7, loss = 3.26715346\n",
      "Iteration 8, loss = 3.24769723\n",
      "Iteration 9, loss = 3.24361195\n",
      "Iteration 10, loss = 3.24306897\n",
      "Iteration 11, loss = 3.24213502\n",
      "Iteration 12, loss = 3.24046734\n",
      "Iteration 13, loss = 3.23948654\n",
      "Iteration 14, loss = 3.23934431\n",
      "Iteration 15, loss = 3.23813286\n",
      "Iteration 16, loss = 3.23730221\n",
      "Iteration 17, loss = 3.23769198\n",
      "Iteration 18, loss = 3.23792115\n",
      "Iteration 19, loss = 3.23773575\n",
      "Iteration 20, loss = 3.23786307\n",
      "Iteration 21, loss = 3.23697876\n",
      "Iteration 22, loss = 3.23739588\n",
      "Iteration 23, loss = 3.23726785\n",
      "Iteration 24, loss = 3.23652422\n",
      "Iteration 25, loss = 3.23694666\n",
      "Iteration 26, loss = 3.23771352\n",
      "Iteration 27, loss = 3.23717607\n",
      "Iteration 28, loss = 3.23796083\n",
      "Iteration 29, loss = 3.23864959\n",
      "Iteration 30, loss = 3.23822423\n",
      "Iteration 31, loss = 3.23734352\n",
      "Iteration 32, loss = 3.23612704\n",
      "Iteration 33, loss = 3.23574954\n",
      "Iteration 34, loss = 3.23575616\n",
      "Iteration 35, loss = 3.23738436\n",
      "Iteration 36, loss = 3.23710425\n",
      "Iteration 37, loss = 3.23654609\n",
      "Iteration 38, loss = 3.23687673\n",
      "Iteration 39, loss = 3.23647355\n",
      "Iteration 40, loss = 3.23678499\n",
      "Iteration 41, loss = 3.23708976\n",
      "Iteration 42, loss = 3.23663349\n",
      "Iteration 43, loss = 3.23657630\n",
      "Iteration 44, loss = 3.23606496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10435136\n",
      "Iteration 2, loss = 3.83327077\n",
      "Iteration 3, loss = 3.63098333\n",
      "Iteration 4, loss = 3.48506774\n",
      "Iteration 5, loss = 3.38083877\n",
      "Iteration 6, loss = 3.31226642\n",
      "Iteration 7, loss = 3.27284036\n",
      "Iteration 8, loss = 3.25352784\n",
      "Iteration 9, loss = 3.24646682\n",
      "Iteration 10, loss = 3.24537943\n",
      "Iteration 11, loss = 3.24407559\n",
      "Iteration 12, loss = 3.24291543\n",
      "Iteration 13, loss = 3.24145747\n",
      "Iteration 14, loss = 3.24150646\n",
      "Iteration 15, loss = 3.24114656\n",
      "Iteration 16, loss = 3.24075963\n",
      "Iteration 17, loss = 3.24064427\n",
      "Iteration 18, loss = 3.24103158\n",
      "Iteration 19, loss = 3.24146522\n",
      "Iteration 20, loss = 3.24246779\n",
      "Iteration 21, loss = 3.24218280\n",
      "Iteration 22, loss = 3.24207054\n",
      "Iteration 23, loss = 3.24219014\n",
      "Iteration 24, loss = 3.24189848\n",
      "Iteration 25, loss = 3.24062442\n",
      "Iteration 26, loss = 3.24131890\n",
      "Iteration 27, loss = 3.24242553\n",
      "Iteration 28, loss = 3.24265043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10337010\n",
      "Iteration 2, loss = 3.83163579\n",
      "Iteration 3, loss = 3.62965291\n",
      "Iteration 4, loss = 3.48439121\n",
      "Iteration 5, loss = 3.37910038\n",
      "Iteration 6, loss = 3.30651713\n",
      "Iteration 7, loss = 3.26291095\n",
      "Iteration 8, loss = 3.24519509\n",
      "Iteration 9, loss = 3.23744152\n",
      "Iteration 10, loss = 3.23647862\n",
      "Iteration 11, loss = 3.23627849\n",
      "Iteration 12, loss = 3.23561002\n",
      "Iteration 13, loss = 3.23572227\n",
      "Iteration 14, loss = 3.23547178\n",
      "Iteration 15, loss = 3.23464042\n",
      "Iteration 16, loss = 3.23430457\n",
      "Iteration 17, loss = 3.23478361\n",
      "Iteration 18, loss = 3.23535108\n",
      "Iteration 19, loss = 3.23461172\n",
      "Iteration 20, loss = 3.23407757\n",
      "Iteration 21, loss = 3.23429017\n",
      "Iteration 22, loss = 3.23471446\n",
      "Iteration 23, loss = 3.23506548\n",
      "Iteration 24, loss = 3.23468296\n",
      "Iteration 25, loss = 3.23463026\n",
      "Iteration 26, loss = 3.23512938\n",
      "Iteration 27, loss = 3.23513598\n",
      "Iteration 28, loss = 3.23580974\n",
      "Iteration 29, loss = 3.23511944\n",
      "Iteration 30, loss = 3.23568612\n",
      "Iteration 31, loss = 3.23456917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10081984\n",
      "Iteration 2, loss = 3.83311015\n",
      "Iteration 3, loss = 3.63283894\n",
      "Iteration 4, loss = 3.48381653\n",
      "Iteration 5, loss = 3.37764081\n",
      "Iteration 6, loss = 3.30880490\n",
      "Iteration 7, loss = 3.26541971\n",
      "Iteration 8, loss = 3.24422204\n",
      "Iteration 9, loss = 3.23726470\n",
      "Iteration 10, loss = 3.23634965\n",
      "Iteration 11, loss = 3.23583945\n",
      "Iteration 12, loss = 3.23501827\n",
      "Iteration 13, loss = 3.23481448\n",
      "Iteration 14, loss = 3.23328542\n",
      "Iteration 15, loss = 3.23290439\n",
      "Iteration 16, loss = 3.23248566\n",
      "Iteration 17, loss = 3.23281030\n",
      "Iteration 18, loss = 3.23255777\n",
      "Iteration 19, loss = 3.23337994\n",
      "Iteration 20, loss = 3.23303040\n",
      "Iteration 21, loss = 3.23326276\n",
      "Iteration 22, loss = 3.23409544\n",
      "Iteration 23, loss = 3.23303865\n",
      "Iteration 24, loss = 3.23376538\n",
      "Iteration 25, loss = 3.23328888\n",
      "Iteration 26, loss = 3.23325369\n",
      "Iteration 27, loss = 3.23222635\n",
      "Iteration 28, loss = 3.23264240\n",
      "Iteration 29, loss = 3.23224831\n",
      "Iteration 30, loss = 3.23247170\n",
      "Iteration 31, loss = 3.23183657\n",
      "Iteration 32, loss = 3.23227254\n",
      "Iteration 33, loss = 3.23275039\n",
      "Iteration 34, loss = 3.23306032\n",
      "Iteration 35, loss = 3.23174693\n",
      "Iteration 36, loss = 3.23237737\n",
      "Iteration 37, loss = 3.23294423\n",
      "Iteration 38, loss = 3.23243763\n",
      "Iteration 39, loss = 3.23265296\n",
      "Iteration 40, loss = 3.23344545\n",
      "Iteration 41, loss = 3.23318843\n",
      "Iteration 42, loss = 3.23353374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.10518858\n",
      "Iteration 2, loss = 3.84056055\n",
      "Iteration 3, loss = 3.64670126\n",
      "Iteration 4, loss = 3.49715673\n",
      "Iteration 5, loss = 3.39218151\n",
      "Iteration 6, loss = 3.31997269\n",
      "Iteration 7, loss = 3.27405314\n",
      "Iteration 8, loss = 3.25177969\n",
      "Iteration 9, loss = 3.24390022\n",
      "Iteration 10, loss = 3.24230883\n",
      "Iteration 11, loss = 3.24154234\n",
      "Iteration 12, loss = 3.24045138\n",
      "Iteration 13, loss = 3.24028349\n",
      "Iteration 14, loss = 3.23990096\n",
      "Iteration 15, loss = 3.23887732\n",
      "Iteration 16, loss = 3.23883073\n",
      "Iteration 17, loss = 3.23989475\n",
      "Iteration 18, loss = 3.23840835\n",
      "Iteration 19, loss = 3.23788112\n",
      "Iteration 20, loss = 3.23837331\n",
      "Iteration 21, loss = 3.23874158\n",
      "Iteration 22, loss = 3.23853898\n",
      "Iteration 23, loss = 3.23918402\n",
      "Iteration 24, loss = 3.23801299\n",
      "Iteration 25, loss = 3.23871397\n",
      "Iteration 26, loss = 3.23900144\n",
      "Iteration 27, loss = 3.23852818\n",
      "Iteration 28, loss = 3.23894225\n",
      "Iteration 29, loss = 3.23848047\n",
      "Iteration 30, loss = 3.23821551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.09849991\n",
      "Iteration 2, loss = 3.62194558\n",
      "Iteration 3, loss = 3.37266121\n",
      "Iteration 4, loss = 3.25450600\n",
      "Iteration 5, loss = 3.24710100\n",
      "Iteration 6, loss = 3.25150353\n",
      "Iteration 7, loss = 3.24021672\n",
      "Iteration 8, loss = 3.23949385\n",
      "Iteration 9, loss = 3.23997753\n",
      "Iteration 10, loss = 3.23808027\n",
      "Iteration 11, loss = 3.23877255\n",
      "Iteration 12, loss = 3.23731146\n",
      "Iteration 13, loss = 3.23847198\n",
      "Iteration 14, loss = 3.23774119\n",
      "Iteration 15, loss = 3.23791671\n",
      "Iteration 16, loss = 3.23823425\n",
      "Iteration 17, loss = 3.23760933\n",
      "Iteration 18, loss = 3.23884202\n",
      "Iteration 19, loss = 3.23850814\n",
      "Iteration 20, loss = 3.23820051\n",
      "Iteration 21, loss = 3.23844027\n",
      "Iteration 22, loss = 3.23831223\n",
      "Iteration 23, loss = 3.23741709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters: {'activation': 'logistic', 'alpha': 0.01, 'hidden_layer_sizes': (200,), 'max_iter': 966}\n",
      "Accuracy: 0.3229166666666667\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c54355595531fb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random_search.fit(X_train_pca, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Get the best estimator\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the best estimator\n",
    "accuracy = best_estimator.score(X_test_pca, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "id": "736f91bf2b9e12c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64fb9f47d3fc2b83",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
